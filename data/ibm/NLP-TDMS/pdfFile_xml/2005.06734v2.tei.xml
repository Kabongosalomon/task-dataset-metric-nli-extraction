<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense-Resolution Network for Point Cloud Classification and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data61-CSIRO</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
							<email>saeed.anwar@data61.csiro.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data61-CSIRO</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
							<email>nick.barnes@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dense-Resolution Network for Point Cloud Classification and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point cloud analysis is attracting attention from Artificial Intelligence research since it can be widely used in applications such as robotics, Augmented Reality, self-driving. However, it is always challenging due to irregularities, unorderedness, and sparsity. In this article, we propose a novel network named Dense-Resolution Network (DRNet) for point cloud analysis. Our DRNet is designed to learn local point features from the point cloud in different resolutions. In order to learn local point groups more effectively, we present a novel grouping method for local neighborhood searching and an error-minimizing module for capturing local features. In addition to validating the network on widely used point cloud segmentation and classification benchmarks, we also test and visualize the performance of the components. Comparing with other stateof-the-art methods, our network shows superiority on Mod-elNet40, ShapeNet synthetic and ScanObjectNN real point cloud datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the help of rapid progress in 3D sensing technology, an increasing number of researchers are now focusing on 3D point clouds. Different from complex 3D data e.g., mesh and volumetric data, point clouds have a simpler data format. Typically, point clouds are easier to collect using different types of scanners <ref type="bibr" target="#b2">[3]</ref> with specific algorithms: e.g., LiDAR scanners <ref type="bibr" target="#b11">[12]</ref> and Simultaneous localization and mapping (SLAM) algorithms. Traditional algorithms addressing point cloud learning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref> used to estimate geometric information and capture indirect clues utilizing complicated models. In contrast, deep learning models provide explicit and effective data-driven approaches to acquire information from 3D point cloud data, leveraging Convolutional Neural Networks (CNN).</p><p>In general, CNN-related methods for 3D point clouds can be divided mainly into two categories <ref type="bibr" target="#b6">[7]</ref>. The first one is conversion-based, which converts the 3D data to some intermediate representations, for example, <ref type="figure" target="#fig_0">Figure 1</ref>. A birdeyes view of our Dense-Resolution Network.</p><p>MVCNN <ref type="bibr" target="#b33">[34]</ref> projects 3D shapes into multi-view 2D images, and VoxNet <ref type="bibr" target="#b22">[23]</ref> transfers point clouds as volumetric grids. The other one is point-based such as PointNet <ref type="bibr" target="#b27">[28]</ref>, which directly processes points. The point-based approach has become popular due to the introduction of the multilayer perceptrons (MLPs) operation in <ref type="bibr" target="#b27">[28]</ref>. The subsequent algorithms <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b34">35]</ref> adopted MLPs to learn the local features of point clouds using graph context and kernel points.</p><p>In order to recognize fine-grained patterns for complex objects or scenes, it is necessary to capture the local spatial context of point clouds. To represent local areas for point clouds, Qi et al. <ref type="bibr" target="#b28">[29]</ref> and Liu et al. <ref type="bibr" target="#b18">[19]</ref> apply the Ball Query algorithm <ref type="bibr" target="#b26">[27]</ref> to group local points, while Wang et al. <ref type="bibr" target="#b38">[39]</ref> uses k-nearest neighbors (knn) to build point neighborhoods. However, when using these methods, the performance is strongly affected by the areas of their pre-defined neighborhoods, i.e. the searching radius of a Ball Query, or the k of knn. If the area is too small, it cannot cover sufficient local patterns; if too large, the overlap may involve redundancies. DPC <ref type="bibr" target="#b5">[6]</ref> proposes an idea of dilated point convolution to increase the size of the receptive field without additional computational cost. Unlike previous works, we attempt to adaptively define such a local area for each point w.r.t. the density distribution around it, by which the point neighborhood would be more reasonable though requiring less manual intervention and parameter tuning.</p><p>Unlike 2D images whose pixels are well-organized in local neighborhoods, learning the feature representations of scattered, unordered, and irregular 3D point clouds are al- <ref type="bibr">Figure 2</ref>. Dense-resolution network architecture. For the FR branch (in green), we learn the full-resolution point cloud features through a series of Error-minimizing modules (denoted as E-M, see Section 3.2) involving the Adaptive Dilated Point Grouping method (denoted as ADPG, see Section 3.1). For the MR branch (in blue), point features of different resolutions are investigated in a down/up-sampling manner with skip connections (dotted lines). DS and US represent our down-sampling and up-sampling processes (more details are in Section 4.1 and the supplementary material), respectively. By merging the feature maps (denoted as M, see Eq. 4) of the two branches, we manage point cloud classification and segmentation tasks using fully-connected (FC) layers. C stands for concatenating along channels.</p><p>ways challenging. Although one can construct local areas based on the spatial distances between points, the process may accumulate biases from different scales of embedding space and further affect the performance. In addition to feature encoding, an effective mechanism is also required to guide the procedure to learn local features.</p><p>Previously, the idea of error feedback has been applied in 2D human pose estimation <ref type="bibr" target="#b3">[4]</ref> and image Super-Resolution (SR) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref>, in order to regulate the network by compensating the estimated error. To leverage the properties of both error-feedback and CNN training mechanism, unlike the complex error-correcting structures in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>, we propose an error-minimizing module with lower complexity but better performance. Meanwhile, we present a new network architecture, named Dense-Resolution Network (DRNet), for basic 3D point cloud classification and segmentation tasks. By merging feature maps of a Full-Resolution (FR) branch that investigates the full size of the point cloud and a Multi-Resolution (MR) branch that explores different resolutions of the point cloud in a novel fusion method, we can obtain more information for a comprehensive analysis. The main contributions are:</p><p>• We propose a novel point grouping method to find neighbors for each point adaptively, considering the density distribution of the neighbors.</p><p>• We design an error-minimizing module leveraging the idea of error feedback mechanism to learn the local features of point clouds.</p><p>• We introduce a new network to comprehensively rep-resent point clouds from different resolutions.</p><p>• We conduct thorough experiments to validate the properties and abilities of our proposals. Our results demonstrate that our approach outperforms state-ofthe-art methods on three point cloud benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Local points grouping. Contrary to the pioneer Point-Net <ref type="bibr" target="#b27">[28]</ref> that relied on global features, subsequent work captured more local features in detail. PointNet++ <ref type="bibr" target="#b28">[29]</ref> firstly applied Ball Query, an algorithm for collecting possible neighbors of a particular point through a ball-like searching space centering at a point, to group local neighbors. Similarly, local features learning methods such as <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">30]</ref> use another simple algorithm knn gathering nearest neighbors based on a distance metric. Although Ball Query and knn grouping are intuitive, sometimes the size of the neighborhood (i.e. the receptive field of the point) is limited due to the range of searching (i.e. the radius of query ball, or the value of k). Meanwhile, merely increasing the searching range may involve substantial computational cost. To solve this problem, DPC <ref type="bibr" target="#b5">[6]</ref> extended regular knn to dilated-knn, which gathers local points over a dilated neighborhood obtained by computing the k · d nearest neighbors (d is the dilation factor <ref type="bibr" target="#b45">[46]</ref>) and preserving only every d-th nearest point. Moreover, recent works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44]</ref> group neighbors through query balls in different scales (e.g., multi-scale grouping) to capture information from various sizes of the local area. However, the existing methods have some issues in common. On the one hand, the performance of grouping methods highly relies on pre-defined settings. For example, DGCNN <ref type="bibr" target="#b38">[39]</ref> provided the results under different k conditions, DPC <ref type="bibr" target="#b5">[6]</ref> compared the effects of d values, and Point-Net++ <ref type="bibr" target="#b28">[29]</ref> discussed the influence of the query ball radius. On the other hand, the grouping methods act on all points without considering each point or object's distinct condition. As far as we are concerned, it is necessary to find an intelligent point-level adaptive grouping method.</p><p>Error feedback structure. Previously in 2D computer vision, Carreira et al. <ref type="bibr" target="#b3">[4]</ref> proposed a framework called Iterative Error Feedback (IEF), which minimized the error loss between current and desired outputs in the backpropagation procedure. In contrast to <ref type="bibr" target="#b3">[4]</ref>, the methods in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref> complimented the output with a back-projection unit in the forward procedure. For 3D point clouds, PU-GAN <ref type="bibr" target="#b14">[15]</ref> leveraged a similar idea for point cloud generation, while <ref type="bibr" target="#b29">[30]</ref> presented a structure with specially designed paths for prominent features learning.</p><p>Basically, current IEF structures for point clouds are redundant and implicit. Considering the complexity of 3D data, a concise and explicit IEF module is needed. More importantly, an IEF module is expected to serve two purposes in the network: first, to make the actual output approach the desired point clouds representations; second, to help the grouping process form the adaptive point neighborhoods.</p><p>Network architecture for point cloud learning. To realize different computer vision tasks using deep learning, many network architectures have been introduced: VGG [33], ResNet <ref type="bibr" target="#b8">[9]</ref>, etc. Besides, some works tried different image resolutions for more clues; for example, the fully convolutional network <ref type="bibr" target="#b20">[21]</ref> keeps the full size of an image, deconvolution network <ref type="bibr" target="#b25">[26]</ref> steps into lower resolutions, and HRNet <ref type="bibr" target="#b37">[38]</ref> shares the features among different resolutions.</p><p>As for 3D point clouds, two popular architectures are 1) PointNet++ <ref type="bibr" target="#b28">[29]</ref>, which downsamples the point clouds using Farthest Point Sampling (FPS) and upsamples using Feature Propagation (FP), and 2) a fully convolutional network, which learns point-wise features from multiple embedding space scales, for example, DGCNN <ref type="bibr" target="#b38">[39]</ref> dynamically updates the crafted point graph around each point. Different from the above mentioned methods, our approach exploits more clues through dense connections between various resolutions of the point clouds. Furthermore, we investigate the characteristics of multi-resolutional features, and then develop a better merging behavior for the feature maps. In general, our DRNet adaptively encodes the local context from more resolutions of point clouds, by which fine-grained output representations benefit point cloud classification and segmentation tasks. </p><formula xml:id="formula_0">E N ×(k·dmax) , where: d i ∈ Z, d i ∈ [1, d max ], D N = [d 1 , ... , d i , ... , d N ] T ; group the indices I N ×k of the k neighbors from I N ×(k·dmax) based on D N ; end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>CNN-based learning of 3D data has become more intuitive due to the introduction of multi-layer perceptrons (MLPs) <ref type="bibr" target="#b27">[28]</ref> that directly process point clouds. Primarily, an MLP, M(·), is described as a composite operation of 1by-1 convolution with a possible batch normalization <ref type="bibr" target="#b10">[11]</ref> (BN) and an activation (e.g., ReLU) on the feature map.</p><p>In addition, recent works [39, 6, 44] craft regional patterns to record more local details via a graph around each point p i ∈ R c , based on both the absolute position of the centroid and relative positions of the neighbors in cdimensional feature space. Specifically, the crafted graph (G) of the centroid p i is formulated as:</p><formula xml:id="formula_1">G(p i ) = (p i , p j − p i ); where ∀p j ∈ N i(p i ).</formula><p>Usually, the quality of the information provided by G(p i ) highly depends on the neighbors, ∀p j ∈ N i(p i ), that are found by the grouping method. Hence, we expect a better grouping method for G(p i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Adaptive Dilated Point Grouping</head><p>The two popular grouping methods i.e. Ball Query and knearest neighbors (knn) (see Section 1) have shortcomings (as analyzed in Section 2), and to overcome these issues, here we propose a novel grouping method named Adaptive Dilated Point Grouping (ADPG), which is shown in Algorithm 1. ADPG aims to generate the indices of neighbors I N ×k for the points, given a feature map P N ×c of the point cloud and consists of the following three main procedures. Searching. The first step of ADPG is searching candidate neighbors for the points. In this paper, we introduce a solution capable of addressing common scales of point cloud data. We define the pairwise Euclidean distances E N ×N in feature space as our metric, which indicates the point density distribution to a certain extent. As for a N × c size feature map P, the pairwise Euclidean distances are:</p><formula xml:id="formula_2">E N ×N = diag(PP T ) · 1 + 1 T · diag(PP T ) T − 2PP T ,</formula><p>where 1 means a 1 × N row vector of ones, and diag(·) forms a N × 1 column vector whose entries are the N diagonal elements of a N × N square matrix. According to the calculated distances metric, we can easily identify the k · d max candidate nearest neighbors of each point. In our implementation, we sort the rows of E N ×N in ascending order, and retain the metric values and indices of the first k · d max elements. Therefore, the elements with the smallest k · d max values in each row of E N ×N are identified as candidate neighbors for each point. Meanwhile, the metric values and indices of the searched candidate neighbors are recorded as E N ×(k·dmax) and I N ×(k·dmax) , respectively. Besides, our implementation is also flexible; that is, the choices for metrics (e.g., density or geometric similarities) and searching techniques (e.g., FLANN <ref type="bibr" target="#b24">[25]</ref> for the sake of efficiency in large-scale point cloud data) can be easily integrated as needed.</p><p>Learning. In order to construct a dilated neighborhood for each point adaptively, it is necessary to determine a dilation factor <ref type="bibr" target="#b45">[46]</ref> for each point based on known information of its candidate neighbors. In practice, we learn the dilation factors based on E N ×(k·dmax) and CNN-related operations.</p><p>To be specific, we apply an MLP (M) and a sigmoid function (σ) to the metric values of candidates E N ×(k·dmax) , in order to summarize the information of the point distribution of local areas. Then, a projection function J (e.g., linear function) can map the values to the expected numerical scale. Finally, we take a scale function S (e.g., round to assign a dilation factor, D N 1 , for each point according to the summarized information:</p><formula xml:id="formula_3">D N = S J σ M(E N ×(k·dmax) )</formula><p>.</p><p>Grouping. As each point has a corresponding dilation factor, we pick up every d i -th index of candidate indices I N ×(k·dmax) to form the selected k neighbors for each point. Following a behavior similar to <ref type="bibr" target="#b5">[6]</ref>, we obtain the final indices of local point groups I N ×k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Error-minimizing Module</head><p>Following the ADPG method, each point gathers a group of neighbors with a larger receptive field. As stated, we apply the crafted graph G, i.e. the absolute position of a centroid and relative positions of the neighbors, to encode the high-dimensional features over each neighborhood. Further projected by an MLP (with c filters), the information of a local graph centering at p i , is represented as:</p><formula xml:id="formula_5">f Gi = M G(p i ) = M (p i , p j − p i ) ,<label>(2)</label></formula><p>1 More implementing details are in the supplementary material.</p><p>where ∀p j ∈ ADP G(p i ) and f Gi ∈ R c ×k . Usually, a max-pooling function is applied over the k neighbors of each crafted local graph to aggregate the local context as the centroid's feature representation. However, possible bias exists in process: on the one hand, the local graphs lack geometric regularization from the initial 3D space; and on the other hand, the max-pooled features only retain prominent outlines while discarding local details in embedding space. In this case, the Iterative Error Feedback (IEF) mechanisms idea helps avoid bias accumulation during the high-dimensional feature learning process.</p><p>Let us assume that the local graph f Gi indeed embeds the full information about the neighborhood, it would be possible to restore the input p i through a back-projection process B(·). Practically, we realize the B(·) operation through a shared 1-by-k convolution followed by BN and ReLU, over the local graphs. Intuitively, this operation acts to aggregate the nodes based on learned weights of the edges in the graph, which implicitly simulates a reverse process of crafting the graph. Therefore, the back-projected feature f Bi is formulated as:</p><formula xml:id="formula_6">f Bi = B(f Gi ); where f Bi ∈ R c .</formula><p>Consequently, an error feature f Ei is defined as the difference between the original input p i and back-projected feature f Bi . In contrast to the methods in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref> that correct the error by extra computations in the forward pass, we use additional 2 loss to minimize the error, f Ei = f Bi − p i , during the back-propagation pass:</p><formula xml:id="formula_7">L er = ||f Ei || 2 .<label>(3)</label></formula><p>The loss in Equation 3 can constrain the feature learning during training by forcing the back-projected feature f Ei to approach the original input p i inside of the module. Following such a regularization, the error and bias in the output representations can be alleviated, especially during the early stages of training. Meanwhile, compared with the regular cross-entropy loss for the whole network, each errorminimizing module's loss can provide more clues for the ADPG in corresponding feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dense-Resolution Network Architecture</head><p>Although the ADPG method and the error-minimizing module seem promising for local feature learning of point clouds, we still need a robust network architecture to leverage the potential offered by both. The architecture of our network is presented in <ref type="figure">Figure 2</ref>. Full-resolution branch. We adopt the idea of basic fully convolutional architecture as the full-resolution (FR) branch of our network. The benefits can be retained based on two aspects; 1) there remains a consistent number of points in different scales of embedding space during feature learning progress; 2) it retains the per-point feature without any confusion caused by the numerical approximation in upsam-pling. Therefore, we expect this structure to learn comprehensive representations for point-wise features.</p><p>Specifically, the FR branch consists of the proposed error-minimizing modules in a cascaded form, which progressively learn the feature representation of each point from its adaptive neighborhood formed by ADPG in different scales of embedding space. In order to acquire a global knowledge about the abstract embedding space, the learned features from different scales are concatenated and aligned to form the output F F R of the FR branch. Multi-resolution branch. Meanwhile, there is a limitation of F F R : it lacks channel-wise clues about semantic/shaperelated information since the FR branch mainly focuses on point-wise context. To overcome this issue, we capture additional features from more resolutions of point clouds. Therefore, we propose the multi-resolution (MR) branch, a light-weight down/up-sampling structure, to investigate the lower resolutions of point clouds. Contrary to competing methods, the propagated features and skip links are densely connected to enhance the relations between multiple point cloud resolutions and feature embedding scales. The output F M R of the MR branch captures thorough channel-wise information about the point clouds.</p><p>Features merging. To leverage the information gathered from both FR and MR branches, it is necessary to find a reasonable merging technique for the two feature maps, i.e. F F R and F M R . Usually, CNNs combine the feature maps by concatenation, summation, or multiplication. These regular operations treat the feature maps equally, without taking their properties into account. Instead, we prefer merging the FR and MR outputs in a unique manner.</p><p>Given the advantages of FR and MR branches that we analyzed before, F F R is applied as the basis of per-point feature representation. In addition, the channel-wise information of F M R is derived to enhance F F R . Empirically, we use a max-pooling and an MLP to summarize the knowledge of F M R channels. After a sigmoid activation σ, the channel-wise enhancement on the per-point context of F F R can be realized by multiplication. The final output of our dense-resolution (DR) network follows:</p><formula xml:id="formula_8">F DR = F F R × σ M max N (F M R ) .<label>(4)</label></formula><p>Loss function. Based on the output feature map (F DR ), the fully-connected (FC) layers regress the confidence scores for all possible categories. In addition to the basic cross-entropy loss (L ce ), the weighted losses of the errorminimizing modules are incorporated. For the DRNet with M error-minimizing modules in its FR branch, by applying Equation 3 and the hyper-parameter w i as weight, the overall loss is formulated as:</p><formula xml:id="formula_9">L = L ce + M i=1 w i · L eri .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, our implementation details are provided, including network parameters, training settings, datasets, etc. By comparing the experimental results with other stateof-the-art methods, we analyze performance quantitatively. Further, we present ablation studies and visualizations to illustrate the properties of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation</head><p>Network details. The FR branch of our DRNet is a series of error-minimizing modules extracting features at different scales of embedding space: i.e. 64, 128, and 256, as in <ref type="figure">Figure 2</ref>. Empirically, we adopt k = 20 and d max = 5 as in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6]</ref>. The FR output F F R is an MLP projected concatenation of the modules' outputs. For the MR branch, we apply the widely-used farthest point sampling (FPS) and feature propagation (FP) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref> for downsampling and upsampling, respectively. Further, single-layer MLPs are used for channel alignment together with the mentioned operations. The MR branch starts from the first output of FR in N size; after that, two lower resolutions: N/4 and N/16, are investigated through the regular knn and local graph encoding as Equation 2. Different from other approaches, more propagated features and dense skip connections are employed to enhance the relations between different point resolutions and feature spaces. Compared with the FR, the MR branch 2 is light-weight due to the fewer scales of embedding space, the limited number of points, and the operations with fewer learnable weights.</p><p>The output F DR is obtained by following Equation 4. For the classification task, we apply a max-pooling function and Fully Connected (FC) layers to regress confidence scores for all possible categories. In terms of the segmentation task, we attach the max-pooled feature to each point feature of F DR and further predict each point's semantic label with FC layers being applied.</p><p>For the loss function, empirically, a larger weight is set for the first error-minimizing module, i.e. w 1 , since its output affects both branches and constrains the network learning initially. In contrast, the weights for other modules can be smaller since they are less critical. Although the additional loss is involved, cross-entropy loss still contributes the most to the training 2 . We implement the project with Py-Torch and Python; all experiments are conducted on Linux and GeForce RTX 2080Ti GPUs. <ref type="bibr" target="#b2">3</ref> Training strategy. For classification, Stochastic Gradient Descent (SGD) <ref type="bibr" target="#b21">[22]</ref> with a momentum of 0.9 is adopted as the optimizer. The learning rate decreases from 0.1 to 0.001 by cosine annealing <ref type="bibr" target="#b21">[22]</ref>  segmentation, we exploit Adam <ref type="bibr" target="#b12">[13]</ref> optimization for 200 epochs of training. The learning rate begins at 0.001 and gradually decays with a rate of 0.5 after every 20 epochs. The batch size for both tasks is 32. Besides, training data is augmented with random scaling and translation; the overall loss follows Equation 5. Part segmentation is evaluated with a ten-votes strategy used by recent approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19]</ref>. Datasets. We test our approach on two main tasks: point cloud segmentation and classification. The ShapeNet Part dataset <ref type="bibr" target="#b44">[45]</ref> is used to predict the semantic class (part label) for each point of the object. In addition, the synthetic ModelNet40 <ref type="bibr" target="#b40">[41]</ref> dataset and the real-world ScanObjectNN <ref type="bibr" target="#b35">[36]</ref> dataset are used to identify the category of the object. <ref type="figure">Figure 3</ref> presents some examples from the datasets.</p><p>• ShapeNet Part. The dataset has 16,881 object point clouds in 16 categories, where each point is labeled as one of 50 parts. As the primary dataset for our experiments, we follow the official data split <ref type="bibr" target="#b4">[5]</ref>. We input the 3D coordinates of 2048 points for each point cloud and feed the object label before FC layers during training. In terms of the metric for evaluation, we adopt Intersection-over-Union (i.e. IoU). The IoU of method input type #points accuracy PointNet <ref type="bibr" target="#b27">[28]</ref> coords 1k 89.2 A-SCN <ref type="bibr" target="#b41">[42]</ref> coords 1k 90.0 PointNet++ <ref type="bibr" target="#b28">[29]</ref> coords 1k 90.7 SO-Net <ref type="bibr" target="#b13">[14]</ref> coords  the shape is calculated by the mean value of IoUs of all parts in that shape, and mIoU (i.e. mean IoU) is the average of IoUs for all testing shapes.</p><p>• ModelNet40. It is a popular dataset because of regular and clean shapes. There are 12,311 meshes in 40 classes, with 9,843 for training and 2,468 for testing. Corresponding point clouds are generated by uniformly sampling from the surfaces, translating to the origin, and scaling within a unit sphere <ref type="bibr" target="#b27">[28]</ref>. In our case, only the 3D coordinates of 1024 points for each point cloud have been used.</p><p>• ScanObjectNN. This real-world object dataset is recently published. Although it has over 15,000 objects in only 15 categories, it is practically more challenging due to the background complexity, object partiality, and different deformation variants. We conduct the experiment using its most challenging variant, PB T50 RS, with background points.   <ref type="table">Table 3</ref>. Classification results (%) on ScanObjectNN dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Segmentation. <ref type="table" target="#tab_0">Table 1</ref> shows the results of related works reported in overall mIoU, which is the most critical evaluation metric on the ShapeNet Part dataset. On the whole, our network achieves 86.4% and outperforms other state-of-theart algorithms based on similar experimental settings. For evaluations inside each class, we surpass others in five out of 16 categories. Especially in categories with a large number of samples, e.g., airplane, chair, or table, we perform even better (two out of these three classes) than others. In <ref type="figure" target="#fig_1">Figure 4</ref>, we provide some samples of our part segmentation results comparing with DGCNN <ref type="bibr" target="#b38">[39]</ref> and RS-CNN <ref type="bibr" target="#b18">[19]</ref>. Classification. <ref type="table" target="#tab_2">Table 2</ref> presents the overall accuracy of the classification on the synthetic object dataset: ModelNet40. Specifically, we achieve 93.1% for overall classification accuracy and exceed other state-of-the-art results with similar input. Essentially, our method performs better than others using more input points or features. <ref type="table">Table 3</ref> presents our results on the ScanObjectNN dataset, which contains practical scans of real-world objects as <ref type="figure">Figure 3</ref> indicates. To be concrete, both overall accuracy 80.3% and average class accuracy 78.0% of our approach are significantly higher than all results on its official leaderboard <ref type="bibr" target="#b9">[10]</ref>. Typically, we lead in four (bag, box, display, and sofa) out of the 15 categories. The inference time of our basic classification model running on a single GeForce RTX 2080Ti GPU is about 19.2ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Visualization of learned dilation factors. <ref type="figure" target="#fig_2">Figure 5</ref> illustrates the effects of our ADPG method, where the color of the points corresponds to the learned dilation factor. Intuitively, the advantages of ADPG can be observed from two aspects: Firstly, for each point cloud, ADPG tends to assign larger dilation factors to points that have relatively sparse local point distributions (e.g., on corners/boundaries/edges) because they need larger neighborhoods for more comprehensive local feature learning. Secondly, within the cascaded structure, ADPG regulates the points' dilation factors in deep layers and turns out to have smaller dilation factors in dense local distribution (e.g., on flat surfaces/central areas), most probably to constrain the neighborhoods and   <ref type="table" target="#tab_5">Table 5</ref>, we observe that the simple ways of merging may not improve performance. In contrast, channel-wise enhancing of the F F R using F M R (model 5) can improve a bit because of the reasons explained in Section 3.3. With ten-votes testing, the overall mIoU can boost to 86.4%.</p><formula xml:id="formula_10">model Network F mer overall mIoU 0 F R F F R 85.6 1 M R F M R 85.3 2 DR Concat(F F R , F M R ) 85.7 3 DR F F R + F M R 85.6 4 DR F F R F M R 85.6 5 DR F DR 86.0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a Dense-Resolution Network for point cloud analysis, which leverages information from different resolutions of the point clouds. Specifically, the Adaptive Dilated Point Grouping method is introduced to realize a flexible point grouping based on the density distribution. Moreover, an error-minimizing module and corresponding loss are presented to capture local information and guide the training network. We conduct experiments and provide ablation studies on both point cloud segmentation and classification benchmarks. The experimental results outperform competing state-of-the-art methods on ShapeNet Part, ModelNet40, and ScanObjectNN datasets. The quantitative reports and qualitative visualizations demonstrate the advantages of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>In this supplementary material, we present more contents of our paper Dense-Resolution Network for Point Cloud Classification and Segmentation. To be specific, we provide the implementation of the Adaptive Dilated Point Grouping (ADPG) method and loss function for the experiments. Besides, we show the details of our Multi-resolution (MR) branch. By comparing the relevant model parameters with others on ModelNet40 dataset, we discuss the complexity of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation</head><p>In the main paper, we introduce the pipeline for the ADPG method and the design of loss function for training. In this section, we provide more practical details in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. ADPG Learning Process</head><p>In practice, d max is an empirical parameter which may vary between the data scales or networks. In our experiments, we set d max = 5 for ShapeNet Part, ModelNet40 and ScanObjectNN datasets, since they share the similar scales of point clouds.</p><p>Assume that we already have the indices I N ×(k·dmax) and metrics E N ×(k·dmax) for k·d max candidates, the crucial step of ADPG is to learn a certain dilation factor for each point based on the known information. In Section 3.1 of the paper, we present the general description for this process:</p><formula xml:id="formula_11">D N = S J σ M(E N ×(k·dmax) )</formula><p>Specifically, we apply a two-layer Multi-Layer-Perceptron (MLP M): Conv (k·dmax/2),1 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1]</ref> first, then activate corresponding negative values using a logistic function: y = 1/(1 + e −x ). Since the values are in between 0 and 1, J can further enlarge the variance by projecting them to another interval. Here we expect the values to be in [0.5, 5.5], thus a simple linear projection function y = 5 · x + 0.5 serves as J . Finally, we adopt round function as S to scale the continuous values in [0.5, 5.5], by which an integer in {1, 2, 3, 4, 5} can be assigned as the dilation factor for each point. To summarize, the dilation factors learning in our implementation follows:  </p><formula xml:id="formula_12">D N =       </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Loss Function</head><p>As discussed in the Section 3.3, the total loss for training is the sum of cross-entropy loss L ce and weighted errorminimizing module losses:</p><p>w i · L eri . In practice, we apply 4 error-minimizing modules in the Full-resolution (FR) branch of our network, adopting the similar layers and feature dimensions as in <ref type="bibr" target="#b38">[39]</ref>. In terms of our experiments on the ShapeNet Part, ModelNet40 and ScanOb-jectNN datasets, we empirically set a larger weight for the first error-minimizing module (w 1 = 0.1) since its output affects the both branches and constrains the network learning at the beginning. In contrast, the weights for other modules' losses can be smaller (w 2 = w 3 = w 4 = 0.01).</p><p>Although the additional losses are incorporated, the crossentropy loss still contributes the most to the training. The overall loss L in our practice is formulated as: L = L ce +0.1 · L er1 +0.01 · L er2 +0.01 · L er3 +0.01 · L er4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-resolution Branch</head><p>As shown in <ref type="figure">Figure 6</ref>, the MR branch is implemented with light-weight operations such as single-layer MLPs, and only investigates 2 more resolutions of the point cloud using basic Local Graph Encoding as Equation 2 in the main paper. For upsampling and downsampling operations, they are implemented based on CUDA without learnable weights. Besides, we use the dense connections and concatenations to enhance the relations between the feature maps of different resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Complexity</head><p>In addition, we adopt the network complexity data provided in <ref type="bibr" target="#b38">[39]</ref> for a fair comparison. As <ref type="table" target="#tab_7">Table 6</ref> shows, our model size is relatively large due to the parameters and operations needed. However, the inference time of our method running on a single GeForce GTX 2080Ti GPU is only 19.2 ms, which indicates the ability of our model in forward procedure thanks to the algorithm optimization and relevant CUDA implementation. <ref type="figure">Figure 6</ref>. The input of the MR branch is the output of the first error-minimizing module in the FR branch, while the output of the MR branch merges with the output of the FR branch following the behavior as Equation 4 in the main paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>The forward pass pipeline of Adaptive Dilated Point Grouping input: feature map P N ×c = [p 1 T , p 2 T , ... , p N T ] in c-dimensional space. parameters: the number of neighbors k, and an empirical maximum dilation factor d max . output: the matrix I N ×k , indices of the selected k neighbors for the point cloud. for each point cloud do search for the (k · d max ) candidate neighbors based on P N ×c , get the candidate metric values E N ×(k·dmax) and the indices I N ×(k·dmax) ; learn the dilation factors D N based on the metrics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Examples of the part segmentation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Learned dilation factors by the ADPG method. For each point cloud, ADPG assigns larger dilation factors for the points in sparse areas. As the network goes deeper, ADPG regulates the dilation factors of the points. (First-row: the learned dilation factors in a shallow layer of our network. Second-row: in a deep layer.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>during the 300 epochs. For Part segmentation results (mIoU(%)) on the ShapeNet Part dataset.</figDesc><table><row><cell></cell><cell>overall</cell><cell>air</cell><cell>bag</cell><cell>cap</cell><cell>car chair</cell><cell>ear</cell><cell cols="9">guitar knife lamp laptop moto mug pistol rocket skate table</cell></row><row><cell></cell><cell cols="2">mIoU plane</cell><cell></cell><cell></cell><cell></cell><cell>phone</cell><cell></cell><cell></cell><cell></cell><cell>bike</cell><cell></cell><cell></cell><cell></cell><cell>board</cell><cell></cell></row><row><cell># shapes</cell><cell>16881</cell><cell>2690</cell><cell>76</cell><cell>55</cell><cell>898 3758</cell><cell>69</cell><cell>787</cell><cell>392 1547</cell><cell>451</cell><cell>202</cell><cell>184</cell><cell>283</cell><cell>66</cell><cell>152</cell><cell>5271</cell></row><row><cell>PointNet [28]</cell><cell>83.7</cell><cell cols="4">83.4 78.7 82.5 74.9 89.6</cell><cell>73.0</cell><cell>91.5</cell><cell>85.9 80.8</cell><cell>95.3</cell><cell cols="3">65.2 93.0 81.2</cell><cell>57.9</cell><cell>72.8</cell><cell>80.6</cell></row><row><cell>A-SCN [42]</cell><cell>84.6</cell><cell cols="4">83.8 80.8 83.5 79.3 90.5</cell><cell>69.8</cell><cell>91.7</cell><cell>86.5 82.9</cell><cell>96.0</cell><cell cols="3">69.2 93.8 82.5</cell><cell>62.9</cell><cell>74.4</cell><cell>80.8</cell></row><row><cell>SO-Net [14]</cell><cell>84.6</cell><cell cols="4">81.9 83.5 84.8 78.1 90.8</cell><cell>72.2</cell><cell>90.1</cell><cell>83.6 82.3</cell><cell>95.2</cell><cell cols="3">69.3 94.2 80.0</cell><cell>51.6</cell><cell>72.1</cell><cell>82.6</cell></row><row><cell>PointNet++ [29]</cell><cell>85.1</cell><cell cols="4">82.4 79.0 87.7 77.3 90.8</cell><cell>71.8</cell><cell>91.0</cell><cell>85.9 83.7</cell><cell>95.3</cell><cell cols="3">71.6 94.1 81.3</cell><cell>58.7</cell><cell>76.4</cell><cell>82.6</cell></row><row><cell>PCNN [1]</cell><cell>85.1</cell><cell cols="4">82.4 80.1 85.5 79.5 90.8</cell><cell>73.2</cell><cell>91.3</cell><cell>86.0 85.0</cell><cell>95.7</cell><cell cols="3">73.2 94.8 83.3</cell><cell>51.0</cell><cell>75.0</cell><cell>81.8</cell></row><row><cell>DGCNN [39]</cell><cell>85.2</cell><cell cols="4">84.0 83.4 86.7 77.8 90.6</cell><cell>74.7</cell><cell>91.2</cell><cell>87.5 82.8</cell><cell>95.7</cell><cell cols="3">66.3 94.9 81.1</cell><cell>63.5</cell><cell>74.5</cell><cell>82.6</cell></row><row><cell>P2Sequence [17]</cell><cell>85.2</cell><cell cols="4">82.6 81.8 87.5 77.3 90.8</cell><cell>77.1</cell><cell>91.1</cell><cell>86.9 83.9</cell><cell>95.7</cell><cell cols="3">70.8 94.6 79.3</cell><cell>58.1</cell><cell>75.2</cell><cell>82.8</cell></row><row><cell>SpiderCNN [43]</cell><cell>85.3</cell><cell cols="4">83.5 81.0 87.2 77.5 90.7</cell><cell>76.8</cell><cell>91.1</cell><cell>87.3 83.3</cell><cell>95.8</cell><cell cols="3">70.2 93.5 82.7</cell><cell>59.7</cell><cell>75.8</cell><cell>82.8</cell></row><row><cell>PointASNL [44]</cell><cell>86.1</cell><cell cols="4">84.1 84.7 87.9 79.7 92.2</cell><cell>73.7</cell><cell>91.0</cell><cell>87.2 84.2</cell><cell>95.8</cell><cell cols="3">74.4 95.2 81.0</cell><cell>63.0</cell><cell>76.3</cell><cell>83.2</cell></row><row><cell>RS-CNN [19]</cell><cell>86.2</cell><cell cols="4">83.5 84.8 88.8 79.6 91.2</cell><cell>81.1</cell><cell>91.6</cell><cell>88.4 86.0</cell><cell>96.0</cell><cell cols="3">73.7 94.1 83.4</cell><cell>60.5</cell><cell>77.7</cell><cell>83.6</cell></row><row><cell>Ours</cell><cell>86.4</cell><cell cols="4">84.3 85.0 88.3 79.5 91.2</cell><cell>79.3</cell><cell>91.8</cell><cell>89.0 85.2</cell><cell>95.7</cell><cell cols="3">72.2 94.2 82.0</cell><cell>60.6</cell><cell>76.8</cell><cell>84.2</cell></row></table><note>Figure 3. Examples from the experimental datasets. The upper row shows the point clouds labeled as Chair from the three datasets, while the lower row presents Table. Particularly, ScanObjectNN dataset contains background points, which are in a lighter color.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Overall classification accuracy (%) on ModelNet40 dataset. (coords: 3D coordinates, norms: surface normal vectors of the points, k:×2 10 )</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Merging the feature maps. Both FR and MR have properties as mentioned, so we need to find an effective way to unify the benefits. We test simple ways of merging the features of F F R and F M R , i.e. concatenating them in channelwise, adding and multiplying them in element-wise. Comparing the results of model 2&amp;3&amp;4 to model 0 in</figDesc><table><row><cell cols="5">model Network ADPG E-M module overall mIoU</cell></row><row><cell>0</cell><cell>F R</cell><cell>-</cell><cell>-</cell><cell>85.2</cell></row><row><cell>1</cell><cell>F R</cell><cell>-</cell><cell></cell><cell>85.4</cell></row><row><cell>2</cell><cell>F R</cell><cell></cell><cell></cell><cell>85.6</cell></row><row><cell>3</cell><cell>M R</cell><cell>-</cell><cell>-</cell><cell>84.9</cell></row><row><cell>4</cell><cell>M R</cell><cell></cell><cell></cell><cell>85.3</cell></row><row><cell>5</cell><cell>DR</cell><cell></cell><cell></cell><cell>86.0</cell></row><row><cell cols="5">Table 4. Ablation study about the effects of different network com-</cell></row><row><cell cols="5">ponents on ShapeNet Part (%). (F R: Full-Resolution branch</cell></row><row><cell cols="5">only, M R: Multi-Resolution branch only, DR: Dense-Resolution</cell></row><row><cell cols="5">Network, ADPG: Adaptive Dilated Point Grouping method, E-M</cell></row><row><cell cols="4">module: Error-minimizing module for local points.)</cell><cell></cell></row><row><cell cols="5">avoid outliers. Unlike regular knn/Ball Query/dilated-knn,</cell></row><row><cell cols="5">which defines a limited and fixed neighborhood for all</cell></row><row><cell cols="5">points in all layers, our ADPG works adaptively and rea-</cell></row><row><cell cols="2">sonably as expected.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Effects of components. We conduct an ablation study</cell></row><row><cell cols="5">about the effects of the network components: the architec-</cell></row><row><cell cols="5">ture, grouping method, and the error-minimizing module.</cell></row><row><cell cols="5">We run tests on the ShapeNet Part dataset under the same</cell></row><row><cell cols="5">settings, and Table 4 presents the results. Comparing model</cell></row><row><cell cols="5">1&amp;2 to model 0 and model 4 to model 3, we observe that</cell></row><row><cell cols="5">the error-minimizing module with ADPG applied can sig-</cell></row><row><cell cols="5">nificantly improve the part segmentation's network perfor-</cell></row><row><cell cols="5">mance. Although the multi-resolution branch is not able to</cell></row><row><cell cols="5">learn the features as comprehensively as a full-resolution</cell></row><row><cell cols="5">branch does, we can take advantage of both by combining</cell></row><row><cell cols="4">them as a dense-resolution network (model 5).</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation study about the different forms of merged feature Fmer on ShapeNet Part (%). (F R: Full-Resolution branch only, M R: Multi-Resolution branch only, DR: Dense-Resolution Network, FF R: the output of FR, FMR: the output of MR, : element-wise multiplication, FDR: merging as in Equation 4.)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Complexity of classification network on ModelNet40. (</figDesc><table /><note>* running on GeForce GTX 2080Ti)</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">More information about the implementation is provided in the supplementary material.<ref type="bibr" target="#b2">3</ref> The code and models are available at https://github.com/ ShiQiu0419/DRNet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10091</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Three-dimensional point cloud classification in real-time using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhak</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anath</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3145" to="3152" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Review of 20 years of range sensor development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Blais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of electronic imaging</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="231" to="243" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12046</idno>
		<title level="m">Dilated point convolutions: On the receptive field of point convolutions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hkust-Vgd</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>3d scene understanding benchmark</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Jaboyedoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Oppikofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Abellán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Henri</forename><surname>Derron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Loye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Pedrazzini</surname></persName>
		</author>
		<title level="m">Use of lidar in landslide investigations: a review. Natural hazards</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="5" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">So-net: Selforganizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pu-gan: A point cloud upsampling adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8778" to="8785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
	<note>Bin Fan, Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical back projection network for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Tak</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Registration of point cloud data from a geometric optimization perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Pottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing</title>
		<meeting>the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VISAPP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="331" to="340" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Five balltree construction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Omohundro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>International Computer Science Institute Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Geometric backprojection network for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12885</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<biblScope unit="page" from="3212" to="3217" />
			<date type="published" when="2009" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient ransac for point-cloud shape detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruwen</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikaela</forename><forename type="middle">Angelina</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">3d building model reconstruction from point clouds and ground plans. International archives of photogrammetry remote sensing and spatial information sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vosselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander Dijkman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07919</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
