<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AtlasNet: A Papier-Mâché Approach to Learning 3D Surface Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIGM (UMR 8049)</orgName>
								<orgName type="institution">UPE</orgName>
								<address>
									<addrLine>École des Ponts</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIGM (UMR 8049)</orgName>
								<orgName type="institution">UPE</orgName>
								<address>
									<addrLine>École des Ponts</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AtlasNet: A Papier-Mâché Approach to Learning 3D Surface Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Given input as either a 2D image or a 3D point cloud (a), we automatically generate a corresponding 3D mesh (b) and its atlas parameterization (c). We can use the recovered mesh and atlas to apply texture to the output shape (d) as well as 3D print the results (e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) autoencoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Significant progress has been made on learning good representations for images, allowing impressive applications in image generation <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b35">35]</ref>. However, learning a representation for generating high-resolution 3D shapes remains an open challenge. Representing a shape as a volumetric function <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">31]</ref> only provides voxel-scale sampling of the underlying smooth and continuous surface. In contrast, a point cloud <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref> provides a representation for generating on-surface details <ref type="bibr" target="#b8">[9]</ref>, efficiently leveraging sparsity of the data. However, points do not directly represent neighborhood * Work done at Adobe Research during TG's summer internship information, making it difficult to approximate the smooth low-dimensional manifold structure with high fidelity.</p><p>To remedy shortcomings of these representations, surfaces are a popular choice in geometric modeling. A surface is commonly modeled by a polygonal mesh: a set of vertices, and a list of triangular or quad primitives composed of these vertices, providing piecewise planar approximation to the smooth manifold. Each mesh vertex contains a 3D (XYZ) coordinate, and, frequently, a 2D (UV) embedding to a plane. The UV parameterization of the surface provides an effective way to store and sample functions on surfaces, such as normals, additional geometric details, textures, and other reflective properties such as BRDF and ambient occlusion. One can imagine converting point clouds or volumetric functions produced with existing learned generative models as a simple post-process. However, this requires solving two fundamental, difficult, and long-standing challenges in geometry processing: global surface parameterization and meshing.</p><p>In this paper we explore learning the surface representation directly. Inspired by the formal definition of a surface as a topological space that locally resembles the Euclidean plane, we seek to approximate the target surface locally by mapping a set of squares to the surface of the 3D shape. The use of multiple such squares allows us to model complex surfaces with non-disk topology. Our representation of a shape is thus extremely similar to an atlas, as we will discuss in Section 3. The key strength of our method is that it jointly learns a parameterization and an embedding of a shape. This (c) Our approach with K patches. <ref type="figure">Figure 2</ref>. Shape generation approaches. All methods take as input a latent shape representation (that can be learned jointly with a reconstruction objective) and generate as output a set of points. (a) A baseline deep architecture would simply decode this latent representation into a set of points of a given size. (b) Our approach takes as additional input a 2D point sampled uniformly in the unit square and uses it to generate a single point on the surface. Our output is thus the continuous image of a planar surface. In particular, we can easily infer a mesh of arbitrary resolution on the generated surface elements. (c) This strategy can be repeated multiple times to represent a 3D shape as the union of several surface elements. helps in two directions. First, by ensuring that our 3D points come from 2D squares we favor learning a continuous and smooth 2-manifold structure. Second, by generating a UV parameterization for each 3D point, we generate a global surface parameterization, which is key to many applications such as texture mapping and surface meshing. Indeed, to generate the mesh, we simply transfer a regular mesh from our 2D squares to the 3D surface, and to generate a regular texture atlas, we simply optimize the metric of the square to become as-isometric-as-possible to the corresponding 3D shape ( <ref type="figure">Fig. 1</ref>).</p><p>Since our work deforms primitive surface elements into a 3D shape, it can be seen as bridging the gap between the recent works that learn to represent 3D shapes as a set of simple primitives, with a fixed, low number of parameters <ref type="bibr" target="#b32">[32]</ref> and those that represent 3D shapes as an unstructured set of points <ref type="bibr" target="#b8">[9]</ref>. It can also be interpreted as learning a factored representation of a surface, where a point on the shape is represented jointly by a vector encoding the shape structure and a vector encoding its position. Finally, it can be seen as an attempt to bring to 3D the power of convolutional approaches for generating 2D images <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b35">35]</ref> by sharing the network parameters for parts of the surface. Our contributions. In this paper:</p><p>• We propose a novel approach to 3D surface generation, dubbed AtlasNet, which is composed of a union of learnable parametrizations. These learnable parametrizations transform a set of 2D squares to the surface, covering it in a way similar to placing strips of paper on a shape to form a papier-mâché. The parameters of the transformations come both from the learned weights of a neural network and a learned representation of the shape.</p><p>• We show that the learned parametric transformation maps locally everywhere to a surface, naturally adapts to its underlying complexity, can be sampled at any desired resolution, and allows for the transfer of a tessellation or texture map to the generated surface.</p><p>• We demonstrate the advantages of our approach both qualitatively and quantitatively on high resolution sur-face generation from (potentially low resolution) point clouds and 2D images</p><p>• We demonstrate the potential of our method for several applications, including shape interpolation, parameterization, and shape collections alignment.</p><p>All the code is available at the project webpage 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>3D shape analysis and generation has a long history in computer vision. In this section, we only discuss the most directly related works for representation learning for 2manifolds and 3D shape generation using deep networks. Learning representations for 2-manifolds. A polygon mesh is a widely-used representation for the 2-manifold surface of 3D shapes. Establishing a connection between the surface of the 3D shape and a 2D domain, or surface parameterization, is a long-standing problem in geometry processing, with applications in texture mapping, re-meshing, and shape correspondence <ref type="bibr" target="#b14">[15]</ref>. Various related representations have been used for applying neural networks on surfaces. The geometry image representation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">28]</ref> views 3D shapes as functions (e.g., vertex positions) embedded in a 2D domain, providing a natural input for 2D neural networks <ref type="bibr" target="#b29">[29]</ref>. Various other parameterization techniques, such as local polar coordinates <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b4">5]</ref> and global seamless maps <ref type="bibr" target="#b22">[22]</ref> have been used for deep learning on 2-manifolds. Unlike these methods, we do not need our input data to be parameterized. Instead, we learn the parameterization directly from point clouds. Moreover, these methods assume that the training and testing data are 2-manifold meshes, and thus cannot easily be used for surface reconstructions from point clouds or images.</p><p>Deep 3D shape generation. Non-parametric approaches retrieve shapes from a large corpus <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24]</ref>, but require having an exact instance in the corpus. One of the most popular shape representation for generation is the voxel representation. Methods for generating a voxel grid have been demonstrated with various inputs, namely one or several images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>, full 3D objects in the form of voxel grids <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">34]</ref>, and 3D objects with missing shape parts <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b11">12]</ref>. Such direct volumetric representation is costly in term of memory and is typically limited to coarser resolutions. To overcome this, recent work has looked at a voxel representation of the surface of a shape via oct-trees <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b31">31]</ref>. Recently, Li et al. also attempted to address this issue via learning to reason over hierarchical procedural shape structures and only generating voxel representations at the part level <ref type="bibr" target="#b20">[20]</ref>. As an alternative to volumetric representations, another line of work has learned to encode <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref> and decode <ref type="bibr" target="#b8">[9]</ref> a 3D point representation of the surface of a shape. A limitation of the learned 3D point representation is there is no surface connectivity (e.g., triangular surface tessellation) embedded into the representation.</p><p>Recently, Sinha et al. <ref type="bibr" target="#b30">[30]</ref> proposed to use a spherical parameterization of a single deformable mesh (if available) or of a few base shapes (composed with authalic projection of a sphere to a plane) to represent training shapes as parameterized meshes. They map vertex coordinates to the resulting UV space and use 2D neural networks for surface generation. This approach relies on consistent mapping to the UV space, and thus requires automatically estimating correspondences from training shapes to the base meshes (which gets increasingly hard for heterogeneous datasets). Surfaces generated with this method are also limited to the topology and tessellation of the base mesh. Overall, learning to generate surfaces of arbitrary topology from unstructured and heterogeneous input still poses a challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Locally parameterized surface generation</head><p>In this section, we detail the theoretical motivation for our approach and present some theoretical guarantees.</p><p>We seek to learn to generate a surface of a 3D shape. A subset S of R 3 is a 2-manifold if, for every point p ∈ S, there is an open set U in R 2 and an open set W in R 3 containing p such that S ∩ W is homeomorphic to U . The set homeomorphism from S ∩ W to U is called a chart, and its inverse a parameterization. A set of charts such that their images cover the 2-manifold is called an atlas of the 2-manifold. The ability to learn an atlas for a 2-manifold would allow a number of applications, such as transfer of a tessellation to the 2-manifold for meshing and texture mapping (via texture atlases). In this paper, we use the word surface in a slightly more generic sense than 2-manifold, allowing for self-intersections and disjoint sets.</p><p>We consider a local parameterization of a 2-manifold and explain how we learn to approximate it. More precisely, let us consider a 2-manifold S, a point p ∈ S and a parameterization ϕ of S in a local neighborhood of p. We can assume that ϕ is defined on the open unit square ]0, 1[ 2 by first restricting ϕ to an open neighborhood of ϕ −1 (p) with disk topology where it is defined (which is possible because ϕ is continuous) and then mapping this neighborhood to the unit square.</p><p>We pose the problem of learning to generate the local 2-manifold previously defined as one of finding a parameterizations ϕ θ (x) with parameters θ which map the open unit 2D square ]0, 1[ 2 to a good approximation of the desired 2-manifold S loc . Specifically, calling S θ = ϕ θ (]0, 1[ 2 ), we seek to find parameters θ minimizing the following objective function, min</p><formula xml:id="formula_0">θ L (S θ , S loc ) + λR (θ) ,<label>(1)</label></formula><p>where L is a loss over 2-manifolds, R is a regularization function over parameters θ, and λ is a scalar weight. In practice, instead of optimizing a loss over 2-manifolds L, we optimize a loss over point sets sampled from these 2manifolds such as Chamfer and Earth-Mover distance. One question is, how do we represent the functions ϕ θ ? A good family of functions should (i) generate 2-manifolds and (ii) be able to produce a good approximation of the desired 2manifolds S loc . We show that multilayer perceptrons (MLPs) with rectified linear unit (ReLU) nonlinearities almost verify these properties, and thus are an adequate family of functions. Since it is difficult to design a family of functions that always generate a 2-manifold, we relax this constraint and consider functions that locally generate a 2-manifold.</p><p>Proposition 1. Let f be a multilayer perceptron with ReLU nonlinearities. There exists a finite set of polygons P i , i ∈ {1, ..., N } such that on each P i f is an affine function:</p><formula xml:id="formula_1">∀x ∈ P i , f (x) = A i x + b, where A i are 3 × 2 matrices.</formula><p>If for all i, rank(A i ) = 2, then for any point p in the interior of one of the P i s there exists a neighborhood N of p such that f (N ) is a 2-manifold.</p><p>Proof. The fact that f is locally affine is a direct consequence of the fact that we use ReLU non-linearities. If rank(A i ) = 2 the inverse of A i x + b is well defined on the surface and continuous, thus the image of the interior of each P i is a 2-manifold.</p><p>To draw analogy to texture atlases in computer graphics, we call the local functions we learn to approximate a 2-manifold learnable parameterizations and the set of these functions A a learnable atlas. Note that in general, an MLP locally defines a rank 2 affine transformation and thus locally generates a 2-manifold, but may not globally as it may intersect or overlap with itself. The second reason to choose MLPs as a family is that they can allow us to approximate any continuous surface.</p><p>Proposition 2. Let S be a 2-manifold that can be parameterized on the unit square. For any &gt; 0 there exists an integer K such that a multilayer perceptron with ReLU non linearities and K hidden units can approximate S with a precision .</p><p>Proof. This is a consequence of the universal representation theorem <ref type="bibr" target="#b16">[16]</ref> In the next section, we show how to train such MLPs to align with a desired surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">AtlasNet</head><p>In this section we introduce our model, AtlasNet, which decodes a 3D surface given an encoding of a 3D shape. This encoding can come from many different representations such as a point cloud or an image (see <ref type="figure">Figure 1</ref> for examples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Learning to decode a surface</head><p>Our goal is, given a feature representation x for a 3D shape, to generate the surface of the shape. As shown in Section 3, an MLP with ReLUs ϕ θ with parameters θ can locally generate a surface by learning to map points in R 2 to surface points in R 3 . To generate a given surface, we need several of these learnable charts to represent a surface. In practice, we consider N learnable parameterizations φ θi for i ∈ {1, ..., N }. To train the MLP parameters θ i , we need to address two questions: (i) how to define the distance between the generated and target surface, and (ii) how to account for the shape feature x in the MLP? To represent the target surface, we use the fact that, independent of the representation that is available to us, we can sample points on it. Let A be a set of points sampled in the unit square [0, 1] 2 and S a set of points sampled on the target surface. Next, we incorporate the shape feature x by simply concatenating them with the sampled point coordinates p ∈ A before passing them as input to the MLPs. Our model is illustrated in <ref type="figure">Figure 2b</ref>. Notice that the MLPs are not explicitly prevented from encoding the same area of space, but their union should cover the full shape. Our MLPs do depend on the random initialization, but similar to convolutional filter weights the network learns to specialize to different regions in the output without explicit biases. We then minimize the Chamfer loss between the set of generated 3D points and S ,</p><formula xml:id="formula_2">L(θ) = p∈A N i=1 min q∈S |φ θi (p; x) − q| 2 + q∈S min i∈{1, ...,N } min p∈A |φ θi (p; x) − q| 2 . (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>We consider two tasks: (i) to auto-encode a 3D shape given an input 3D point cloud, and (ii) to reconstruct a 3D shape given an input RGB image. For the auto-encoder, we used an encoder based on PointNet <ref type="bibr" target="#b25">[25]</ref>, which has proven to be state of the art on point cloud analysis on ShapeNet and ModelNet40 benchmarks. This encoder transforms an input point cloud into a latent vector of dimension k = 1024.</p><p>We experimented with input point clouds of 250 to 2500 points. For images, we used ResNet-18 <ref type="bibr" target="#b13">[14]</ref> as our encoder. The architecture of our decoder is 4 fully-connected layers of size 1024, 512, 256, 128 with ReLU non-linearities on the first three layers and tanh on the final output layer. We always train with output point clouds of size 2500 evenly sampled across all of the learned parameterizations -scaling above this size is time-consuming because our implementation of Chamfer loss has a compute cost that is quadratic in the number of input points. We experimented with different basic weight regularization options but did not notice any generalization improvement. Sampling of the learned parameterizations as well as the ground truth point-clouds is repeated at each training step to avoid over-fitting. To train for single-view reconstruction, we obtained the best results by training the encoder and using the decoder from the point cloud autoencoder with fixed parameters. Finally, we noticed that sampling points regularly on a grid on the learned parameterization yields better performance than sampling points randomly. All results used this regular sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Mesh generation</head><p>The main advantage of our approach is that during inference, we can easily generate a mesh of the shape.</p><p>Propagate the patch-grid edges to the 3D points. The simplest way to generate a mesh of the surface is to transfer a regular mesh on the unit square to 3D, connecting in 3D the images of the points that are connected in 2D. Note that our method allows us to generate such meshes at very high resolution, without facing memory issues, since the points can be processed in batches. We typically use 22500 points. As shown in the results section, such meshes are satisfying, but they can have several drawbacks: they will not be closed, may have small holes between the images of different learned parameterizations, and different patches may overlap.</p><p>Generate a highly dense point cloud and use Poisson surface reconstruction (PSR) <ref type="bibr" target="#b18">[18]</ref>. To avoid the previously mentioned drawbacks, we can additionally densely sample the surface and use a mesh reconstruction algorithm. We start by generating a surface at a high resolution, as explained above. We then shoot rays at the model from infinity and obtain approximately 100000 points, together with their oriented normals, and then can use a standard oriented cloud reconstruction algorithm such as PSR to produce a triangle mesh. We found that high quality normals as well as high density point clouds are critical to the success of PSR, which are naturally obtained using this method.</p><p>Sample points on a closed surface rather than patches.</p><p>To obtain a closed mesh directly from our method, without requiring the PSR step described above, we can sample the input points from the surface of a 3D sphere instead of a 2D square. The quality of this method depends on how well the underlying surface can be represented by a sphere, which we will explore in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section we show qualitative and quantitative results on the tasks of auto-encoding 3D shapes and single-view reconstruction and compare against several baselines. In addition to these tasks, we also demonstrate several additional applications of our approach. More results are available in the supplementary material <ref type="bibr" target="#b0">[1]</ref>.</p><p>Data. We evaluated our approach on the standard ShapeNet Core dataset (v2) <ref type="bibr" target="#b5">[6]</ref>. The dataset consists of 3D models covering 13 object categories with 1K-10K shapes per category. We used the training and validation split provided by <ref type="bibr" target="#b6">[7]</ref> for our experiments to be comparable with previous approaches. We used the rendered views provided by <ref type="bibr" target="#b6">[7]</ref> and sampled 3D points on the shapes using <ref type="bibr" target="#b33">[33]</ref>.</p><p>Evaluation criteria. We evaluated our generated shape outputs by comparing to ground truth shapes using two criteria. First, we compared point sets for the output and ground-truth shapes using Chamfer distance ("CD"). While this criteria compares two point sets, it does not take into account the surface/mesh connectivity. To account for mesh connectivity, we compared the output and ground-truth meshes using the "Metro" criteria using the publicly available METRO software <ref type="bibr" target="#b7">[8]</ref>, which is the average Euclidean distance between the two meshes.</p><p>Points baseline. In addition to existing baselines, we compare our approach to the multi-layer perceptron "Points baseline" network shown in <ref type="figure">Figure 2a</ref>. The Points baseline network consists of four fully connected layers with output dimensions of size 1024, 512, 256, 7500 with ReLU nonlinearities, batch normalization on the first three layers, and a hyperbolic-tangent non-linearity after the final fully connected layer. The network outputs 2500 3D points and has comparable number of parameters to our method with 25 learned parameterizations. The baseline architecture was designed to be as close as possible to the MLP used in At-lasNet. As the network outputs points and not a mesh, we also trained a second network that outputs 3D points and normals, which are then passed as inputs to Poisson surface reconstruction (PSR) <ref type="bibr" target="#b18">[18]</ref> to generate a mesh ("Points baseline + normals"). The network generates outputs in R 6 representing both the 3D spatial position and normal. We optimized Chamfer loss in this six-dimensional space and normalized the normals to 0.1 length as we found this trade-off between the spatial coordinates and normals in the loss worked best. As density is crucial to PSR quality, we augmented the number of points by sampling 20 points in a small radius in the tangent plane around each point <ref type="bibr" target="#b18">[18]</ref>. We noticed significant qualitative and quantitative improvements  <ref type="table">Table 1</ref>. 3D reconstruction. Comparison of our approach against a point-generation baseline ("CD" -Chamfer distance, multiplied by 10 3 , computed on 2500 points; "Metro" values are multiplied by 10). Note that our approach can be directly evaluated by Metro while the baseline requires performing PSR <ref type="bibr" target="#b18">[18]</ref>. These results can be compared with an Oracle sampling points directly from the ground truth 3D shape followed by PSR (top two rows). See text for details. and the results shown in this paper use this augmentation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Auto-encoding 3D shapes</head><p>In this section we evaluate our approach to generate a shape given an input 3D point cloud and compare against the Points baseline. We evaluate how well our approach can generate the shape, how it can generalize to object categories not seen during training, and its sensitivity to the number of patches. Evaluation on surface generation. We report quantitative results for shape generation from point clouds in <ref type="table">Table 1</ref>, where each approach is trained over all ShapeNet categories and results are averaged over all categories. Notice that our approach out-performs the Points baseline on both the Chamfer distance and Metro criteria, even when using a single learned parameterization (patch). Also, the Points baseline + normals has worse Chamfer distance than the Points baseline without normals indicating that predicting the normals decreases the quality of the point cloud generation.</p><p>We also report performance for two "oracle" outputs indicating upper bounds in <ref type="table">Table 1</ref>. The first oracle ("Oracle 2500 pts") randomly samples 2500 points+normals from the ground truth shape and applies PSR. The Chamfer distance between the random point set and the ground truth gives an upper bound on performance for point-cloud generation. Notice that our method out-performs the surface generated from the oracle points. The second oracle ("Oracle 125K pts") applies PSR on all 125K points+normals from the groundtruth shape. It is interesting to note that the Metro distance from this result to the ground truth is not far from the one obtained with our method.</p><p>We show qualitative comparisons in <ref type="figure" target="#fig_1">Figure 3</ref>. Notice that the PSR from the baseline point clouds <ref type="figure" target="#fig_1">(Figure 3b</ref>) look noisy and lower quality than the meshes produced directly by our method and PSR performed on points generated from our method as described in Section 4.3 <ref type="figure" target="#fig_1">(Figure 3c</ref>).</p><p>Sensitivity to number of patches. We show in <ref type="table">Table 1</ref> our approach with varying number of learnable parameterizations (patches) in the atlas. Notice how our approach improves as we increase the number of patches. Moreover, we also compare with the approach described in Section 4.3 which samples points on the 3D unit sphere instead of 2D patches to obtain a closed mesh. Notice that sampling from a sphere quantitatively out-performs a single patch, but multiple patches perform better.</p><p>We show qualitative results for varying number of learnable parameterizations in <ref type="figure" target="#fig_1">Figure 3</ref>. As suggested by the quantitative results, the visual quality improves with the number of parameterizations. However, more artifacts appear with more parameterizations, such as close-but-disconnected patches (e.g., sail of the sailboat) . We thus used 25 patches for the single-view reconstruction experiments (Section 5.2)</p><p>Generalization across object categories. An important desired property of a shape auto-encoder is that it generalizes well to categories it has not been trained on. To evaluate this, we trained our method on all categories but one target cate- gory ("LOO") for chair, car, watercraft, and plane categories, and evaluated on the held-out category. The corresponding results are reported in <ref type="table">Table 2</ref> and <ref type="figure">Figure 4</ref>. We also include performance when the methods are trained on all of the categories including the target category ("All") for comparison. Notice that we again out-perform the point-generating baseline on this leave-one-out experiment and that performance improves with more patches. The car category is especially interesting since when trained on all categories the baseline has better results than our method with 1 patch and similar to our method with 125 patches. If not trained on cars, both our approaches clearly outperform the baseline, showing that at least in this case, our approach generalizes better than the baseline. The visual comparison shown <ref type="figure">Figure 4</ref> gives an intuitive understanding of the consequences of not  training for a specific category. When not trained on chairs, our method seems to struggle to define clear thin structures, like legs or armrests, especially when they are associated to a change in the topological genus of the surface. This is expected as these types of structures are not often present in the categories the network was trained on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Single-view reconstruction</head><p>We evaluate the potential of our method for single-view reconstruction. We compare qualitatively our results with three state-of-the-art methods, PointSetGen <ref type="bibr" target="#b8">[9]</ref>, 3D-R2N2 <ref type="bibr" target="#b6">[7]</ref> and HSP <ref type="bibr" target="#b12">[13]</ref> in <ref type="figure" target="#fig_2">Figure 5</ref>. To perform the comparison for PointSetGen <ref type="bibr" target="#b8">[9]</ref> and 3D-R2N2 <ref type="bibr" target="#b6">[7]</ref>, we used the trained models made available online by the authors. For HSP <ref type="bibr" target="#b12">[13]</ref>, we asked the authors to run their method on the images in <ref type="figure" target="#fig_2">Fig. 5</ref>. Note that since their model was trained on images generated with a different renderer, this comparison is not absolutely fair. To remove the bias we also compared our results with HSP on real images for which none of the methods was trained ( <ref type="figure" target="#fig_3">Fig. 6</ref>) which also demonstrates the ability of our network to generalize to real images. <ref type="figure" target="#fig_2">Figure 5</ref> emphasizes the importance of the type of output (voxels for 3D-N2D2 and HSP, point cloud for PointSetGen, mesh for us) for the visual appearance of the results. Notice the small details visible on our meshes that may be hard to see on the unstructured point cloud or volumetric representation. Also, it is interesting to see that PointSetGen tends to generate points inside the volume of the 3D shape while our result, by construction, generates points on a surface.</p><p>To perform a quantitative comparison against PointSet-Gen <ref type="bibr" target="#b8">[9]</ref>, we evaluated the Chamfer distance between generated points and points from the original mesh for both PointSetGen and our method with 25 learned parameterizations. However, the PointSetGen network was trained with a translated, rotated, and scaled version of ShapeNet with parameters we did not have access to. We thus first had to align the point clouds resulting from PointSetGen to the ShapeNet models used by our algorithm. We randomly selected 260 shapes, 20 from each category, and ran the iterative closest point (ICP) algorithm <ref type="bibr" target="#b2">[3]</ref> to optimize a similarity transform between PointSetGen and the target point cloud. Note that this optimization improves the Chamfer distance between the resulting point clouds, but is not globally convergent. We checked visually that the point clouds from PointSetGen were correctly aligned, and display all alignments on the project webpage 2 . To have a fair comparison we ran the same ICP alignment on our results. In <ref type="table">Table 3</ref> we compared the resulting Chamfer distance. Our method provides the best results on 6 categories whereas PointSetGen and the baseline are best on 4 and 3 categories, respectively. Our method is better on average and generates point clouds of a quality similar to the state of the art. We also report the Metro distance to the original shape, which is the most meaningful measure for our method.</p><p>To quantitatively compare against HSP <ref type="bibr" target="#b12">[13]</ref>, we retrained our method on their publicly available data since train/test splits are different from 3D-R2N2 <ref type="bibr" target="#b6">[7]</ref> and they made their own renderings of ShapeNet data. Results are in <ref type="table">Table 4</ref>. More details are in the supplementary <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Additional applications</head><p>Shape interpolation. <ref type="figure" target="#fig_5">Figure 7a</ref> shows shape interpolation. Each row shows interpolated shapes generated by our Atlas-Net, starting from the shape in the first column to the shape in the last. Each intermediate shape is generated using a weighted sum of the latent representations of the two extreme shaped. Notice how the interpolated shapes gradually add armrests in the first row, and chair legs in the last.  <ref type="table">Table 3</ref>. Single-View Reconstruction (per category). The mean is taken category-wise. The Chamfer Distance reported is computed on 1024 points, after running ICP alignment with the GT point cloud, and multiplied by 10 3 . The Metro distance is multiplied by 10.</p><p>(a) Shape interpolation.  Chamfer Metro HSP <ref type="bibr" target="#b12">[13]</ref> 11.6 1.49 Ours (25 patches) 9.52 1.09 <ref type="table">Table 4</ref>. Single-view reconstruction. Quantitative comparison against HSP <ref type="bibr" target="#b12">[13]</ref>, a state of the art octree-based method. The average error is reported, on 100 shapes from each category. The Chamfer Distance reported is computed on 10 4 points, and multiplied by 10 3 . The Metro distance is multiplied by 10. More details are in the supplemetary <ref type="bibr" target="#b0">[1]</ref>.</p><p>Finding shape correspondences. <ref type="figure" target="#fig_5">Figure 7b</ref> shows shape correspondences. We colored the surface of reference chair (left) according to its 3D position. We transfer the surface colors from the reference shape to the inferred atlas (middle). Finally, we transfer the atlas colors to other shapes (right) such that points with the same color are parametrized by the same point in the atlas. Notice that we get semantically meaningful correspondences, such as the chair back, seat, and legs without any supervision from the dataset on semantic information.</p><p>Mesh parameterization Most existing rendering pipelines require an atlas for texturing a shape <ref type="figure" target="#fig_5">(Figure 7c</ref>). A good parameterization should minimize amount of area distortion (E a ) and stretch (E s ) of a UV map. We computed average per-triangle distortions for 20 random shapes from each category and found that our inferred atlas usually has relatively high texture distortion (E a = 1.9004, E s = 6.1613, where undistorted map has E a = E s = 1). Our result, however, is well-suited for distortion minimization because all meshes have disk-like topology and inferred map is bijective, making it easy to further minimize distortion with off-theshelf geometric optimization <ref type="bibr" target="#b19">[19]</ref>, yielding small distortion (E a = 1.0016, E s = 1.025, see bottom row for example).</p><p>Limitations and future work are detailed in the supplementary materials [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have introduced an approach to generate parametric surface elements for 3D shapes. We have shown its benefits for 3D shape and single-view reconstruction, out-performing existing baselines. In addition, we have shown its promises for shape interpolation, finding shape correspondences, and mesh parameterization. Our approach opens up applications in generation and synthesis of meshes for 3D shapes, similar to still image generation <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b35">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Overview</head><p>This document provides more detailed quantitative and qualitative results highlighting the strengths and limitations of AtlasNet.</p><p>Detailed results, per category, for the autoencoder These tables report the metro reconstruction error and the chamfer distance error. It surprisingly shows that our method with 25 learned parameterizations outperforms our method with 125 learned parameterizations in 7 categories out of 13 for the metro distance, but is significantly worse on the cellphone category, resulting in the 125 learned parameterizations approach being better on average. This is not mirrored in the Chamfer distance.</p><p>Regularisation In the autoencoder experiment, we tried using weight decay with different weight. The best results were obtained without any regularization.</p><p>Limitations We describe two limitations with our approach. First, when a small number of learned parameterizations are used, the network has to distort them too much to recreate the object. This leads, when we try to recreate a mesh, to small triangles in the learned parameterization space being distorted and become large triangles in 3D covering undesired regions. On the other hand, as the number of learned parameterization increases, errors in the topology of the reconstructed mesh can be sometimes observed. In practice, it means that the reconstructed patches overlap, or are not stiched together.</p><p>Additional Single View Reconstruction qualitative results In this figure, we show one example of single-view reconstruction per category and compare with the state of the art, PointSetGen and 3D-R2N2. We consistently show that our method produces a better reconstruction.</p><p>Additional Autoencoder qualitative results In this figure, we show one example per category of autoencoder reconstruction for the baseline and our various approaches to reconstruct meshes, detailed in the main paper. We show how we are able to recreate fine surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Shape Correspondences qualitative results</head><p>We color each vertex of the reference object by its distance to the gravity center of the object, and transfer these colors to the inferred atlas. We then propagate them to other objects of the same category, showing semantically meaningful correspondences between them. Results for the plane and watercraft categories are shown and generalize to all categories.</p><p>Deformable shapes. We ran an experiment on human shape to show that our method is also suitable for reconstructing deformable shapes. The FAUST dataset <ref type="bibr" target="#b3">[4]</ref> is a collection of meshes representing several humans in different poses. We used 250 shapes for training, and 50 for validation (without using the ground truth correspondences in any way). In table 5, we report the reconstruction error in term of Chamfer distance and Metro distance for our method with 25 squarred parameterizations, our methods with a sphere parametrization, and for the baseline. We found results to be consistent with the analysis on ShapeNet. Qualitative results are shown in <ref type="figure" target="#fig_8">figure 14</ref>, revealing that our method leads to qualitatively good reconstructions.  <ref type="table">Table 5</ref>. 3D Reconstruction on FAUST <ref type="bibr" target="#b3">[4]</ref>. We trained the baseline and our method sampling the points according from 25 square patches, and from a sphere on the human shapes from the FAUST dataset. We report Chamfer distance (x 10 4 ) on the points and Metro distance (x10) on the meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chamfer</head><p>Point cloud super-resolution AtlasNet can generate pointclouds or meshes of arbitrary resolution simply by sampling more points. <ref type="figure">Figure 8</ref> shows qualitative results of our approach with 25 patches generating high resolution meshes with 122500 points. Moreover, PointNet is able to take an arbitrary number of points as input and encodes a minimal shape based on a subset of the input points. This is a double-edged sword : while it allows the autoencoder to work with varying number of input points, it also prevent it from reconstructing very fine details, as they are not used by PointNet and thus not present in the latent code. We show good results using only 250 input points, despite the fact that we train using 2500 input points which shows the capacity of our decoder to interpolate a surface from a small number of input points, and the flexibility of our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details on the comparison against HSP [13]</head><p>We perform a quantitative comparison against an octree-based state of the art method. AtlasNet is trained with 25 learned parameterizations on the same data as their publicly available trained model <ref type="bibr" target="#b2">3</ref> . 100 random samples are drawn from each category from the test split. We evaluated the the quality of the reconstruction using the Chamfer distance on the unnormalized meshes, and the metro distance. Voxelised versions points 250 points (a) Low-Res Input (b) High-Res reconstruction <ref type="figure">Figure 8</ref>. Super resolution. Our approach can generate meshes at arbitrary resolutions, and the pointnet encoder <ref type="bibr" target="#b25">[25]</ref> can take pointclouds of varying resolution as input. Given the same shape sampled at the training resolution of 2500, or 10 times less points, we generate high resolution meshes with 122500 vertices. This can be viewed as the 3D equivalent of super-resolution on 2D pixels. of meshes often appear inflated. This bias can appear for HSP, where we observed that the generated meshes were slightly larger than the original meshes. We ran an ICP alignment procedure on the generated meshes for both methods to remove this bias. In table 8, we report per category results. As AtlasNet was specifically trained to optimise the chamfer distance, we outperform HSP in every category. AtlasNet also outperforms HSP in metro distance in each category for the metro distance, for which none of the two algorithm where trained to optimise. List of sampled used, ans trained model for AtlasNet are available in the github repository.</p><p>Limitations and future work Our results have limitations that lead to many open question and perspective for future work. First, the patches for our generated shapes are not guaranteed to be connected (except if the surface the input points are sampled from is already closed, as in the sphere experiment). An open question is how to effectively stitch the patches together to form a closed shape. Second, we have demonstrated results on synthetic object shapes. Ideally, we would like to extend to entire real scenes. Third, we have optimized the parameterization of the generated meshes post-hoc. It would be good to directly learn to generate the surfaces with low distortion parameterizations. Fourth, this work generates surfaces by minimizing an energy computed from point clouds. An open question is how to define a loss on meshes that is easy to optimize? Finally, as the atlases provide promising correspondences across different shapes, an interesting future direction is to leverage them for shape recognition and segmentation.  <ref type="table">Table 8</ref>. Single-view reconstruction. Quantitative comparison against HSP <ref type="bibr" target="#b12">[13]</ref>, a state of the art octree-based method. The average error is reported, on 100 shapes from each category. The Chamfer Distance reported is computed on 10 4 points, and multiplied by 10 3 . The Metro distance is multiplied by 10.</p><p>Weight Decay Ours : 25 patches 10 −3 8.57 10 −4 4.84 10 −5 3.42 0 1.56 <ref type="table">Table 9</ref>. Regularization on Auto-Encoder (per category). The mean is taken category-wise. The Chamfer Distance is reported, multiplied by 10 3 . pt <ref type="figure">Figure 11</ref>. Shape correspondences: a reference watercraft (left) is colored by distance to the center, with the jet colormap. We transfer the surface colors to the inferred atlas for the reference shape (middle). Finally, we transfer the atlas colors to other shapes (right). Notice that we get semantically meaningful correspondences, without any supervision from the dataset on semantic information. All objects are generated by the autoencoder, with 25 learned parametrizations. <ref type="figure">Figure 12</ref>. Shape correspondences: a reference plane (left) is colored by distance to the center, with the jet colormap. We transfer the surface colors to the inferred atlas for the reference shape (middle). Finally, we transfer the atlas colors to other shapes (right). Notice that we get semantically meaningful correspondences, such as the nose and tail of the plane, and the tip of the wings, without any supervision from the dataset on semantic information. All objects are generated by the autoencoder, with 25 learned parametrizations.</p><p>(a) Excess of distortion. Notice how, compared to the original point cloud (left), the generated pointcloud (middle) with 1 learned parameterization is valid, but the mapping from squares to surfaces enforces too much distortion leading to error when propagating the grid edges in 3D (right).</p><p>(b) Topological issues. Notice how, compared to the original point cloud (left), the generated pointcloud (middle) with 125 learned parameterizations is valid, but the 125 generated surfaces overlap and are not stiched together (right).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>(a) Ground truth (b) Pts baseline (c) PSR on ours (d) Ours sphere (Auto-encoder. We compare the original meshes (a) to meshes obtained by running PSR on the point clouds generated by the baseline (b) and on the densely sampled point cloud from our generated mesh (c), and to our method generating a surface from a sphere (d), 1 (e), 5 (f), 25 (g), and 125(h) learnable parameterizations. Notice the fine details in (g) and (h) : e.g. the plane's engine and the jib of the ship.(a) Not trained on chairs (b) Trained on all categoriesFigure 4. Generalization. (a) Our method (25 patches) can generate surfaces close to a category never seen during training. It, however, has more artifacts than if it has seen the category during training (b), e.g., thin legs and armrests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Single-view reconstruction comparison. From a 2D RGB image (a), 3D-R2N2 [7] reconstructs a voxel-based 3D model (b), HSP [13] reconstructs a octree-based 3D model (c), PointSet-Gen [9] a point cloud based 3D model (d), and our AtlasNet a triangular mesh (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Single-view reconstruction comparison on natural images. From a 2D RGB image taken from internet (a), HSP<ref type="bibr" target="#b12">[13]</ref> reconstructs a octree-based 3D model (b), and our AtlasNet a triangular mesh (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Shape correspondences. (c) Mesh parameterization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Applications. Results from three applications of our method. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Single-view reconstruction comparison: From a 2D RGB image (a), 3D-R2N2 reconstructs a voxel-based 3D model (b), PointSetGen a point cloud based 3D model (c), and our AtlasNet a triangular mesh (d). pt (a) Ground truth (b) PSR on ours (c) Ours sphere (d) Ours 25 Autoencoder comparison: We compare the original meshes (a) to meshes obtained by running PSR (b) on the dense point cloud sampled from our generated mesh, and to our method generating a surface from a sphere (c), and 25 (d) learnable parameterizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 .</head><label>13</label><figDesc>Limitations. Two main artifacts are highlighted : (a) Excess of distortion when too small a number of learned parameterizations is used, and (b) growing errors in the topology of the reconstructed mesh as the number of learned parameterization increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 .</head><label>14</label><figDesc>Deformable shapes. Our method learned on 250 shapes from the FAUST dataset to reconstructs a human in different poses. Each color represent one of the 25 parametrizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.39 6.01 4.45 7.24 5.95 7.42 10.4 1.83 6.65 4.83 4.66 4.65 5.50 PSG CD 3.36 4.31 8.51 8.63 6.35 6.47 7.66 15.9 1.58 6.92 3.93 3.76 5.94 6.41 Ours CD 2.54 3.91 5.39 4.18 6.77 6.71 7.24 8.18 1.63 6.76 4.35 3.91 4.91 5.11 Ours Metro 1.31 1.89 1.80 2.04 2.11 1.68 2.81 2.39 1.57 1.78 2.28 1.03 1.84 1.89</figDesc><table><row><cell></cell><cell>pla.</cell><cell>ben. cab.</cell><cell>car</cell><cell>cha. mon. lam. spe.</cell><cell>fir.</cell><cell>cou.</cell><cell>tab.</cell><cell>cel.</cell><cell>wat. mean</cell></row><row><cell>Ba CD</cell><cell cols="2">2.91 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>.12 1.98 2.24 2.68 1.78 2.58 2.29 1.03 1.90 2.66 1.15 2.46 2.12 Baseline PSR PA 1.38 1.97 1.75 2.04 2.08 1.53 2.51 2.25 1.46 1.57 2.06 1.15 1.80 Auto-Encoder (per category). The mean is taken category-wise. The Metro Distance is reported, multiplied by 10. The meshes were contructed by propagating the patch grid edges. .21 1.64 1.76 1.60 1.66 2.51 2.55 0.68 1.64 1.52 1.25 1.46 1.57 Ours 25 patch 0.87 1.25 1.78 1.58 1.56 1.72 2.30 2.61 0.68 1.83 1.52 1.27 1.33 1.56 Ours 125 patch 0.86 1.15 1.76 1.56 1.55 1.69 2.26 2.55 0.59 1.69 1.47 1.31 1.23 1.51 Table 7. Auto-Encoder (per category). The mean is taken category-wise. The Chamfer Distance is reported, multiplied by 10 3 . .84 1.28 1.06 1.61 1.66 1.93 1.77 1.05 1.37 1.93 1.39 1.34 1.49 Ours 25 patch 0.77 1.01 1.04 0.92 1.19 1.22 1.26 1.46 0.95 1.19 1.27 0.83 1.09 1.09 chamfer HSP 2.60 17.4 14.3 1.77 10.0 19.4 9.46 21.7 2.34 12.9 20.2 13.2 4.89 11.6 Ours 25 patch 1.33 14.1 12.5 1.29 7.23 17.5 6.99 17.8 1.69 11.2 17.0 10.6 4.20 9.52</figDesc><table><row><cell></cell><cell></cell><cell>pla.</cell><cell cols="2">ben. cab.</cell><cell>car</cell><cell cols="2">cha. mon. lam. spe.</cell><cell>fir.</cell><cell>cou.</cell><cell>tab.</cell><cell>cel.</cell><cell>wat. mean</cell></row><row><cell cols="2">Baseline PSR</cell><cell cols="9">2.71 21.82</cell></row><row><cell cols="2">Ours 1 patch</cell><cell cols="9">1.11 1.41 1.70 1.93 1.76 1.35 2.01 2.30 1.01 1.46 1.46 0.87 1.46</cell><cell>1.53</cell></row><row><cell cols="2">Ours 1 sphere</cell><cell cols="9">1.03 1.33 1.64 1.99 1.76 1.30 2.06 2.33 0.93 1.41 1.59 0.79 1.54</cell><cell>1.52</cell></row><row><cell cols="2">Ours 5 patch</cell><cell cols="9">0.99 1.36 1.65 1.90 1.79 1.28 2.00 2.27 0.92 1.37 1.57 0.76 1.40</cell><cell>1.48</cell></row><row><cell cols="2">Ours 25 patch</cell><cell cols="9">0.96 1.35 1.63 1.96 1.49 1.22 1.86 2.22 0.93 1.36 1.31 1.41 1.35</cell><cell>1.47</cell></row><row><cell cols="2">Ours 125 patch</cell><cell cols="9">1.01 1.30 1.58 1.90 1.36 1.29 1.95 2.29 0.85 1.38 1.34 0.76 1.37</cell><cell>1.41</cell></row><row><cell></cell><cell></cell><cell>pla.</cell><cell cols="2">ben. cab.</cell><cell>car</cell><cell></cell><cell>cha. mon. lam. spe.</cell><cell>fir.</cell><cell cols="2">cou.</cell><cell>tab.</cell><cell>cel.</cell><cell>wat. mean</cell></row><row><cell cols="2">Baseline</cell><cell cols="9">1.11 1.46 1.91 1.59 1.90 2.20 3.59 3.07 0.94 1.83 1.83 1.71 1.69</cell><cell>1.91</cell></row><row><cell cols="11">Baseline + normal 1.25 1.73 2.19 1.74 2.19 2.52 3.89 3.51 0.98 2.13 2.17 1.87 1.88</cell><cell>2.15</cell></row><row><cell cols="2">Ours 1 patch</cell><cell cols="9">1.04 1.43 1.79 2.28 1.97 1.83 3.06 2.95 0.76 1.90 1.95 1.29 1.69</cell><cell>1.84</cell></row><row><cell cols="2">Ours 1 sphere</cell><cell cols="9">0.98 1.31 2.02 1.75 1.81 1.83 2.59 2.94 0.69 1.73 1.88 1.30 1.51</cell><cell>1.72</cell></row><row><cell cols="2">Ours 5 patch</cell><cell cols="2">0.96 1pla.</cell><cell cols="2">ben. cab.</cell><cell>car</cell><cell cols="2">cha. mon. lam. spe.</cell><cell>fir.</cell><cell>cou.</cell><cell>tab.</cell><cell>cel.</cell><cell>wat. mean</cell></row><row><cell>metro</cell><cell>HSP</cell><cell cols="3">1.10 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ThibaultGROUEIX/AtlasNet.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://imagine.enpc.fr/˜groueixt/atlasnet/PSG. html.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/chaene/hsp.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was partly supported by ANR project EnHerit ANR-17-CE23-0008, Labex Bézout, and gifts from Adobe toÉcole des Ponts. We thank Aaron Herzmann for fruitful discussions, Christian Häne for his help in comparing to <ref type="bibr" target="#b12">[13]</ref> and Kevin Wampler for helping with geometric optimization for surface parameterization.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Supplementary material (appendix) for the paper</title>
		<ptr target="https://http://imagine.enpc.fr/groueixt/atlasnet/arxiv" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Marr revisited: 2d-3d alignment via surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">FAUST: Dataset and evaluation for 3D mesh registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<title level="m">Learning shape correspondence with anisotropic convolutional neural networks. NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D-R2N2: A unified approach for single and multi-view 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Metro: Measuring error on simplified surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cignoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rocchini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Scopigno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<title level="m">Geometry images. SIG-GRAPH</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Highresolution shape completion using deep neural networks for global structure and local geometry inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical surface prediction for 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mesh parameterization: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hormann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Polthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH ASIA</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siggraph</forename><surname>Courses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACM</publisher>
			<biblScope unit="volume">08</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Screened poisson surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accelerated quadratic proxy for geometric optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Kovalsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (proceedings of ACM SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GRASS: Generative recursive autoencoders for shape structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH 2017)</title>
		<meeting>of SIGGRAPH 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint embeddings of shapes and images via CNN image purification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aigerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<title level="m">Convolutional neural networks on surfaces via seamless toric covers. SIGGRAPH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<title level="m">Geodesic convolutional neural networks on riemannian manifolds. 3dRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep exemplar 2D-3D detection by adapting from real to rendered views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">OctNet-Fusion: Learning depth fusion from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<title level="m">Multi-chart geometry images. SGP</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning 3d shape surfaces using geometry images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Surfnet: Generating 3d shape surfaces using deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Unmesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3D outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00404</idno>
		<title level="m">Learning shape abstractions by assembling volumetric primitives</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
