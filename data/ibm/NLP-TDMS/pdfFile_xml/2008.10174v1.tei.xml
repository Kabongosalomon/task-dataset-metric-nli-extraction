<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Zakharov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Ivakhnenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandra</forename><surname>Shysheya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural avatars</term>
					<term>talking heads</term>
					<term>neural rendering</term>
					<term>head syn- thesis</term>
					<term>head animation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a neural rendering-based system that creates head avatars from a single photograph. Our approach models a person's appearance by decomposing it into two layers. The first layer is a posedependent coarse image that is synthesized by a small neural network. The second layer is defined by a pose-independent texture image that contains high-frequency details. The texture image is generated offline, warped and added to the coarse image to ensure a high effective resolution of synthesized head views. We compare our system to analogous state-of-the-art systems in terms of visual quality and speed. The experiments show significant inference speedup over previous neural head avatar models for a given visual quality. We also report on a real-time smartphone-based implementation of our system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Personalized head avatars driven by keypoints or other mimics/pose representation is a technology with manifold applications in telepresence, gaming, AR/VR applications, and special effects industry. Modeling human head appearance is a daunting task, due to complex geometric and photometric properties of human heads including hair, mouth cavity and surrounding clothing. For at least two decades, creating head avatars (talking head models) was done with computer graphics tools using mesh-based surface models and texture maps. The resulting systems fall into two groups. Some <ref type="bibr" target="#b2">[5]</ref> are able to model specific people with very high realism after significant acquisition and design efforts are spent on those particular people. Others <ref type="bibr" target="#b16">[19]</ref> are able to create talking head models from as little as a single photograph, but do not aim to achieve photorealism.</p><p>In recent years, neural talking heads have emerged as an alternative to classic computer graphics pipeline striving to achieve both high realism and ease of acquisition. The first works required a video <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b36">39]</ref> or even multiple videos <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b32">35]</ref> to create a neural network that can synthesize talking head view of a person. Most recently, several works <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b38">41]</ref>  Existing few-shot neural head avatar systems achieve remarkable results. Yet, unlike some of the graphics-based avatars, the neural systems are too slow to be deployed on mobile devices and require a high-end desktop GPU to run in real-time. We note that most application scenarios of neural avatars, especially those related to telepresence, would benefit highly from the capability to run in real-time on a mobile device. While in theory neural architectures within stateof-the-art approaches can be scaled down in order to run faster, we show that such scaling down results in a very unfavourable speed-realism tradeoff.</p><p>In this work, we address the speed limitataions of one-shot neural head avatar systems, and develop an approach that can run much faster than previous models. To achieve this, we adopt a bi-layer representation, where the image of an avatar in a new pose is generated by summing two components: a coarse image directly predicted by a rendering network, and a warped texture image. While the warping itself is also predicted by the rendering network, the texture is estimated at the time of avatar creation and is static at runtime. To enable the few-shot capability, we use the meta-learning stage on a dataset of videos, where we (meta)-train the inference (rendering) network, the embedding network, as well as the texture generation network.</p><p>The separation of the target frames into two layers allows us both to improve the effective resolution and the speed of neural rendering. This is because we can use off-line avatar generation stage to synthesize high-resolution texture, while at test time both the first component (coarse image) and the warping of the texture need not contain high frequency details and can therefore be predicted by a relatively small rendering network. These advantages of our system are validated by extensive comparisons with previously proposed neural avatar systems. We also report on the smartphone-based real-time implementation of our system, which was beyond the reach of previously proposed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>As discussed above, methods for the neural synthesis of realistic talking head sequences can be divided into many-shot (i.e. requiring a video or multiple videos of the target person for learning the model) <ref type="bibr" target="#b18">[21,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b36">39]</ref> and a more recent group of few-shot/singe-shot methods capable of acquiring the model of a person from a single or a handful photographs <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b37">40,</ref><ref type="bibr" target="#b38">41]</ref>. Our method falls into the latter category as we focus on the one-shot scenario (modeling from a single photograph).</p><p>Along another dimension, these methods can be divided according to the architecture of the generator network. Thus, several methods <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b36">39,</ref><ref type="bibr" target="#b38">41]</ref> use generators based on direct synthesis, where the image is generated using a sequence of convolutional operators, interleaved with elementwise non-linearities, and normalizations. Person identity information may be injected into such architecture, either with a lengthy learning process (in the many-shot scenario) <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b36">39]</ref> or by using adaptive normalizations conditioned on person embeddings <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b38">41]</ref>. The method <ref type="bibr" target="#b38">[41]</ref> effectively combines both approaches by injecting identity through adaptive normalizations, and then fine-tuning the resulting generator on the fewshot learning set. The direct synthesis approach for human heads can be traced back to <ref type="bibr" target="#b32">[35]</ref> that generated lips of a famous person in the talking head sequence, and further towards first works on conditional convolutional neural synthesis of generic objects such as <ref type="bibr" target="#b8">[11]</ref>.</p><p>The alternative to the direct image synthesis is to use differentiable warping <ref type="bibr" target="#b19">[22]</ref> inside the architecture. The X2Face approach <ref type="bibr" target="#b37">[40]</ref> applies warping twice, first from the source image to a standardized image (texture), and then to the target image. The Codec Avatar system <ref type="bibr" target="#b25">[28]</ref> synthesizes a pose-dependent texture for a simplified mesh geometry. The MarioNETte system <ref type="bibr" target="#b14">[17]</ref> applies warping to the intermediate feature representations. The Few-shot Vid-to-Vid system <ref type="bibr" target="#b34">[37]</ref> combines direct synthesis with the warping of the previous frame in order to obtain temporal continuity. The First Order Motion Model <ref type="bibr" target="#b30">[33]</ref> learns to warp the intermediate feature representation of the generator based on keypoints that are learned from data. Beyond heads, differentiable warping/texturing have recently been used for full body re-rendering <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b29">32]</ref>. Earlier, DeepWarp system <ref type="bibr" target="#b11">[14]</ref> used neural warping to alter the appearance of eyes for the purpose of gaze redirection, and <ref type="bibr" target="#b40">[43]</ref> also used neural warping for the resynthesis of generic scenes. Our method combines direct image synthesis with warping in a new way, as we obtain the fine layer by warping an RGB pose-independent texture, while the coarse-grained pose-dependent RGB component is synthesized by a neural network directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We use video sequences annotated with keypoints and, optionally, segmentation masks, for training. We denote t-th frame of the i-th video sequence as x i (t), corresponding keypoints as y i (t), and segmentation masks as m i (t) We will use</p><formula xml:id="formula_0">Result … …</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embeddings</head><p>New pose Image composition <ref type="figure">Fig. 2</ref>: During training, we first encode a source frame into the embeddings, then we initialize adaptive parameters of both inference and texture generators, and predict a high-frequency texture. These operations are only done once per avatar. Target keypoints are then used to predict a low-frequency component of the output image and a warping field, which, applied to the texture, provides the high-frequency component. Two components are then added together to produce an output.</p><p>an index t to denote a target frame, and s -a source frame. Also, we mark all tensors, related to generated images, with a hat symbol, ex.x i (t). We assume the spatial size of all frames to be constant and denote it as H × W . In some modules, input keypoints are encoded as an RGB image, which is a standard approach in a large body of previous works <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b38">41]</ref>. In this work, we will call it a landmark image. But, contrary to these approaches, at test-time we input the keypoints into the inference generator directly as a vector. This allows us to significantly reduce the inference time of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>In our approach, the following networks are trained in an end-to-end fashion:</p><p>-The embedder network E x i (s), y i (s) encodes a concatenation of a source image and a landmark image into a stack of embeddings {ê i k (s)}, which are used for initialization of the adaptive parameters inside the generators.</p><p>-The texture generator network G tex {ê i k (s)} initializes its adaptive parameters from the embeddings and decodes an inpainted high-frequency component of the source image, which we call a textureX i (s).</p><p>-The inference generator network G y i (t), {ê i k (s)} maps target poses into a predicted imagex i (t). The network accepts vector keypoints as an input and outputs a low-frequency layer of the output imagex i LF (t), which encodes basic facial features, skin color and lighting, andω i (t) -a mapping between coordinate spaces of the texture and the output image. Then, the highfrequency layer of the output image is obtained by warping the predicted texture:x i HF (t) =ω i (t) •X i (s), and is added to a low-frequency component to produce the final image:</p><formula xml:id="formula_1">x i (t) =x i LF (t) +x i HF (t) .<label>(1)</label></formula><p>-Finally, the discriminator network D x i (t), y i (t) , which is a conditional <ref type="bibr" target="#b26">[29]</ref> relativistic <ref type="bibr" target="#b21">[24]</ref> PatchGAN <ref type="bibr" target="#b18">[21]</ref>, maps a real or a synthesised target image, concatenated with the target landmark image, into realism scores s i (t).</p><p>During training, we first input a source image x i (s) and a source pose y i (s), encoded as a landmark image, into the embedder. The outputs of the embedder are K tensorsê i k (s), which are used to predict the adaptive parameters of the texture generator and the inference generator. A high-frequency textureX i (s) of the source image is then synthesized by the texture generator. Next, we input corresponding target keypoints y i (t) into the inference generator, which predicts a low-frequency component of the output imagex i LF (t) directly and a highfrequency componentx i HF (t) by warping the texture with a predicted fieldω i (t). Finally, the output imagex i (t) is obtained as a sum of these two components.</p><p>It is important to note that while the texture generator is manually forced to generate only a high-frequency component of the image via the design of the loss functions, which is described in the next section, we do not specifically constrain it to perform texture inpainting for occluded head parts. This behavior is emergent from the fact that we use two different images with different poses for initialization and loss calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training process</head><p>We use multiple loss functions for training. The main loss function responsible for the realism of the outputs is trained in an adversarial way <ref type="bibr" target="#b13">[16]</ref>. We also use pixelwise loss to preserve source lightning conditions and perceptual <ref type="bibr" target="#b20">[23]</ref> loss to match the source identity in the outputs. Finally, a regularization of the texture mapping adds robustness to the random initialization of the model.</p><p>Pixelwise and perceptual losses ensure that the predicted images match the ground truth, and are respectively applied to low-and high-frequency components of the output images. Since usage of pixelwise losses assumes independence of all pixels in the image, the optimization process leads to blurry images <ref type="bibr" target="#b18">[21]</ref>, which is suitable for the low-frequency component of the output. Thus the pixelwise loss is calculated by simply measuring mean L 1 distance between the target image and the low-frequency component:</p><formula xml:id="formula_2">L G pix = 1 HW ||x i LF (t) − x i (t)|| 1 .<label>(2)</label></formula><p>On the contrary, the optimization of the perceptual loss leads to crisper and more realistic images <ref type="bibr" target="#b20">[23]</ref>, which we utilize to train the high-frequency component. To calculate the perceptual loss, we use the stop-gradient operator SG, which allows us to prevent the gradient flow into a low-frequency component. The input generated image is, therefore, calculated as following:</p><formula xml:id="formula_3">x i (t) = SG x i LF (t) +x i HF (t) .</formula><p>(3)</p><p>Following <ref type="bibr" target="#b14">[17]</ref> and <ref type="bibr" target="#b38">[41]</ref>, our variant of the perceptual loss consists of two components: features evaluated using an ILSVRC (ImageNet) pre-trained VGG19 network <ref type="bibr" target="#b31">[34]</ref>, and the VGGFace network <ref type="bibr" target="#b28">[31]</ref>, trained for face recognition. If we denote the intermediate features of these networks as f i k,IN (t) and f i k,face (t), and their spatial size as H k × W k , the objectives can be written as follows:</p><formula xml:id="formula_4">L G IN = 1 K k 1 H k W k ||f i k,IN (t) − f i k,IN (t)|| 1 ,<label>(4)</label></formula><formula xml:id="formula_5">L G face = 1 K k 1 H k W k ||f i k,face (t) − f i k,face (t)|| 1 .<label>(5)</label></formula><p>Texture mapping regularization is proposed to improve the stability of the training. In our model, the coordinate space of the texture is learned implicitly, and there are two degrees of freedom that can mutually compensate each other: the position of the face in the texture, and the predicted warping. If, after initial iterations, the major part of the texture is left unused by the model, it can easily compensate that with a more distorted warping field. This artifact of an initialization is not fixed during training, and clearly is not the behavior we need, since we want all the texture to be used to achieve the maximum effective resolution in the outputs. We address the problem by regularizing the warping in the first iterations to be close to an identity mapping:</p><formula xml:id="formula_6">L G reg = 1 HW ||ω i (t) − I|| 1 .<label>(6)</label></formula><p>Adversarial loss is optimized by both generators, the embedder and the discriminator networks. Usually, it resembles a binary classification loss function between real and fake images, which discriminator is optimized to minimize, and generators -maximize <ref type="bibr" target="#b13">[16]</ref>. We follow a large body of previous works <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b38">41]</ref> and use a hinge loss as a substitute for the original binary cross entropy loss. We also perform relativistic realism score calculation <ref type="bibr" target="#b21">[24]</ref>, following its recent success in tasks such as super-resolution <ref type="bibr" target="#b36">[39]</ref> and denoising <ref type="bibr" target="#b22">[25]</ref>. Additionally, we use PatchGAN <ref type="bibr" target="#b18">[21]</ref> formulation of the adversarial learning. The discriminator is trained only with respect to its adversarial loss L D adv , while the generators and the embedder are trained via the adversarial loss L G adv , and also a feature matching loss L FM <ref type="bibr" target="#b35">[38]</ref>. The latter is introduced for better stability of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Texture enhancement</head><p>To minimize the identity gap, <ref type="bibr" target="#b38">[41]</ref> suggested to fine-tune the generator weights to the few-shot training set. Training on a person-specific source data leads to significant improvement in realism and identity preservation of the synthesized images <ref type="bibr" target="#b38">[41]</ref>, but is computationally expensive. Moreover, when the source data is scarce, like in one-shot scenario, fine-tuning may lead to over-fitting and performance degradation, which is observed in <ref type="bibr" target="#b38">[41]</ref>. We address both of these problems by using a learned gradient descend (LGD) method <ref type="bibr" target="#b3">[6]</ref> to optimize only the synthesized textureX i (s). Optimizing with respect to the texture tensor prevents the model from overfitting, while the LGD allows us to perform optimization with respect to computationally expensive objectives by doing forward passes through a pre-trained network. Specifically, we introduce a lightweight loss function L upd (we use a sum of squared errors), that measures the distance between a generated image and a ground-truth in the pixel space, and a texture updating network G upd , that uses the current state of the texture and the gradient of L upd with respect to the texture to produce an update ∆X i (s). During fine-tuning we perform M update steps, each time measuring the gradients of L upd with respect to an updated texture. The visualization of the process can be seen in <ref type="figure" target="#fig_0">Figure 14</ref>. More formally, each update is computed as:</p><formula xml:id="formula_7">X i m+1 (s) =X i m (s) + G upd X i m (s), ∂L upd ∂X i m (s) ,<label>(7)</label></formula><p>where m ∈ {0, . . . , M − 1} denotes an iteration number, withX i 0 (s) ≡X i (s).</p><p>The network G upd is trained by back-propagation through all M steps. For training, we use the same objective L G total that was used during the training of the base model. We evaluate it using a target frame x i (t) and a generated framê</p><formula xml:id="formula_8">x i M (t) =x i LF (t) +ω i (t) •X i M (s) .<label>(8)</label></formula><p>It is important to highlight that L upd is not used for training of G upd , but simply guides the updates to the texture. Also, the gradients with respect to this loss are evaluated using the source image, while the objective in Eq. 8 is calculated using the target image, which implies that the network has to produce updates for the whole texture, not just a region "visible" on the source image. Lastly, while we do not propagate any gradients into the generator part of the base model, we keep training the discriminator using the same objective L D adv . Even though training the updater network jointly with the base generator is possible, and can lead to better quality (following the success of model agnostic meta-learning <ref type="bibr" target="#b9">[12]</ref> method), we resort to two-stage training due to memory constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Segmentation</head><p>The presence of static background leads to a certain degradation of our model for two reasons. Firstly, part of the capacity of the texture and the inference generators has to be spent on modeling high variety of background patterns. Secondly, and more importantly, the static nature of backgrounds in most training videos biases the warping towards identity mapping. We therefore, have found it advantageous to include background segmentation into our model.</p><p>We use a state-of-the-art face and body segmentation model <ref type="bibr" target="#b12">[15]</ref> to obtain the ground truth masks. Then, we add the mask prediction outputm i (t) to our inference generator alongside with its other outputs, and train it via a binary cross-entropy loss L seg to match the ground truth mask m i (t). To filter out the training signal, related to the background, we have explored multiple options. Simple masking of the gradients that are fed into the generator leads to severe overfitting of the discriminator. We also could not simply apply the ground truth masks to all the images in the dataset, since the model <ref type="bibr" target="#b12">[15]</ref> works so well that it produces a sharp border between the foreground and the background, leading to border artifacts that emerge after adversarial training.</p><p>Instead, we have found out that masking the ground truth images that are fed to the discriminator with the predicted masksm i (t) works well. Indeed, these masks are smooth and prevent the discriminator from overfitting to the lack of background, or sharpness of the border. We do not backpropagate the signal from the discriminator and from perceptual losses to the generator via the mask pathway (i.e. we use stop gradient/detach operator SG m i (t) before applying the mask). The stop-gradient operator also ensures that the training does not converge to a degenerate state (empty foreground).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation details</head><p>All our networks consist of pre-activation residual blocks <ref type="bibr" target="#b15">[18]</ref> with LeakyReLU activations. We set a minimum number of features in these blocks to 64, and a maximum to 512. By default, we use half the number of features in the inference generator, but we also evaluate our model with full-and quater-capacity inference part, with the results provided in the experiments section.</p><p>We use batch normalization <ref type="bibr" target="#b17">[20]</ref> in all the networks except for the embedder and the texture updater. Inside the texture generator, we pair batch normalization with adaptive SPADE layers <ref type="bibr" target="#b34">[37]</ref>. We modify these layers to predict pixelwise scale and bias coefficients using feature maps, which are treated as model parameters, instead of being input from a different network. This allows us to save memory by removing additional networks and intermediate feature maps from the optimization process, and increase the batch size. Also, following <ref type="bibr" target="#b34">[37]</ref>, we predict weights for all 1 × 1 convolutions in the network from the embeddings {ê i k (s)}, which includes the scale and bias mappings in AdaSPADE layers, and skip connections in the residual upsampling blocks. In the inference generator, we use standard adaptive batch normalization layers <ref type="bibr" target="#b4">[7]</ref>, but also predict weights for the skip connections from the embeddings.</p><p>We do simultaneous gradient descend on parameters of the generator networks and the discriminator using Adam <ref type="bibr" target="#b24">[27]</ref> with a learning rate of 2 · 10 −4 . We use 0.5 weight for adversarial losses, and 10 for all other losses, except for the VGGFace perceptual loss (Eq. 5), which is set to 0.01. The weight of the regularizer (Eq. 6) is then multiplicatively reduced by 0.9 every 50 iterations. We train our models on 8 NVIDIA P40 GPUs with the batch size of 48 for the base model, and a batch size of 32 for the updater model. We set unrolling depth M of the updater to 4 and use a sum of squared errors as the lightweight objective. Batch normalization statistics are synchronized across all GPUs during training. During inference they are replaced with "standing" statistics, similar to <ref type="bibr" target="#b4">[7]</ref>, which significantly improves the quality of the outputs, compared to the usage of running statistics. Spectral normalization is also applied in all linear and convolutional layers of all networks.</p><p>Please refer to the supplementary material for a detailed description of our model's architecture, as well as the discussion of training and architectural features that we have adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform evaluation in multiple scenarios. First, we use the original Vox-Celeb2 <ref type="bibr" target="#b6">[9]</ref> dataset to compare with state-of-the-art systems. To do that, we annotated this dataset using an off-the-shelf facial landmarks detector <ref type="bibr" target="#b5">[8]</ref>. Overall, the dataset contains 140697 videos of 5994 different people. We also use a high-quality version of the same dataset, additionally annotated with the segmentation masks (which were obtained using a model <ref type="bibr" target="#b12">[15]</ref>), to measure how the performance of our model scales with a dataset of a significantly higher quality.</p><p>We obtained this version by downloading the original videos via the links provided in the VoxCeleb2 dataset, and filtering out the ones with low resolution. This dataset is, therefore, significantly smaller and contains only 14859 videos of 4242 people, with each video having at most 250 frames (first 10 seconds). Lastly, we do ablation studies on both VoxCeleb2 and VoxCeleb2-HQ, and report on a smartphone-based implementation of the method. For comparisons and ablation studies we show the results qualitatively and also evaluate the following metrics:</p><p>-Learned perceptual image patch similarity <ref type="bibr" target="#b39">[42]</ref> (LPIPS), which measures overall predicted image similarity to ground truth. -Cosine similarity between the embedding vectors of a state-of-the-art face recognition network <ref type="bibr" target="#b7">[10]</ref> (CSIM), calculated using the synthesized and the target images. This metric evaluates the identity mismatch. -Normalized mean error of the head pose in the synthesized image (NME). We use the same network <ref type="bibr" target="#b5">[8]</ref>, which was used for the annotation of the dataset, to evaluate the pose of the synthesized image. We normalize the error, which is a mean euclidean distance between the predicted and the target points, by the distance between the eyes in the target pose, multiplied by 10. -Multiply-accumulate operations (MACs), which measure the complexity of each method. We exclude from the evaluation initialization steps, which are calculated only once per avatar.</p><p>The test set in both datasets does not intersect with the train set in terms of videos or identities. For evaluation, we use a subset of 50 test videos with different identities (for VoxCeleb2, it is the same as in <ref type="bibr" target="#b38">[41]</ref>). The first frame in each sequence is used as a source. Target frames are taken sequentially at 1 FPS.</p><p>We only discuss most important results in the main paper. For additional qualitative results and comparisons please refer to the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with the state-of-the-art methods</head><p>We compare against three state-of-the-art systems: Few-shot Talking Heads <ref type="bibr" target="#b38">[41]</ref>, Few-shot Vid-to-Vid <ref type="bibr" target="#b34">[37]</ref> and First Order Motion Model <ref type="bibr" target="#b30">[33]</ref>. The first system is a problem-specific model designed for avatar creation. Few-shot Vid-to-Vid is a state-of-the-art video-to-video translation system, which has also been successfully applied to this problem. First Order Motion Model (FOMM) is a general motion transfer system that does not use precomputed keypoints, but can also be used as an avatar. We believe that these models are representative of the most recent and successful approaches to one-shot avatar generation. We also acknowledge the work of <ref type="bibr" target="#b14">[17]</ref>, but do not compare to them extensively due to unavailability of the source code, pretrained models or pre-calculated results. A small-scale qualitative comparison is provided in the supplementary materials. Additionally, their method is limited to the usage of 3D keypoints, while our method does not have such restriction. Lastly, since Few-shot Vid-to-Vid is an autoregressive model, we use a full test video sequence for evaluation <ref type="bibr">(25 FPS)</ref> and save the predicted frames at 1 FPS. For quality metrics, we have compared synthesized images to their targets using a perceptual image similarity (LPIPS ↓), identity preservation metric (CSIM ↑), and a normalized pose error (NME ↓). We highlight a model which was used for the comparison in <ref type="figure">Figure 5</ref> with a bold marker. We observe that our model outperforms the competitors in terms of identity preservation (CSIM) and pose matching (NME) in the settings, when models' complexities are comparable. In order to better compare with FOMM, we did a user study, where users have preferred the image generated by our model to FOMM 59.6% of the time.</p><p>Importantly, the base models in these approaches have a lot of computational complexity, so for each method we evaluate a family of models by varying the number of parameters. The performance comparison for each family is reported in <ref type="figure" target="#fig_2">Figure 4</ref> (with Few-shot Talking Heads being excluded from this evaluation, since their performance is much worse than the compared methods). Overall, we can see that our model's family outperforms competing methods in terms of pose error and identity preservation, while being, on average, up to an order of magnitude faster. To better compare with FOMM in terms of image similarity, we have performed a user study, where we asked crowd-sourced users which generated image better matches the ground truth. In total, 361 users evaluated 1600 test pairs of images, with each one seeing on average 21 pairs. In 59.6% of comparisons, the result of our medium model was preferred to a medium sized model of FOMM.</p><p>Another important note is on how the complexity was evaluated. In Fewshot Vid-to-Vid we have additionally excluded from the evaluation parts that are responsible for the temporal consistency, since other compared methods are evaluated frame-by-frame and do not have such overhead. Also, in FOMM we have excluded the keypoints extractor network, because this overhead is shared implicitly by all the methods via usage of the precomputed keypoints.</p><p>We visualize the results for medium-sized models of each of the compared methods in <ref type="figure">Figure 5</ref>. Since all methods perform similarly in case when source and target images have marginal differences, we have shown the results where a source and a target have different head poses. In this extrapolation setting,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Target Few-shot T. Heads</p><p>Few-shot Vid-to-Vid FOMM Ours <ref type="figure">Fig. 5</ref>: Comparison on a VoxCeleb2 dataset. The task is to reenact a target image, given a source image and target keypoints. The compared methods are Fewshot Talking Heads <ref type="bibr" target="#b38">[41]</ref>, Few-shot Vid-to-Vid <ref type="bibr" target="#b34">[37]</ref>, First Order Motion Model (FOMM) <ref type="bibr" target="#b30">[33]</ref> and our proposed Bi-layer Model. For each method, we used the models with a similar number of parameters, and picked source and target images to have diverse poses and expressions, in order to highlight the differences between the compared methods.</p><p>our method has a clear advantage, while other methods either introduce more artifacts or more blurriness.</p><p>Evaluation on high-quality images. Next, we evaluate our method on the high-quality dataset and present the results in <ref type="figure" target="#fig_4">Figure 6</ref>. Overall, in this case, our method is able to achieve a smaller identity gap, compared to the dataset with the background. We also show the decomposition between the texture and a low frequency component in <ref type="figure">Figure 7</ref>. Lastly, in <ref type="figure">Figure 8</ref>   several frameworks which provide APIs for mobile inference on such devices. From our experiments, we measured the Snapdragon Neural Processing Engine (SNPE) [3] to be about 1.5 times faster than PyTorch Mobile [2] and up to two times faster than TensorFlow Lite <ref type="bibr" target="#b1">[4]</ref>. The medium-sized model ported to the Snapdragon 855 (Adreno 640 GPU, FP16 mode) takes 42 ms per frame, which is sufficient for real-time performance, given that the keypoint tracking is being run in parallel, e.g. on a mobile CPU.</p><p>Ablation study. Finally, we evaluate the contribution of individual components. First, we evaluate the contribution of adaptive SPADE layers in the texture generator (by replacing them with adaptive batch normalization and perpixel biases) and adaptive skip-connections in both generators. A model with these features removed makes up our baseline. Lastly, we evaluate the contribution of the updater network. The results can be seen in <ref type="table">Table 1</ref> and <ref type="figure">Figure 9</ref>. We evaluate the baseline approach only on a VoxCeleb2 dataset, while the full models with and without the updater network are evaluated on both low-and highquality datasets. Overall, we see a significant contribution of each component with respect to all metrics, which is particularly noticeable in the high-quality scenario. In all ablation comparisons, medium-sized models were used.  <ref type="table">Table 1</ref>: Ablation studies of our approach. We first evaluate the baseline method without AdaSPADE or adaptive skip connections. Then we add these layers, following <ref type="bibr" target="#b34">[37]</ref>, and observe significant quality improvement. Finally, our updater network provides even more improvement across all metrics, especially noticeable in the highquality scenario.</p><p>Source Pose Ours +Upd. <ref type="figure">Fig. 9</ref>: Examples from the ablation study on VoxCeleb2 (first two rows) and VoxCeleb2-HQ (last two rows).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a new neural rendering-based system that creates head avatars from a single photograph. Our approach models person appearance by decomposing it into two layers. The first layer is a pose-dependent coarse image that is synthesized by a small neural network. The second layer is defined by a poseindependent texture image that contains high-frequency details and is generated offline. During test-time it is warped and added to the coarse image to ensure high effective resolution of synthesized head views. We compare our system to analogous state-of-the-art systems in terms of visual quality and speed. The experiments show up to an order of magnitude inference speedup over previous neural head avatar models, while achieving state-of-the-art quality. We also report on a real-time smartphone-based implementation of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Methods</head><p>We start by explaining training process of our method in much more details. Then, we describe the architecture that we use and how different choices affect the final performance. Finally, we provide a more extended explanation of the mobile inference pipeline that we have adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Training details</head><p>We optimize all networks using Adam <ref type="bibr" target="#b24">[27]</ref> with a learning rate equal to 2 · 10 −4 β 1 = 0.5 and β 2 = 0.999. Before testing, we calculate "standing" statistics for all batch normalization layers using 500 mini-batches. Below we provide additional details for the losses that we use.</p><p>Texture mapping regularization. Below we provide additional implementation details as well as better describe the reasons why this loss is used. The training signal that the texture generator G tex receives is first warped by the warping field ω i (t) predicted by the inference generator. Because of this, random initializations of the networks typically lead to subpotimal textures, in which the face of the source person occupies a small fraction of the total area of the texture. As the training progresses, this leads to a lower effective resolution of the output image, since the optimization process is unable to escape this bad local optima.</p><p>In practice, we address the problem by treating the network's output as a delta to an identity mapping, and also by applying a magnitude penalty on that delta in the early iterations. As mentioned in the main paper, the weight of this penalty is multiplicatively reduced to zero during training, so it does not affect the final performance of the model. More formally, we decompose the output warping field into a sum of two terms: ω i (t) = I + ∆ω i (t), where I denotes an identity mapping, and apply an L 1 penalty, averaged by a number of spatial positions in the mapping, to the second term:</p><formula xml:id="formula_9">L G reg = 1 HW ||∆ω i (t)|| 1 .<label>(9)</label></formula><p>To understand why this regularization helps, we need to briefly describe the implicit properties of the VoxCeleb2 dataset. Since it was obtained using a face detector, a weak from of face alignment is present in the training images, with face occupying more or less the same region.</p><p>On the other hand, our regularization allows the gradients to initially flow unperturbed into the texture generator. Therefore, gradients with respect to the texture, averaged over the minibatch, consistently force the texture to produce a high-frequency component of a mean face in the minibatch. This allows the face in the texture to fill the same area as it does in the training images, leading to better generalization.</p><p>Adversarial loss. Below we elaborate in more details on the type of adversarial loss that is used. We use the terms (10) and <ref type="bibr" target="#b8">(11)</ref> to calculate realism scores for real and fake images respectively, with i n and t n denoting indices of mini-batch elements, N -a mini-batch size and i ∈ {i 1 , . . . , i n }:</p><formula xml:id="formula_10">s i (t) = D x i (t), y i (t) − 1 N N n D x in (t n ), y in (t n ) ,<label>(10)</label></formula><formula xml:id="formula_11">s i (t) = D x i (t), y i (t) − 1 N N n D x in (t n ), y in (t n ) .<label>(11)</label></formula><p>Moreover, we use PatchGAN <ref type="bibr" target="#b18">[21]</ref> formulation of the adversarial learning. In it, the discriminator outputs a matrix of realism scores instead of a single prediction, and each element of this matrix is treated as a realism score for a corresponding patch in the input image. This formulation is also used in a large body of relevant works <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b35">38]</ref> and improves the stability of the adversarial training. If we denote the size of a scores matrix s i (t) as H s × W s , the resulting objectives can be written as follows:</p><formula xml:id="formula_12">L D adv = 1 H s W s h,w max 0, 1 − s i h,w (t) + max 0, 1 +ŝ i h,w (t) ,<label>(12)</label></formula><formula xml:id="formula_13">L G adv = 1 H s W s h,w max 0, 1 + s i h,w (t) + max 0, 1 −ŝ i h,w (t) .<label>(13)</label></formula><p>The loss (12) serves as the discriminator objective. For the generator, we also calculate the feature matching loss <ref type="bibr" target="#b35">[38]</ref>, which has now become a standard component of supervised image-to-image translation models. In this objective, we minimize the distance between the intermediate feature maps of discriminator, calculated using corresponding target and generated images. If we denote as f i k,D (t) the features at different spatial resolutions H k × W k , then the feature mathing objective is computed as follows:</p><formula xml:id="formula_14">L G FM = 1 K k 1 H k W k ||f i k,D (t) − f i k,D (t)|| 1 .<label>(14)</label></formula><p>A.2 Architecture description</p><p>All our networks consist of pre-activation residual blocks. The layout is visualized in the <ref type="figure" target="#fig_0">Figures 10-14</ref>. In all networks, except for the inference generator at the updater, we set the minimum number of channels to 64, and increase (decrease) it by a factor of two each time we perform upsampling (downsampling). We pick the first convolution in each block to increase (decrease) the number of channels. The maximum number of channels is set to 512. In the inference generator we set the minimum number of channels to 32, and the maximum to 256. Also, all linear layers (except for the last one) have their dimensionality set to 256.</p><p>Moreover, as described in <ref type="figure" target="#fig_0">Figure 11</ref>, in the inference generator we employ more efficient blocks, with upsampling performed after the first convolution, and not before it. This allows us to halve the number of MACs per inference.</p><p>In the embedder network ( <ref type="figure" target="#fig_0">Figure 12</ref>) each block operating at the same resolution reduces the number of channels, similarly to what is done in the generators. In fact, the output number of channels in each block is excatly equal to the input number of channels in the corresponding generator block. We borrowed this scheme from <ref type="bibr" target="#b34">[37]</ref>, and assume that is it done to botteleneck the embedding tensors, which will be used for the prediction of the adaptive parameters at high resolution. This forces the generators to use all their capacity to generate the image bottom-up, instead of using a shortcut between the source and the target at high resolution, which is present in the architecture.</p><p>We do not use batch normalization in the embedder network, because we want it to be trained more slowly, compared to other networks. Otherwise, the whole system overfits to the dataset and the textures become correlated with the source image in terms of head pose. We believe that this is related to the VoxCeleb2 dataset, since in it there is a strong correlation in terms of pose between the randomly sampled source and target frames. This implies that the dataset is lacking diversity with respect to the head movement, and we believe that our system would perform much better either with a better disentangling mechanism of head pose and identity, which we did not come up with, or with a more diverse dataset.</p><p>On contrary, we find it highly beneficial to use batch normalization in the discriminator <ref type="figure" target="#fig_0">(Figure 13</ref>). This is less memory efficient, compared to the classical scheme, since "real" and "fake" batches have to be concatenated and fed into the discriminator together. We concatenate these batches to ensure that the first and second order statistics inside the discriminator's features are not whitened with respect to the label ("real" or "fake"), which significantly improves the quality of the outputs.</p><p>We also tried using instance normalization, but found this to be more sensitive to hyperparameters. For example, the config working on a high-quality dataset cannot be transferred to the low-quality dataset without the occurring instabilities during the adversarial training.</p><p>We predict adaptive parameters following the procedure inspired by a matrix decomposition. The basic idea is to predict a weight tensor for the convolution via a decomposition of the embedding tensor. In our work, we use the following procedure (taken from <ref type="bibr" target="#b34">[37]</ref>) to predict the weights for all 1 × 1 convolutions and adaptive batch normalization layers in the texture and the inference generators:</p><p>-Resize all embedding tensorsê i k (s), with the number of channels C k , by nearest upsampling to 32 × 32 resolution for the texture generator, and 16 × 16 for the medium-sized inference generator.</p><p>-Flatten the resized tensor across its spatial resoluton, converting it to a matrix of the shape C k × 1024 for the texture generator, and 1 2 C k × 512 for the inference generator (the first dimensionality has to match the reduced number of channels in the convolutions of the medium-sized model).</p><p>-Three linear layers (with no nonlinearities in between) are then applied, performing the decomposition. A resulting matrix should match the shape of the weights, combined with the biases, for each specific adaptive layer. These linear layers are trained separately for each adaptive convolution and adaptive batch normalization.</p><p>Each embedding tensorê i k (s) is therefore used to predict all adaptive parameters inside the layers of the k-th block in the texture and inference generators. We do not perform an ablation study with respect to this scheme, since it was used in an already published work on a similar topic.</p><p>Finally, we describe the architecture of the texture enhancer in <ref type="figure" target="#fig_0">Figure 14</ref>. This architecture is standard for image-to-image translation tasks. The spatial dimensionality and the number of channels in the bottleneck is equal to 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Mobile inference</head><p>As mentioned in main paper, we train our models using PyTorch and then port them to smartphones with Qualcomm Snapdragon 855 chips. For inference, we use a native Snapdragon Neural Processing Engine (SNPE) APK, which provides a significant speed-up compared to TF-Lite and PyTorch mobile. In order to convert the models trained in PyTorch into SNPE-compatible containers, we first use the PyTorch-ONNX parser, as it is simple to get an ONNX model right from PyTorch. However, it does not guarantee that the obtained model can be converted into a mobile-compatible container, since some operations may be unsupported by SNPE. Moreover, there is a collision between different versions of ONNX and SNPE operation sets, with some versions of the operations being incompatible with each other. We have solved this problem by using PyTorch 1.3 and SNPE 1.32, but solely for operations used our inference generator. This is part of the reason why we had to resort to simple layers, like BathNorm-s, convolutions and nonlinearities in our network.. All ported models have spectral normalization removed, and adaptive parameters fixed and merged into their base layers. In our experiments the target platform is Adreno 640 GPU, utilized in FP16 mode. We do not observe any noticeable quality degradation from running our model in FP16 (although training in FP16 or mixed precision settings leads to instabilities and early explosion of the gradients). Since our model includes bilinear sampling from texture (using a predicted warping field), that is not supported by SNPE, we implement it ourselves, as a part of application, called after each inferred frame on a CPU. The GPU implementation should be possible as well, but is more time-consuming to implement. Our reported mobile timings (42 ms, averaged by 100 runs) do not include the bilinear sampling and copy operations from GPU to CPU. On CPU, bilinear sampling takes additional 2 milliseconds, but for a GPU implementation, the timing would be negligible. First Order Motion model was trained using a config provided with the official implementation of the model. In order to obtain a family of models, we modify minimum and maximum number of channels in the generator from default 64 and 512 to 32 and 256 for the medium, and 16 and 128 for the small models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiments</head><p>For Few-shot Vid-to-Vid, we have also used a default config from the official implementation, but with slight modifications. Since we train on a dataset with videos already being cropped, we removed the random crop and scale augmentations in order to avoid a domain gap between training and testing. In our case, that would lead to black borders appearing on the training images, and a suboptimal performance on a test set with no such artifacts. In order to obtain a family of models, we also reduce the minimum and maximum number of channels in the generator from the default 32 and 1024 to 32 and 256 for the medium model and 16 and 128 for the small model.</p><p>To calculate the number of multiply-accumulate operations, we used an offthe-shelf tool that evaluates this number for all internal PyTorch modules. That way of calculation, while being easy, is not perfect as, for example, it does not account for the number of operations in PyTorch functionals, which may be called inside the model. Other forms of complexity evaluation would require significant refactor of the code of the competitors, which lies out of the scope of our comparison. For our model, we have provided accurate complexity esimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Extended evaluations.</head><p>We provide extended quantitative data for our experiments in <ref type="table">Table 2</ref>, and additional qualitative comparisons in <ref type="figure" target="#fig_0">Figures 15-17</ref>, which extend the comparisons provided in the main paper. We additionally perform a small comparison with a representative mesh-based avatar system <ref type="bibr" target="#b0">[1]</ref> in <ref type="figure" target="#fig_0">Figure 18</ref> and compare our method with MarioNETte system <ref type="bibr" target="#b14">[17]</ref> in <ref type="figure" target="#fig_7">Figure 20</ref>. Also we extend our ablation study to highlight the contribution of the texture enhancement network in the <ref type="figure" target="#fig_0">Figure 19</ref>. Finally, we show cross-person reenactment results in <ref type="figure" target="#fig_0">Figure 21</ref>.  <ref type="table">Table 2</ref>: We present numerical data for the comparison of the models. Some of it duplicates the data available in <ref type="figure">Figure 5</ref> of the main paper. F-s V2V denotes Few-shot Vid-to-Vid <ref type="bibr" target="#b34">[37]</ref>, FOMM denotes First Order Motion Model <ref type="bibr" target="#b30">[33]</ref>, and NTH denotes Neural Talking Heads <ref type="bibr" target="#b38">[41]</ref>. Here we also include SSIM evaluation, which we found to correlate with LPIPS, and therefore excluded it from the main paper. We also provide evaluation for initialization and inference time (in milliseconds) for the medium-sized models of each method, measured on NVIDIA P40 GPU. We did not include this measurement in the main paper since we cannot calculate it using target low-performance devices (due to difficulties with porting the competitor models to the SNPE [3] framework), while evaluation on much more powerful (in terms of FLOPs) desktop GPUs may be an inaccurate way to measure the performance on less capable devices. We, therefore, decided to stick with MACs as our performance metric, which is more common in the literature, but still provide our obtained numbers for desktop GPUs here. We report median values out of a thousand iterations with random inputs.  <ref type="figure" target="#fig_0">Fig. 12</ref>: Architecture for the embedder. Here we do not use normalization layers. First, we downsample input images and stickmen to 8 × 8 resolution. After that, we obtain embeddings for each of the blocks in the texture and the inference generators. Each embedding is a feature map, and has the same number of channels as the corresponding block in the texture generator. Therefore, we reduce the number of channels in the final blocks, from the maximum of 512 to the minimum of 64 at the end. In the blocks operating at the same resolution, we insert a convolution into a skip connection only when the input and the output number of channels is different.  <ref type="figure" target="#fig_0">Fig. 13</ref>: Architecture of the discriminator. We use 5 downsampling blocks and one block operating at final 8×8 resolution. Additionally, in each block we output features after the second nonlinearity. These features are later used in the feature matching loss. For downsampling, we use average pooling. The architecture of the final block, operating at the same resolution, is similar to the one in the embedder: it is without a convolution in the skip connection, but with batch normalization layers.  <ref type="figure" target="#fig_0">Fig. 14:</ref> We employ a simple encoder-decoder style architecture, similar to the one used in <ref type="bibr" target="#b18">[21]</ref>. We replace downsampling and upsampling layers with residual blocks. We also do not employ batch normalization inside the enhancer.  <ref type="figure" target="#fig_0">Fig. 18</ref>: Comparison of our method with a closed-source product <ref type="bibr" target="#b0">[1]</ref>, which is representative of the state-of-the-art in real-time one-shot avatar creation, based on explicit 3D modelling. The first row represents reenactment results, since the frontal image was used for initialization of both methods. We can see that our model does a much better job of modelling the face shape and the hair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HF Texture + Gradients</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Pose Texture + Upd. Ours + Upd. <ref type="figure" target="#fig_0">Fig. 19</ref>: Ablation study for the contribution of the texture updater on a VoxCeleb2-HQ dataset. The results are presented with and without the updater.  <ref type="bibr" target="#b14">[17]</ref> system in a one-shot selfreenactment task. The results for <ref type="bibr" target="#b14">[17]</ref> are taken from the respective paper, as no source code is available. The evaluation of the computational complexity of this system was also beyond our reach since it would require re-implementation from scratch. However, since it utilizes an encoder-decoder architecture with a large number of channels <ref type="bibr" target="#b14">[17]</ref>, it can be assumed to have a similar complexity to the largest variant of FOMM <ref type="bibr" target="#b30">[33]</ref>. For our method, we use a medium-sized model. Lastly, the evaluation for <ref type="bibr" target="#b14">[17]</ref> is done on the same videos as training (on the hold-out frames), while our method is applied without any fine-tuning. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•Fig. 1 :</head><label>1</label><figDesc>presented systems that arXiv:2008.10174v1 [cs.CV] 24 Aug 2020 Our new architecture creates photorealistic neural avatars in one-shot mode and achieves considerable speed-up over previous approaches. Rendering takes just 42 milliseconds on Adreno 640 (Snapdragon 855) GPU, FP16 mode. create neural head avatars from a handful of photographs (few-shot setting) or a single photograph (one-shot setting), causing both excitement and concerns about potential misuse of such technology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Texture enhancement network (updater) accepts the current state of the texture and the guiding gradients to produce the next state. The guiding gradients are obtained by reconstructing the source image from the current state of the texture and matching it to the ground-truth via a lightweight updater loss. These gradients are only used as inputs and are detached from the computational graph. This process is repeated M times. The final state of the texture is then used to obtain a target image, which is matched to the ground-truth via the same loss as the one used during training of the main model. The gradients from this loss are then backpropagated through all M copies of the updater network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>In order to evaluate a quality against performance trade off, we train a family of models with varying complexity for each of the compared methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, we show that our texture enhancement pipeline allows us to render small person-specific features like wrinkles and moles on out-of-domain examples. For more qualitative examples, as well as reenactment examples with a driver of a different person, please refer to the supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>High quality synthesis results. We can see that our model is both capable of viewpoint extrapolation and low identity gap synthesis. The architecture in this experiment has the same number of parameters as the medium architecture in the previous comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>B. 1</head><label>1</label><figDesc>Training details for the state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 15 :Fig. 16 :Fig. 17 :</head><label>151617</label><figDesc>Extended comparison of the medium-sized models from all method families on the VoxCeleb2 dataset. For Few-shot Talking Heads we use the results obtained using the original full-sized model. Detailed qualitative results for our medium-sized model trained on the VoxCeleb2-HQ dataset. Qualitative comparison between the small, medium and large models for all compared families of methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 20 :</head><label>20</label><figDesc>A comparison with MarioNETte</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>5 Fig. 21 :</head><label>521</label><figDesc>The results for cross-person reenactment. While our method does preserve the texture of the original image, the driving identity leakage remains noticeable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>3x3 Conv 3x3 Tanh Sigmoid Pose Tanh Inference Generator Linear LeakyReLU … Linear Reshape x4 AdaBN Conv 3x3 AdaBN LeakyReLU LeakyReLU Upsampling Conv 3x3 AdaConv 1x1 Upsampling Add Upsampling Block Upsampling Block* Upsampling Block AdaSPADE LeakyReLU Conv 3x3 x6 …Same Block x6 … Embeddings 6 LeakyReLU LeakyReLU Conv 3x3 Conv 3x3 Add Same Resolution Block Embeddings Conv 1x1</head><label></label><figDesc>Description of the texture generator's architecture. The first normalization layer in the first upsampling block (marked with a star) is replaced by a regular batch normalization. For the spatial resolution increase, nearest upsampling is performed. All trainable tensors in adaptive SPADE layers have the same size as an output of the previous layer. The first trainable tensor, which is a network's input, has a spatial resolution of 4 × 4. The architecture of the inference generator. As in the texture generator, in the first upsampling block the first normalization layer is replaced by a regular batch normalization. Similarly, nearest upsamping is used. Input pose is reshaped into a vector and fed into a stack of linear layers. Then, the output of the last linear layer is reshaped to have a spatial resolution of 4 × 4.</figDesc><table><row><cell></cell><cell>Embedder</cell></row><row><cell>Image + Stickmen</cell><cell>Texture Generator Downsampling Block</cell></row><row><cell cols="2">Tanh Trainable Tensor AdaConv 1x1 AdaConv 1x1 Scales Biases Trainable Tensor Upsampling Block* Upsampling Block AdaSPADE LeakyReLU Conv 3x3 x6 … Adaptive SPADE Conv 3x3 High-freq. Texture Fig. 10: Conv Warping Low. freq. RGB Down. Block Down. x5 … Fig. 11: LeakyReLU AdaSPADE Upsampling Conv 3x3 LeakyReLU AdaSPADE LeakyReLU Conv 3x3 Add Upsampling Block AdaConv 1x1 Upsampling Seg. LeakyReLU Conv 3x3 Conv 1x1 Add sampling Block Down-Conv 3x3 Block Down-Embeddings 1 Same sampling</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Conv 3x3 BN LeakyReLU LeakyReLU Conv 3x3 Down- sampling Down- sampling Conv 1x1 Add Downsampling Block Discriminator Image + Stickmen Conv 3x3 Down. Block Down. Block BN LeakyReLU Conv 1x1 x5 …</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Features 1</cell></row><row><cell></cell><cell>Features 5</cell></row><row><cell>Same Block</cell><cell>Features 6</cell></row><row><cell></cell><cell>Features</cell></row><row><cell>Scores</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Texture Enhancer Conv 3x3 Same Block Same Block LeakyReLU Conv 3x3 x8 … Update for HF Texture Upsampling Block Down. Block LeakyReLU Conv 3x3 LeakyReLU Upsampling Upsampling Add Upsampling Block Conv 3x3 Conv 1x1</head><label></label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sdk</forename><surname>Avatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Homepage</surname></persName>
		</author>
		<ptr target="https://avatarsdk.com2.PyTorchhomepage.https://pytorch.org3.SNPEhomepage.https://developer.qualcomm.com/sites/default/files/docs/snpe" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tensorflow Lite Homepage</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/lite" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Digital Emily project: Achieving a photorealistic digital actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lambeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="20" to="31" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230, 000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Voxceleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Niannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1538" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">High fidelity face manipulation with extreme pose and expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12003</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepwarp: Photorealistic image resynthesis for gaze manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sungatullina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="311" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graphonomy: Universal human parsing via graph transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Marionette: Few-shot face reenactment preserving identity of unseen targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<idno>abs/1911.08139</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Avatar digitization from a single image for real-time rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fursund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The relativistic discriminator: a key element missing from standard GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GRDN: grouped residual dense network for real image denoising and gan-based real-world noise modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11714</idno>
		<title level="m">Deep video portraits</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep appearance models for face rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">68</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno>abs/1411.1784</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dense pose transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lempitsky</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aliev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bashirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ivakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pasechnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vakhitov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Textured neural avatars. In: IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">First order motion model for image animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Synthesizing Obama: learning lip sync from audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Icface: Interpretable and controllable face reenactment using gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<idno>abs/1904.01909</idno>
		<ptr target="http://arxiv.org/abs/1904.01909" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Few-shot video-tovideo synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yakovenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sophia Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Few-shot adversarial learning of realistic neural talking head models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
