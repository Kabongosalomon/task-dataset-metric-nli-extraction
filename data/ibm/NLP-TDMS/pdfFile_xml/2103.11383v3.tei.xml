<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-level Metric Learning for Few-shot Image Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxiong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Li</surname></persName>
							<email>yaohuili@smail.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlin</forename><surname>Chen</surname></persName>
							<email>clchen@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-level Metric Learning for Few-shot Image Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot learning is devoted to training a model on few samples. Recently, the method based on local descriptor metric-learning has achieved great performance. Most of these approaches learn a model based on a pixel-level metric. However, such works can only measure the relations between them on a single level, which is not comprehensive and effective. We argue that if query images can simultaneously be well classified via three distinct level similarity metrics, the query images within a class can be more tightly distributed in a smaller feature space, generating more discriminative feature maps. Motivated by this, we propose a novel Multi-level Metric Learning (MML) method for fewshot learning, which not only calculates the pixel-level similarity but also considers the similarity of part-level features and the similarity of distributions. First, we use a feature extractor to get the feature maps of images. Second, a multi-level metric module is proposed to calculate the part-level, pixel-level, and distribution-level similarities simultaneously. Specifically, the distribution-level similarity metric calculates the distribution distance (i.e., Wasserstein distance, Kullback-Leibler divergence) between query images and the support set, the pixel-level, and the partlevel metric calculates the pixel-level and part-level similarities respectively. Finally, the fusion layer fuses three kinds of relation scores to obtain the final similarity score. Extensive experiments on popular benchmarks demonstrate that the MML method significantly outperforms the current state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans can learn novel concepts and objects with just a few samples. Recently, many methods were proposed to learn new concepts with limited labeled data, such as semisupervised learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b34">35]</ref>, zero-shot learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, and few-shot learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b2">3]</ref>. Facing with the problem of data scarcity, these three paradigms propose solutions from different perspectives. Semi-supervised learning aims to train a model with few labeled data and a large amount of unlabeled data, and zero-shot learning devoted to identifying unseen categories with no labeled data, while few-shot learning focuses on learning new concepts with few labeled data. We propose a novel few-shot learning method to address the problem of data scarcity in this paper.</p><p>The few-shot learning methods can be roughly classified into two categories: meta-learning based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref> and metric-learning based methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b2">3]</ref>. Metric-based few-shot learning methods have achieved remarkable success due to their fewer parameters and effectiveness. In this work, we focus on this branch.</p><p>The basic idea of the metric-learning based few-shot learning method is to learn a good metric to calculate the similarity between query images and the support set. Therefore, how to learn good feature embedding representation and similarity metric are the key problem of metric-learning based few-shot learning method. For feature embedding representation, ProtoNet <ref type="bibr" target="#b29">[30]</ref> and RelationNet <ref type="bibr" target="#b31">[32]</ref> adopt image-level feature representations. However, due to the scarcity of data, it is not sufficient to measure the relation at the image-level <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>. Recently, CovaMNet <ref type="bibr" target="#b20">[21]</ref>, DN4 <ref type="bibr" target="#b19">[20]</ref> and MATANet <ref type="bibr" target="#b2">[3]</ref> introduce local representations (LRs) into few-shot learning and utilize these LRs to represent the image features, which can achieve better recognition results.</p><p>For similarity metrics, these existing methods calculate similarities by different metrics. For example, Relation Nets <ref type="bibr" target="#b31">[32]</ref> proposes a network to learn the most suitable image-level similarity metric functions. DN4 <ref type="bibr" target="#b2">[3]</ref> proposes a cosine-based image-to-class metric to measure the similarity on pixel-level.</p><p>However, all methods mentioned above only consider the distribution of support set, ignoring the natural distribution of query images. We argue that the distribution of the query image is also important for few-shot learning. It is necessary to design a distribution-level similarity metric to capture the distribution-level relations between query images and support set. Moreover, these methods only calculate similarities on a single level, i.e., pixel-level or imagelevel, which is not effective enough. Intuitively, under the few-shot learning setting, the features obtained by adopting a single similarity measure are not comprehensive, and the single similarity measure may lead to a certain similarity deviation, thus reducing the generalization ability of the model. It is necessary to adopt multi-level similarity metric, generating more discriminative features rather than using a single measure.</p><p>To this end, we propose a novel multi-level metric learning method (MML), which can be trained in an end-to-end manner. First, we represent all images as a set of LRs, rather than a global feature representation at the imagelevel. Moreover, inspired by CBAM <ref type="bibr" target="#b36">[37]</ref>, we not only consider these features as a pixel-level set but also consider these features as a part-level set. Afterward, we employ a multi-level metric module to calculate the relations at different feature levels as illustrated in <ref type="figure">Figure 1</ref>, i.e., pixel-level, part-level, and distribution-level. Specifically, we use DN4 <ref type="bibr" target="#b19">[20]</ref> as the pixel-level similarity metric, to capture local relationships. We propose a novel part-level similarity metric to calculate the different semantic part relations, and employ a distribution similarity metric (i.e., Kullback-Leibler divergence and Wasserstein distance) to get the distribution-level relations. Finally, a fusion layer is proposed to fuse three kinds of relation scores to obtain the final similarity score.</p><p>The main contributions are summarized as follows:</p><p>• We propose a part-level similarity metric for few-shot learning, which can capture the part-level semantic similarity between query images and support images. • We propose a novel multi-level metric learning method by computing the semantic similarities on pixel-level, part-level, and distribution-level simultaneously, aiming to find more comprehensive semantic similarities. • We conduct sufficient experiments on popular benchmark datasets to verify the advancement of our model and the performance of our model achieves the stateof-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Recently, the domain of few-shot learning has shown increased interest. In this section, we first review recent metric-learning based few-shot learning methods and then introduce the work that inspire our work.</p><p>Metric-learning based few-shot learning: Metriclearning based methods can be roughly classified into two groups:</p><p>(1) Learning feature embedding representation: Koch et al. <ref type="bibr" target="#b13">[14]</ref> used a Siamese Neural Network to tackle the oneshot learning problem, in which the feature extractor is of VGG styled structure and L 1 distance is used to measure the similarity between query images and support images. Snell et al. <ref type="bibr" target="#b29">[30]</ref> proposed Prototypical Networks, in which the Euclidean distance is used to compute the distance between class-specific prototypes. Li et al. <ref type="bibr" target="#b19">[20]</ref> proposed an image-to-class mechanism to find the relation at pixel-level, in which the image features are represented as a local descriptor collection.</p><p>(2) Learning similarity metric: Sung et al. <ref type="bibr" target="#b31">[32]</ref> replaced the existing metric with the Relation Network, which measures the similarity between each query instance and support classes. Li et al. <ref type="bibr" target="#b19">[20]</ref> proposed a Deep Nearest Neighbor Neural Network (DN4) to learn an image-to-class metric by measuring the cosine similarity between the deep local descriptors of a query instance and its neighbors from each support class. Li et al. <ref type="bibr" target="#b20">[21]</ref>explored the distribution consistency-based metric by introducing local covariance representation and deep covariance metric. Unlike these methods, the proposed MML measures the similarity at three different feature levels, i.e., pixel-level, part-level, and distribution-level.</p><p>Other important works: Woo et at. <ref type="bibr" target="#b36">[37]</ref> proposed a Convolutional Block Attention Module (CBAM), which is a kind of attention mechanism module combining spatial information and channel information. According to CBAM, each channel of the feature map can be regarded as a descriptor of a part. Inspired by this, we represent the feature map as a set of part-level feature descriptors and propose a component-level metric to capture semantic correlations between different components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Definition and Formulation</head><p>Standard few-shot image recognition problems are often formalized as N-way M-shot classification problem, in which models are given M seen images from each of N classes, and required to correctly classify unseen images. Different from traditional image recognition tasks, few-shot learning aims to classify novel classes after training. This requires that samples used for training, validation, and testing should come from disjoint label space. To be more specific, given a dataset of visual concepts C, we devide it into three parts: C train , C val and C test , and their label space sat-</p><formula xml:id="formula_0">isfy L train ∩ L val ∩ L test = ∅.</formula><p>To obtain a trained model, we train our model in an episodic way. That is, in each episode, a new task is randomly sampled from the training set C train to train the current model. Each task consists of two subsets, includ- ing support set A S and query set A Q . The A S contains N previously unseen classes, with M samples for each class. We focus on training our model to correctly determine which category each image in the A Q belongs to. Similarly, we randomly sample tasks from C val and C test for meta-validation and meta-testing scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Approach</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, our MML is mainly composed of three modules: a feature extractor F θ , a multi-level metriclearning module, and a fusion layer F ω . All images are first fed into the F θ to get feature embeddings. Then, we represented them as pixel-level and part-level feature descriptors. Afterward, the multi-level metric-learning module calculates similarities on part-level, pixel-level, and distributionlevel simultaneously. Finally, we adaptively fuse the partlevel, pixel-level, and distribution-level similarities together by a fusion layer F ω . All the modules can be trained jointly in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature Embedding with Local Representations</head><p>As proved by some recent studies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>, feature representation based on local descriptor is more abundant than image-level feature representation, which can alleviate the problem of sample scarcity. Therefore, we employ local descriptors to represent each image.</p><p>Specifically, we employ a feature extractor F θ , which can extract informative local descriptors. Given an image X, through F θ , we can get a three-dimensional (3D) feature vector F θ (X) ∈ R C×H×W . As proposed by <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>, the 3D feature vector can be regarded as a set of HW Cdimensional pixel-level feature descriptors:</p><formula xml:id="formula_1">L pixel = F θ (X) = [x 1 , ..., x HW ] ∈ R C×HW<label>(1)</label></formula><p>where x i is the i-th pixel-level feature descriptor. Inspired by <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b9">10]</ref>, each channel in F θ (X) can be regarded as a semantic representation of a specific part in the image, as shown in <ref type="figure">Figure 1(b)</ref>. Therefore, the 3D feature vector can also be regarded as a set of C HW -dimensional part-level feature descriptors:</p><formula xml:id="formula_2">L part = F θ (X) = [x 1 , ..., x C ] ∈ R HW ×C<label>(2)</label></formula><p>where x j is the j-th part-level feature descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-level Metric Learning</head><p>Under N-way M-shot few-shot learning setting, given a query image Q and a certain support class S, through feature extractor F θ , we can get the fearure representation F θ (Q) ∈ R C×H×W and F θ (S) ∈ R M ×C×H×W , respectively. The F θ (Q) can be regards as</p><formula xml:id="formula_3">L pixel Q = [u pixel 1 , ..., u pixel HW ] ∈ R C×HW (3) L part Q = [u part 1 , ..., u part C ] ∈ R HW ×C<label>(4)</label></formula><p>Also, the F θ (S) can be regards as</p><formula xml:id="formula_4">L pixel S = [v pixel 1 , ..., v pixel M HW ] ∈ R C×M HW (5) L part S = [v part 1 , ..., v part M C ] ∈ R HW ×M C<label>(6)</label></formula><p>Part-level similarity metric. After obtaining the partlevel feature representation of the query image Q and the support class S, we calculate the correlation matrix R part ∈ R C×M C between the query image and the support class on part-level:</p><formula xml:id="formula_5">R part i,j = cos(u part i , v part j ) (7) cos(u part i , v part j ) = (u part i ) T v part j u part i · v part j (8) where i ∈ {1, ..., C} , j ∈ {1, ..., M C}, R part i,j</formula><p>is (i, j) element of R part reflecting the distance between the i-th part-level feature descriptor of the query image and the j-th part-level feature descriptor of support clss and cos(·, ·) is Cosine similarity function. Each row in R part represents the semantic relation of each part-level feature descriptor in the query image to all part-level feature descriptors of all images in the support class. For each part-level feature descriptor u part i of the query image Q, we find its ξ most similar part-level feature descriptors. Then, we sum C × ξ selected part-level feature descriptors as the part-level similarity between the query image and the support class</p><formula xml:id="formula_6">D part (Q, S) = Topξ(R part i )<label>(9)</label></formula><p>where Topξ(·) means selecting the ξ largest elements in each row of the R part . Typically, ξ is set as 1 in our work. Pixel-level similarity metric We adopt DN4 <ref type="bibr" target="#b19">[20]</ref> as our pixel-level similarity metric, which proposed an image-toclass measure to capture the local relations between query images and support class. Here, we have a brief review of DN4. Given a query image and a support class, we can get their pixel-level feature set L pixel Q and L pixel S . Then, we calculate the correlation matrix R pixel between the query image and the support class on pixel-level and select the k largest element in each row of the correlation matrix:</p><formula xml:id="formula_7">R pixel i,j = cos(u pixel i , v pixel j )<label>(10)</label></formula><formula xml:id="formula_8">cos(u pixel i , v pixel j ) = (u pixel i ) T v pixel j u pixel i · v pixel j<label>(11)</label></formula><formula xml:id="formula_9">D pixel (Q, S) = Topk(R pixel i )<label>(12)</label></formula><p>Typically, k is set as 1 in our work. Distribution-level similarity metric Inspired by <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19]</ref>, we assume the distribution of pixel-level feature descriptors are multivariate Gaussian. Therefore, we can use Q = N (µ Q , Σ Q ) and S = N (µ S , Σ S ) as the query image's distribution and the support class's distribution, respectively. µ ∈ R C indicate the mean vector and Σ ∈ R C×C indicate covariance matrix of a specific distribution. Then the distribution-level metric can be denoted as</p><formula xml:id="formula_10">D dis (Q, S) = F(Q, S)<label>(13)</label></formula><p>where F(·) is a certain distribution metric. There are many options for distance metric function. In this paper, Kullback-Leibler divergence <ref type="bibr" target="#b15">[16]</ref> and Wasserstein distance are selected for experiments:</p><p>(1) Kullback-Leibler divergence: We use KL divergence to match the distribution of Q to the one of support class S, so the KL divergence can be denoted as</p><formula xml:id="formula_11">F KL (S||Q) = 1 2 trace(Σ −1 Q Σ S ) + ln detΣ Q detΣ S + (µ Q − µ S ) T Σ −1 Q (µ Q − µ S ) − c<label>(14)</label></formula><p>where det is the value of the determinant of a square matrix, trace(·) denotes the trace operation of matrixes.</p><p>(2) Wasserstein distance: 2-Wasserstein distance can also be taken as a choice of F(·), whose formulation is defined as follows</p><formula xml:id="formula_12">F Wass (Q, S) = ||µ Q − µ S || 2 2 + trace Σ Q + Σ S − 2 Σ 1 2 Q Σ S Σ 1 2 Q 1 2</formula><p>(15) However, since the square root of the matrix incurs huge computational time, we use the approximate form proposed by <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_13">F Wass (Q, S) = ||µ Q − µ S || 2 2 + ||Σ Q − Σ S || 2 F<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fusion Layer</head><p>Since three different level similarities have been calculated, i.e., part-level, pixel-level, and distribution-level, we need to design a fusion module to integrate them. In order to tackle this problem, we adopt an adaptive fusion strategy, which use a learnable vertor w = [w 1 , w 2 , w 3 ] to integrate these three parts. Specifically, the final similarity can be obtained by the following equation</p><formula xml:id="formula_14">D(Q, S) =w 1 · D part (Q, S) + w 2 · D pixel (Q, S) − w 3 · D dis (Q, S)<label>(17)</label></formula><p>Note that since the distribution level measure represents difference rather than sameness, we use negative numbers to indicate similarity. Under the 5-way 1-shot few-shot learning setting, input a query image, we will get fivedimensional vectors on each level. We first concatenate these three vectors and balance the size of these three vectors by a Batch Normalization layer. Then, we use a 1D convolution layer with the kernel size of 1 × 1 and the dilation value of 3. Finally, we can get a weighted 5-dimensional similarity vector P, we use it for final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we perform extensive experiments to verify the advance and effectiveness of MML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>To verify the advance and effectiveness of our proposed MML, we performed experiments on five benchmark datasets: miniImageNet <ref type="bibr" target="#b6">[7]</ref>, tieredImageNet <ref type="bibr" target="#b26">[27]</ref>, CUB Birds <ref type="bibr" target="#b33">[34]</ref>, Stanford Dogs <ref type="bibr" target="#b11">[12]</ref> and StanfordCars <ref type="bibr" target="#b14">[15]</ref>.</p><p>ImageNet derivatives: Both miniImageNet dataset and tieredImageNet dataset are subsets of ImageNet <ref type="bibr" target="#b4">[5]</ref>. The miniImageNet dataset consists 100 classes, each of which contains 600 samples, and the tieredImageNet contains 608 classes.</p><p>Fine-grained dataset: CUB Birds contains 11,788 images of 200 classes of birds. Stanford Dogs contains 20,580 annotated images from 120 dog species. Stanford Cars including 16,185 annotated images of 196 breeds of cars.</p><p>The partition of all data sets is shown in <ref type="table" target="#tab_0">Table 1</ref>. All images are resized to 84 × 84.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Network Architecture</head><p>In order to make a fair comparison with other works, we adopt the shallow Conv64F network <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref> and ResNet-12 network <ref type="bibr" target="#b16">[17]</ref> as our feature extrator F θ .</p><p>Conv64F has four convolution blocks, each convolutional block consists of a convolutional layer with 64 3 × 3 filters, a batch normalization layer, and a Leaky ReLU activation layer. Besides, the 2 × 2 max-pooling layer is added in the first two blocks.</p><p>ResNet-12 has four residual blocks, each residual block has 3 convolutional layers with 3×3 kernel, and a 2×2 maxpooling layer is added in the first residual block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation Details</head><p>We use pytorch <ref type="bibr" target="#b24">[25]</ref> to implement all the experiments. <ref type="bibr" target="#b0">1</ref> We conduct our experiments on a series of N-way M-shot tasks, i.e., 5-way 1-shot and 5-way 5-shot. On ImageNet derivatives and fine-grained datasets, we train our model 50 epochs. In each epoch, we randomly sampled 10000 tasks. We use the Adam <ref type="bibr" target="#b12">[13]</ref> optimizer and the cross-entropy (CE) loss to train our MML. Our batch size is set to 4, the initial learning rate is 0.001, and multiplied by 0.5 every 10 epochs. During the test stage, we report our performance based on an average of 1000 tasks. We report the average accuracy as well as the corresponding 95% confidence interval over these 1000 tasks.</p><p>As we all know, using deeper networks to extract features or using pre-trained models can achieve higher accuracy. To make a fair comparison, following the previous works, when using Conv-64F as feature extractor F θ , we do not apply the pre-training strategy. And when ResNet-12 is used as feature extractor F θ , we apply an additional pre-training strategy as suggested in <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison Against Related Approaches</head><p>Results on ImageNet derivatives. As seen from Table 2, when adopting Conv-64F as feature extractor, our MML(KL) achieves the highest accuracy on miniImageNet with 55.14% and 73.33% on 5-way 1-shot and 5-way 5-shot tasks respectively, which make a great improvement compared to the previous pixel-level metric-learning based methods. For example, our MML(KL) is 7.7% and 7.6% better than CovaMNet and DN4 on the 5-way 1-shot task, respectively. Although the experimental results of MML(Wasserstein) are not as good as MML(KL), MML(Wasserstein) is also better than other methods. Moreover, when adopting ResNet-12 as feature extractor, our MML(KL) achieves 65.55% and 81.86% in 5-way 1-shot and 5-way 5-shot respectively, which achieves competitive performance. <ref type="table">Table 3</ref> summarizes the results on the tieredImageNet dataset. Our results outperform the state-of-the-art by a significant margin. For example, with Conv-64F feature extrator, our MML(Wass) achieves new state-of-the-art results on tieredImageNet benchmark (57.89% -up 5.3% over the previous best) on 5-way 1-shot and (75.49% -up 1.4% over the previous best) on 5-way 5-shot.</p><p>Results on fine-grained datasets. <ref type="table">Table 4</ref> evaluates our method on three fine-grained datasets, i.e., Stanford Dogs, Stanford Cars, and CUB Birds. It can be seen that the proposed MML obtains significant improvements compared with previous state-of-the-art methods. For the 5-way 1shot task, the proposed MML(KL) obtains state-of-the-art performance on all three fine-grained datasets. And for the The reason why our MML can achieve these state-of-theart performances is that MML can measure the semantic similarities on multiple levels, i.e., part-level, pixel-level, and distribution-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Cross-Domain Few-Shot Learning</head><p>Cross-domain few-shot learning assumes that images in the training set and test set can come from different domains. We proceed by meta-training the models on the miniImageNet training set and evaluate the model on three fine-grained test sets. This setting of domain shift results in a large margin between the distribution of training set and test set. We use the same dataset partitioning as shown in   <ref type="table">Table 5</ref> gives the quantitative results. It can be seen that MML outperforms all the baseline methods. This verifies that our model is more robust under these more challenging tasks. The reason why our MML can achieve these results is that our MML can capture more comprehensive semantic similarities than previous metric-learning based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Ablation Study</head><p>To further explore the effect of the multi-level metric learning module, we prune any of three similarity branches in the multi-level metric-learning module. Specifically, we remove one or two branches from the multi-level metriclearning module and experiment on the miniImageNet and tieredImageNet datasets.</p><p>As seen in <ref type="table">Table 6</ref>, each part of the MML is indispensable. It can be observed that the accuracy of few-shot image recognition using only one level of features is very low. The results were significantly improved when two or three lev-els of features were used together, and the results were best when all three levels were used together. Specifically, compared with the method that only using pixel-level features, our MML gains 7.6% and 3.1% improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Complexity Analysis</head><p>As shown in <ref type="table" target="#tab_3">Table 7</ref>, we compare the trainable parameters to prove that our model is both simple and effective. Although our multi-level metric-learning module has three branches, each branch has no trainable parameters. Therefore, if we ignore three trainable parameters in the fusion layer, our MML is non-parametric if not considering the feature extractor F θ . In the part-level and pixel-level similarity metrics, we need to choose suitable ξ and k. For this purpose, we conduct a contrast experiment on miniImageNet dataset under both 5-way 1-shot and 5-way 5-shot settings by varying the value of ξ ∈ {1, 3, 5, 7, 9} and the value of k ∈ {1, 3, 5, 7, 9}. Experimental results are shown in <ref type="figure">Figure 3</ref>. It can be seen that when ξ = 1 and k = 1, the experimental result of MML(KL) is the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we revisit the metric-learning based method and proposed a novel Multi-level Metric Learning (MML) method for few-shot image recognition, aiming to capture more comprehensive semantic similarities. Specifically, the MML can measure the semantic similarities on multiple levels and produce more discriminative features. Extensive experiments show the effectiveness and the superiority of the MML.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Part-level Distribution-level Figure 1. An example of feature representation at different level. (Best view in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The framework of MML under the 5-way 1-shot image classification setting. The model mainly consists of three modules: the feature extractor F θ to learn local representations (LRs), the multi-level metric-learning module to capture the semantic similarity at a different level, i.e., pixel-level, part-level, and distribution-level, and the fusion layer Fω to fuse three kinds of relation score to get the final similarity score. (Best view in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset N all N train N val N test The splits of evaluation datasets. N all is the number of all classes. Ntrain, N val and Ntest indicate the number of classes in training set, validation set and test set.</figDesc><table><row><cell>miniImageNet</cell><cell>100</cell><cell>64</cell><cell>16</cell><cell>20</cell></row><row><cell cols="2">tieredImageNet 608</cell><cell>351</cell><cell>97</cell><cell>160</cell></row><row><cell>Stanford Dogs</cell><cell>120</cell><cell>70</cell><cell>20</cell><cell>30</cell></row><row><cell>Stanford Cars</cell><cell>196</cell><cell>130</cell><cell>17</cell><cell>49</cell></row><row><cell>CUB Birds</cell><cell>200</cell><cell>100</cell><cell>50</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with other state-of-the-art methods with 95% confidence intervals on miniImageNet. The third column shows which kind of embedding is employed. The fourth column shows which type of the method belongs to, i.e, meta-learning based, metric-learning based, and other kinds of methods. (Top two performances are shown in red and blue.)</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell>Venue</cell><cell>Backbone</cell><cell cols="2">Type 5-way 1-shot 5-way 5-shot</cell></row><row><cell></cell><cell>MAML+L2F [2]</cell><cell></cell><cell>CVPR'20</cell><cell>Conv-32F</cell><cell>Meta</cell><cell>52.10±0.49</cell><cell>69.38±0.46</cell></row><row><cell></cell><cell>BOIL [24]</cell><cell></cell><cell>ICLR'21</cell><cell>Conv-64F</cell><cell>Meta</cell><cell>49.61±0.16</cell><cell>66.45±0.37</cell></row><row><cell cols="2">MatchingNet [33]</cell><cell></cell><cell cols="3">NeurIPS'16 Conv-64F Metric</cell><cell>43.56±0.84</cell><cell>55.31±0.73</cell></row><row><cell></cell><cell>ProtoNet [30]</cell><cell></cell><cell cols="3">NeurIPS'17 Conv-64F Metric</cell><cell>49.42±0.78</cell><cell>68.20±0.66</cell></row><row><cell></cell><cell>RelationNet [32]</cell><cell></cell><cell>CVPR'18</cell><cell cols="2">Conv-64F Metric</cell><cell>50.44±0.82</cell><cell>65.32±0.70</cell></row><row><cell></cell><cell>CovaMNet [21]</cell><cell></cell><cell>AAAI'19</cell><cell cols="2">Conv-64F Metric</cell><cell>51.19±0.76</cell><cell>67.65±0.63</cell></row><row><cell></cell><cell>DN4 [20]</cell><cell></cell><cell>CVPR'19</cell><cell cols="2">Conv-64F Metric</cell><cell>51.24±0.74</cell><cell>71.02±0.64</cell></row><row><cell></cell><cell>CTM [18]</cell><cell></cell><cell>CVPR'19</cell><cell cols="2">Conv-64F Metric</cell><cell>41.62±0.00</cell><cell>58.77±0.00</cell></row><row><cell></cell><cell>DSN [29]</cell><cell></cell><cell>CVPR'20</cell><cell cols="2">Conv-64F Metric</cell><cell>51.78±0.96</cell><cell>68.99±0.69</cell></row><row><cell></cell><cell>ADM [19]</cell><cell></cell><cell>IJCAI'20</cell><cell cols="2">Conv-64F Metric</cell><cell>54.26±0.63</cell><cell>72.54±0.50</cell></row><row><cell cols="3">Align(Centroid) [1]</cell><cell>ECCV'20</cell><cell cols="2">Conv-64F Metric</cell><cell>53.14±1.06</cell><cell>71.45±0.72</cell></row><row><cell></cell><cell>Neg-Margin [22]</cell><cell></cell><cell>ECCV'20</cell><cell cols="2">Conv-64F Others</cell><cell>52.68±0.76</cell><cell>70.41±0.66</cell></row><row><cell></cell><cell>Two-stage [4]</cell><cell></cell><cell>T-IP'2020</cell><cell cols="2">Conv-64F Others</cell><cell>52.68±0.51</cell><cell>70.91±0.85</cell></row><row><cell></cell><cell>MML(KL)</cell><cell></cell><cell>Ours</cell><cell cols="2">Conv-64F Metric</cell><cell>55.14±0.63</cell><cell>73.33±0.50</cell></row><row><cell></cell><cell>MML(Wass)</cell><cell></cell><cell>Ours</cell><cell cols="2">Conv-64F Metric</cell><cell>54.94±0.64</cell><cell>72.81±0.50</cell></row><row><cell></cell><cell>ProtoNet [30]</cell><cell></cell><cell cols="3">NeurIPS'17 ResNet-12 Metric</cell><cell>60.37±0.83</cell><cell>78.02 ±0.57</cell></row><row><cell></cell><cell>CTM [18]</cell><cell></cell><cell>CVPR'19</cell><cell cols="2">ResNet-12 Metric</cell><cell>64.12±0.82</cell><cell>80.51±0.13</cell></row><row><cell></cell><cell>DSN [29]</cell><cell></cell><cell>CVPR'20</cell><cell cols="2">ResNet-12 Metric</cell><cell>62.64±0.66</cell><cell>78.73±0.45</cell></row><row><cell></cell><cell>MetaOptNet [17]</cell><cell></cell><cell>CVPR'19</cell><cell cols="2">ResNet-12 Others</cell><cell>62.64±0.61</cell><cell>78.63±0.46</cell></row><row><cell></cell><cell>Neg-Margin [22]</cell><cell></cell><cell>ECCV'20</cell><cell cols="2">ResNet-12 Others</cell><cell>63.85±0.81</cell><cell>81.57±0.56</cell></row><row><cell></cell><cell>DeepEMD [41]</cell><cell></cell><cell>CVPR'20</cell><cell cols="2">ResNet-12 Metric</cell><cell>65.91±0.82</cell><cell>79.74±0.56</cell></row><row><cell></cell><cell>MML(KL)</cell><cell></cell><cell>Ours</cell><cell cols="2">ResNet-12 Metric</cell><cell>65.55±0.77</cell><cell>81.86±0.51</cell></row><row><cell></cell><cell>MML(Wass)</cell><cell></cell><cell>Ours</cell><cell cols="2">ResNet-12 Metric</cell><cell>65.32±0.76</cell><cell>81.77±0.53</cell></row><row><cell>Model</cell><cell>Backbone</cell><cell cols="3">5-Way Accuracy(%) 1-shot 5-shot</cell><cell cols="2">Car , and a comparable performance on CUB Birds. Specif-ically, compared with pixel-level metric-learning based</cell></row><row><cell cols="5">ProtoNet [30] RelationNet [32] Conv-64F 54.48±0.93 71.32±0.78 Conv-64F 48.67±0.87 69.57±0.75 CovaMNet [21] Conv-64F 54.98±0.90 71.51±0.75 DN4 [20] Conv-64F 53.37±0.86 74.45±0.70</cell><cell cols="2">methods (i.e., CovaMNet, DN4, and LRPABN), MML(KL) is 20.3%, 20.2%, and 0.4% better than the best one of them on Stanford Dogs, Stanford Cars, and CUB Birds under 5-way 1-shot setting.</cell></row><row><cell>ADM [19]</cell><cell cols="4">Conv-64F 56.01±0.69 75.18±0.56</cell><cell></cell></row><row><cell>MML(KL)</cell><cell cols="4">Conv-64F 57.37±0.70 74.98±0.55</cell><cell></cell></row><row><cell>MML(Wass)</cell><cell cols="4">Conv-64F 57.89±0.69 75.49±0.55</cell><cell></cell></row><row><cell>ProtoNet [30]</cell><cell cols="4">ResNet-12 68.37±0.23 83.43±0.16</cell><cell></cell></row><row><cell cols="5">RelationNet [32] ResNet-12 58.99±0.86 75.78±0.76</cell><cell></cell></row><row><cell>DSN [29]</cell><cell cols="4">ResNet-12 67.39±0.82 82.85±0.56</cell><cell></cell></row><row><cell cols="5">DeepEMD [41] ResNet-12 71.16±0.87 83.95±0.58</cell><cell></cell></row><row><cell>MML(KL)</cell><cell cols="4">ResNet-12 71.32±0.75 84.15±0.59</cell><cell></cell></row><row><cell>MML(Wass)</cell><cell cols="4">ResNet-12 71.82±0.76 84.53±0.61</cell><cell></cell></row><row><cell cols="5">Table 3. Comparison with other state-of-the-art methods with 95%</cell><cell></cell></row><row><cell cols="5">confidence intervals on tieredImageNet. (Top two performances</cell><cell></cell></row><row><cell cols="2">are shown in red and blue.)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">5-way 5-shot task, the proposed MML(KL) also achieves</cell><cell></cell></row><row><cell cols="5">state-of-the-art performance on Stanford Dogs and Stanford</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>53±0.98 41.25±1.03 24.88±0.95 40.55±0.98 33.75±0.95 53.14±0.77 ProtoNet [30] 33.24±0.99 42.16±1.02 31.47±1.03 48.75±1.02 42.09±0.88 62.02±0.65 RelationNet [32] 31.99±0.55 50.35±0.49 37.14±0.51 44.95±0.43 38.25±0.99 57.73±0.76 CovaMNet [21] 38.03±0.77 56.22±0.63 32.33±0.75 49.86±0.63 43.11±0.65 63.22±0.69 DN4 [20] 38.37±0.72 56.50±0.72 32.51±0.62 50.19±0.73 43.65±0.77 63.78±0.69 MML(KL) 40.94±0.57 58.43±0.54 34.05±0.51 53.19±0.56 44.86±0.58 65.20±0.56 MML(Wass) 41.26±0.60 57.73±0.57 34.22±0.50 52.80±0.54 44.63±0.58 64.98±0.52 Table 5. Experimental results compared with other methods on miniImageNet under cross-domain few-shot learning setting. (Backbone: Conv-64F. Top two performances are shown in red and blue.)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">5-Way Accuracy(%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Stanford Dogs</cell><cell cols="2">Stanford Cars</cell><cell cols="2">CUB Birds</cell></row><row><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5shot</cell></row><row><cell>MatchingNet [33]</cell><cell>35.80±0.99</cell><cell>47.50±1.03</cell><cell>34.80±0.98</cell><cell>44.70±1.03</cell><cell>61.16±0.89</cell><cell>72.86±0.70</cell></row><row><cell>ProtoNet [30]</cell><cell>37.59±1.00</cell><cell>48.19±1.03</cell><cell>40.90±1.01</cell><cell>52.93±1.03</cell><cell>51.31±0.91</cell><cell>70.77±0.69</cell></row><row><cell>GNN [28]</cell><cell>46.98±0.98</cell><cell>62.27±0.95</cell><cell>55.85±0.97</cell><cell>71.25±0.89</cell><cell>51.83±0.98</cell><cell>63.69±0.94</cell></row><row><cell>MAML [7]</cell><cell>44.81±0.34</cell><cell>58.68±0.31</cell><cell>47.22±0.39</cell><cell>61.21±0.28</cell><cell>55.92±0.95</cell><cell>72.09±0.76</cell></row><row><cell>RelationNet [32]</cell><cell>43.33±0.42</cell><cell>55.23±0.41</cell><cell>47.67±0.47</cell><cell>60.59±0.40</cell><cell>62.45±0.98</cell><cell>76.11±0.69</cell></row><row><cell>adaCNN [23]</cell><cell>41.87±0.42</cell><cell>53.93±0.44</cell><cell>42.14±0.41</cell><cell>50.12±0.34</cell><cell>56.57±0.47</cell><cell>61.21±0.42</cell></row><row><cell>PCM [36]</cell><cell>28.78±2.33</cell><cell>46.92±2.00</cell><cell>29.63±2.38</cell><cell>52.28±1.46</cell><cell>42.10±1.96</cell><cell>62.48±1.21</cell></row><row><cell>CovaMNet [21]</cell><cell>49.10±0.76</cell><cell>63.04±0.65</cell><cell>56.65±0.86</cell><cell>71.33±0.62</cell><cell>60.58±0.69</cell><cell>74.24±0.68</cell></row><row><cell>DN4 [20]</cell><cell>45.41±0.76</cell><cell>63.51±0.62</cell><cell>59.84±0.80</cell><cell>88.65±0.44</cell><cell>52.79±0.86</cell><cell>81.45±0.70</cell></row><row><cell>PABN +cpt [11]</cell><cell>45.65±0.71</cell><cell>61.24±0.62</cell><cell>54.44±0.71</cell><cell>67.36±0.61</cell><cell>63.56±0.79</cell><cell>75.35±0.58</cell></row><row><cell>LRPABN +cpt [11]</cell><cell>45.72±0.75</cell><cell>60.94±0.66</cell><cell>60.28±0.76</cell><cell>73.29±0.58</cell><cell>63.63±0.77</cell><cell>76.06±0.58</cell></row><row><cell>MML(KL)</cell><cell>59.05±0.68</cell><cell>75.59±0.51</cell><cell>72.43±0.65</cell><cell>91.05±0.30</cell><cell>63.86±0.67</cell><cell>80.73 ±0.46</cell></row><row><cell>MML(Wass)</cell><cell>58.07±0.68</cell><cell>75.15±0.49</cell><cell>72.40±0.63</cell><cell>91.01±0.31</cell><cell>63.64±0.69</cell><cell>80.63 ±0.47</cell></row><row><cell cols="7">Table 4. Experimental results compared with other methods on three fine-grained datasets. (Backbone: Conv-64F. Top two performances</cell></row><row><cell>are shown in red and blue.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">5-Way Accuracy(%)</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">miniImageNet→Dogs</cell><cell cols="2">miniImageNet→Cars</cell><cell cols="2">miniImageNet→Birds</cell></row><row><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5shot</cell></row><row><cell cols="2">MatchingNet [33] 31.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>Our model achieved a great improvement in accuracy while maintaining the same number of trainable parameters as MatchingNet, ProtoNet, CovaMNet, and DN4. Specifically, on the Stanford Dogs dataset, our MML(KL) obtains 64.9%, 57.1%, 20.3% and 30.0% improvements over MatchingNet, ProtoNet, CovaM-82±0.65 65.33±0.98 51.04±0.93 70.25±1.13 51.32±0.73 70.95±0.67 53.22±0.85 74.33±0.71 52.87±0.64 69.32±0.50 55.43±0.68 74.21±0.56 50.31±0.62 67.62±0.53 52.88±0.71 73.67±0.57 51.95±0.68 71.35±0.65 53.65±0.87 74.43±0.76 53.15±0.62 70.16±0.53 55.74±0.71 74.07±0.55 51.33±0.65 68.56±0.55 53.62±0.76 74.32±0.59 53.82±0.63 72.13±0.50 55.69±0.71 74.46±0.56 53.42±0.59 72.08±0.52 54.85±0.68 73.85±0.53 55.14±0.63 73.12±0.69 57.37±0.70 74.98±0.55 54.94±0.64 72.81±0.50 57.89±0.69 75.49±0.55 Table 6. Ablation study on miniImageNet. (Backbone: Conv-64F.)Figure 3. Influence of superparameters ξ and k. The number of trainable parameters in different models and the corresponding classification accuracies on Stanford Dogs. (Backbone: Conv-64F. Top two performances are shown in red and blue.)Net, and DN4 under the 5-way 1-shot few-shot learning setting, respectively. Moreover, on the 5-way 5-shot task, our MML(KL) obtains 59.1%, 56.9%, 19.9% and 19.0% im-provements over MatchingNet, ProtoNet, CovaMNet, and DN4 respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">5-Way Accuracy(%)</cell></row><row><cell>Part</cell><cell>Pixel</cell><cell>Dis.(KL) Dis.(Wass)</cell><cell cols="2">miniImageNet</cell><cell>tieredImageNet</cell></row><row><cell></cell><cell></cell><cell cols="2">1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell></cell><cell></cell><cell>49.</cell><cell></cell><cell></cell></row></table><note>5.8. Influence of Superparameter ξ and k .</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/chenhaoxing/M2L.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Table 1. To verify the effectiveness of our model, we compared five classical metric-learning based few-shot learning methods: MatchingNet, ProtoNet, RelationNet, CovaMNet, and DN4.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Associative alignment for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Afrasiyabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-François</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Gagné</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12350</biblScope>
			<biblScope unit="page" from="18" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to forget for meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokil</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2376" to="2384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multi-scale adaptive task attention network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxiong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14479</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A two-stage approach to few-shot learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debasmit</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>George Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3336" to="3350" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Melr: Meta-learning via modeling episode-level relationships for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyi</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent. (ICLR)</title>
		<meeting>Int. Conf. Learn. Represent. (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Metalearning with warped gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Flennerhag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent. (ICLR)</title>
		<meeting>Int. Conf. Learn. Represent. (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wasserstein CNN: learning invariant features for NIR-VIS face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1761" to="1773" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Low-rank pairwise alignment bilinear network for few-shot fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01313</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops (CVPRW)</title>
		<meeting>CVPR Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent. (ICLR)</title>
		<meeting>Int. Conf. Learn. Represent. (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML Workshops (ICMLW)</title>
		<meeting>ICML Workshops (ICMLW)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Workshops (ICCVW)</title>
		<meeting>ICCV Workshops (ICCVW)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for fewshot learning by category traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Asymmetric distribution measure for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artif. Intell. (IJ-CAI)</title>
		<meeting>Int. Joint Conf. Artif. Intell. (IJ-CAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2957" to="2963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based image-to-class measure for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7260" to="7268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distribution consistency based covariance metric networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell. (AAAI)</title>
		<meeting>AAAI Conf. Artif. Intell. (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8642" to="8649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Negative margin matters: Understanding margin in few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12349</biblScope>
			<biblScope unit="page" from="438" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn. (ICML)</title>
		<editor>Jennifer G. Dy and Andreas Krause</editor>
		<meeting>Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3661" to="3670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Boil: Towards representation change for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se-Young</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent. (ICLR)</title>
		<meeting>Int. Conf. Learn. Represent. (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<editor>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent. (ICLR)</title>
		<meeting>Int. Conf. Learn. Represent. (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent. (ICLR)</title>
		<meeting>Int. Conf. Learn. Represent. (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garcia</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">Bruna</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Estrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent. (ICLR)</title>
		<meeting>Int. Conf. Learn. Represent. (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive subspaces for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4135" to="4144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst. (NeurIPS)</title>
		<meeting>Adv. Neural Inf. ess. Syst. (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst. (NeurIPS)</title>
		<meeting>Adv. Neural Inf. ess. Syst. (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for few-shot image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad Shahbaz</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4452" to="4461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Piecewise classifier mappings: Learning fine-grained learners for novel categories with few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6116" to="6125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. CBAM: convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-supervised domainaware generative network for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12764" to="12773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Episode-based prototype generating network for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14032" to="14041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transmatch: A transfer-learning scheme for semi-supervised few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12853" to="12861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12200" to="12210" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
