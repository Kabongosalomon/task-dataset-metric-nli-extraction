<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Graph Completion via Complex Tensor Factorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
							<email>theo.trouillon@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Univ. Grenoble Alpes</orgName>
								<address>
									<addrLine>700 avenue Centrale</addrLine>
									<postCode>38401</postCode>
									<settlement>Saint Martin d&apos;Hères</settlement>
									<country>France Christopher R. Dance</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
							<email>eric.gaussier@imag.fr</email>
							<affiliation key="aff1">
								<orgName type="department">NAVER LABS Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan, Francé</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
							<email>j.welbl@cs.ucl.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">Univ. Grenoble Alpes</orgName>
								<address>
									<addrLine>700 avenue Centrale</addrLine>
									<postCode>38401</postCode>
									<settlement>Saint Martin d&apos;Hères</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<email>s.riedel@cs.ucl.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="institution">University College London</orgName>
								<address>
									<addrLine>Gower St</addrLine>
									<postCode>WC1E 6BT</postCode>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
							<email>g.bouchard@cs.ucl.ac.uk</email>
							<affiliation key="aff4">
								<orgName type="institution">University College London</orgName>
								<address>
									<addrLine>115 Hampstead Road, Gower St</addrLine>
									<postCode>NW1 3EE, WC1E 6BT</postCode>
									<settlement>London, London</settlement>
									<country>United Kingdom, United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Graph Completion via Complex Tensor Factorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>complex embeddings</term>
					<term>tensor factorization</term>
					<term>knowledge graph</term>
					<term>matrix comple- tion</term>
					<term>statistical relational learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In statistical relational learning, knowledge graph completion deals with automatically understanding the structure of large knowledge graphs-labeled directed graphsand predicting missing relationships-labeled edges. State-of-the-art embedding models propose different trade-offs between modeling expressiveness, and time and space complexity. We reconcile both expressiveness and complexity through the use of complex-valued embeddings and explore the link between such complex-valued embeddings and unitary diagonalization. We corroborate our approach theoretically and show that all real square matrices-thus all possible relation/adjacency matrices-are the real part of some unitarily diagonalizable matrix. This results opens the door to a lot of other applications of square matrices factorization. Our approach based on complex embeddings is arguably simple, as it only involves a Hermitian dot product, the complex counterpart of the standard dot product between real vectors, whereas other methods resort to more and more complicated composition functions to increase their expressiveness. The proposed complex embeddings are scalable to large data sets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks. 1 Trouillon, Dance, Gaussier, Welbl, Riedel and Bouchard bases-has stimulated research into predicting missing entries, a task known as link prediction or knowledge graph completion. The need for high quality predictions required by link prediction applications made it progressively become the main problem in statistical relational learning <ref type="bibr" target="#b28">(Getoor and Taskar, 2007)</ref>, a research field interested in relational data representation and modeling.</p><p>Knowledge graphs were born with the advent of the Semantic Web, pushed by the World Wide Web Consortium (W3C) recommendations. Namely, the Resource Description Framework (RDF) standard, that underlies knowledge graphs' data representation, provides for the first time a common framework across all connected information systems to share their data under the same paradigm. Being more expressive than classical relational databases, all existing relational data can be translated into RDF knowledge graphs <ref type="bibr" target="#b54">(Sahoo et al., 2009)</ref>.</p><p>Knowledge graphs express data as a directed graph with labeled edges (relations) between nodes (entities). Natural redundancies between the recorded relations often make it possible to fill in the missing entries of a knowledge graph. As an example, the relation CountryOfBirth could not be recorded for all entities, but it can be inferred if the relation CityOfBirth is known. The goal of link prediction is the automatic discovery of such regularities. However, many relations are non-deterministic: the combination of the two facts IsBornIn(John,Athens) and IsLocatedIn(Athens,Greece) does not always imply the fact HasNationality(John,Greece). Hence, it is natural to handle inference probabilistically, and jointly with other facts involving these relations and entities. To this end, an increasingly popular method is to state the knowledge graph completion task as a 3D binary tensor completion problem, where each tensor slice is the adjacency matrix of one relation in the knowledge graph, and compute a decomposition of this partially-observed tensor from which its missing entries can be completed.</p><p>Factorization models with low-rank embeddings were popularized by the Netflix challenge <ref type="bibr" target="#b35">(Koren et al., 2009)</ref>. A partially-observed matrix or tensor is decomposed into a product of embedding matrices with much smaller dimensions, resulting in fixed-dimensional vector representations for each entity and relation in the graph, that allow completion of the missing entries. For a given fact r(s,o) in which the subject entity s is linked to the object entity o through the relation r, a score for the fact can be recovered as a multilinear product between the embedding vectors of s, r and o, or through more sophisticated composition functions <ref type="bibr" target="#b44">(Nickel et al., 2016a)</ref>.</p><p>Binary relations in knowledge graphs exhibit various types of patterns: hierarchies and compositions like FatherOf, OlderThan or IsPartOf, with strict/non-strict orders or preorders, and equivalence relations like IsSimilarTo. These characteristics maps to different combinations of the following properties: reflexivity/irreflexivity, symmetry/antisymmetry and transitivity. As described in <ref type="bibr" target="#b9">Bordes et al. (2013a)</ref>, a relational model should (i) be able to learn all combinations of such properties, and (ii) be linear in both time and memory in order to scale to the size of present-day knowledge graphs, and keep up with their growth.</p><p>A natural way to handle any possible set of relations is to use the classic canonical polyadic (CP) decomposition <ref type="bibr" target="#b30">(Hitchcock, 1927)</ref>, which yields two different embeddings for each entity and thus low prediction performances as shown in Section 5. With unique entity embeddings, multilinear products scale well and can naturally handle both symmetry and (ir)-reflexivity of relations, and when combined with an appropriate loss function,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Web-scale knowledge graph provide a structured representation of world knowledge, with projects such as the Google Knowledge Vault <ref type="bibr" target="#b21">(Dong et al., 2014)</ref>. They enable a wide range of applications including recommender systems, question answering and automated personal agents. The incompleteness of these knowledge graphs-also called knowledge dot products can even handle transitivity <ref type="bibr" target="#b13">(Bouchard et al., 2015)</ref>. However, dealing with antisymmetric-and more generally asymmetric-relations has so far almost always implied superlinear time and space complexity <ref type="bibr" target="#b42">(Nickel et al., 2011;</ref><ref type="bibr" target="#b55">Socher et al., 2013</ref>) (see Section 2), making models prone to overfitting and not scalable. Finding the best trade-off between expressiveness, generalization and complexity is the keystone of embedding models.</p><p>In this work, we argue that the standard dot product between embeddings can be a very effective composition function, provided that one uses the right representation: instead of using embeddings containing real numbers, we discuss and demonstrate the capabilities of complex embeddings. When using complex vectors, that is vectors with entries in C, the dot product is often called the Hermitian (or sesquilinear) dot product, as it involves the conjugate-transpose of one of the two vectors. As a consequence, the dot product is not symmetric any more, and facts about one relation can receive different scores depending on the ordering of the entities involved in the fact. In summary, complex embeddings naturally represent arbitrary relations while retaining the efficiency of a dot product, that is linearity in both space and time complexity. This paper extends a previously published article <ref type="bibr" target="#b60">(Trouillon et al., 2016)</ref>. This extended version adds proofs of existence of the proposed model in both single and multi-relational settings, as well as proofs of the non-uniqueness of the complex embeddings for a given relation. Bounds on the rank of the proposed decomposition are also demonstrated and discussed. The learning algorithm is provided in more details, and more experiments are provided, especially regarding the training time of the models.</p><p>The remainder of the paper is organized as follows. We first provide justification and intuition for using complex embeddings in the square matrix case (Section 2), where there is only a single type of relation between entities, and show the existence of the proposed decomposition for all possible relations. The formulation is then extended to a stacked set of square matrices in a third-order tensor to represent multiple relations (Section 3). The stochastic gradient descent algorithm used to learn the model is detailed in Section 4, where we present an equivalent reformulation of the proposed model that involves only real embeddings. This should help practitioners when implementing our method, without requiring the use of complex numbers in their software implementation. We then describe experiments on large-scale public benchmark knowledge graphs in which we empirically show that this representation leads not only to simpler and faster algorithms, but also gives a systematic accuracy improvement over current state-of-the-art alternatives (Section 5). Related work is discussed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Relations as the Real Parts of Low-Rank Normal Matrices</head><p>We consider in this section a simplified link prediction task with a single relation, and introduce complex embeddings for low-rank matrix factorization.</p><p>We will first discuss the desired properties of embedding models, show how this problem relates to the spectral theorems, and discuss the classes of matrices these theorems encompass in the real and in the complex case. We then propose a new matrix decomposition-to the best of our knowledge-and a proof of its existence for all real square matrices. Finally we discuss the rank of the proposed decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modeling Relations</head><p>Let E be a set of entities, with |E| = n. The truth of the single relation holding between two entities is represented by a sign value y so ∈ {−1, 1}, where 1 represents true facts and -1 false facts, s ∈ E is the subject entity and o ∈ E is the object entity. The probability for the relation holding true is given by</p><formula xml:id="formula_0">P (y so = 1) = σ(x so )<label>(1)</label></formula><p>where X ∈ R n×n is a latent matrix of scores indexed by the subject (rows) and object entities (columns), Y is a partially-observed sign matrix indexed in identical fashion, and σ is a suitable sigmoid function. Throughout this paper we used the logistic inverse link function σ(x) = 1 1+e −x .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Handling Both Asymmetry and Unique Entity Embeddings</head><p>In this work we pursue three objectives: finding a generic structure for X that leads to (i) a computationally efficient model, (ii) an expressive enough approximation of common relations in real world knowledge graphs, and (iii) good generalization performances in practice. Standard matrix factorization approximates X by a matrix product U V , where U and V are two functionally-independent n × K matrices, K being the rank of the matrix. Within this formulation it is assumed that entities appearing as subjects are different from entities appearing as objects. In the Netflix challenge <ref type="bibr" target="#b35">(Koren et al., 2009</ref>) for example, each row u i corresponds to the user i and each column v j corresponds to the movie j. This extensively studied type of model is closely related to the singular value decomposition (SVD) and fits well with the case where the matrix X is rectangular. However, in many knowledge graph completion problems, the same entity i can appear as both subject or object and will have two different embedding vectors, u i and v i , depending on whether it appears as subject or object of a relation. It seems natural to learn unique embeddings of entities, as initially proposed by <ref type="bibr" target="#b42">Nickel et al. (2011)</ref> and <ref type="bibr" target="#b8">Bordes et al. (2011)</ref> and since then used systematically in other prominent approaches <ref type="bibr" target="#b10">(Bordes et al., 2013b;</ref><ref type="bibr" target="#b64">Yang et al., 2015;</ref><ref type="bibr" target="#b55">Socher et al., 2013)</ref>. In the factorization setting, using the same embeddings for left-and right-side factors boils down to a specific case of eigenvalue decomposition: orthogonal diagonalization.</p><p>Definition 1 A real square matrix X ∈ R n×n is orthogonally diagonalizable if it can be written as X = EW E , where E, W ∈ R n×n , W is diagonal, and E orthogonal so that EE = E E = I where I is the identity matrix.</p><p>The spectral theorem for symmetric matrices tells us that a matrix is orthogonally diagonalizable if and only if it is symmetric <ref type="bibr" target="#b14">(Cauchy, 1829)</ref>. It is therefore often used to approximate covariance matrices, kernel functions and distance or similarity matrices.</p><p>However as previously stated, this paper is explicitly interested in problems where matrices-and thus the relation patterns they represent-can also be antisymmetric, or even not have any particular symmetry pattern at all (asymmetry). In order to both use a unique embedding for entities and extend the expressiveness to asymmetric relations, researchers have generalised the notion of dot products to scoring functions, also known as</p><formula xml:id="formula_1">Model Scoring Function φ Relation Parameters O time O space CP (Hitchcock, 1927) w r , u s , v o w r ∈ R K O(K) O(K)</formula><p>RESCAL <ref type="bibr" target="#b42">(Nickel et al., 2011)</ref> </p><formula xml:id="formula_2">e T s W r e o W r ∈ R K 2 O(K 2 ) O(K 2 ) TransE (Bordes et al., 2013b) −||(e s + w r ) − e o || p w r ∈ R K O(K) O(K)</formula><p>NTN <ref type="bibr" target="#b55">(Socher et al., 2013</ref>)</p><formula xml:id="formula_3">u r f (e s W [1..D] r e o +V r e s e o +b r ) W r ∈ R K 2 D , b r ∈ R K V r ∈ R 2KD , u r ∈ R K O(K 2 D) O(K 2 D)</formula><p>DistMult <ref type="bibr" target="#b64">(Yang et al., 2015)</ref> w x is the complex conjugate, and D is an additional latent dimension of the NTN model. F and F −1 denote respectively the Fourier transform and its inverse, is the element-wise product between two vectors, Re(.) denotes the real part of a complex vector, and ·, ·, · denotes the trilinear product.</p><formula xml:id="formula_4">r , e s , e o w r ∈ R K O(K) O(K) HolE (Nickel et al., 2016b) w T r (F −1 [F[e s ] F[e o ]])) w r ∈ R K O(K log K) O(K) ComplEx (this paper) Re( w r , e s ,ē o ) w r ∈ C K O(K) O(K)</formula><p>composition functions, that allow more general combinations of embeddings. We briefly recall several examples of scoring functions in <ref type="table" target="#tab_0">Table 1</ref>, as well as the extension proposed in this paper. These models propose different trade-offs between the three essential points:</p><p>• Expressiveness, which is the ability to represent symmetric, antisymmetric and more generally asymmetric relations.</p><p>• Scalability, which means keeping linear time and space complexity scoring function.</p><p>• Generalization, for which having unique entity embeddings is critical.</p><p>RESCAL <ref type="bibr" target="#b42">(Nickel et al., 2011)</ref> and NTN <ref type="bibr" target="#b55">(Socher et al., 2013)</ref> are very expressive, but their scoring functions have quadratic complexity in the rank of the factorization. More recently the HolE model <ref type="bibr" target="#b45">(Nickel et al., 2016b)</ref> proposes a solution that has quasi-linear complexity in time and linear space complexity. DistMult <ref type="bibr" target="#b64">(Yang et al., 2015)</ref> can be seen as a joint orthogonal diagonalization with real embeddings, hence handling only symmetric relations. Conversely, TransE <ref type="bibr" target="#b10">(Bordes et al., 2013b)</ref> handles symmetric relations to the price of strong constraints on its embeddings. The canonical-polyadic decomposition (CP) <ref type="bibr" target="#b30">(Hitchcock, 1927)</ref> generalizes poorly with its different embeddings for entities as subject and as object.</p><p>We reconcile expressiveness, scalability and generalization by going back to the realm of well-studied matrix factorizations, and making use of complex linear algebra, a scarcely used tool in the machine learning community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Decomposition in the Complex Domain</head><p>We introduce a new decomposition of real square matrices using unitary diagonalization, the generalization of orthogonal diagonalization to complex matrices. This allows decomposition of arbitrary real square matrices with unique representations of rows and columns.</p><p>Let us first recall some notions of complex linear algebra as well as specific cases of diagonalization of real square matrices, before building our proposition upon these results.</p><p>A complex-valued vector x ∈ C K , with x = Re(x) + iIm(x) is composed of a real part Re(x) ∈ R K and an imaginary part Im(x) ∈ R K , where i denotes the square root of −1. The conjugate x of a complex vector inverts the sign of its imaginary part:</p><formula xml:id="formula_5">x = Re(x) − iIm(x).</formula><p>Conjugation appears in the usual dot product for complex numbers, called the Hermitian product, or sesquilinear form, which is defined as:</p><formula xml:id="formula_6">u, v :=ū v = Re(u) Re(v) + Im(u) Im(v) +i(Re(u) Im(v) − Im(u) Re(v)) .</formula><p>A simple way to justify the Hermitian product for composing complex vectors is that it provides a valid topological norm in the induced vector space. For example,x x = 0 implies x = 0 while this is not the case for the bilinear form x x as there are many complex vectors x for which x x = 0.</p><p>This yields an interesting property of the Hermitian product concerning the order of the involved vectors: u, v = v, u , meaning that the real part of the product is symmetric, while the imaginary part is antisymmetric.</p><p>For matrices, we shall write X * ∈ C n×m for the conjugate-transpose X * = (X) = X . The conjugate transpose is also often written X † or X H .</p><p>Definition 2 A complex square matrix X ∈ C n×n is unitarily diagonalizable if it can be written as X = EW E * , where E, W ∈ C n×n , W is diagonal, and E is unitary such that EE * = E * E = I.</p><p>Definition 3 A complex square matrix X is normal if it commutes with its conjugatetranspose so that XX * = X * X.</p><p>We can now state the spectral theorem for normal matrices.</p><p>Theorem 1 (Spectral theorem for normal matrices, von Neumann (1929)) Let X be a complex square matrix. Then X is unitarily diagonalizable if and only if X is normal.</p><p>It is easy to check that all real symmetric matrices are normal, and have pure real eigenvectors and eigenvalues. But the set of purely real normal matrices also includes all real antisymmetric matrices (useful to model hierarchical relations such as IsOlder), as well as all real orthogonal matrices (including permutation matrices), and many other matrices that are useful to represent binary relations, such as assignment matrices which represent bipartite graphs. However, far from all matrices expressed as X = EW E * are purely real, and Equation (1) requires the scores X to be purely real.</p><p>As we only focus on real square matrices in this work, let us summarize all the cases where X is real square and X = EW E * if X is unitarily diagonalizable, where E, W ∈ C n×n , W is diagonal and E is unitary:</p><p>• X is symmetric if and only if X is orthogonally diagonalizable and E and W are purely real.</p><p>• X is normal and non-symmetric if and only if X is unitarily diagonalizable and E and W are not both purely real.</p><p>• X is not normal if and only if X is not unitarily diagonalizable.</p><p>We generalize all three cases by showing that, for any X ∈ R n×n , there exists a unitary diagonalization in the complex domain, of which the real part equals X:</p><formula xml:id="formula_7">X = Re(EW E * ) .<label>(2)</label></formula><p>In other words, the unitary diagonalization is projected onto the real subspace.</p><p>Theorem 2 Suppose X ∈ R n×n is a real square matrix. Then there exists a normal matrix Z ∈ C n×n such that Re(Z) = X.</p><p>Proof Let Z := X + iX . Then</p><formula xml:id="formula_8">Z * = X − iX = −i(iX + X) = −iZ , so that ZZ * = Z(−iZ) = (−iZ)Z = Z * Z .</formula><p>Therefore Z is normal.</p><p>Note that there also exists a normal matrix Z = X + iX such that Im(Z) = X. Following Theorem 1 and Theorem 2, any real square matrix can be written as the real part of a complex diagonal matrix through a unitary change of basis.</p><p>Corollary 1 Suppose X ∈ R n×n is a real square matrix. Then there exist E, W ∈ C n×n , where E is unitary, and W is diagonal, such that X = Re(EW E * ).</p><p>Proof From Theorem 2, we can write X = Re(Z), where Z is a normal matrix, and from Theorem 1, Z is unitarily diagonalizable.</p><p>Applied to the knowledge graph completion setting, the rows of E here are vectorial representations of the entities corresponding to rows and columns of the relation score matrix X. The score for the relation holding true between entities s and o is hence</p><formula xml:id="formula_9">x so = Re(e s Wē o )<label>(3)</label></formula><p>where e s , e o ∈ C n and W ∈ C n×n is diagonal. For a given entity, its subject embedding vector is the complex conjugate of its object embedding vector.</p><p>To illustrate this difference of expressiveness with respect to real-valued embeddings, let us consider two complex embeddings e s , e o ∈ C of dimension 1, with arbitrary values: e s = 1 − 2i, and e o = −3 + i; as well as their real-valued, twice-bigger counterparts: e s = 1 −2 ∈ R 2 and e o = −3 1 ∈ R 2 . In the real-valued case, that corresponds to the DistMult model <ref type="bibr" target="#b64">(Yang et al., 2015)</ref>, the score is x so = e s W e o . <ref type="figure" target="#fig_0">Figure 1</ref> represents the heatmaps of the scores x so and x os , as a function of W ∈ C in the complex-valued case, and as a function of W ∈ R 2 diagonal in the real-valued case. In the real-valued case, that is symmetric in the subject and object entities, the scores x so and x os are equal for any value of W ∈ R 2 diagonal. Whereas in the complex-valued case, the variation of W ∈ C allows to score x so and x os with any desired pair of values.</p><p>This decomposition however is non-unique, a simple example of this non-uniqueness is obtained by adding a purely imaginary constant to the eigenvalues. Let X ∈ R n×n , and X = Re(EW E * ) where E is unitary, W is diagonal. Then for any real constant c ∈ R we have:</p><formula xml:id="formula_10">X = Re(E(W + icI)E * ) = Re(EW E * + icEIE * ) = Re(EW E * + icI) = Re(EW E * ) .</formula><p>In general, there are many other possible couples of matrices E and W that preserve the real part of the decomposition. In practice however this is no synonym of low generalization abilities, as many effective matrix and tensor decomposition methods used in machine learning lead to non-unique solutions <ref type="bibr" target="#b47">(Paatero and Tapper, 1994;</ref><ref type="bibr" target="#b42">Nickel et al., 2011)</ref>. In this case also, the learned representations prove useful as shown in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Low-Rank Decomposition</head><p>Addressing knowledge graph completion with data-driven approaches assumes that there is a sufficient regularity in the observed data to generalize to unobserved facts. When formulated as a matrix completion problem, as it is the case in this section, one way of implementing this hypothesis is to make the assumption that the matrix has a low rank or approximately low rank. We first discuss the rank of the proposed decomposition, and then introduce the sign-rank and extend the bound developed on the rank to the sign-rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Rank Upper Bound</head><p>First, we recall one definition of the rank of a matrix <ref type="bibr" target="#b31">(Horn and Johnson, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4</head><p>The rank of an m-by-n complex matrix rank(X) = rank(X ) = k, if X has exactly k linearly independent columns.</p><p>Also note that if X is diagonalizable so that X = EW E −1 with rank(X) = k, then W has k non-zero diagonal entries for some diagonal W and some invertible matrix E. From this it is easy to derive a known additive property of the rank: and e o = −3 1 . By varying W ∈ C, the proposed complex-valued decomposition can attribute any pair of scores to x so and x os , whereas x so = x os for all W ∈ R 2 with the real-valued decomposition.</p><formula xml:id="formula_11">rank(B + C) ≤ rank(B) + rank(C)<label>(4)</label></formula><p>where B, C ∈ C m×n .</p><p>We now show that any rank k real square matrix can be reconstructed from a 2kdimensional unitary diagonalization.</p><p>Corollary 2 Suppose X ∈ R n×n and rank(X) = k. Then there exist E ∈ C n×2k such that the columns of E form an orthonormal basis of C 2k , W ∈ C 2k×2k is diagonal, and X = Re(EW E * ).</p><p>Proof Consider the complex square matrix Z := X + iX . We have rank(iX ) = rank(X ) = rank(X) = k.</p><p>From Equation <ref type="formula" target="#formula_11">(4)</ref>, rank(Z) ≤ rank(X) + rank(iX ) = 2k.</p><p>The proof of Theorem 2 shows that Z is normal.</p><formula xml:id="formula_12">Thus Z = EW E * with E ∈ C n×2k , W ∈ C 2k×2k</formula><p>where the columns of E form an orthonormal basis of C 2k , and W is diagonal.</p><p>Since E is not necessarily square, we replace the unitary requirement of Corollary 1 by the requirement that its columns form an orthonormal basis of its smallest dimension, 2k.</p><p>Also, given that such decomposition always exists in dimension n (Theorem 2), this upper bound is not relevant when rank(X) ≥ n 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Sign-Rank Upper Bound</head><p>Since we encode the truth values of each fact with ±1, we deal with square sign matrices: Y ∈ {−1, 1} n×n . Sign matrices have an alternative rank definition, the sign-rank.</p><p>Definition 5 The sign-rank rank ± (Y ) of an m-by-n sign matrix Y, is the rank of the mby-n real matrix of least rank that has the same sign-pattern as Y, so that</p><formula xml:id="formula_13">rank ± (Y ) := min X∈R m×n {rank(X) | sign(X) = Y } , where sign(X) ij = sign(x ij ).</formula><p>We define the sign function of c ∈ R as</p><formula xml:id="formula_14">sign(c) = 1 if c ≥ 0 −1 otherwise</formula><p>where the value c = 0 is here arbitrarily assigned to 1 to allow zero entries in X, conversely to the stricter usual definition of the sign-rank.</p><p>To make generalization possible, we hypothesize that the true matrix Y has a low signrank, and thus can be reconstructed by the sign of a low-rank score matrix X. The low sign-rank assumption is theoretically justified by the fact that the sign-rank is a natural complexity measure of sign matrices <ref type="bibr" target="#b39">(Linial et al., 2007)</ref> and is linked to learnability <ref type="bibr" target="#b0">(Alon et al., 2016)</ref> and empirically confirmed by the wide success of factorization models <ref type="bibr" target="#b44">(Nickel et al., 2016a)</ref>.</p><p>Using Corollary 2, we can now show that any square sign matrix of sign-rank k can be reconstructed from a rank 2k unitary diagonalization.</p><formula xml:id="formula_15">Corollary 3 Suppose Y ∈ {−1, 1} n×n , rank ± (Y ) = k. Then there exists E ∈ C n×2k , W ∈ C 2k×2k</formula><p>where the columns of E form an orthonormal basis of C 2k , and W is diagonal, such that Y = sign(Re(EW E * )).</p><p>Proof By definition, if rank ± (Y ) = k, there exists a real square matrix X such that rank(X) = k and sign(X) = Y . From Corollary 2, X = Re(EW E * ) where E ∈ C n×2k , W ∈ C 2k×2k where the columns of E form an orthonormal basis of C 2k , and W is diagonal.</p><p>Previous attempts to approximate the sign-rank in relational learning did not use complex numbers. They showed the existence of compact factorizations under conditions on the sign matrix , or only in specific cases <ref type="bibr" target="#b13">(Bouchard et al., 2015)</ref>. In contrast, our results show that if a square sign matrix has sign-rank k, then it can be exactly decomposed through a 2k-dimensional unitary diagonalization.</p><p>Although we can only show the existence of a complex decomposition of rank 2k for a matrix with sign-rank k, the sign rank of Y is often much lower than the rank of Y , as we do not know any matrix Y ∈ {−1, 1} n×n for which rank ± (Y ) &gt; √ n <ref type="bibr" target="#b0">(Alon et al., 2016)</ref>. For example, the n × n identity matrix has rank n, but its sign-rank is only 3! By swapping the columns 2j and 2j − 1 for j in 1, . . . , n 2 , the identity matrix corresponds to the relation marriedTo, a relation known to be hard to factorize over the reals , since the rank is invariant by row/column permutations. Yet our model can express it at most in rank 6, for any n.</p><p>Hence, by enforcing a low-rank K n on EW E * , individual relation scores x so = Re(e s Wē o ) between entities s and o can be efficiently predicted, as e s , e o ∈ C K and W ∈ C K×K is diagonal.</p><p>Finding the K that matches the sign-rank of Y corresponds to finding the smallest K that brings the 0-1 loss on X to 0, as link prediction can be seen as binary classification of the facts. In practice, and as classically done in machine learning to avoid this NP-hard problem, we use a continuous surrogate of the 0-1 loss, in this case the logistic loss as described in Section 4, and validate models on different values of K, as described in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Rank Bound Discussion</head><p>Corollaries 2 and 3 use the aforementioned subadditive property of the rank to derive the 2k upper bound. Let us give an example for which this bound is strictly greater than k.</p><p>Consider the following 2-by-2 sign matrix:</p><formula xml:id="formula_16">Y = −1 −1 1 1 .</formula><p>Not only is this matrix not normal, but one can also easily check that there is no real normal 2-by-2 matrix that has the same sign-pattern as Y . Clearly, Y is a rank 1 matrix since its columns are linearly dependent, hence its sign-rank is also 1. From Corollary 3, we know that there is a normal matrix whose real part has the same sign-pattern as Y , and whose rank is at most 2.</p><p>However, there is no rank 1 unitary diagonalization of which the real part equals Y . Otherwise we could find a 2-by-2 complex matrix Z such that Re(z 11 ) &lt; 0 and Re(z 22 ) &gt; 0, where z 11 = e 1 wē 1 = w|e 1 | 2 , z 22 = e 2 wē 2 = w|e 2 | 2 , e ∈ C 2 , w ∈ C. This is obviously unsatisfiable. This example generalizes to any n-by-n square sign matrix that only has −1 on its first row and is hence rank 1, the same argument holds considering Re(z 11 ) &lt; 0 and Re(z nn ) &gt; 0.</p><p>This example shows that the upper bound on the rank of the unitary diagonalization showed in Corollaries 2 and 3 can be strictly greater than k, the rank or sign-rank, of the decomposed matrix. However, there might be other examples for which the addition of an imaginary part could-additionally to making the matrix normal-create some linear dependence between the rows/columns and thus decrease the rank of the matrix, up to a factor of 2.</p><p>We summarize this section in three points:</p><p>1. The proposed factorization encompasses all possible score matrices X for a single binary relation.</p><p>2. By construction, the factorization is well suited to represent both symmetric and antisymmetric relations.</p><p>3. Relation patterns can be efficiently approximated with a low-rank factorization using complex-valued embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Extension to Multi-Relational Data</head><p>Let us now extend the previous discussion to models with multiple relations. Let R be the set of relations, with |R| = m. We shall now write X ∈ R m×n×n for the score tensor, X r ∈ R n×n for the score matrix of the relation r ∈ R, and Y ∈ {−1, 1} m×n×n for the partially-observed sign tensor. Given one relation r ∈ R and two entities s, o ∈ E, the probability that the fact r(s,o) is true given by:</p><formula xml:id="formula_17">P (y rso = 1) = σ(x rso ) = σ(φ(r, s, o; Θ))<label>(5)</label></formula><p>where φ is the scoring function of the model considered and Θ denotes the model parameters.</p><p>We denote the set of all possible facts (or triples) for a knowledge graph by T = R × E × E. While the tensor X as a whole is unknown, we assume that we observe a set of true and false triples Ω = {((r, s, o), y rso ) | (r, s, o) ∈ T Ω } where y rso ∈ {−1, 1} and T Ω ⊆ T is the set of observed triples. The goal is to find the probabilities of entries y r s o for a set of targeted unobserved triples {(r , s , o ) ∈ T \ T Ω }.</p><p>Depending on the scoring function φ(r, s, o; Θ) used to model the score tensor X, we obtain different models. Examples of scoring functions are given in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Complex Factorization Extension to Tensors</head><p>The single-relation model is extended by jointly factorizing all the square matrices of scores into a 3 rd -order tensor X ∈ R m×n×n , with a different diagonal matrix W r ∈ C K×K for each relation r, and by sharing the entity embeddings E ∈ C n×K across all relations:</p><formula xml:id="formula_18">φ(r, s, o; Θ) = Re(e s W rēo ) = Re( K k=1 w rk e skēok ) = Re( w r , e s ,ē o )<label>(6)</label></formula><p>where K is the rank hyperparameter, e s , e o ∈ C K are the rows in E corresponding to the entities s and o, w r = diag(W r ) ∈ C K is a complex vector, and a, b, c := k a k b k c k is the component-wise multilinear dot product 2 . For this scoring function, the set of parameters Θ is {e i , w r ∈ C K , i ∈ E, r ∈ R}. This resembles the real part of a complex matrix decomposition as in the single-relation case discussed above. However, we now have a different vector of eigenvalues for every relation. Expanding the real part of this product gives:</p><formula xml:id="formula_19">Re( w r , e s ,ē o ) = Re(w r ), Re(e s ), Re(e o ) + Re(w r ), Im(e s ), Im(e o ) + Im(w r ), Re(e s ), Im(e o ) − Im(w r ), Im(e s ), Re(e o ) .<label>(7)</label></formula><p>These equations provide two interesting views of the model:</p><p>• Changing the representation: Equation <ref type="formula" target="#formula_18">(6)</ref> would correspond to DistMult with real embeddings (see <ref type="table" target="#tab_0">Table 1</ref>), but handles asymmetry thanks to the complex conjugate of the object-entity embedding.</p><p>• Changing the scoring function: Equation <ref type="formula" target="#formula_19">(7)</ref> only involves real vectors corresponding to the real and imaginary parts of the embeddings and relations.</p><p>By separating the real and imaginary parts of the relation embedding w r as shown in Equation <ref type="formula" target="#formula_19">(7)</ref>, it is apparent that these parts naturally act as weights on each latent dimension: Re(w r ) over the real part of e o , e s which is symmetric, and Im(w) over the imaginary part of e o , e s which is antisymmetric.</p><p>Indeed, the decomposition of each score matrix X r for each r ∈ R can be written as the sum of a symmetric matrix and an antisymmetric matrix. To see this, let us rewrite the decomposition of each score matrix X r in matrix notation. We write the real part of matrices with primes E = Re(E) and imaginary parts with double primes E = Im(E):</p><formula xml:id="formula_20">X r = Re(EW r E * ) = Re((E + iE )(W r + iW r )(E − iE ) ) = (E W r E + E W r E ) + (E W r E − E W r E ) .<label>(8)</label></formula><p>It is trivial to check that the matrix E W r E + E W r E is symmetric and that the matrix E W r E − E W r E is antisymmetric. Hence this model is well suited to model jointly symmetric and antisymmetric relations between pairs of entities, while still using the same entity representations for subjects and objects. When learning, it simply needs to collapse W r = Im(W r ) to zero for symmetric relations r ∈ R, and W r = Re(W r ) to zero for antisymmetric relations r ∈ R, as X r is indeed symmetric when W r is purely real, and antisymmetric when W r is purely imaginary.</p><p>From a geometrical point of view, each relation embedding w r is an anisotropic scaling of the basis defined by the entity embeddings E, followed by a projection onto the real subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Existence of the Tensor Factorization</head><p>Let us first discuss the existence of the multi-relational model where the rank of the decomposition K ≤ n, which relates to simultaneous unitary decomposition.</p><formula xml:id="formula_21">Definition 6 A family of matrices X 1 , . . . , X m ∈ C n×n is simultaneously unitarily diago- nalizable, if there is a single unitary matrix E ∈ C n×n , such that X i = EW i E * for all i in 1, . . . , m, where W i ∈ C n×n are diagonal.</formula><p>Definition 7 A family of normal matrices X 1 , . . . , X m ∈ C n×n is a commuting family of normal matrices, if X i X * j = X * i X j , for all i, j in 1, . . . , m.</p><p>Theorem 3 (see Horn and Johnson <ref type="formula" target="#formula_0">(2012)</ref>) Suppose F is the family of matrices X 1 , . . . , X m ∈ C n×n . Then F is a commuting family of normal matrices if and only if F is simultaneously unitarily diagonalizable.</p><p>To apply Theorem 3 to the proposed factorization, we would have to make the hypothesis that the relation score matrices X r are a commuting family, which is too strong a hypothesis. Actually, the model is slightly different since we take only the real part of the tensor factorization. In the single-relation case, taking only the real part of the decomposition rids us of the normality requirement of Theorem 1 for the decomposition to exist, as shown in Theorem 2.</p><p>In the multiple-relation case, it is an open question whether taking the real part of the simultaneous unitary diagonalization will enable us to decompose families of arbitrary real square matrices-that is with a single unitary matrix E that has at most n columns. Though it seems unlikely, we could not find a counter-example yet.</p><p>However, by letting the rank of the tensor factorization K to be greater than n, we can show that the proposed tensor decomposition exists for families of arbitrary real square matrices, by simply concatenating the decomposition of Theorem 2 of each real square matrix X i .</p><p>Theorem 4 Suppose X 1 , . . . , X m ∈ R n×n . Then there exists E ∈ C n×nm and W i ∈ C nm×nm are diagonal, such that X i = Re(EW i E * ) for all i in 1, . . . , m.</p><p>Proof From Theorem 2 we have</p><formula xml:id="formula_22">X i = Re(E i W i E * i ), where W i ∈ C n×n is diagonal, and each E i ∈ C n×n is unitary for all i in 1, . . . , m. Let E = [E 1 . . . E m ], and Λ i =   0 ((i−1)n)×((i−1)n) W i 0 ((m−i)n)×((m−i)n)  </formula><p>where 0 l×l the zero l × l matrix. Therefore X i = Re(EΛ i E * ) for all i in 1, . . . , m.</p><p>By construction, the rank of the decomposition is at most nm. When m ≤ n, this bound actually matches the general upper bound on the rank of the canonical polyadic (CP) decomposition <ref type="bibr" target="#b30">(Hitchcock, 1927;</ref><ref type="bibr" target="#b37">Kruskal, 1989)</ref>. Since m corresponds to the number of relations and n to the number of entities, m is always smaller than n in real world knowledge graphs, hence the bound holds in practice.</p><p>Though when it comes to relational learning, we might expect the actual rank to be much lower than nm for two reasons. The first one, as discussed above, is that we are dealing with sign tensors, hence the rank of the matrices X r need only match the sign-rank of the partially-observed matrices Y r . The second one is that the matrices are related to each other, as they all represent the same entities in different relations, and thus benefit from sharing latent dimensions. As opposed to the construction exposed in the proof of Theorem 4, where other relations dimensions are canceled out. In practice, the rank needed to generalize well is indeed much lower than nm as we show experimentally in <ref type="figure" target="#fig_7">Figure 5</ref>.</p><p>Also, note that with the construction of the proof of Theorem 4, the matrix E = [E 1 . . . E m ] is not unitary any more. However the unitary constraints in the matrix case serve only the proof of existence, which is just one solution among the infinite ones of same rank. In practice, imposing orthonormality is essentially a numerical commodity for the decomposition of dense matrices, through iterative methods for example <ref type="bibr" target="#b53">(Saad, 1992)</ref>. When it comes to matrix and tensor completion, and thus generalisation, imposing such constraints is more of a numerical hassle than anything else, especially for gradient methods. As there is no apparent link between orthonormality and generalisation properties, we did not impose these constraints when learning this model in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Algorithm</head><p>Algorithm 1 describes stochastic gradient descent (SGD) to learn the proposed multirelational model with the AdaGrad learning-rate updates <ref type="bibr" target="#b22">(Duchi et al., 2011)</ref>. We refer to the proposed model as ComplEx, for Complex Embeddings. We expose a version of the algorithm that uses only real-valued vectors, in order to facilitate its implementation. To do so, we use separate real-valued representations of the real and imaginary parts of the embeddings.</p><p>These real and imaginary part vectors are initialized with vectors having a zero-mean normal distribution with unit variance. If the training set Ω contains only positive triples, negatives are generated for each batch using the local closed-world assumption as in <ref type="bibr" target="#b10">Bordes et al. (2013b)</ref>. That is, for each triple, we randomly change either the subject or the object, to form a negative example. In this case the parameter η &gt; 0 sets the number of negative triples to generate for each positive triple. Collision with positive triples in Ω is not checked, as it occurs rarely in real world knowledge graphs as they are largely sparse, and may also be computationally expensive.</p><p>Squared gradients are accumulated to compute AdaGrad learning rates, then gradients are updated. Every s iterations, the parameters Θ are evaluated over the evaluation set Ω v (evaluate AP or MRR(Ω v ; Θ) function in Algorithm 1). If the data set contains both positive and negative examples, average precision (AP) is used to evaluate the model. If the data set contains only positives, then mean reciprocal rank (MRR) is used as average precision cannot be computed without true negatives. The optimization process is stopped when the measure considered decreases compared to the last evaluation (early stopping). Bern(p) is the Bernoulli distribution, the one random sample(E) function sample uniformly one entity in the set of all entities E, and the sample batch of size b(Ω, b) function sample b true and false triples uniformly at random from the training set Ω.</p><p>For a given embedding size K, let us rewrite Equation <ref type="formula" target="#formula_19">(7)</ref>, by denoting the real part of embeddings with primes and the imaginary part with double primes: e i = Re(e i ), e i = Im(e i ), w r = Re(w r ), w r = Im(w r ). The set of parameters is Θ = {e i , e i , w r , w r ∈ R K , i ∈ E, r ∈ R}, and the scoring function involves only real vectors: </p><p>where each entity and each relation has two real embeddings.</p><p>Gradients are now easy to write: where is the element-wise (Hadamard) product. We optimized the negative log-likelihood of the logistic model described in Equation <ref type="formula" target="#formula_17">(5)</ref>  where λ ∈ R + is the regularization parameter.</p><p>To handle regularization, note that using separate representations for the real and imaginary parts does not change anything as the squared L 2 -norm of a complex vector v = v +iv is the sum of the squared modulus of each entry:</p><formula xml:id="formula_24">||v|| 2 2 = j v 2 j + v 2 j 2 = j v 2 j + j v 2 j = ||v || 2 2 + ||v || 2 2 ,</formula><p>which is actually the sum of the L 2 -norms of the vectors of the real and imaginary parts. We can finally write the gradient of γ with respect to a real embedding v for one triple (r, s, o) and its truth value y: </p><p>Algorithm 1 Stochastic gradient descent with AdaGrad for the ComplEx model Input Training set Ω, validation set Ω v , learning rate α ∈ R ++ , rank K ∈ Z ++ , L 2 regularization factor λ ∈ R + , negative ratio η ∈ Z ++ , batch size b ∈ Z ++ , maximum iteration m ∈ Z ++ , validate every s ∈ Z ++ iterations, AdaGrad regularizer = 10 −8 . Output Embeddings e , e , w , w .</p><formula xml:id="formula_26">e i ∼ N (0 k , I k×k ) , e i ∼ N (0 k , I k×k ) for each i ∈ E w i ∼ N (0 k , I k×k ), w i ∼ N (0 k , I k×k ) for each i ∈ R g e i ← 0 k , g e i ← 0 k for each i ∈ E g w i ← 0 k , g w i ← 0 k for each i ∈ R previous score ← 0 for i = 1, . . . , m do for j = 1, . . . , |Ω|/b do Ω b ← sample batch of size b(Ω, b) // Negative sampling: Ω n ← {∅} for ((r, s, o), y) in Ω b do for l = 1, . . . , η do e ← one random sample(E) if Bern(0.5) &gt; 0.5 then Ω n ← Ω n ∪ {((r, e, o), −1)} else Ω n ← Ω n ∪ {((r, s, e), −1)} end if end for end for Ω b ← Ω b ∪ Ω n for ((r, s, o), y) in Ω b do</formula><p>for v in Θ do // AdaGrad updates: </p><formula xml:id="formula_27">g v ← g v + (∇ v γ({((r, s, o), y)}; Θ)) 2 // Gradient updates: v ← v − α gv+ ∇ v γ({((r,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluated the method proposed in this paper on both synthetic and real data sets. The synthetic data set contains both symmetric and antisymmetric relations, whereas the real data sets are standard link prediction benchmarks based on real knowledge graphs.</p><p>We compared ComplEx to state-of-the-art models, namely TransE <ref type="bibr" target="#b10">(Bordes et al., 2013b)</ref>, DistMult <ref type="bibr" target="#b64">(Yang et al., 2015)</ref>, RESCAL <ref type="bibr" target="#b42">(Nickel et al., 2011)</ref> and also to the canonical polyadic decomposition (CP) <ref type="bibr" target="#b30">(Hitchcock, 1927)</ref>, to emphasize empirically the importance of learning unique embeddings for entities. For experimental fairness, we reimplemented these models within the same framework as the ComplEx model, using a Theanobased SGD implementation 3 <ref type="bibr" target="#b5">(Bergstra et al., 2010)</ref>.</p><p>For the TransE model, results were obtained with its original max-margin loss, as it turned out to yield better results for this model only. To use this max-margin loss on data sets with observed negatives (Sections 5.1 and 5.2), positive triples were replicated when necessary to match the number of negative triples, as described in <ref type="bibr" target="#b26">Garcia-Duran et al. (2016)</ref>. All other models are trained with the negative log-likelihood of the logistic model (Equation <ref type="formula" target="#formula_0">(10)</ref>). In all the following experiments we used a maximum number of iterations m = 1000, a batch size b = |Ω| 100 , and validated the models for early stopping every s = 50 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Task</head><p>To assess our claim that ComplEx can accurately model jointly symmetry and antisymmetry, we randomly generated a knowledge graph of two relations and 30 entities. One relation is entirely symmetric, while the other is completely antisymmetric. This data set corresponds to a 2 × 30 × 30 tensor. <ref type="figure" target="#fig_4">Figure 2</ref> shows a part of this randomly generated tensor, with a symmetric slice and an antisymmetric slice, decomposed into training, validation and test sets. To ensure that all test values are predictable, the upper triangular parts of the matrices are always kept in the training set, and the diagonals are unobserved. We conducted a 5-fold cross-validation on the lower-triangular matrices, using the uppertriangular parts plus 3 folds for training, one fold for validation and one fold for testing. Each training set contains 1392 observed triples, whereas validation and test sets contain 174 triples each. <ref type="figure" target="#fig_5">Figure 3</ref> shows the best cross-validated average precision (area under the precisionrecall curve) for different factorization models of ranks ranging up to 50. The regularization parameter λ is validated in {0.1, 0.03, 0.01, 0.003,0.001, 0.0003, 0.00001, 0.0} and the learning rate α was initialized to 0.5.</p><p>As expected, DistMult <ref type="bibr" target="#b64">(Yang et al., 2015)</ref> is not able to model antisymmetry and only predicts the symmetric relations correctly. Although TransE <ref type="bibr" target="#b10">(Bordes et al., 2013b)</ref> is not a symmetric model, it performs poorly in practice, particularly on the antisymmetric relation. RESCAL <ref type="bibr" target="#b42">(Nickel et al., 2011)</ref>, with its large number of parameters, quickly overfits as the rank grows. Canonical Polyadic (CP) decomposition <ref type="bibr" target="#b30">(Hitchcock, 1927</ref>) fails on both relations as it has to push symmetric and antisymmetric patterns through the entity embeddings. Surprisingly, only ComplEx succeeds even on such simple data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real Fully-Observed Data Sets: Kinships and UMLS</head><p>We then compare all models on two fully observed data sets, that contain both positive and negative triples, also called the closed-world assumption. The Kinships data set <ref type="bibr" target="#b20">(Denham, 1973)</ref>    We performed a 10-fold cross-validation, keeping 8 for training, one for validation and one for testing. <ref type="figure" target="#fig_6">Figure 4</ref> shows the best cross-validated average precision for ranks ranging up to 50, and error bars show the standard deviation over the 10 runs. The regularization parameter λ is validated in {0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.00001, 0.0} and the learning rate α was initialized to 0.5.</p><p>On both data sets ComplEx, RESCAL and CP are very close, with a slight advantage for ComplEx on Kinships, and for RESCAL on UMLS. DistMult performs poorly here as many relations are antisymmetric both in UMLS (causal links, anatomical hierarchies) and Kinships (being father, uncle or grand-father).</p><p>The fact that CP, RESCAL and ComplEx work so well on these data sets illustrates the importance of having an expressive enough model, as DistMult fails because   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Real Sparse Data Sets: FB15K and WN18</head><p>Finally, we evaluated ComplEx on the FB15K and WN18 data sets, as they are well established benchmarks for the link prediction task. FB15K is a subset of Freebase <ref type="bibr" target="#b7">(Bollacker et al., 2008)</ref>, a curated knowledge graph of general facts, whereas WN18 is a subset of Word-Net <ref type="bibr" target="#b23">(Fellbaum, 1998)</ref>, a database featuring lexical relations between words. We used the same training, validation and test set splits as in <ref type="bibr" target="#b10">Bordes et al. (2013b)</ref>. <ref type="table" target="#tab_6">Table 3</ref> summarizes the metadata of the two data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Experimental Setup</head><p>As both data sets contain only positive triples, we generated negative samples using the local closed-world assumption, as described in Section 4. For evaluation, we measure the quality of the ranking of each test triple among all possible subject and object substitutions : r(s , o) and r(s, o ), for each s , o in E, as used in previous studies <ref type="bibr" target="#b10">(Bordes et al., 2013b;</ref><ref type="bibr" target="#b45">Nickel et al., 2016b)</ref>. Mean Reciprocal Rank (MRR) and Hits at N are standard evaluation measures for these data sets and come in two flavours: raw and filtered. The filtered metrics are computed after removing all the other positive observed triples that appear in either training, validation or test set from the ranking, whereas the raw metrics do not remove these.</p><p>Since ranking measures are used, previous studies generally preferred a max-margin ranking loss for the task <ref type="bibr" target="#b10">(Bordes et al., 2013b;</ref><ref type="bibr" target="#b45">Nickel et al., 2016b)</ref>. We chose to use the negative log-likelihood of the logistic model-as described in the previous section-as it is a continuous surrogate of the sign-rank, and has been shown to learn compact representations for several important relations, especially for transitive relations <ref type="bibr" target="#b13">(Bouchard et al., 2015)</ref>. As previously stated, we tried both losses in preliminary work, and indeed training the models with the log-likelihood yielded better results than with the max-margin ranking loss, especially on FB15K-except with TransE.</p><p>We report both filtered and raw MRR, and filtered Hits at 1, 3 and 10 in <ref type="table" target="#tab_8">Table 4</ref> for the evaluated models. The HolE model has recently been shown to be equivalent to ComplEx <ref type="bibr" target="#b29">(Hayashi and Shimbo, 2017)</ref>, we record the original results for HolE as reported in <ref type="bibr" target="#b45">Nickel et al. (2016b)</ref> and briefly discuss the discrepancy of results obtained with ComplEx.</p><p>Reported results are given for the best set of hyper-parameters evaluated on the validation set for each model, after a distributed grid-search on the following values: K ∈ {10, 20, 50, 100, 150, 200}, λ ∈ {0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0}, α ∈ {1.0, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01}, η ∈ {1, 2, 5, 10} with λ the L 2 regularization parameter, α the initial learning rate, and η the number of negatives generated per positive training triple. We also   <ref type="bibr" target="#b29">(Hayashi and Shimbo, 2017)</ref>, score divergence on FB15K is only due to the loss function used <ref type="bibr" target="#b59">(Trouillon and Nickel, 2017)</ref>.</p><p>tried varying the batch size but this had no impact and we settled with 100 batches per epoch. With the best hyper-parameters, training the ComplEx model on a single GPU (NVIDIA Tesla P40) takes 45 minutes on WN18 (K = 150, η = 1), and three hours on FB15K (K = 200, η = 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Results</head><p>WN18 describes lexical and semantic hierarchies between concepts and contains many antisymmetric relations such as hypernymy, hyponymy, and being part of. Indeed, the Dist-Mult and TransE models are outperformed here by ComplEx and HolE, which are on a par with respective filtered MRR scores of 0.941 and 0.938, which is expected as both models are equivalent. <ref type="table" target="#tab_10">Table 5</ref> shows the filtered MRR for the reimplemented models and each relation of WN18, confirming the advantage of ComplEx on antisymmetric relations while losing nothing on the others. 2D projections of the relation embeddings <ref type="figure" target="#fig_10">(Figures 8 &amp; 9)</ref> visually corroborate the results.</p><p>On FB15K, the gap is much more pronounced and the ComplEx model largely outperforms HolE, with a filtered MRR of 0.692 and 59.9% of Hits at 1, compared to 0.524 and 40.2% for HolE. This difference of scores between the two models, though they have been proved to be equivalent <ref type="bibr" target="#b29">(Hayashi and Shimbo, 2017)</ref>, is due to the use of the aforementioned max-margin loss in the original HolE publication <ref type="bibr" target="#b45">(Nickel et al., 2016b)</ref> that performs worse than the log-likelihood on this dataset, and to the generation of more than one negative sample per positive in these experiments. This has been confirmed and discussed in details by <ref type="bibr" target="#b59">Trouillon and Nickel (2017)</ref>. The fact that DistMult yields fairly high scores (0.654 filtered MRR) is also due to the task itself and the evaluation measures used. As the dataset only involves true facts, the test set never includes the opposite facts r(o, s) of each test fact r(s, o) for antisymmetric relations-as the opposite fact is always false. Thus highly scoring the opposite fact barely impacts the rankings for antisymmetric relations. This is not the case in the fully observed experiments (Section 5.2), as the opposite fact is known  to be false-for antisymmetric relations-and largely impacts the average precision of the DistMult model <ref type="figure" target="#fig_6">(Figure 4)</ref>. RESCAL, that represents each relation with a K ×K matrix, performs well on WN18 as there are few relations and hence not so many parameters. On FB15K though, it probably overfits due to the large number of relations and thus the large number of parameters to learn, and performs worse than a less expressive model like DistMult. On both data sets, TransE and CP are largely left behind. This illustrates again the power of the multilinear product in the first case, and the importance of learning unique entity embeddings in the second. CP performs especially poorly on WN18 due to the small number of relations, which magnifies this subject/object difference. <ref type="figure" target="#fig_7">Figure 5</ref> shows that the filtered MRR of the ComplEx model quickly converges on both data sets, showing that the low-rank hypothesis is reasonable in practice. The little gain of performances for ranks comprised between 50 and 200 also shows that ComplEx does not perform better because it has twice as many parameters for the same rank-the real and imaginary parts-compared to other linear space complexity models but indeed thanks to its better expressiveness.</p><p>Best ranks were generally 150 or 200, in both cases scores were always very close for all models, suggesting there was no need to grid-search on higher ranks. The number of negative samples per positive sample also had a large influence on the filtered MRR on FB15K (up to +0.08 improvement from 1 to 10 negatives), but not much on WN18. On both data sets regularization was important (up to +0.05 on filtered MRR between λ = 0 and optimal one). We found the initial learning rate to be very important on FB15K, while not so much on WN18. We think this may also explain the large gap of improvement ComplEx provides on this data set compared to previously published results-as DistMult results are also better than those previously reported <ref type="bibr" target="#b64">(Yang et al., 2015)</ref>-along with the use of the log-likelihood objective. It seems that in general AdaGrad is relatively insensitive to the initial learning rate, perhaps causing some overconfidence in its ability to tune the step size online and consequently leading to less efforts when selecting the initial step size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training time</head><p>As defended in Section 2, having a linear time and space complexity becomes critical when the dataset grows. To illustrate this, we report in <ref type="figure" target="#fig_8">Figure 6</ref> the evolution of the filtered MRR on the validation set as a function of time, for the best set of validated hyper-parameters for each model. The convergence criterion used is the decrease of the validation filtered MRR-computed every 50 iterations-with a maximum number of iterations of 1000 (see Algorithm 1). All models have a linear complexity except for RESCAL that has a quadratic one in the rank of the decomposition, as it learns one matrix embedding for each relation r ∈ R. Timings are measured on a single NVIDIA Tesla P40 GPU. On WN18, all models reach convergence in a reasonable time, between 15 minutes and 1 hour and 20 minutes. The difference between RESCAL and the other models is not sharp there, first because its optimal embedding size (K = 50) is lower compared to the other models. Secondly, there are only |R| = 18 relations in WN18, hence the memory footprint of RESCAL is pretty similar to the other models-because it represents only relations with matrices and not entities. On FB15K, the difference is much more pronounced, as RESCAL optimal rank is similar to the other models; and with |R| = 1345 relations, RESCAL has a much higher memory footprint, which implies more processor cache misses due to the uniformly-random nature of the SGD sampling.</p><p>RESCAL took more than four days to train on FB15K, whereas other models took between 40 minutes and 3 hours. While a few days might seem manageable, this could not be the case on larger data sets, as FB15K is but a small subset of Freebase that contains |R| = 35000 relations <ref type="bibr" target="#b7">(Bollacker et al., 2008)</ref>. This experimentally supports our claim that linear complexity is required for scalability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Influence of Negative Samples</head><p>We further investigated the influence of the number of negatives generated per positive training sample. In the previous experiment, due to computational limitations, the number of negatives per training sample, η, was validated over the set {1, 2, 5, 10}. On WN18 it proved to be of no help to have more than one generated negative per positive. Here we explore in which proportions increasing the number of generated negatives leads to better results on FB15K. To do so, we fixed the best validated λ, K, α obtained from the previous experiment. We then let η vary in <ref type="bibr">{1, 2, 5, 10, 20, 50, 100, 200}</ref>. <ref type="figure" target="#fig_9">Figure 7</ref> shows the influence of the number of generated negatives per positive training triple on the performance of ComplEx on FB15K. Generating more negatives clearly improves the results up to 100 negative triples, with a filtered MRR of 0.737 and 64.8% of Hits@1, before decreasing again with 200 negatives, probably due to the too large class imbalance. The model also converges with fewer epochs, which compensates partially for the additional training time per epoch, up to 50 negatives. It then grows linearly as the number of negatives increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">WN18 Embeddings Visualization</head><p>We used principal component analysis (PCA) to visualize embeddings of the relations of the WordNet data set (WN18). We plotted the four first components of the best DistMult and ComplEx model's embeddings in <ref type="figure" target="#fig_10">Figures 8 &amp; 9</ref>. For the ComplEx model, we simply concatenated the real and imaginary parts of each embedding.  Most of WN18 relations describe hierarchies, and are thus antisymmetric. Each of these hierarchic relations has its inverse relation in the data set. For example: hypernym / hyponym, part of / has part, synset domain topic of / member of domain topic. Since DistMult is unable to model antisymmetry, it will correctly represent the nature of each pair of opposite relations, but not the direction of the relations. Loosely speaking, in the hypernym / hyponym pair the nature is sharing semantics, and the direction is that one entity generalizes the semantics of the other. This makes DistMult representing the opposite relations with very close embeddings. It is especially striking for the third and fourth principal component <ref type="figure" target="#fig_11">(Figure 9</ref>). Conversely, ComplEx manages to oppose spatially the opposite relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>We first discuss related work about complex-valued matrix and tensor decompositions, and then review other approaches for knowledge graph completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Complex Numbers</head><p>When factorization methods are applied, the representation of the decomposition is generally chosen in accordance with the data, despite the fact that most real square matrices only have eigenvalues in the complex domain. Indeed in the machine learning community, the data is usually real-valued, and thus eigendecomposition is used for symmetric matrices, or other decompositions such as (real-valued) singular value decomposition <ref type="bibr" target="#b4">(Beltrami, 1873)</ref>, non-negative matrix factorization <ref type="bibr" target="#b47">(Paatero and Tapper, 1994)</ref>, or canonical polyadic decomposition when it comes to tensors <ref type="bibr" target="#b30">(Hitchcock, 1927)</ref>.</p><p>Conversely, in signal processing, data is often complex-valued <ref type="bibr" target="#b56">(Stoica and Moses, 2005</ref>) and the complex-valued counterparts of these decompositions are then used. Joint diagonalization is also a much more common tool than in machine learning for decomposing sets of (complex) dense square matrices <ref type="bibr" target="#b3">(Belouchrani et al., 1997;</ref><ref type="bibr" target="#b19">De Lathauwer et al., 2001)</ref>.</p><p>Some works on recommender systems use complex numbers as an encoding facility, to merge two real-valued relations, similarity and liking, into one single complex-valued matrix which is then decomposed with complex embeddings <ref type="bibr" target="#b38">(Kunegis et al., 2012;</ref><ref type="bibr" target="#b63">Xie et al., 2015)</ref>. Still, unlike our work, it is not real data that is decomposed in the complex domain.</p><p>In deep learning, <ref type="bibr" target="#b18">Danihelka et al. (2016)</ref> proposed an LSTM extended with an associative memory based on complex-valued vectors for memorization tasks, and <ref type="bibr" target="#b32">Hu et al. (2016)</ref> a complex-valued neural network for speech synthesis. In both cases again, the data is first encoded in complex vectors that are then fed into the network.</p><p>Conversely to these contributions, this work suggests that processing real-valued data with complex-valued representation, through a projection onto the real-valued subspace, can be a very simple way of increasing the expressiveness of the model considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Knowledge Graph Completion</head><p>Many knowledge graphs have recently arisen, pushed by the W3C recommendation to use the resource description framework (RDF) <ref type="bibr" target="#b17">(Cyganiak et al., 2014)</ref> for data representation. Examples of such knowledge graphs include DBPedia <ref type="bibr" target="#b1">(Auer et al., 2007)</ref>, Freebase <ref type="bibr" target="#b7">(Bollacker et al., 2008)</ref> and the Google Knowledge Vault <ref type="bibr" target="#b21">(Dong et al., 2014)</ref>. Motivating applications of knowledge graph completion include question answering <ref type="bibr" target="#b12">(Bordes et al., 2014b)</ref> and more generally probabilistic querying of knowledge bases <ref type="bibr" target="#b33">(Huang and Liu, 2009;</ref><ref type="bibr" target="#b36">Krompaß et al., 2014)</ref>.</p><p>First approaches to relational learning relied upon probabilistic graphical models <ref type="bibr" target="#b28">(Getoor and Taskar, 2007)</ref>, such as bayesian networks <ref type="bibr" target="#b25">(Friedman et al., 1999)</ref> and markov logic networks <ref type="bibr" target="#b50">(Richardson and Domingos, 2006;</ref><ref type="bibr" target="#b49">Raedt et al., 2016)</ref>.</p><p>With the first embedding models, asymmetry of relations was quickly seen as a problem and asymmetric extensions of tensors were studied, mostly by either considering independent embeddings <ref type="bibr" target="#b24">(Franz et al., 2009)</ref> or considering relations as matrices instead of vectors in the RESCAL model <ref type="bibr" target="#b42">(Nickel et al., 2011)</ref>, or both <ref type="bibr" target="#b57">(Sutskever, 2009)</ref>. Direct extensions were based on uni-,bi-and trigram latent factors for triple data <ref type="bibr" target="#b26">(Garcia-Duran et al., 2016)</ref>, as well as a low-rank relation matrix <ref type="bibr" target="#b34">(Jenatton et al., 2012)</ref>. <ref type="bibr" target="#b11">Bordes et al. (2014a)</ref> propose a two-layer model where subject and object embeddings are first separately combined with the relation embedding, then each intermediate representation is combined into the final score.</p><p>Pairwise interaction models were also considered to improve prediction performances. For example, the Universal Schema approach <ref type="bibr" target="#b51">(Riedel et al., 2013)</ref> factorizes a 2D unfolding of the tensor (a matrix of entity pairs vs. relations) while  extend this also to other pairs. <ref type="bibr" target="#b51">Riedel et al. (2013)</ref> also consider augmenting the knowledge graph facts by exctracting them from textual data, as does <ref type="bibr" target="#b58">Toutanova et al. (2015)</ref>. Injecting prior knowledge in the form of Horn clauses in the objective loss of the Universal Schema model has also been considered <ref type="bibr" target="#b52">(Rocktaschel et al., 2015)</ref>. <ref type="bibr" target="#b15">Chang et al. (2014)</ref> enhance the RESCAL model to take into account information about the entity types. For recommender systems (thus with different subject/object sets of entities), <ref type="bibr" target="#b2">Baruch (2014)</ref> proposed a noncommutative extension of the CP decomposition model. More recently, Gaifman models that learn neighborhood embeddings of local structures in the knowledge graph showed competitive performances <ref type="bibr" target="#b46">(Niepert, 2016)</ref>.</p><p>In the Neural Tensor Network (NTN) model, <ref type="bibr" target="#b55">Socher et al. (2013)</ref> combine linear transformations and multiple bilinear forms of subject and object embeddings to jointly feed them into a nonlinear neural layer. Its non-linearity and multiple ways of including interactions between embeddings gives it an advantage in expressiveness over models with simpler scoring function like DistMult or RESCAL. As a downside, its very large number of parameters can make the NTN model harder to train and overfit more easily.</p><p>The original multilinear DistMult model is symmetric in subject and object for every relation <ref type="bibr" target="#b64">(Yang et al., 2015)</ref> and achieves good performance on FB15K and WN18 data sets. However it is likely due to the absence of true negatives in these data sets, as discussed in Section 5.3.2.</p><p>The TransE model from <ref type="bibr" target="#b10">Bordes et al. (2013b)</ref> also embeds entities and relations in the same space and imposes a geometrical structural bias into the model: the subject entity vector should be close to the object entity vector once translated by the relation vector.</p><p>A recent novel way to handle antisymmetry is via the Holographic Embeddings (HolE) model by <ref type="bibr" target="#b45">Nickel et al. (2016b)</ref>. In HolE the circular correlation is used for combining entity embeddings, measuring the covariance between embeddings at different dimension shifts. This model has been shown to be equivalent to the ComplEx model <ref type="bibr" target="#b29">(Hayashi and Shimbo, 2017;</ref><ref type="bibr" target="#b59">Trouillon and Nickel, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and Future Work</head><p>Though the decomposition proposed in this paper is clearly not unique, it is able to learn meaningful representations. Still, characterizing all possible unitary diagonalizations that preserve the real part is an interesting open question. Especially in an approximation setting with a constrained rank, in order to characterize the decompositions that minimize a given reconstruction error. That might allow the creation of an iterative algorithm similar to eigendecomposition iterative methods <ref type="bibr" target="#b53">(Saad, 1992)</ref> for computing such a decomposition for any given real square matrix.</p><p>The proposed decomposition could also find applications in many other asymmetric square matrices decompositions applications, such as spectral graph theory for directed graphs <ref type="bibr" target="#b16">(Cvetković et al., 1997)</ref>, but also factorization of asymmetric measures matrices such as asymmetric distance matrices <ref type="bibr" target="#b40">(Mao and Saul, 2004)</ref> and asymmetric similarity matrices <ref type="bibr" target="#b48">(Pirasteh et al., 2015)</ref>.</p><p>From an optimization point of view, the objective function (Equation <ref type="formula" target="#formula_0">(10)</ref>) is clearly non-convex, and we could indeed not be reaching a globally optimal decomposition using stochastic gradient descent. Recent results show that there are no spurious local minima in the completion problem of positive semi-definite matrix <ref type="bibr" target="#b27">(Ge et al., 2016;</ref><ref type="bibr" target="#b6">Bhojanapalli et al., 2016)</ref>. Studying the extensibility of these results to our decomposition is another possible line of future work. The first step would be generalizing these results to symmetric real-valued matrix completion, then generalization to normal matrices should be straightforward. The two last steps would be extending to matrices that are expressed as real part of normal matrices, and finally to the joint decomposition of such matrices as a tensor. We indeed noticed a remarkable stability of the scores across different random initialization of ComplEx for the same hyper-parameters, which suggests the possibility of such theoretical property.</p><p>Practically, an obvious extension is to merge our approach with known extensions to tensor factorization models in order to further improve predictive performance. For example, the use of pairwise embeddings <ref type="bibr" target="#b51">(Riedel et al., 2013;</ref> together with complex numbers might lead to improved results in many situations that involve non-compositionality. Adding bigram embeddings to the objective could also improve the results as shown on other models <ref type="bibr" target="#b26">(Garcia-Duran et al., 2016)</ref>. Another direction would be to develop a more intelligent negative sampling procedure, to generate more informative negatives with respect to the positive sample from which they have been sampled. This would reduce the number of negatives required to reach good performance, thus accelerating training time. Extension to relations between more than two entities, n-tuples, is not straightforward, as ComplEx's expressiveness comes from the complex conjugation of the object-entity, that breaks the symmetry between the subject and object embeddings in the scoring function. This stems from the Hermitian product, which seems to have no standard multilinear extension in the linear algebra literature, this question hence remains largely open.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We described a new matrix and tensor decomposition with complex-valued latent factors called ComplEx. The decomposition exists for all real square matrices, expressed as the real part of normal matrices. The result extends to sets of real square matrices-tensorsand answers to the requirements of the knowledge graph completion task : handling a large variety of different relations including antisymmetric and asymmetric ones, while being scalable. Experiments confirm its theoretical versatility, as it substantially improves over the state-of-the-art on real knowledge graphs. It shows that real world relations can be efficiently approximated as the real part of low-rank normal matrices. The generality of the theoretical results and the effectiveness of the experimental ones motivate for the application to other real square matrices factorization problems. More generally, we hope that this paper will stimulate the use of complex linear algebra in the machine learning community, even and especially for processing real-valued data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: Scores x so = Re(e s Wē o ) (top) and x os = Re(e o W e s ) (bottom) for the proposed complex-valued decomposition, plotted as a function of W ∈ C, for fixed entity embeddings e s = 1−2i, and e o = −3+i. Right: Scores x so = e s W e o (top) and x os = e o W e s (bottom) for the corresponding real-valued decomposition with the same number of free real-valued parameters (i.e. in twice the dimension), plotted as a function of W ∈ R 2 diagonal, for fixed entity embeddings e s = 1 −2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>∇</head><label></label><figDesc>e s φ(r, s, o; Θ) = (w r e o ) + (w r e o ), ∇ e s φ(r, s, o; Θ) = (w r e o ) − (w r e o ), ∇ e o φ(r, s, o; Θ) = (w r e s ) − (w r e s ), ∇ e o φ(r, s, o; Θ) = (w r e s ) + (w r e s ), ∇ w r φ(r, s, o; Θ) = (e s e o ) + (e s e o ), ∇ w r φ(r, s, o; Θ) = (e s e o ) − (e s e o ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>with L 2 regularization on the parameters Θ: γ(Ω; Θ) = ((r,s,o),y)∈Ω log(1 + exp(−yφ(r, s, o; Θ))) + λ||Θ|| 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>∇</head><label></label><figDesc>v γ({((r, s, o), y)}; Θ) = −yσ(−yφ(r, s, o; Θ))∇ v φ(r, s, o; Θ) + 2λv .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Parts of the training, validation and test sets of the generated experiment with one symmetric and one antisymmetric relation. Red pixels are positive triples, blue are negatives, and green missing ones. Top: Plots of the symmetric slice (relation) for the 10 first entities. Bottom: Plots of the antisymmetric slice for the 10 first entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Average precision (AP) for each factorization rank ranging from 5 to 50 for different state-of-the-art models on the synthetic task. Learning is performed jointly on the symmetric relation and on the antisymmetric relation. Top-left: AP over the symmetric relation only. Top-right: AP over the antisymmetric relation only. Bottom: Overall AP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Average precision (AP) for each factorization rank ranging from 5 to 50 for different state-of-the-art models on the Kinships data set (top) and on the UMLS data set (bottom</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Best filtered MRR for ComplEx on the FB15K and WN18 data sets for different ranks. Increasing the rank gives little performance gain for ranks of 50 − 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Evolution of the filtered MRR on the validation set as a function of time, on WN18 (top) and FB15K (bottom) for each model for its best set of hyper-parameters. The best rank K is reported in legend. Final black marker indicates that the maximum number of iterations (1000) has been reached (RESCAL on WN18, TransE on FB15K).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Influence of the number of negative triples generated per positive training example on the filtered test MRR and on training time to convergence on FB15K for the ComplEx model with K = 200, λ = 0.01 and α = 0.5. Times are given relative to the training time with one negative triple generated per positive training sample (= 1 on time scale).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Plots of the first and second components of the WN18 relations embeddings using principal component analysis. Red arrows link the labels to their point. Top: ComplEx embeddings. Bottom: DistMult embeddings. Opposite relations are clustered together by DistMult while correctly separated by ComplEx.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Plots of the third and fourth components of the WN18 relations embeddings using principal component analysis. Red arrows link the labels to their point. Top: ComplEx embeddings. Bottom: DistMult embeddings. Opposite relations are clustered together by DistMult while correctly separated by ComplEx.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Scoring functions of state-of-the-art latent factor models for a given fact r(s, o), along with the representation of their relation parameters, and time and space</figDesc><table /><note>(memory) complexity. K is the dimensionality of the embeddings. The entity embeddings e s and e o of subject s and object o are in R K for each model, except for ComplEx, where e s , e o ∈ C K .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>describes the 26 different kinship relations of the Alyawarra tribe in Australia, among 104 individuals. The unified medical language system (UMLS) data set<ref type="bibr" target="#b41">(McCray, 2003)</ref> represents 135 medical concepts and diseases, linked by 49 relations describing their interactions. Metadata for the two data sets is summarized inTable 2.</figDesc><table><row><cell>Data set</cell><cell cols="2">|E| |R| Total number of triples</cell></row><row><cell cols="2">Kinships 104 26</cell><cell>281,216</cell></row><row><cell>UMLS</cell><cell>135 49</cell><cell>893,025</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Number of entities |E|, relations |R|, and observed triples (all are observed) for the Kinships and UMLS data sets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>). of antisymmetry; the power of the multilinear product-that is the tensor factorization approach-as TransE can be seen as a sum of bilinear products (Garcia-Duran et al., 2016); but not yet the importance of having unique entity embeddings, as CP works well. We believe having separate subject and object-entity embeddings works well under the closed-world assumption, because of the amount of training data compared to the number of embeddings to learn. Though when only a fractions of the positive training examples are observed (as it is most often the case), we will see in the next experiments that enforcing unique entity embeddings is key to good generalization.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Number of triples in sets:</cell></row><row><cell>Data set</cell><cell>|E|</cell><cell cols="3">|R| Training Validation</cell><cell>Test</cell></row><row><cell>WN18</cell><cell>40,943</cell><cell>18</cell><cell>141,442</cell><cell>5,000</cell><cell>5,000</cell></row><row><cell>FB15K</cell><cell cols="2">14,951 1,345</cell><cell>483,142</cell><cell cols="2">50,000 59,071</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Number of entities |E|, relations |R|, and observed triples in each split for the FB15K and WN18 data sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Filtered and raw mean reciprocal rank (MRR) for the models tested on the FB15K and WN18 data sets. Hits@N metrics are filtered. *Results reported from<ref type="bibr" target="#b45">Nickel et al. (2016b)</ref> for the HolE model, that has been shown to be equivalent to ComplEx</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Filtered Mean Reciprocal Rank (MRR) for the models tested on each relation of the WordNet data set (WN18).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. This is not the Hermitian extension of the multilinear dot product as there appears to be no standard definition of the Hermitian multilinear product in the linear algebra literature.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. https://github.com/lmjohns3/downhill</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the Association Nationale de la Recherche et de la Technologie through the CIFRE grant 2014/0121, in part by the Paul Allen Foundation through an Allen Distinguished Investigator grant, and in part by a Google Focused Research Award. We would like to thank Ariadna Quattoni, Stéphane Clinchant, Jean-Marc Andréoli, Sofia Michel, Alejandro Blumentals, Léo Hubert and Pierre Comon for their helpful comments and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sign rank versus vc dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Yehudayoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DBpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A ternary non-commutative latent factor model for scalable three-way real tensor completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Baruch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.7383</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A blind source separation technique using second-order statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Belouchrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Abed-Meraim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-F</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="434" to="444" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Giornale di Matematiche ad Uso degli Studenti Delle Universita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Beltrami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1873" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
	<note>Sulle funzioni bilineari</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Python for Scientific Computing Conference (SciPy)</title>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Global optimality of local search for low rank matrix recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07221</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Irreflexive and hierarchical relations as translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<idno>abs/1304.7158</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On approximate reasoning capabilities of low-rank vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sur l&apos;équationà l&apos;aide de laquelle on détermine les inégalités séculaires des mouvements des planètes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustin-Louis</forename><surname>Cauchy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OEuvres complètes</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="174" to="195" />
			<date type="published" when="1829" />
		</imprint>
	</monogr>
	<note>II e série</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Typed tensor decomposition of knowledge bases for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dragoš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Cvetković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Rowlinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simic</surname></persName>
		</author>
		<title level="m">Eigenspaces of graphs. Number 66</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rdf 1.1 concepts and abstract syntax. W3C Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Lanthaler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Associative long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03032</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Independent component analysis and (simultaneous) third-order tensor diagonalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><forename type="middle">De</forename><surname>Lieven De Lathauwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joos</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2262" to="2271" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The detection of patterns in Alyawara nonverbal behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woodrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
		<respStmt>
			<orgName>University of Washington, Seattle.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wordnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Triplerank: Ranking semantic web data by tensor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antje</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergej</forename><surname>Sizov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Probabilistic Relational Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Pfeffer</surname></persName>
		</author>
		<idno>3540422897. doi: 10.1.1.101.3165</idno>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence, number August</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1300" to="1309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining two and three-way embedding models for link prediction in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="715" to="742" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Matrix completion has no spurious local minimum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07272</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Introduction to Statistical Relational Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>ISBN 0262072882</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On the equivalence of holographic and complex embeddings for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05563</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Phys</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles R Johnson</forename><surname>Horn</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Initial investigation of speech synthesis based on complex-valued neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korin</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartick</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Stylianou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5630" to="5634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Query evaluation on probabilistic rdf databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web Information Systems Engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="307" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Latent Factor Model for Highly Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Querying factorized probabilistic triple databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Krompaß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="114" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rank, decomposition, and uniqueness for 3-way and n-way arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kruskal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiway data analysis</title>
		<imprint>
			<publisher>North-Holland Publishing Co</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="7" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online dating recommender systems: The split-complex number approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Kunegis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerd</forename><surname>Gröner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gottron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM RecSys Workshop on Recommender Systems and the Social Web</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Complexity measures of sign matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Schechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adi</forename><surname>Shraibman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="439" to="463" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling distances in large-scale networks by matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM conference on Internet Measurement</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An upper-level ontology for the biomedical domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexa T</forename><surname>Mccray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comparative and Functional Genomics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="84" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reducing the rank in relational factorization models by including observable patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1179" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Discriminative gaifman models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3405" to="3413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pentti</forename><surname>Paatero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unto</forename><surname>Tapper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmetrics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="126" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploiting matrix factorization to asymmetric user similarities in recommendation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parivash</forename><surname>Pirasteh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dosam</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="51" to="57" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Statistical relational artificial intelligence: Logic, probability, and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Luc De Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriraam</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="189" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="107" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Injecting Logical Background Knowledge into Embeddings for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rocktaschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Numerical methods for large eigenvalue problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youcef</forename><surname>Saad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>SIAM</publisher>
			<biblScope unit="volume">158</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A survey of current approaches for mapping of relational databases to rdf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Halb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kingsley</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Idehen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Thibodeau</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Sequeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ezzat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">W3C RDB2RDF Incubator Group Report</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="113" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Spectral analysis of signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petre</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Randolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moses</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">452</biblScope>
			<pubPlace>Pearson Prentice Hall Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Modelling relational data using bayesian clustered tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Complex and holographic embeddings of knowledge graphs: a comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Statistical Relational AI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Zur algebra der funktionaloperationen und der theorie der normalen operatoren</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John Von Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematische Annalen</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="370" to="427" />
			<date type="published" when="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A factorization machine framework for testing bigram embeddings in knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Automated Knowledge Base Construction AKBC@NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="103" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">A link prediction approach for item recommendation with complex numbers. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="148" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
