<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CubifAE-3D: Monocular Camera Space Cubification for Auto-Encoder based 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Shrivastava</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ford</forename><forename type="middle">Greenfield</forename><surname>Labs</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palo</forename><surname>Alto</surname></persName>
						</author>
						<title level="a" type="main">CubifAE-3D: Monocular Camera Space Cubification for Auto-Encoder based 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a method for 3D object detection using a single monocular image. Starting from a synthetic dataset, we pre-train an RGB-to-Depth Auto-Encoder (AE). The embedding learnt from this AE is then used to train a 3D Object Detector (3DOD) CNN which is used to regress the parameters of 3D object poses after the encoder from the AE generates a latent embedding from the RGB image. We show that we can pre-train the AE using paired RGB and depth images from simulation data once and subsequently only train the 3DOD network using real data, comprising of RGB images and 3D object pose labels (without the requirement of dense depth). Our 3DOD network utilizes a particular 'cubification' of 3D space around the camera, where each cuboid is tasked with predicting N object poses, along with their class and confidence values. The AE pre-training and this method of dividing the 3D space around the camera into cuboids give our method its name -CubifAE-3D. We demonstrate results for monocular 3D object detection in the Autonomous Vehicle (AV) use-case with the Virtual KITTI 2 and the KITTI datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Detecting objects around the ego-vehicle is one of the fundamental tasks for the perception system on an AV. The perception of objects such as other vehicles, pedestrians, bicycles, and their relative positions in 3D space around the ego-vehicle are then used by the vehicle's path planning system to chart a collision free route through it.</p><p>Detecting objects as 2D bounding boxes in monocular camera images is a relatively mature technology, and techniques like YOLO <ref type="bibr" target="#b15">[16]</ref> work reasonably well in the real world. However, the same cannot be said for detecting these objects in 3D space using monocular cameras. Most methods for 3D object detection available today use high-cost sensors such as LIDAR <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>. Some work has also suggested the use of depth-maps to generate pseudo-lidar representation, which can subsequently be used for 3D object detection using state-of-theart (SoTA) LIDAR based object detection methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Here, we present a method for performing 3D object detection using a single RGB camera during inference. We assume that a dense depth map (as obtained from stereo / RGB-D camera) is available during training. Sparse depth maps (as obtained from LIDAR) could also be used for training by first transforming it into a dense representation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref>. We first pre-train an RGB-to-depth auto-encoder in a fully-supervised manner and let the latent space learn an RGB-to-depth embedding, after which, the Decoder is disconnected and the output of the latent space is fed to the 3D Object Detection (3DOD) network, which is trained to predict 3D object poses. We call our model CubifAE-3D, pronounced Cubify-3D. The first part of the name refers to the cubification or voxellization of monocular camera space as a pre-processing step, and the AE refers to the Auto-Encoding of RGB-to-depth space.</p><p>Furthermore, we show that the RGB-to-depth autoencoder can to be pre-trained using simulation data. This pre-trained latent encoding of the RGB image can subse-quently be used to train the 3DOD network on a separate real dataset. The only annotation needed for the real dataset therefore, is the object pose itself corresponding to each camera frame. We do not use LIDAR or stereo data for training the network on the real dataset (though these sensing sources might be required to annotate the 3D labels themselves). <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the 3D object pose predictions from a single RGB image made by our model on the vKITTI2 <ref type="bibr" target="#b2">[3]</ref> and KITTI <ref type="bibr" target="#b5">[6]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Due to the expense of LIDAR scanners and the simultaneous advances in Deep Learning, camera-only techniques for 3D object detection have gained currency over the past few years. Pseudo-LIDAR <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref> demonstrated that they could retrieve LIDAR-like point clouds from cameras. Camera images (monocular and stereo) are converted into a depth map, which is unprojected into the 3D camera space to get a point-cloud. 3D object poses are subsequently detected directly in the point cloud using a Point-net-like architecture <ref type="bibr" target="#b13">[14]</ref> or converted to a Bird's Eye View (BEV) image, before being fed to a CNN for detecting the 3D pose of objects. The BEV projection has the advantage of preserving object size and scale throughout the representation and is faster to process, allowing real-time detections. The BEV can be treated as an image, that can be scanned using 2D convolutions that are faster to compute compared to the 3D convolutions required in the point cloud generated from stereo.</p><p>Determining the 6-Dof pose of an object from a single 2D image is essentially an ill-posed problem, because the camera projection equation squashes the depth dimension so that a 2D point in an image corresponds to a ray in 3D space, and the 3D world point corresponding to its projected 2D point could lie along any point along that ray. Approaches like Mousavian et. al. <ref type="bibr" target="#b12">[13]</ref> and MonoGRNet <ref type="bibr" target="#b14">[15]</ref> combine Deep Learning with the geometric properties for the rigid bodies of vehicles to constrain this problem. Mousavian et. al. <ref type="bibr" target="#b12">[13]</ref> uses Deep Learning to predict the orientation angle and the dimensions of the 3D bounding box corresponding to a 2D detection. These are then used to instantiate an oriented and sized 3D box in 3D space, along the ray corresponding to the centre of the 2D detection, and the depth (distance along the ray) is optimized by minimizing the 2D projection error of the instantiated 3D box relative to the 2D detection.</p><p>Deep MANTA <ref type="bibr" target="#b4">[5]</ref> and Mono3D++ <ref type="bibr" target="#b6">[7]</ref> use 2D vehicle part detectors in monocular images to further constrain the determination of their 3D poses using optimization. Deeplearnt vehicle part detectors for the location of vehicle parts in the 2D image are combined with the actual relative 3D locations of these parts to determine the pose of the vehicle in the camera coordinate frame. Mono3D++ <ref type="bibr" target="#b6">[7]</ref> adds ad-ditional priors like monocular depth estimation and ground plane detection to the 3D part locations to improve performance.</p><p>An efficient way of training an object detector is to divide the image into a grid and learning anchors that are typical of the sizes and aspect ratios of the bounding boxes normally detected in that part of the image. Each grid cell is associated with a fixed number of anchors, each with a prototypical size and aspect ratio, where a training sample is associated with an anchor based on an intersection-over-union (IoU) score. The network is trained to output an offset from the anchor in terms of centre and dimensions of the ground truth bounding box relative to the anchor. This is the approach adopted by YOLO <ref type="bibr" target="#b15">[16]</ref> for 2D object detection and more recently, by M3D-RPN <ref type="bibr" target="#b1">[2]</ref>, a method that extends the anchor box idea to 3D space for monocular 3D object detection. M3D-RPN learns both 2D and 3D bounding box priors for anchors from the data and the network is trained to output 3D bounding box parameters that are offsets from these anchors.</p><p>Our CubifAE-3D network takes inspiration from these works. We divide the space around the camera into a 3D grid, that starts by subdividing the camera image into a 4x4 grid and extends this outwards into the 3rd (z) axis. Each subdivision or cuboid in our 3D grid is each allowed to predict N 3D object poses and their confidence scores. However, in contrast to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b1">2]</ref>, our network is anchor-free. Anchor-based approaches are limited to predicting objects whose sizes and aspect ratios resemble the anchor, and the number of objects per grid cell is limited to the number of anchors allowed for that cell. In contrast, each cuboid in our method is allowed to regress 3D bounding boxes of any size, shape, orientation and relative depth for N objects.</p><p>A major advantage of methods, including ours, that operate in the 3D voxel space <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref> or indeed the orthographic/BEV/ground-plane space <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17]</ref>, over methods that operate in the camera image space <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref> is that they can detect objects farther away from the camera, and we further extend this advantage and demonstrate that the accuracy of our method remains constant with distance from the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our method of performing 3D object detection relies on first learning the latent space embeddings for per-pixel RGB-to-depth predictions in an image. This is achieved by training an auto-encoder to predict the dense depth map from a single RGB image. Once trained, the decoder is detached, and the latent space embedding is fed to our 3DOD network. The high-level model architecture is shown in figure 2 and a detailed model architecture is shown in <ref type="figure" target="#fig_1">figure  3</ref>.</p><p>By training the auto-encoder first, we force its latent <ref type="figure">Figure 2</ref>. CubifAE-3D high-level architecture space to learn a compact RGB-to-depth embedding representation which is encoded in the latent space. A model which then operates on these encodings is thus able to formulate a relationship between the structures present in the RGB image and its real world depth. We then cubify the monocular camera space and train our 3DOD model to detect object poses (x center , y center , z center , width, height, length, orientation). So, at the test time, only an RGB image is needed for detecting object poses. Orientation term in the predicted pose refers to the rotation about y-axis in the camera frame. For the sake of simplicity, we assume rotation about the other two axes to be zero. An additional classifier network with a small number of parameters is then used to classify all of these detected objects at once by resizing and stacking the object crops and feeding them to the classifier model. This is done instead of predicting the classes directly as a part of the vector corresponding to each object from the 3DOD network in the favour of reducing number of parameters in the fully-connected layers and hence the inference time. We apply non-max suppression to further filter out the object pose predictions with high IoU by retaining only the objects with the highest confidence.</p><p>In our experiments, we train the RGB-to-depth AE only in simulation and do not use/require dense depth information from the real datasets. We realize that an RGB-to-depth model trained on one dataset is incompatible with another because of the camera intrinsics mismatch. However, because we only use the encoder part of the RGB-to-depth AE as an embedding, our 3DOD network automatically learns to compensate for this. We train the latent representation once, using one set of simulation data and use the same latent representation in the 3DOD network which then learns to predict object poses on new real data. We also experiment with replacing the encoder head by a pre-trained backbone model (VGG-16 <ref type="bibr" target="#b20">[21]</ref>) and observe an improved performance over our baseline CubifAE-3D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">RGB-to-Depth Auto-Encoder</head><p>The RGB-to-depth auto-encoder is trained with a combination of a Mean Squared Error (eqn 1) and an Edge-Aware Smoothing Loss (eqn 2) to perform depth map prediction from monocular RGB images. The network contains a U-Net <ref type="bibr" target="#b17">[18]</ref> like encoder-decoder architecture with skip connections and is shown in the first row of <ref type="figure" target="#fig_1">figure 3</ref>.</p><formula xml:id="formula_0">mse loss = λ mse u * v u,v i=0,j=0 (d i,j −d i,j ) 2 (1) eas loss = λ eas u * v u,v i=0,j=0 | ∂ xdi,j | e −|∂xIi,j | + | ∂ ydi,j | e −|∂yIi,j | (2) depth loss = mse loss + eas loss<label>(3)</label></formula><p>This model learns to predict accurate depth maps from monocular RGB images. The edge-aware smoothing loss penalizes high edge gradients if the image gradient is low and vice-versa, thus forcing the depth to be continuous and locally smooth within object boundaries while ensuring a clear depth-difference at the edges. This improves the accuracy of depth along the silhouette of the objects and avoids noisy holes within the object boundaries. Prediction results of this network is shown in figure 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The 3DOD Network</head><p>Once the RGB-to-depth auto-encoder (AE) is trained, the head (encoder) of this network is detached, its weights are frozen, and the latent space is then fed to the 3DOD network for object pose estimation. This combined network thus learns to perform 3D object detection based on just a monocular RGB image as an input. We do not need the ground-truth depth map from a real dataset and instead pre-train the RGB-to-depth AE in simulation and then use the latent representation to train the 3DOD network. Ideally, the camera intrinsics between the simulated and real datasets need to match -as in our choice of datasets with KITTI and its simulated counterpart, vKITTI2. Experiments (not included in this paper) of using the pre-trained AE from vKITTI2 for 3DOD on a real dataset with a different camera, nuScenes <ref type="bibr" target="#b3">[4]</ref> resulted in reasonable results, but worse than SoTA. This means that given the ground truth 3D pose labels in the real dataset, the 3DOD network is able to compensate for the focal length mismatch to some extent, but there is a degradation of accuracy. We hope to further explore this in future work.</p><p>We prepare training labels for the 3DOD network in a way that allows each part of the network to only be responsible for detecting objects within a certain physical space relative to the ego-vehicle camera. We cubify the 3D regionof-interest (ROI) of the ego-camera into a 3-dimensional grid. This 3D grid is of size 4xM , where the camera image plane is divided into 4 regions along the (x,y) dimensions of the camera coordinate frame, with z axis further quantified into M cuboids for each of these 4 regions. Each cuboid Once trained, the decoder is detached, encoder weights are frozen, and the encoder output is fed to the 3DOD model (middle branch), which is trained with a combination of xyz loss , whl loss , orientation loss , iou loss , and conf loss . A 2D bounding-box is obtained for each object by projecting its detected 3D bounding-box onto the camera image plane, cropped, and resized to 64x64 and fed to the classifier model (bottom branch) along with the normalized whl vector for class prediction. The dimensions indicated correspond to the output tensor for each block. In our experiments, we also replace the encoder head by a pretrained backbone network (VGG-16) and observe an improved performance. in this 4xM grid is responsible for predicting up to N objects in an increasing order of z (depth) from the center of the ego-camera. The model predicts a vector of length 8 for each possible object (conf idence, x center , y center , z center , width, height, length, orientation). This results in the model output being a vector of size 4xM xN x8. The intuition behind dividing the visible ROI into a 4-dimensional grid in the x and y directions comes from the fact that the center of the image very closely corresponds to the optic center of the lens, and hence a 4x4 grid in the image space demarcates the 3D space in the same directions. This simplifies our normalization process given the x and y maximum limits. This also ensures that each grid in the camera plane contains an object within the corresponding 3D space; for example, the top-left grid contains objects with their center at negative x and negative y 3D coordinates, and the top-right grid contains objects with their center at positive x and negative y 3D coordinates.</p><p>This Cubification of the camera space is visualized in <ref type="figure">figure 5</ref>. We assume the standard camera coordinate system for our work (as used in computer vision applications), where, x points to the right and is aligned with the image x-axis, y points downward and is aligned with the image yaxis, and z points in the direction that the camera is pointed at.</p><p>We normalize 3D coordinates of the center of the object (x, y, z) and dimensions (width, height, length) between (0, 1) in accordance with a prior that is computed from data statistics. Orientation is normalized by simply scaling the values from (−π, π) to (0, 1). We use a Sigmoid activation function at the output of the final fully-connected layer for these predictions. Each predicted object ROI with high confidence is then projected to the 2D camera plane, cropped, and resized to a 64x64 patch, which is then fed to a classifier network that predicts a class for the detected object. <ref type="figure">Figure 5</ref>. Cubification of the camera space: The perception region of interest is divided into a 4x4xM grid (4x4 in the x and y directions aligned with the camera image plane, where each grid has stacked on it, M cuboids in the z direction). Each cuboid is responsible for predicting up to N object poses. The object coordinates and dimensions are then normalized between 0 and 1 in accordance with a prior that is computed from data statistics.</p><p>The total loss function for this model is a weighted sum of 5 individual loss terms for minimizing the detection loss of object center coordinates, object dimensions, orientation, and their detection confidence. We take inspiration from YOLO <ref type="bibr" target="#b15">[16]</ref> for the development of these loss functions.</p><p>Additionally, we also try to maximize 3D intersectionover-union (IoU) explicitly by minimizing the iou loss (eqn.7) which is computed as negative log-likelihood of 3D IOU between ground-truth and prediction. Attempting to minimize this loss improves training by allowing the network to converge faster while improving the accuracy and mean IoU as shown in <ref type="figure">figure 6</ref>. However, a very small weight, λ iou is chosen for this loss function to avoid the log term from exploding.</p><formula xml:id="formula_1">xyz loss = λ xyz n true objs 4 i=0 M j=0 N k=0 T n objs ijk [(x ijk −x ijk ) 2 ]+ [(y ijk −ŷ ijk ) 2 ] + [(z ijk −ẑ ijk ) 2 ]<label>(4)</label></formula><formula xml:id="formula_2">whl loss = λ whl n true objs 4 i=0 M j=0 N k=0 T n objs ijk [( √ w ijk − ŵ ijk ) 2 ]+ [( h ijk − ĥ ijk ) 2 ] + [( l ijk − l ijk ) 2 ]<label>(5)</label></formula><p>orientation loss = λ orientation n true objs</p><formula xml:id="formula_3">4 i=0 M j=0 N k=0 T n objs ijk [(o ijk −ô ijk ) 2 ]<label>(6)</label></formula><formula xml:id="formula_4">iou loss = λ iou n true objs 4 i=0 M j=0 N k=0 −T n objs ijk [log (iou ijk )] (7) conf loss = λ conf 4 * N * M 4 i=0 M j=0 N k=0 T n objs ijk [(c ijk −ĉ ijk ) 2 ]+ (1 − T n objs ijk )[(c ijk −ĉ ijk ) 2 ]<label>(8)</label></formula><p>total loss = xyz loss + whl loss + orientation loss + iou loss + conf loss</p><p>In equations <ref type="bibr">[4 -9]</ref>, M is the number of cuboids for each (of the 4) 2D image grid-sections, N is the maximum number of possible objects per cuboid, and n true objs is the number of ground-truth objects. T ijk in these equations indicates whether or not an object appears at that position in the ground-truth label vector, its value being 1 if it does, 0 otherwise. <ref type="figure">Figure 6</ref>. Introducing the iou loss improves error convergence and mean intersection-over-union (IoU). x-axis shows number of training epochs, left y-axis shows the xyz loss which is the meansquared error of object center coordinate predictions (lower is better), right y-axis shows the mean 3D intersection-over-union between detected objects and ground-truth (higher is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classifier</head><p>The output of our 3DOD model is a vector of objects poses along with a confidence value for their prediction. We compute 8 vertices for each bounding-box and project them onto the 2D camera plane. A 2D bounding-box is then obtained for each detected object, cropped, and resized to a 64x64 patch. All object crops are then stacked and fed to the classifier network which is responsible for predicting a class for each detected object using Softmax activation function. Additionally, we also feed the normalized whl (width, height, length) vector for each detected object to the network which is concatenated with the output of first fullyconnected layer as shown in <ref type="figure" target="#fig_1">figure 3</ref>. This further helps the network establish a relationship between object classes and their dimensions. Our classifier model has a few residual blocks followed by fully-connected layers and contains only 430k parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Details</head><p>We start with training the RGB-to-depth auto-encoder on the Virtual KITTI 2 dataset by considering depths up to 100 meters. Any depth value higher than that is set to 100 meters. The dense depth map is then normalized between [−0.5, 0.5]. Once trained, the pre-trained encoder of RGB-to-depth auto-encoder is used to train our 3DOD model. We perform 3D Object Detection experiments on the Virtual KITTI 2 <ref type="bibr" target="#b2">[3]</ref>, and KITTI <ref type="bibr" target="#b5">[6]</ref> datasets. For Virtual KITTI 2, we use 18,943 training and 1,049 validation frames for training. 1000 samples were randomly chosen from the dataset and separated as a test set. KITTI 3D object detection dataset contains 7481 training samples and 7518 testing samples (test split). Following <ref type="bibr" target="#b19">[20]</ref>, we divide the training samples into train split (3712 samples) and val split (3769 samples) for evaluation of our method. During the training of our classifier model, we randomly perturb the center of object position (x, y, z) by adding a small amount of noise in the range of [−0.2 meters, 0.2 meters] to improve the robustness of classification. We also perform data augmentation by flipping the images in a leftright fashion and obtaining x center ← x center , orientation ← (π -orientation); further augmentation is obtained by adjusting saturation and brightness by first converting the RGB images into HSV colorspace and then multiplying the saturation (S) and value (V ) channel by a factor randomly chosen between [0.5, 1.5], and then converting it back to RGB. Other types of commonly used data augmentation techniques such as cropping, rotation, and offsetting the image plane, would need more sophisticated algorithms because of the requirement for the determination of new object poses as a result of image warping and will be considered as future work.</p><p>For our work presented here, we use 5 cuboids per 2D (xy plane) grid section -chosen after ablation study on model performance with various combinations (see <ref type="table" target="#tab_3">table 3</ref> for details), and allow the detection of a maximum of 10 objects per cuboid. As a result, our model is able to predict up to 4(grids)x5(cuboids/grid)x10(objects/cuboid)=200 objects within a 100 meter range. We specify a region-ofinterest (ROI) and only consider objects within the ROI for our experiments. Furthermore, we obtain a prior on object dimensions from the dataset statistics. The ROI around the ego-camera used in our work is: [x = 40m, y = 10m, z = 100m]. We compute priors for each dataset separately and normalize them between Additionally, we also experiment by replacing the encoder head with a pre-trained backbone network (VGG-16). This results in slightly improved performance as tabulated in table 2. For our VGG-16 based model, we do not use U-Net like skip connections between encoder and decoder; we extract all the VGG-16 layers up until the last 16x16 spatial resolution layer (namely block5 conv3) to replace the encoder head with. The choice of using 5 cuboids per grid and 10 objects per cuboid was made after an exhaustive search through various combinations of cuboids and objects per cuboid, shown in the ablation study table 3. The table shows results for the car, pedestrian and cyclist classes for 3D and BEV IoUs in the KITTI object detection eval kit. IoU thresholds for these classes for computing metrics were 0.7, 0.5 and 0.5 respectively. 5 cuboids and 10 objects gave the best results for the car (easy), pedestrian (easy) and cyclist (easy, moderate, and hard) classes and difficulty levels. While 6 cuboids, 10 objects per cuboid gave better results on some classes, we chose the former to save on compute.</p><p>We train our models for 1500 epochs with a batch size of 16, learning rate of 10 −4 and decay rate of 0.001. The Adam Optimizer with β 1 = 0.9, β 2 = 0.999 is used for error optimization. Lambda weights used for loss functions are: λ mse = 0.8, λ eas = 0.2, λ xyz = 5.0, λ whl = 5.0, λ orientation = 1.0, λ iou = 0.01, λ conf = 0.5. Our complete model (Encoder of RGB-to-depth AE + 3DOD + Classifier), has 38.9M parameters and runs at 7.2 FPS on a single NVIDIA RTX 2070 GPU during inference. To avoid overfitting, we use dropout layers with dropout rate of 0.5 in our auto-encoder as well as 3DOD model. In the classifier model we employ L2 regularization with a factor of 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We compute the Mean Average Precision for the vKITTI2 and KITTI datasets using 101-point interpolation as used in MS COCO <ref type="bibr" target="#b8">[9]</ref> for 3D IoU thresholds of 0.3, 0.5, and 0.7 by varying the confidence threshold between [0.0, 1.0] across all classes as shown in table 1 (number of classes: KITTI -8, Virtual KITTI -3). <ref type="figure">Figure 8</ref> shows the Precision-Recall curves as computed using the KITTI object detection benchmark eval kit for 3D object detection problem (easy, medium, and hard difficulty levels). IOU thresholds used for obtaining these graphs were 0.7, 0.5, and 0.5 for car, pedestrian, and cyclist respectively. Additionally, we do comprehensive quantitative benchmarking <ref type="figure">Figure 7</ref>. Distribution of errors for the car class. Because of the cubification method, our errors are localized within each cuboid and does not increase with respect to absolute distance from the ego-vehicle. and comparisions with other SoTA methods for the KITTI dataset and summarize them in table 2. Aside from the most recent M3D-RPN <ref type="bibr" target="#b1">[2]</ref> work, we notice that our bestperforming method (with 5 cuboids, 10 objects per cuboid and the vgg-16 head), is able to compete with other SoTA techniques, especially in the moderate and hard difficulty levels, even beating M3D-RPN in the cyclist class. Our method is second-best in terms of moderate and hard difficulty levels for the car class (the only class reported in all competing methods except for M3D-RPN). Qualitative results of our model (w/vgg-16) on the KITTI object detection dataset (val split) is shown in <ref type="figure" target="#fig_3">figure 9</ref>.  <ref type="figure">Figure 8</ref>. Precision-Recall curves obtained using KITTI 3D object detection eval kit. These were computed on KITTI val split for car, pedestrian, and cyclist classes using 3D IOU of 0.7, 0.5, and 0.5 respectively.</p><p>We also attempt to model the error distribution with respect to the absolute distance from the ego-vehicle for each class. We find that, because of our cubification techniques, the error is highly localized within each cuboid and does not increase with increasing distance from the ego-vehicle. Errors are found to be the minimum at the center of each cuboid and maximum at the beginning and end of each cuboid for cars as evident from figure 7. This figure shows the plot of error distribution for the car and pedestrian class. Note that the cuboid size in the depth (z) direction was chosen to be 20 meters and hence error peaks in z can be seen at 0, 20, 40, 60, 80, and 100 meters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Future Work</head><p>CubifAE-3D leverages the power of simulation to learn an embedding for RGB-to-depth, which is then used to train a network to predict object poses from RGB images on real datasets. We find that our method does surprisingly well on objects that are far from the ego-vehicle and is represented by only a few pixels, as is evident from our results on the KITTI moderate and hard difficulty levels. Additionally, with the cubification technique, we are able to localize the position error to within each cuboid so that it does not increase with increasing distance from the ego-camera. This encourages us to believe that with further quantification of the 3D space, we will be able to achieve higher accuracy and this will be explored in our future work. A non-uniform or a coarse-to-fine cubification technique is also something of interest to fill the gap between short-range and long-range performance and reduce errors at cuboid boundaries. Furthermore, we would like to perform projective data augmentation in order to virtually zoom and move the camera through space to increase the amount of labelled 3D pose training data so that the 3DOD network can generalize better on real datasets.</p><p>The techniques presented in this paper: pre-training an  RGB-to-depth embedding from simulation and cubifying 3D space are also likely to be useful for other AV perception tasks that are dependent on pixel-level depth embeddings, without the need for a ground-truth dense depth map, like free space/occupancy grid estimation or 6 DoF monocular localization of the ego-camera/AV.</p><p>We show that our method already outperforms some SoTA methods for monocular 3D object detection and we are hopeful that our continuing improvements to CubifAE-3D will serve as a baseline for monocular 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>3D object detection is a primary task in the perception pipeline for an AV and we present CubifAE-3D, a method for doing this using monocular imagery. Our method utilizes dense depth information corresponding to RGB images from a Gaming Engine based simulator and this is used to pre-train a RGB-to-depth auto-encoder (AE). The encoder of this pre-trained AE is then detached and used as an embedder for RGB images. This embedding is sub-sequently used to train our 3D Object Detector (3DOD), which is trained in a fully supervised manner and requires RGB images and corresponding 3D object pose annotations. Our 3DOD cubifies the 3D space around the camera into a grid of cuboids, where each cuboid is trained to output N 3D object poses and their confidence scores. Once trained, our aggregate detector CubifAE-3D (comprising of the AE-encoder and the 3DOD) is able to generate accurate 3D object poses and classifications from RGB images. The particular scheme of cubification of the 3D space gives results whose accuracy is maintained with distance from the camera, with an average localization error of 0.2m and a never-exceed localization error of 0.4m upto a detection distance of 100m from the ego-vehicle for vehicle, pedestrian and cyclist classes in the KITTI dataset. We believe that CubifAE-3D represents an important advancement in the field for fast, accurate 3D object detection from monocular images. The ability to do this robustly and accurately from cheap cameras (instead of expensive LIDAR) that are already present in a surround-view configuration in present generation commercial vehicles will be crucial in the development and democratization of AV technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials -CubifAE-3D: Monocular Camera Space Cubification</head><p>for Auto-Encoder based 3D Object Detection S1. Supplementary Materials S1.1. Qualitative Results    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Object pose predictions using CubifAE-3D on monocular RGB camera images from the vKITTI2 (top) and KITTI (bottom) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Detailed model architecture of CubifAE-3D. The RGB-to-depth auto-encoder (top branch) is first trained in a supervised way with a combination of MSE and Edge-Aware Smoothing Loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Monocular RGB to Depth Map prediction for the vKITTI2 dataset. Top: input RGB images. Centre: ground-truth depth maps. Bottom: the depth map predictions from our RGB to Depth auto-encoder</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 .</head><label>9</label><figDesc>Samples of qualitative results on the KITTI dataset. The top part of each image shows a bounding box obtained as a 2D projection of their 3D poses (red: car, green: van, blue: pedestrian, yellow: truck, cyan: cyclist, tram, others). The bottom part shows a birds-eye view of the object poses with the ego-vehicle positioned at the center of red circle drawn on the left; pointing towards the right of the image. Front of each object is shown by a green line to indicate its orientation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>This section demonstrates qualitative results of our method on KITTI object detection dataset (val split) [figure Sa, figure Sb] and Virtual KITTI 2 dataset [figure Sc]. Number of classes considered for the former is 8 and for the latter is 3. Different classes are colored differently, more details in each figure caption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><label></label><figDesc>Sa. Qualitative results on the KITTI dataset. The top part of each image shows a bounding box obtained as a 2D projection of their 3D poses (red: car, yellow: truck, green: van, blue: pedestrian, cyan: tram, cyclist, and others). The bottom part shows a birds-eye view of the object poses with the ego-vehicle positioned at the center of red circle drawn on the left; pointing towards the right of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><label></label><figDesc>Sb. Qualitative results on the KITTI dataset. The top part of each image shows a bounding box obtained as a 2D projection of their 3D poses (red: car, yellow: truck, green: van, blue: pedestrian, cyan: tram, cyclist, and others). The bottom part shows a birds-eye view of the object poses with the ego-vehicle positioned at the center of red circle drawn on the left; pointing towards the right of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure</head><label></label><figDesc>Sc. Qualitative results on the Virtual KITTI 2 dataset across various scenes, weather, and lighting conditions. The top part of each image shows a bounding box obtained as a 2D projection of their 3D poses (red: car, yellow: truck, cyan: van). The bottom part shows a birds-eye view of the object poses with the ego-vehicle positioned at the center of red circle drawn on the left; pointing towards the right of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>[0, 1] for training. Priors used for the datasets were: Virtual KITTI 2: [width min = 1.13, width max = 3.02, height min = 1.22, height max = 4.20, length min = 2.22, length max = 16.44]. KITTI: [width min = 0.30, width max = 3.01, height min = 0.76, height max = 4.20, length min = 0.20, length max = 35.24].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Monocular 3D Object Detection results on the vKITTI2 and KITTI datasets. mAPx stands for % of Mean Average Precision (mAP) for 3D IoU threshold of x. Higher is better.</figDesc><table><row><cell></cell><cell cols="3">Method</cell><cell></cell><cell cols="4">Dataset</cell><cell cols="9">mAP .3 mAP .5 mAP .7</cell></row><row><cell></cell><cell cols="10">CubifAE-3D vKITTI 2 86.6</cell><cell></cell><cell cols="3">66.7</cell><cell cols="2">34.1</cell><cell></cell></row><row><cell></cell><cell cols="7">CubifAE-3D KITTI</cell><cell></cell><cell cols="2">83.8</cell><cell></cell><cell cols="3">59.2</cell><cell cols="2">27.0</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Car</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pedestrian</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cyclist</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>Easy</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Easy</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Easy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Moderate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Moderate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Moderate</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Hard</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Monocular 3D Object Detection Results on the KITTI validation split for car, pedestrian, and cyclist. These metrics are evaluated by AP |R11 at 0.7 IoU threshold for car, and 0.5 IoU threshold for pedestrian and cyclist. Higher is better. Best performing method is highlighted in bold and second best in green.</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell cols="3">Easy Mod. Hard</cell></row><row><cell>OFT-Net[17]</cell><cell>Car</cell><cell>4.07</cell><cell>3.27</cell><cell>3.29</cell></row><row><cell>FQNet[10]</cell><cell>Car</cell><cell>5.98</cell><cell>5.50</cell><cell>4.75</cell></row><row><cell>Mono3D++[7]</cell><cell>Car</cell><cell cols="2">10.60 7.90</cell><cell>5.70</cell></row><row><cell>ROI-10D[12]</cell><cell>Car</cell><cell>9.61</cell><cell>6.63</cell><cell>6.29</cell></row><row><cell>MF3D[24]</cell><cell>Car</cell><cell cols="2">10.53 5.69</cell><cell>5.39</cell></row><row><cell>GS3D[8]</cell><cell>Car</cell><cell cols="3">11.63 10.51 10.51</cell></row><row><cell>MonoGRNet[15]</cell><cell>Car</cell><cell cols="3">13.88 10.19 7.62</cell></row><row><cell cols="2">Barabanau et al.[1] Car</cell><cell cols="2">13.96 7.37</cell><cell>4.54</cell></row><row><cell>M3D-RPN[2]</cell><cell>Car</cell><cell cols="3">20.27 17.06 15.21</cell></row><row><cell>M3D-RPN[2]</cell><cell>Ped.</cell><cell>-</cell><cell cols="2">11.28 -</cell></row><row><cell>M3D-RPN[2]</cell><cell cols="2">Cyclist -</cell><cell cols="2">10.01 -</cell></row><row><cell>ours</cell><cell>Car</cell><cell>7.94</cell><cell>9.21</cell><cell>10.43</cell></row><row><cell>ours</cell><cell>Ped.</cell><cell>4.64</cell><cell>4.79</cell><cell>4.96</cell></row><row><cell>ours</cell><cell cols="2">Cyclist 8.27</cell><cell>8.61</cell><cell>9.01</cell></row><row><cell>ours (w/vgg-16)</cell><cell>Car</cell><cell>9.89</cell><cell cols="2">11.20 12.33</cell></row><row><cell>ours (w/vgg-16)</cell><cell>Ped.</cell><cell>6.74</cell><cell>6.82</cell><cell>7.04</cell></row><row><cell>ours (w/vgg-16)</cell><cell cols="4">Cyclist 12.93 12.18 12.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Cubification ablation study: results with varying cuboids and objects per grid. Green and blue highlights show the best performing variant for the BEV and 3D IOUs respectively in the KITTI object detection dataset. n cuboids n objects 3D / BEV Car (Easy/Mod./Hard) Pedestrian (Easy/Mod./Hard) Cyclist (Easy/Mod./Hard) 4 10 3D 5.30 / 7.40 / 8.32 2.56 / 2.47 / 2.78 4.91 / 5.73 / 5.91 4</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection via geometric reasoning on keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Barabanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vyacheslav</forename><surname>Murashkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Virtual kitti 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naila</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Humenberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Céline</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mono3d++: Monocular 3d vehicle detection with two-scale 3d hypotheses and task priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection. CoRR, abs/1506.02640</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08188</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Depth completion from sparse lidar data with depth-normal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pixor: Realtime 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06310</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Jiquan Ngiam, and Vijay Vasudevan. End-to-end multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
