<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
							<email>tianyug@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
							<email>danqic@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We hypothesize that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we draw inspiration from the recent success of learning sentence embeddings from natural language inference (NLI) datasets and incorporate annotated pairs from NLI datasets into contrastive learning by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 74.5% and 81.6% Spearman's correlation respectively, a 7.9 and 4.6 points improvement compared to previous best results. We also show that contrastive learning theoretically regularizes pretrained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning universal sentence embeddings is a fundamental problem in natural language processing and has been studied extensively in the literature <ref type="bibr" target="#b27">(Kiros et al., 2015;</ref><ref type="bibr" target="#b22">Hill et al., 2016;</ref><ref type="bibr" target="#b12">Conneau et al., 2017;</ref><ref type="bibr" target="#b31">Logeswaran and Lee, 2018;</ref><ref type="bibr" target="#b8">Cer et al., 2018;</ref><ref type="bibr">Reimers and Gurevych, 2019, inter alia)</ref>. In this work, we advance state-of-the-art sentence embedding methods and demonstrate that a * The first two authors contributed equally (listed in alphabetical order). This work was done when Xingcheng visited the Princeton NLP group remotely. <ref type="bibr">1</ref> Our code and pre-trained models are publicly available at https://github.com/princeton-nlp/SimCSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERTbase</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised</head><p>Avg. embeddings 56.7 IS- <ref type="bibr">BERT (prev. SoTA)</ref> 66.6 SimCSE 74.5 (+7.9%) Supervised SBERT 74.9 SBERT-whitening (prev. SoTA) 77.0 SimCSE 81.6 (+4.6%) <ref type="table" target="#tab_15">Table 1</ref>: Comparison between SimCSE and previous state-of-the-art (unsupervised and supervised). The reported numbers are the average of seven STS tasks (Spearman's correlation), see <ref type="table" target="#tab_6">Table 6</ref> for details. IS-BERT, SBERT, SBERT-whitening: <ref type="bibr" target="#b58">Zhang et al. (2020)</ref>, <ref type="bibr" target="#b43">Reimers and Gurevych (2019)</ref> and <ref type="bibr" target="#b46">Su et al. (2021)</ref>.</p><p>contrastive objective can be extremely effective in learning sentence embeddings, coupled with pre-trained language models such as BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> and RoBERTa . We present SimCSE, a simple contrastive sentence embedding framework, which can be used to produce superior sentence embeddings, from either unlabeled or labeled data.</p><p>Our unsupervised SimCSE simply predicts the input sentence itself, with only dropout <ref type="bibr" target="#b45">(Srivastava et al., 2014)</ref> used as noise <ref type="figure" target="#fig_1">(Figure 1(a)</ref>). In other words, we pass the same input sentence to the pretrained encoder twice and obtain two embeddings as "positive pairs", by applying independently sampled dropout masks. Although it may appear strikingly simple, we find that this approach largely outperforms training objectives such as predicting next sentences <ref type="bibr" target="#b27">(Kiros et al., 2015;</ref><ref type="bibr" target="#b31">Logeswaran and Lee, 2018)</ref> and common data augmentation techniques, e.g., word deletion and replacement. More surprisingly, this unsupervised embedding method already matches all the previous supervised approaches. Through careful analysis, we find that dropout essentially acts as minimal data augmentation, while removing it leads to a representation collapse.</p><p>The pets are sitting on a couch. Different dropout masks in two forward passes There are animals outdoors.</p><p>There is a man.</p><p>The man wears a business suit.</p><p>A kid is skateboarding.</p><p>A kit is inside the house.</p><p>Two dogs are running.</p><p>A man surfing on the sea.</p><p>A kid is on a skateboard.  In our supervised SimCSE, we build upon the recent success of leveraging natural language inference (NLI) datasets for sentence embeddings <ref type="bibr" target="#b12">(Conneau et al., 2017;</ref><ref type="bibr" target="#b43">Reimers and Gurevych, 2019)</ref> and incorporate supervised sentence pairs in contrastive learning <ref type="figure" target="#fig_1">(Figure 1(b)</ref>). Unlike previous work that casts it as a 3-way classification task (entailment/neutral/contradiction), we take advantage of the fact that entailment pairs can be naturally used as positive instances. We also find that adding corresponding contradiction pairs as hard negatives further improves performance. This simple use of NLI datasets achieves a greater performance compared to prior methods using the same datasets. We also compare to other (annotated) sentence-pair datasets and find that NLI datasets are especially effective for learning sentence embeddings.</p><p>To better understand the superior performance of SimCSE, we borrow the analysis tool from <ref type="bibr" target="#b50">Wang and Isola (2020)</ref>, which takes alignment between semantically-related positive pairs and uniformity of the whole representation space to measure the quality of learned embeddings. We prove that theoretically the contrastive learning objective "flattens" the singular value distribution of the sentence embedding space, hence improving the uniformity. We also draw a connection to the recent findings that pre-trained word embeddings suffer from anisotropy <ref type="bibr" target="#b16">(Ethayarajh, 2019;</ref><ref type="bibr" target="#b29">Li et al., 2020)</ref>. We find that our unsupervised SimCSE essentially improves uniformity while avoiding degenerated alignment via dropout noise, thus greatly improves the expressiveness of the representations. We also demonstrate that the NLI training signal can further improve alignment between positive pairs and hence produce better sentence embeddings.</p><p>We conduct a comprehensive evaluation of Sim-CSE, along with previous state-of-the-art models on 7 semantic textual similarity (STS) tasks and 7 transfer tasks. On STS tasks, we show that our unsupervised and supervised models achieve a 74.5% and 81.6% averaged Spearman's correlation respectively using BERT base , largely outperforming previous best <ref type="table" target="#tab_15">(Table 1)</ref>. We also achieve competitive performance on the transfer tasks. Additionally, we identify an incoherent evaluation issue in existing work and consolidate results of different evaluation settings for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Contrastive Learning</head><p>Contrastive learning aims to learn effective representation by pulling semantically close neighbors together and pushing apart non-neighbors <ref type="bibr" target="#b19">(Hadsell et al., 2006)</ref>. It assumes a set of paired examples</p><formula xml:id="formula_0">D = {(x i , x + i )} m i=1</formula><p>, where x i and x + i are semantically related. We follow the contrastive framework in  and take a cross-entropy objective with in-batch negatives <ref type="bibr" target="#b10">(Chen et al., 2017;</ref><ref type="bibr" target="#b21">Henderson et al., 2017)</ref>: let h i and h + i denote the representations of x i and x + i , for a mini-batch with N pairs, the training objective for (</p><formula xml:id="formula_1">x i , x + i ) is: i = log e sim(h i ,h + i )/τ N j=1 e sim(h i ,h + j )/τ ,<label>(1)</label></formula><p>where τ is a temperature hyperparameter and sim(h 1 , h 2 ) is the cosine similarity</p><formula xml:id="formula_2">h 1 h 2 h 1 · h 2 .</formula><p>In this work, we encode input sentences using a pre-trained language model such as BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> or RoBERTa : h = f θ (x), and then fine-tune all the parameters using the contrastive learning objective (Eq. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive instances</head><p>One critical question in contrastive learning is how to construct (x i , x + i ) pairs. In visual representations, an effective solution is to take two random transformations of the same image (e.g., cropping, flipping, distortion and rotation) as x i and x + i <ref type="bibr" target="#b15">(Dosovitskiy et al., 2014)</ref>. A similar approach has been recently adopted in language representations <ref type="bibr" target="#b33">Meng et al., 2021)</ref>, by applying augmentation techniques such as word deletion, reordering, and substitution. However, data augmentation in NLP is inherently difficult because of its discrete nature. As we will see in §3, using standard dropout on intermediate representations outperforms these discrete operators.</p><p>In NLP, a similar contrastive learning objective has been also explored in different contexts <ref type="bibr" target="#b21">(Henderson et al., 2017;</ref><ref type="bibr" target="#b18">Gillick et al., 2019;</ref><ref type="bibr" target="#b25">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b28">Lee et al., 2020)</ref>. In these cases, (x i , x + i ) are collected from supervised datasets such as mention-entity, or question-passage pairs. Because of the distinct nature of x i and x + i by definition, these approaches always use a dualencoder framework, i.e., using two independent encoders f θ 1 and f θ 2 for x i and x + i . For sentence embeddings, <ref type="bibr" target="#b31">Logeswaran and Lee (2018)</ref> also use contrastive learning with a dual-encoder approach, by forming (current sentence, next sentence) as (x i , x + i ). <ref type="bibr" target="#b58">Zhang et al. (2020)</ref> consider global sentence representations and local token representations of the same sentence as positive instances.</p><p>Alignment and uniformity Recently, <ref type="bibr" target="#b50">Wang and Isola (2020)</ref> identify two key properties related to contrastive learning: alignment and uniformity and propose metrics to measure the quality of representations. Given a distribution of positive pairs p pos , alignment calculates expected distance between embeddings of the paired instances (assuming representations are already normalized),</p><formula xml:id="formula_3">align E (x,x + )∼ppos f (x) − f (x + ) 2 .<label>(2)</label></formula><p>On the other hand, uniformity measures how well the embeddings are uniformly distributed:</p><formula xml:id="formula_4">uniform log E x,y i.i.d. ∼ p data e −2 f (x)−f (y) 2 ,<label>(3)</label></formula><p>where p data denotes the data distribution. These two metrics are well aligned with the objective of contrastive learning: positive instances should stay close and embeddings for random instances should scatter on the hypersphere. In the following sections, we will also use the two metrics to justify the inner workings of our approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsupervised SimCSE</head><p>In this section, we describe our unsupervised Sim-CSE model. The idea is extremely simple: we take a collection of sentences {x i } m i=1 and use x + i = x i . The key ingredient to get this to work with identical positive pairs is through the use of independently sampled dropout masks. In standard training of Transformers <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>, there is a dropout mask placed on fully-connected layers as well as attention probabilities (default p = 0.1). We denote h z i = f θ (x i , z) where z is a random mask for dropout. We simply feed the same input to the encoder twice by applying different dropout masks z, z and the training objective becomes:</p><formula xml:id="formula_5">i = − log e sim(h z i i ,h z i i )/τ N j=1 e sim(h z i i ,h z j j )/τ ,<label>(4)</label></formula><p>for a mini-batch with N sentences. Note that z is just the standard dropout mask in Transformers and we do not add any additional dropout.</p><p>Dropout noise as data augmentation We view this approach as a minimal form of data augmentation: the positive pair takes exactly the same sentence, and their embeddings only differ in dropout masks. We compare this approach to common augmentation techniques and other training objectives on the STS-B development set <ref type="bibr" target="#b7">(Cer et al., 2017)</ref>.  <ref type="table" target="#tab_15">Table 3</ref>: Comparison of different unsupervised objectives. Results are Spearman's correlation on the STS-B development set using BERT base , trained on 1million pairs from Wikipedia. The two columns denote whether we use one encoder f θ or two independent encoders f θ1 and f θ2 ("dual-encoder"). Next 3 sentences: randomly sample one from the next 3 sentences. Delete one word: delete one word randomly (see <ref type="table" target="#tab_15">Table 2</ref>).  We use N = 512 and m = 10 6 sentences randomly drawn from English Wikipedia in these experiments. <ref type="table" target="#tab_15">Table 2</ref> compares our approach to common data augmentation techniques such as crop, word deletion and replacement, which can be viewed as h = f θ (g(x), z) and g is a (random) discrete operator on x. We find that even deleting one word would hurt performance and none of the discrete augmentations outperforms basic dropout noise. We also compare this self-prediction training objective to next-sentence objective used in Logeswaran and <ref type="bibr" target="#b31">Lee (2018)</ref>, taking either one encoder or two independent encoders. As shown in <ref type="table" target="#tab_15">Table 3</ref>, we find that SimCSE performs much better than the next-sentence objectives (79.1 vs 69.7 on STS-B) and using one encoder instead of two makes a significant difference in our approach.</p><p>Why does it work? To further understand the role of dropout noise in unsupervised SimCSE, we try out different dropout rates in <ref type="table" target="#tab_2">Table 4</ref> and observe that all the variants underperform the default dropout probability p = 0.1 from Transformers. We find two extreme cases particularly interesting: "no dropout" (p = 0) and "fixed 0.1" (using default dropout p = 0.1 but the same dropout masks for the pair). In both cases, the resulting embeddings   <ref type="figure">Figure 2</ref>: alignuniform plot for unsupervised SimCSE, "no dropout", "fixed 0.1" (same dropout mask for x i and x + i with p = 0.1), and "delete one word". We visualize checkpoints every 10 training steps and the arrows indicate the training direction. For both align and uniform , lower numbers are better.</p><formula xml:id="formula_6">X w = " &gt; A A A C A H i c b V A 9 S w N B E N 2 L X z F + n V p Y 2 B w m g l W 4 C 4 q W Q R v L C O Y D c k f Y 2 0 y S J X s f 7 M 6 J 4 b j G v 2 J j o Y i t P 8 P O f + M m u U I T H w w 8 3 p t h Z p 4 f C 6 7 Q t r + N w s r q 2 v p G c b O 0 t b 2 z u 2 f u H 7 R U l E g G T R a J S H Z 8 q k D w E J r I U U A n l k A D X 0 D b H 9 9 M / f Y D S M W j 8 B 4 n M X g B H Y Z 8 w B l F L f X M o 4 o L Q v R S F + E R 0 0 R b k Q y y r N I z y 3 b V n s F a J k 5 O y i R H o 2 d + u f 2 I J Q G E y A R V q u v Y M X o p l c i Z g K z k J g p i y s Z 0 C F 1 N Q x q A 8 t L Z A 5 l 1 q p W + p V f r C t G a q b 8 n U h o o N Q l 8 3 R l Q H K l F b y r + 5 3 U T H F x 5 K Q / j B C F k 8 0 W D R F g Y W d M 0 r D 6 X w F B M N K F M c n 2 r x U Z U U o Y 6 s 5 I O w V l 8 e Z m 0 a l X n v H p x V y v X r / M 4 i u S Y n J A z 4 p B L U i e 3 p E G a h J G M P J N X 8 m Y 8 G S / G u / E x b y 0 Y + c w h + Q P j 8 w d c I Z b o &lt; / l a t e x i t &gt; uniform align &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L X r C a B I B F U / V J M 8 f r P g L j L C Y e c c = " &gt; A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A V P J W k K H o s e v F Y w X 5 A W 8 p m O 2 2 X b j Z h d y K W E P C v e P G g i F d / h z f / j d s 2 B 2 1 9 M P B 4 b 4 a Z e X 4 k u E b X / b Z y K 6 t r 6 x v 5 z c L W 9 s 7 u n r 1 / 0 N B h r B j U W S h C 1 f K p B s E l 1 J G j g F a k g A a + g K Y / v p n 6 z Q d Q m o f y H i c R d A M 6 l H z A G U U j 9 e y j U g e E 6 C U d h E d M q O B D m a a l n l 1 0 y + 4 M z j L x M l I k G W o 9 + 6 v T D 1 k c g E Q m q N Z t z 4 2 w m 1 C F n A l I C 5 1 Y Q 0 T Z m A 6 h b a i k A e h u M j s / d U 6 N 0 n c G o T I l 0 Z m p v y c S G m g 9 C X z T G V A c 6 U V v K v 7 n t W M c X H U T L q M Y Q b L 5 o k E s H A y d a R Z O n y t g K C a G U K a 4 u d V h I 6 o o Q 5 N Y w Y T g L b 6 8 T B q V s n d e v r i r F K v X W R x 5 c k x O y B n x y C W p k l t S I 3 X C S E K e</formula><p>for the pair are exactly the same, and it leads to a dramatic performance degradation. We take the checkpoints of these models every 10 steps during training and visualize the alignment and uniformity metrics 2 in <ref type="figure">Figure 2</ref>, along with a simple data augmentation model "delete one word". As is clearly shown, all models largely improve the uniformity. However, the alignment of the two special variants also degrades drastically, while our unsupervised SimCSE keeps a steady alignment, thanks to the use of dropout noise. On the other hand, although "delete one word" slightly improves the alignment, it has a smaller gain on the uniformity, and eventually underperforms unsupervised SimCSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Supervised SimCSE</head><p>We have demonstrated that adding dropout noise is able to learn a good alignment for positive pairs (x, x + ) ∼ p pos . In this section, we study whether we can leverage supervised datasets to provide better training signals for improving alignment of our approach. Prior work <ref type="bibr" target="#b12">(Conneau et al., 2017;</ref><ref type="bibr" target="#b43">Reimers and Gurevych, 2019)</ref> has demonstrated that supervised natural language inference (NLI) datasets <ref type="bibr" target="#b6">(Bowman et al., 2015;</ref><ref type="bibr" target="#b54">Williams et al., 2018)</ref> are effective for learning sentence embeddings, by predicting whether the relationship between two sentences is entailment, neutral or contradiction. In our contrastive learning framework, we instead directly take (x i , x + i ) pairs from supervised datasets and use them to optimize Eq. 1.  Exploiting supervised data We first explore which annotated datasets are especially suitable for constructing positive pairs (x i , x + i ). We experiment with a number of datasets with sentencepair examples, including QQP 4 : Quora question pairs; Flickr30k <ref type="bibr" target="#b57">(Young et al., 2014)</ref>: each image is annotated with 5 human-written captions and we consider any two captions of the same image as a positive pair; ParaNMT <ref type="bibr" target="#b52">(Wieting and Gimpel, 2018)</ref>: a large-scale back-translation paraphrase dataset 5 ; and finally NLI datasets: SNLI <ref type="bibr" target="#b6">(Bowman et al., 2015)</ref> and MNLI <ref type="bibr" target="#b54">(Williams et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>We train the contrastive learning model (Eq. 1) with different datasets and compare the results in <ref type="table" target="#tab_4">Table 5</ref> (for a fair comparison, we also run experiments with the same # of training pairs). We find that most of these models using supervised datasets outperform our unsupervised approach, showing a clear benefit from supervised signals. Among all the options, using entailment pairs from the NLI (SNLI + MNLI) datasets perform the best. We think this is reasonable, as the NLI datasets consist of high-quality and crowd-sourced pairs, and human annotators are expected to write the hypotheses manually based on the premises, and 3 Though our final model only takes entailment pairs as positives, here we also try neutral and contradiction pairs. 4 https://www.quora.com/q/quoradata/ 5 ParaNMT is automatically constructed by machine translation systems and we should not call it a supervised dataset, although it even underperforms our unsupervised SimCSE. hence two sentences tend to have less lexical overlap. For instance, we find that the lexical overlap (F1 measured between two bags of words) for the entailment pairs (SNLI + MNLI) is 39%, while they are 60% and 55% for QQP and ParaNMT.</p><p>Contradiction as hard negatives Finally, we further take the advantage of the NLI datasets by using its contradiction pairs as hard negatives 6 . In NLI datasets, given one premise, annotators are required to manually write one sentence that is absolutely true (entailment), one that might be true (neutral), and one that is definitely false (contradiction). Thus for each premise and its entailment hypothesis, there is an accompanying contradiction hypothesis 7 (see <ref type="figure" target="#fig_1">Figure 1</ref> for an example).</p><p>Formally, we extend (</p><formula xml:id="formula_7">x i , x + i ) to (x i , x + i , x − i ), where x i is the premise, x +</formula><p>i and x − i are entailment and contradiction hypotheses. The training objective i is then defined by (N is the mini-batch size):</p><formula xml:id="formula_8">− log e sim(h i ,h + i )/τ N j=1 e sim(h i ,h + j )/τ + e sim(h i ,h − j )/τ .</formula><p>(5) As shown in <ref type="table" target="#tab_4">Table 5</ref>, adding hard negatives can further improve performance (84.9 → 86.2) and this is our final supervised SimCSE. We also tried to add the ANLI dataset <ref type="bibr" target="#b37">(Nie et al., 2020)</ref> or combine it with our unsupervised SimCSE approach, but didn't find a meaningful improvement. We also considered a dual encoder framework in supervised SimCSE and it hurt performance (86.2 → 84.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Connection to Anisotropy</head><p>Recent work identifies an anisotropy problem in language representations <ref type="bibr" target="#b16">(Ethayarajh, 2019;</ref><ref type="bibr" target="#b29">Li et al., 2020)</ref>, i.e., the learned embeddings occupy a narrow cone in the vector space, which largely limits their expressiveness. <ref type="bibr" target="#b17">Gao et al. (2019)</ref> term it as a representation degeneration problem and demonstrate that language models trained with tied input/output embeddings lead to anisotropic word embeddings, and this is further observed by Ethayarajh (2019) in pre-trained contextual embeddings.  show that the singular values of the word embedding matrix decay drastically. In other words, except for a few dominating singular values, all others are close to zero.</p><p>A simple way to alleviate the problem is postprocessing, either to eliminate the dominant principal components <ref type="bibr" target="#b5">(Arora et al., 2017;</ref><ref type="bibr" target="#b36">Mu and Viswanath, 2018)</ref>, or to map embeddings to an isotropic distribution <ref type="bibr" target="#b29">(Li et al., 2020;</ref><ref type="bibr" target="#b46">Su et al., 2021)</ref>. Alternatively, one can add regularization during training <ref type="bibr" target="#b17">(Gao et al., 2019;</ref>. In this section, we show that the contrastive objective can inherently "flatten" the singular value distribution of the sentence-embedding matrix.</p><p>Following <ref type="bibr" target="#b50">Wang and Isola (2020)</ref>, the asymptotics of the contrastive learning objective can be expressed by the following equation when the number of negative instances approaches infinity (assuming f (x) is normalized):</p><formula xml:id="formula_9">− 1 τ E (x,x + )∼ppos f (x) f (x + ) + E x∼p data log E x − ∼p data e f (x) f (x − )/τ ,<label>(6)</label></formula><p>where the first term keeps positive instances similar and the second pushes negative pairs apart. When p data is uniform over finite samples</p><formula xml:id="formula_10">{x i } m i=1 , with h i = f (x i )</formula><p>, we can derive the following formula from the second term with Jensen's inequality:</p><formula xml:id="formula_11">E x∼p data log E x − ∼p data e f (x) f (x − )/τ = 1 m m i=1 log   1 m m j=1 e h i h j /τ   ≥ 1 τ m 2 m i=1 m j=1 h i h j .<label>(7)</label></formula><p>Let W be the sentence embedding matrix corresponding to {x i } m i=1 , i.e., the i-th row of W is h i . Ignoring the constant terms, optimizing the second term in Eq. 6 essentially minimizes an upper bound of the summation of all elements in WW , i.e.,</p><formula xml:id="formula_12">Sum(WW ) = m i=1 m j=1 h i h j .</formula><p>Since we normalize h i , all elements on the diagonal of WW are 1 and then tr(WW ), also the sum of all eigenvalues, is a constant. According to <ref type="bibr" target="#b34">Merikoski (1984)</ref>, if all elements in WW are positive, which is the case in most times from <ref type="bibr" target="#b17">Gao et al. (2019)</ref>, then Sum(WW ) is an upper bound for the largest eigenvalue of WW . Therefore, when minimizing the second term in Eq. 6, we are reducing the top eigenvalue of WW and inherently "flattening" the singular spectrum of the embedding space. Hence contrastive learning can potentially tackle the representation degeneration problem and improve the uniformity.</p><p>Compared to postprocessing methods in <ref type="bibr" target="#b29">Li et al. (2020)</ref>; <ref type="bibr" target="#b46">Su et al. (2021)</ref>, which only aim to encourage isotropic representations, contrastive learning also optimizes for aligning positive pairs by the first term in Eq. 6, which is the key to the success of SimCSE (a quantitative analysis is given in §7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation setup</head><p>We conduct our experiments on 7 standard semantic textual similarity (STS) tasks and also 7 transfer learning tasks. We use the SentEval toolkit (Conneau and Kiela, 2018) for evaluation. Note that we share a similar sentiment with <ref type="bibr" target="#b43">Reimers and Gurevych (2019)</ref> that the main goal of sentence embeddings is to cluster semantically similar sentences. Hence, we take STS results as the main comparison of sentence embedding methods and provide transfer task results for reference.</p><p>Semantic textual similarity tasks We evaluate on 7 STS tasks: STS 2012-2016 <ref type="bibr" target="#b3">(Agirre et al., 2012</ref><ref type="bibr" target="#b4">(Agirre et al., , 2013</ref><ref type="bibr" target="#b1">(Agirre et al., , 2014</ref><ref type="bibr" target="#b0">(Agirre et al., , 2015</ref><ref type="bibr" target="#b2">(Agirre et al., , 2016</ref>, STS Benchmark <ref type="bibr" target="#b7">(Cer et al., 2017)</ref> and SICK-Relatedness <ref type="bibr" target="#b32">(Marelli et al., 2014)</ref> and compute cosine similarity between sentence embeddings. When comparing to previous work, we identify invalid comparison patterns in published papers in the evaluation settings , including (a) whether to use an additional regressor, (b) Spearman's vs Pearson's correlation, (c) how the results are aggregated <ref type="table" target="#tab_15">(Table B</ref>.1). We discuss the detailed differences in Appendix B and choose to follow the setting of <ref type="bibr" target="#b43">Reimers and Gurevych (2019)</ref> in our evaluation. We also report our replicated study of previous work, as well as our results evaluated in a different setting in <ref type="table" target="#tab_15">Table B.2 and Table B</ref>.3. We also call for unifying the setting in evaluating sentence embeddings for future research.</p><p>Transfer tasks We also evaluate on the following transfer tasks: MR <ref type="bibr" target="#b40">(Pang and Lee, 2005)</ref>, CR <ref type="bibr" target="#b23">(Hu and Liu, 2004)</ref>, SUBJ <ref type="bibr" target="#b39">(Pang and Lee, 2004)</ref>, MPQA <ref type="bibr" target="#b51">(Wiebe et al., 2005)</ref>, SST-2 <ref type="bibr" target="#b44">(Socher et al., 2013)</ref>, <ref type="bibr">TREC (Voorhees and Tice, 2000)</ref> and <ref type="bibr">MRPC (Dolan and Brockett, 2005)</ref>. A logistic regression classifier is trained on top of (frozen) sentence embeddings produced by different methods. We follow default configurations from SentEval 8 .   <ref type="bibr">vlin et al., 2019)</ref> as an auxiliary loss to Eq. 1: +λ· mlm (λ is a hyperparameter). This helps Sim-CSE avoid catastrophic forgetting of token-level knowledge. As we will show in <ref type="table" target="#tab_15">Table 9</ref>, we find that adding this term can help improve performance on transfer tasks (not on sentence-level STS tasks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Main Results</head><p>We compare SimCSE to previous state-of-the-art unsupervised and supervised sentence embedding methods. Unsupervised methods include averaging GloVe embeddings <ref type="bibr" target="#b41">(Pennington et al., 2014)</ref>, Skipthought <ref type="bibr" target="#b27">(Kiros et al., 2015)</ref>, and IS-BERT <ref type="bibr" target="#b58">(Zhang et al., 2020)</ref>. We also compare our models to average BERT or RoBERTa embeddings 10 , and post-processing methods such as BERT-flow <ref type="bibr" target="#b29">(Li et al., 2020)</ref> and BERT-whitening <ref type="bibr" target="#b46">(Su et al., 2021)</ref>. Supervised methods include InferSent <ref type="bibr" target="#b12">(Conneau et al., 2017)</ref>, Universal Sentence Encoder <ref type="bibr" target="#b8">(Cer et al., 2018)</ref> and SBERT/SRoBERTa <ref type="bibr" target="#b43">(Reimers and Gurevych, 2019)</ref> along with applying BERT-flow and whitening on them. More details about each baseline are provided in Appendix C.</p><p>Semantic textual similarity   <ref type="bibr" target="#b43">Reimers and Gurevych (2019)</ref>; ♥: results from <ref type="bibr" target="#b58">Zhang et al. (2020)</ref>. We highlight the highest numbers among models with the same pre-trained encoder. MLM: adding MLM as an auxiliary task ( § 6.1) with λ = 0.1. <ref type="table" target="#tab_8">Table 7</ref> shows the evaluation results on transfer tasks. We find that supervised SimCSE performs on par or better than previous approaches, although the trend of unsupervised models remains unclear. We find that adding this MLM term consistently improves performance on transfer tasks, confirming our intuition that sentence-level objective may not directly benefit transfer tasks. We also experiment with post-processing methods (BERT-flow/whitening) and find that they both hurt performance compared to their base models, showing that good uniformity of representations does not lead to better embeddings for transfer learning. As we argued earlier, we think that transfer tasks are not a major goal for sentence embeddings, and thus we take the STS results for main comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Study</head><p>We investigate how different batch sizes, pooling methods and MLM auxiliary objectives affect our models' performance. All results are using our supervised SimCSE model, evaluated on the development set of STS-B or transfer tasks. A more detailed ablation study is provided in Appendix D.  Batch size We explore the impact of batch sizes (N in Eq. 5) in <ref type="table" target="#tab_10">Table 8</ref>. We find that the performance increases as N increases but it will not further increase after 512. This is slightly divergent from the batch sizes used in visual representations , mostly caused by the smaller training data size we use.</p><p>Pooling methods Reimers and Gurevych (2019); <ref type="bibr" target="#b29">Li et al. (2020)</ref> show that taking the average embeddings of pre-trained models, especially from both the first and last layers, leads to better performance than <ref type="bibr">[CLS]</ref>. <ref type="table" target="#tab_15">Table 9</ref> shows the comparison between the two settings and we find that they do not make a significant difference in our approach. Thus we choose to use the [CLS] representation for simplicity and to be consistent with the common practice of using pre-trained embeddings.  <ref type="table" target="#tab_15">Table 9</ref>: Ablation studies of different pooling methods and incorporating the MLM objective. The results are based on the development sets using BERT base .</p><p>MLM auxiliary task Finally, we study the impact of the MLM auxiliary objective with different λ. As shown in <ref type="table" target="#tab_15">Table 9</ref>, the token-level MLM objective improves the averaged performance on transfer tasks modestly, yet it brings a consistent drop in semantic textual similarity tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>In this section, we further conduct analyses to understand the inner workings of SimCSE. Uniformity and alignment <ref type="figure" target="#fig_5">Figure 3</ref> shows the uniformity and alignment of different sentence embeddings along with their averaged STS results. In general, models that attain both better alignment and uniformity will achieve better performance, confirming the findings in <ref type="bibr" target="#b50">Wang and Isola (2020)</ref>. We also observe that (1) though pre-trained embedding has good alignment, its uniformity is poor, i.e., it is highly anisotropic; (2) post-processing methods like BERT-flow and BERT-whitening largely improve the uniformity but also suffer a degeneration in alignment; (3) unsupervised SimCSE effectively improves the uniformity of pre-trained embeddings, while keeping a good alignment; (4) incorporating supervised data in SimCSE further amends the alignment. In Appendix E, we further show that SimCSE can effectively flatten singular value distribution of pre-trained embeddings.</p><p>Cosine-similarity distribution To directly show the strengths of our approaches on STS tasks, we illustrate the cosine similarity distributions of STS-B pairs with different groups of human ratings in <ref type="figure">Figure 4</ref>. Compared to all the baseline models, both unsupervised and supervised SimCSE better distinguish sentence pairs with different levels of similarities, thus lead to a better performance on STS tasks. In addition, we observe that SimCSE generally shows a more scattered distribution than BERT or SBERT, but also preserves a lower variance on semantically similar sentence pairs compared to whitened distribution. This observation further validates that SimCSE can achieve a better alignment-uniformity balance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative comparison</head><p>We conduct a smallscale retrieval experiment using SBERT base and SimCSE-BERT base . We use 150k captions from Flickr30k dataset and take any random sentence as query to retrieve similar sentences (based on cosine similarity). As several examples shown in <ref type="table" target="#tab_13">Table 10</ref>, the retrieved instances by SimCSE have a higher quality compared to those retrieved by SBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Early work in sentence embeddings builds upon the distributional hypothesis by predicting surrounding sentences of a given sentence <ref type="bibr" target="#b27">(Kiros et al., 2015;</ref><ref type="bibr" target="#b22">Hill et al., 2016;</ref><ref type="bibr" target="#b31">Logeswaran and Lee, 2018)</ref>. <ref type="bibr" target="#b38">Pagliardini et al. (2018)</ref> show that simply augmenting the idea of word2vec <ref type="bibr" target="#b35">(Mikolov et al., 2013)</ref> with n-gram embeddings leads to strong results. Several recent models adopt contrastive objectives <ref type="bibr" target="#b58">(Zhang et al., 2020;</ref><ref type="bibr" target="#b33">Meng et al., 2021)</ref> with unsupervised data by taking different views of the same sentence.</p><p>Compared to unsupervised approaches, supervised sentence embeddings demonstrate stronger performance. <ref type="bibr" target="#b12">Conneau et al. (2017)</ref> propose to fine-tune a Siamese model on NLI datasets, which is further extended to other encoders or pre-trained models <ref type="bibr" target="#b8">(Cer et al., 2018;</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-5</head><p>Supervised SimCSE-BERTbase <ref type="figure">Figure 4</ref>: Density plots of cosine similarities between sentence pairs in full STS-B. Pairs are divided into 5 groups based on ground truth ratings (higher means more similar) along the y-axis, and x-axis is the cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SBERT base Supervised SimCSE-BERT base</head><p>Query: A man riding a small boat in a harbor.</p><p>#1 A group of men traveling over the ocean in a small boat. A man on a moored blue and white boat. #2 Two men sit on the bow of a colorful boat. A man is riding in a boat on the water. #3 A man wearing a life jacket is in a small boat on a lake. A man in a blue boat on the water.</p><p>Query: A dog runs on the green grass near a wooden fence. #1 A dog runs on the green grass near a grove of trees.</p><p>The dog by the fence is running on the grass. #2 A brown and white dog runs through the green grass.</p><p>Dog running through grass in fenced area. #3 The dogs run in the green field.</p><p>A dog runs on the green grass near a grove of trees. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2019</head><p>). Furthermore, <ref type="bibr" target="#b52">Wieting and Gimpel (2018)</ref>; <ref type="bibr" target="#b53">Wieting et al. (2020)</ref> demonstrate that bilingual and back-translation corpora provide useful supervision for learning semantic similarity. Another line of work focuses on regularizing embeddings <ref type="bibr" target="#b29">(Li et al., 2020;</ref><ref type="bibr" target="#b46">Su et al., 2021;</ref><ref type="bibr" target="#b24">Huang et al., 2021)</ref> to alleviate the representation degeneration problem (as discussed in §5), and yields substantial improvement over pre-trained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this work, we propose SimCSE, a simple contrastive learning framework, which largely improves state-of-the-art sentence embedding performance on semantic textual similarity tasks. We present an unsupervised approach which predicts input sentence itself with dropout noise and a supervised approach utilizing NLI datasets. We fur-ther justify the inner workings of our approach by analyzing the alignment and uniformity of Sim-CSE along with other baseline models.</p><p>We believe that our contrastive training objective, especially the unsupervised approach, may have a broader application in NLP. It provides a new perspective on data augmentation with text input in contrastive learning, and may be extended to other continuous representations and integrated in language model pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Details</head><p>We implement SimCSE based on Huggingface's transformers package <ref type="bibr" target="#b55">(Wolf et al., 2020)</ref>. For supervised SimCSE, we train our models for 3 epochs with a batch size of 512 and temperature τ = 0.05 using an Adam optimizer <ref type="bibr" target="#b26">(Kingma and Ba, 2015)</ref>. The learning rate is set as 5e-5 for base models and 1e-5 for large models. We evaluate the model every 250 training steps on the development set of STS-B and keep the best checkpoint for the final evaluation on test sets. For unsupervised Sim-CSE, we take 5e-5 as the learning rate for both base and large models and only train for one epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Different Settings for STS Evaluation</head><p>We elaborate the differences in STS evaluation settings in previous work in terms of (a) whether to use additional regressors; (b) reported metrics; (c) different ways to aggregate results.</p><p>Additional regressors The default SentEval implementation applies a linear regressor on top of frozen sentence embeddings for STS-B and SICK-R, and train the regressor on the training sets of the two tasks, while most sentence representation papers take the raw embeddings and evaluate in an unsupervised way. In our experiments, we do not apply any additional regressors and directly take cosine similarities for all STS tasks.</p><p>Metrics Both Pearson's and Spearman's correlation coefficients are used in the literature. <ref type="bibr" target="#b42">Reimers et al. (2016)</ref> argue that Spearman correlation, which measures the rankings instead of the actual scores, better suits the need of evaluating sentence embeddings. For all of our experiments, we report Spearman's rank correlation.</p><p>Aggregation methods Given that each year's STS challenge contains several subsets, there are different choices to gather results from them: one way is to concatenate all the topics and report the overall Spearman's correlation (denoted as "all"), and the other is to calculate results for different subsets separately and average them (denoted as "mean" if it is simple average or "wmean" if weighted by the subset sizes). However, most papers do not claim the method they take, making it challenging for a fair comparison. We take some of the most recent work: SBERT <ref type="bibr" target="#b43">(Reimers and Gurevych, 2019)</ref>, BERT-flow <ref type="bibr" target="#b29">(Li et al., 2020)</ref> and BERT-whitening <ref type="bibr" target="#b46">(Su et al., 2021)</ref>   <ref type="bibr" target="#b12">Conneau et al. (2017)</ref> Pearson mean <ref type="bibr" target="#b11">Conneau and Kiela (2018)</ref> Pearson mean <ref type="bibr" target="#b43">Reimers and Gurevych (2019)</ref> Spearman all <ref type="bibr" target="#b58">Zhang et al. (2020)</ref> Spearman all <ref type="bibr" target="#b29">Li et al. (2020)</ref> Spearman wmean <ref type="bibr" target="#b46">Su et al. (2021)</ref> Spearman wmean <ref type="bibr" target="#b53">Wieting et al. (2020)</ref> Pearson mean Ours Spearman all <ref type="table" target="#tab_15">Table B</ref>.1: STS evaluation protocols used in different papers. "Reg.": whether an additional regressor is used; "aggr.": methods to aggregate different subset results.</p><p>In <ref type="table" target="#tab_15">Table B</ref>.2, we compare our reproduced results to reported results of SBERT and BERT-whitening, and find that Reimers and Gurevych (2019) take the "all" setting but <ref type="bibr" target="#b29">Li et al. (2020)</ref>; <ref type="bibr" target="#b46">Su et al. (2021)</ref> take the "wmean" setting, even though <ref type="bibr" target="#b29">Li et al. (2020)</ref> claim that they take the same setting as <ref type="bibr" target="#b43">Reimers and Gurevych (2019)</ref>. Since the "all" setting fuses data from different topics together, it makes the evaluation closer to real-world scenarios, and unless specified, we take the "all" setting. We list evaluation settings for a number of previous work in <ref type="table" target="#tab_15">Table B</ref>.1. Some of the settings are reported by the paper and some of them are inferred by comparing the results and checking their code. As we can see, the evaluation protocols are very incoherent across different papers. We call for unifying the setting in evaluating sentence embeddings for future research. We also release our evaluation code for better reproducibility. Since previous work uses different evaluation protocols from ours, we further evaluate our models in these settings to make a direct comparison to the published numbers. We evaluate SimCSE with "wmean" and Spearman's correlation to directly compare to <ref type="bibr" target="#b29">Li et al. (2020)</ref> and <ref type="bibr" target="#b46">Su et al. (2021)</ref> in <ref type="table" target="#tab_15">Table B</ref>.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Baseline Models</head><p>We elaborate on how we obtain different baselines for comparison:</p><p>• For average GloVe embedding <ref type="bibr" target="#b41">(Pennington et al., 2014)</ref>, InferSent <ref type="bibr" target="#b12">(Conneau et al., 2017)</ref> and Universal Sentence Encoder <ref type="bibr" target="#b8">(Cer et al., 2018)</ref>, we directly report the results from <ref type="bibr" target="#b43">Reimers and Gurevych (2019)</ref>, since our evaluation setting is the same with theirs. so we assume that they take the same evaluation and just take BERT-whitening in experiments here. • For BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> and RoBERTa , we download the pretrained model weights from HuggingFace's Transformers 12 , and evaluate the models with our own scripts.</p><p>• For SBERT and SRoBERTa <ref type="bibr" target="#b43">(Reimers and Gurevych, 2019)</ref>, we reuse the results from the original paper. For results not reported by <ref type="bibr" target="#b43">Reimers and Gurevych (2019)</ref>, such as the performance of SRoBERTa on transfer tasks, we download the model weights from Sen-tenceTransformers 13 and evaluate them.</p><p>• For BERT-flow <ref type="bibr" target="#b29">(Li et al., 2020)</ref>, since their original numbers take a different setting, we retrain their models using their code 14 , and evaluate the models using our own script.</p><p>• For BERT-whitening <ref type="bibr" target="#b46">(Su et al., 2021)</ref>, we implemented our own version of whitening script 12 https://github.com/huggingface/ transformers 13 https://www.sbert.net/ 14 https://github.com/bohanli/BERT-flow following the same pooling method in <ref type="bibr" target="#b46">Su et al. (2021)</ref>, i.e. first-last average pooling. Our implementation can reproduce the results from the original paper (see <ref type="table" target="#tab_15">Table B</ref>.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More Ablation Studies</head><p>τ N/A 0.001 0.01 0.05 0.1 1 STS-B 85.9 84.9 85.4 86.2 82.0 64.0 For both BERT-flow and BERT-whitening, they have two variants of postprocessing: one takes the NLI data ("NLI") and one directly learns the embedding distribution on the target sets ("target"). We find that in our evaluation setting, "target" is generally worse than "NLI" <ref type="table" target="#tab_15">(Table D.</ref>3), so we only report the NLI variant in the main results.</p><p>Normalization and temperature We train Sim-CSE using both dot product and cosine similarity with different temperatures and evaluate them on the STS-B development set. As shown in <ref type="table" target="#tab_15">Table D</ref>.1, with a carefully tuned temperature τ = 0.05, cosine similarity is better than dot product.</p><p>The use of hard negatives Intuitively, it may be not reasonable to use contradiction hypotheses equally with other in-batch negatives. Therefore, we extend the supervised training objective defined in Eq. 5 to a weighted one as follows: </p><p>where 1 j i ∈ {0, 1} is an indicator that equals 1 if and only if i = j. We train SimCSE with different α and evaluate the trained models on the development set of STS-B. Moreover, we also consider taking neutral hypotheses as hard negatives. As shown in <ref type="table" target="#tab_15">Table D</ref>.2, α = 1 performs the best, and neutral hypotheses do not bring further gains.  <ref type="figure">Figure E</ref>.1: Singular value distributions of sentence embedding matrix from sentences in STS-B. We normalize the singular values so that the largest one is 1. even more since they directly aim for the goal of mapping embeddings to an isotropic distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Distribution of Singular Values</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) Unsupervised SimCSE predicts the input sentence itself from in-batch negatives, with different dropout masks applied. (b) Supervised SimCSE leverages the NLI datasets and takes the entailment (premisehypothesis) pairs as positives, and contradiction pairs as well as other in-batch instances as negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " z H v i h v H 3 4 7 c G s m j N G f y W A F 4 1 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>y S t 5 s 5 6 s F + v d + p i 3 5 q x s 5 p D 8 g f X 5 A 4 w J l d 8 = &lt; / l a t e x i t &gt; align</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " L X r C a B I B F U / V J M 8 f r P g L j L C Y e c c = " &gt; A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A V P J W k K H o s e v F Y w X 5 A W 8 p m O 2 2 X b j Z h d y K W E P C v e P G g i F d / h z f / j d s 2 B 2 1 9 M P B 4 b 4 a Z e X 4 k u E b X / b Z y K 6 t r 6 x v 5 z c L W 9 s 7 u n r 1 / 0 N B h r B j U W S h C 1 f K p B s E l 1 J G j g F a k g A a + g K Y / v p n 6 z Q d Q m o f y H i c R d A M 6 l H z A G U U j 9 e y j U g e E 6 C U d h E d M q O B D m a a l n l 1 0 y + 4 M z j L x M l I k G W o 9 + 6 v T D 1 k c g E Q m q N Z t z 4 2 w m 1 C F n A l I C 5 1 Y Q 0 T Z m A 6 h b a i k A e h u M j s / d U 6 N 0 n c G o T I l 0 Z m p v y c S G m g 9 C X z T G V A c 6 U V v K v 7 n t W M c X H U T L q M Y Q b L 5 o k E s H A y d a R Z O n y t g K C a G U K a 4 u d V h I 6 o o Q 5 N Y w Y T g L b 6 8 T B q V s n d e v r i r F K v X W R x 5 c k x O y B n x y C W p k l t S I 3 X C S E K e y S t 5 s 5 6 s F + v d + p i 3 5 q x s 5 p D 8 g f X 5 A 4 w J l d 8 = &lt; / l a t e x i t &gt; uniform &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z H v i h v H 3 4 7 c G s m j N G f y W A F 4 1 4 X w = " &gt; A A A C A H i c b V A 9 S w N B E N 2 L X z F + n V p Y 2 B w m g l W 4 C 4 q W Q R v L C O Y D c k f Y 2 0 y S J X s f 7 M 6 J 4 b j G v 2 J j o Y i t P 8 P O f + M m u U I T H w w 8 3 p t h Z p 4 f C 6 7 Q t r + N w s r q 2 v p G c b O 0 t b 2 z u 2 f u H 7 R U l E g G T R a J S H Z 8 q k D w E J r I U U A n l k A D X 0 D b H 9 9 M / f Y D S M W j 8 B 4 n M X g B H Y Z 8 w B l F L f X M o 4 o L Q v R S F + E R 0 0 R b k Q y y r N I z y 3 b V n s F a J k 5 O y i R H o 2 d + u f 2 I J Q G E y A R V q u v Y M X o p l c i Z g K z k J g p i y s Z 0 C F 1 N Q x q A 8 t L Z A 5 l 1 q p W + p V f r C t G a q b 8 n U h o o N Q l 8 3 R l Q H K l F b y r + 5 3 U T H F x 5 K Q / j B C F k 8 0 W D R F g Y W d M 0 r D 6 X w F B M N K F M c n 2 r x U Z U U o Y 6 s 5 I O w V l 8 e Z m 0 a l X n v H p x V y v X r / M 4 i u S Y n J A z 4 p B L U i e 3 p E G a h J G M P J N X 8 m Y 8 G S / G u / E x b y 0 Y + c w h + Q P j 8 w d c I Z b o &lt; / l a t e x i t &gt; uniformalign alignuniform plot of models based on BERT base . Color of points and numbers in brackets represent average STS performance (Spearman's correlation). Next3Sent: "next 3 sentences" fromTable 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>− log e sim(h i ,h + i )/τ N j=1 e sim(h i ,h + j )/τ + α 1 j i e sim(h i ,h − j )/τ ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure E. 1</head><label>1</label><figDesc>shows the singular value distribution of SimCSE together with other baselines. For both unsupervised and supervised cases, singular value drops the fastest for vanilla BERT or SBERT embeddings, while SimCSE helps flatten the spectrum distribution. Postprocessing-based methods such as BERT-flow or BERT-whitening flatten the curve</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Effects of different dropout probabilities p</cell></row><row><cell>on the STS-B development set (Spearman's correlation,</cell></row><row><cell>BERT base ). Fixed 0.1: use the default 0.1 dropout rate but apply the same dropout mask on both x i and x + i .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of different supervised datasets as positive pairs. Results are Spearman's correlation on the STS-B development set using BERT base . Numbers in brackets denote the # of pairs. Sample: subsampling 134k positive pairs for a fair comparison between datasets; full: using the full dataset. In the last block, we use entailment pairs as positives and contradiction pairs as hard negatives (our final model).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Training details We start from pre-trained check-</cell></row><row><cell>points of BERT (Devlin et al., 2019) (uncased) or</cell></row><row><cell>RoBERTa (Liu et al., 2019) (cased), and add an</cell></row><row><cell>MLP layer on top of the [CLS] representation as</cell></row><row><cell>the sentence embedding 9 (see  §6.3 for comparison</cell></row><row><cell>between different pooling methods). More training</cell></row><row><cell>details can be found in Appendix A. Finally, we</cell></row><row><cell>introduce one more optional variant which adds a</cell></row><row><cell>masked language modeling (MLM) objective (De</cell></row></table><note>Sentence embedding performance on STS tasks (Spearman's correlation, "all" setting). We highlight the highest numbers among models with the same pre-trained encoder. ♣: results from Reimers and Gurevych (2019); ♥: results from Zhang et al. (2020); all other results are reproduced or reevaluated by ourselves. For BERT-flow (Li et al., 2020) and whitening (Su et al., 2021), we only report the "NLI" setting (see Table D.3).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>shows the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Transfer task results of different sentence embedding models (measured as accuracy). ♣: results from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Effect of different batch sizes (STS-B develop- ment set, Spearman's correlation, BERT base ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Retrieved top-3 examples by SBERT and supervised SimCSE from Flickr30k (150k sentences).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table B .</head><label>B</label><figDesc>2: Comparisons of our reproduced results using different evaluation protocols and the original numbers. ♣: results from<ref type="bibr" target="#b43">Reimers and Gurevych (2019)</ref>; ♠: results from<ref type="bibr" target="#b46">Su et al. (2021)</ref>; Other results are reproduced by us. From the table we see that SBERT takes the "all" evaluation and BERT-whitening takes the "wmean" evaluation.Table B.3: STS results with "wmean" setting (Spearman). ♠: from<ref type="bibr" target="#b29">Li et al. (2020)</ref>;<ref type="bibr" target="#b46">Su et al. (2021)</ref>.</figDesc><table><row><cell>Model</cell><cell cols="7">STS12 STS13 STS14 STS15 STS16 STS-B SICK-R</cell><cell>Avg.</cell></row><row><cell>BERTbase (first-last avg.) ♠</cell><cell>57.86</cell><cell>61.97</cell><cell>62.49</cell><cell>70.96</cell><cell>69.76</cell><cell>59.04</cell><cell>63.75</cell><cell>63.69</cell></row><row><cell>+ flow (NLI) ♠</cell><cell>59.54</cell><cell>64.69</cell><cell>64.66</cell><cell>72.92</cell><cell>71.84</cell><cell>58.56</cell><cell>65.44</cell><cell>65.38</cell></row><row><cell>+ flow (target) ♠</cell><cell>63.48</cell><cell>72.14</cell><cell>68.42</cell><cell>73.77</cell><cell>75.37</cell><cell>70.72</cell><cell>63.11</cell><cell>69.57</cell></row><row><cell>+ whitening (NLI) ♠</cell><cell>61.69</cell><cell>65.70</cell><cell>66.02</cell><cell>75.11</cell><cell>73.11</cell><cell>68.19</cell><cell>63.60</cell><cell>67.63</cell></row><row><cell>+ whitening (target) ♠</cell><cell>63.62</cell><cell>73.02</cell><cell>69.23</cell><cell>74.52</cell><cell>72.15</cell><cell>71.34</cell><cell>60.60</cell><cell>69.21</cell></row><row><cell>*  Unsup. SimCSE-BERTbase</cell><cell>68.92</cell><cell>78.70</cell><cell>73.35</cell><cell>79.72</cell><cell>79.42</cell><cell>75.49</cell><cell>69.92</cell><cell>75.07</cell></row><row><cell>SBERTbase (first-last avg.) ♠</cell><cell>68.70</cell><cell>74.37</cell><cell>74.73</cell><cell>79.65</cell><cell>75.21</cell><cell>77.63</cell><cell>74.84</cell><cell>75.02</cell></row><row><cell>+ flow (NLI) ♠</cell><cell>67.75</cell><cell>76.73</cell><cell>75.53</cell><cell>80.63</cell><cell>77.58</cell><cell>79.10</cell><cell>78.03</cell><cell>76.48</cell></row><row><cell>+ flow (target) ♠</cell><cell>68.95</cell><cell>78.48</cell><cell>77.62</cell><cell>81.95</cell><cell>78.94</cell><cell>81.03</cell><cell>74.97</cell><cell>77.42</cell></row><row><cell>+ whitening (NLI) ♠</cell><cell>69.11</cell><cell>75.79</cell><cell>75.76</cell><cell>82.31</cell><cell>79.61</cell><cell>78.66</cell><cell>76.33</cell><cell>76.80</cell></row><row><cell>+ whitening (target) ♠</cell><cell>69.01</cell><cell>78.10</cell><cell>77.04</cell><cell>80.83</cell><cell>77.93</cell><cell>80.50</cell><cell>72.54</cell><cell>76.56</cell></row><row><cell>*  Sup. SimCSE-BERTbase</cell><cell>70.90</cell><cell>81.49</cell><cell>80.19</cell><cell>83.79</cell><cell>81.89</cell><cell>84.25</cell><cell>80.39</cell><cell>80.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table D</head><label>D</label><figDesc>Comparison of using NLI or target data for postprocessing methods ("all", Spearman's correlation).</figDesc><table><row><cell></cell><cell>77.57</cell><cell>74.66</cell><cell>82.27</cell><cell>78.39</cell><cell>79.52</cell><cell cols="2">76.91</cell><cell>77.00</cell></row><row><cell>SBERT-whitening (target) 52.91</cell><cell>81.91</cell><cell>75.44</cell><cell>72.24</cell><cell>72.93</cell><cell>80.50</cell><cell cols="2">72.54</cell><cell>72.64</cell></row><row><cell>Table D.3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">.1: STS-B development results (Spearman's</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">correlation) with different temperatures. "N/A": Dot</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">product instead of cosine similarity.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Hard neg N/A</cell><cell cols="3">Contradiction</cell><cell>Contra.+ Neutral</cell></row><row><cell></cell><cell></cell><cell></cell><cell>α</cell><cell>-</cell><cell>0.5</cell><cell>1.0</cell><cell>2.0</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>STS-B</cell><cell cols="4">84.9 86.1 86.2 86.2</cell><cell>85.3</cell></row><row><cell></cell><cell></cell><cell cols="7">Table D.2: STS-B development results with different</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">hard negative policies. "N/A": no hard negative.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We take STS-B pairs with a score higher than 4 as ppos and all STS-B sentences as p data .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We do not use the neutral pairs for hard negatives. 7 In fact, one premise can have multiple contradiction hypotheses. In our implementation, we only sample one as the hard negative and we did not find a difference by using more.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/facebookresearch/ SentEval</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">There is an MLP pooler in BERT's original implementation and we just use the layer with random initialization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10"> Following Su et al. (2021), we take the average of the first and the last layer, which is better than only taking the last.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11"><ref type="bibr" target="#b29">Li et al. (2020)</ref> and<ref type="bibr" target="#b46">Su et al. (2021)</ref> have consistent results,</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Tao Lei, Jason Lee, Zhengyan Zhang, Jinhyuk Lee, Alexander Wettig, Zexuan Zhong, and the members of the Princeton NLP group for helpful discussion and valuable feedback on our paper. TG is currently supported by a Graduate Fellowship at Princeton University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">and pilot on interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S15-2045</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>English, Spanish</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
	<note>SemEval-2015 task 2: Semantic textual similarity</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1081</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">*SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">*SEM 2013 shared task: Semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity</title>
		<meeting>the Main Conference and the Shared Task: Semantic Textual Similarity</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal sentence encoder for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurzweil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2029</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On sampling strategies for neural networkbased collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/3097983.3098202</idno>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="767" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SentEval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1070</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
		<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1006</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representation degeneration problem in training natural language generation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning dense representations for entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Garcia-Olano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="528" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient natural language response suggestion for smart reply</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laszlo</forename><surname>Yun Hsuan Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Lukacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Sanjiv Kumar, Balint Miklos, and Ray Kurzweil</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Whiteningbert: An easy unsupervised sentence embedding approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01767</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning dense representations of phrases at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mujeen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the sentence embeddings from pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9119" to="9130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08473</idno>
		<title level="m">Coco-lm: Correcting and contrasting text sequences for language model pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the trace and the sum of elements of a matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Merikoski</forename><surname>Jorma Kaarlo</surname></persName>
		</author>
		<idno type="DOI">10.1016/0024-3795(84)90078-8</idno>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="177" to="185" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">All-but-thetop: Simple and effective postprocessing for word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod</forename><surname>Viswanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adversarial NLI: A new benchmark for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.441</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4885" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1049</idno>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="528" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Task-oriented intrinsic evaluation of semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Whitening sentence representations for better semantics and faster retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyiwen</forename><surname>Ou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Building a question answering test collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improving neural language generation with spectrum control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1042</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A bilingual generative transformer for semantic sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.122</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1581" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Clear: Contrastive learning for sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15466</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00166</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An unsupervised sentence embedding method by mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.124</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1601" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<title level="m">Model STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert-Whitening</surname></persName>
		</author>
		<imprint>
			<pubPlace>NLI, all</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert-Whitening</surname></persName>
		</author>
		<imprint>
			<pubPlace>NLI, wmean</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<title level="m">Model STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert-Flow</surname></persName>
		</author>
		<imprint>
			<publisher>NLI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
