<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">INFOBERT: IMPROVING ROBUSTNESS OF LANGUAGE MODELS FROM AN INFORMATION THEORETIC PERSPECTIVE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Wang</surname></persName>
							<email>boxinw2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
							<email>shuohang.wang@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
							<email>yu.cheng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
							<email>zhe.gan@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Jia</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">INFOBERT: IMPROVING ROBUSTNESS OF LANGUAGE MODELS FROM AN INFORMATION THEORETIC PERSPECTIVE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale pre-trained language models such as BERT and RoBERTa have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing the threats of textual adversarial attacks. We aim to address this problem from an information-theoretic perspective, and propose InfoBERT, a novel learning framework for robust fine-tuning of pre-trained language models. InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) an Anchored Feature regularizer, which increases the mutual information between local stable features and global features. We provide a principled way to theoretically analyze and improve the robustness of language models in both standard and adversarial training. Extensive experiments demonstrate that InfoBERT achieves stateof-the-art robust accuracy over several adversarial datasets on Natural Language Inference (NLI) and Question Answering (QA) tasks. Our code is available at https://github.com/AI-secure/InfoBERT.</p><p>Published as a conference paper at ICLR 2021 BERT are vulnerable to adversarial attacks, which raises the challenge of building robust real-world LM applications against unknown adversarial attacks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Self-supervised representation learning pre-trains good feature extractors from massive unlabeled data, which show promising transferability to various downstream tasks. Recent success includes large-scale pre-trained language models (e.g., <ref type="bibr">BERT, RoBERTa, and GPT-3 (Devlin et al., 2019;</ref><ref type="bibr" target="#b27">Liu et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref>), which have advanced state of the art over a wide range of NLP tasks such as NLI and QA, even surpassing human performance. Specifically, in the computer vision domain, many studies have shown that self-supervised representation learning is essentially solving the problem of maximizing the mutual information (MI) I(X; T ) between the input X and the representation T (van den <ref type="bibr" target="#b36">Oord et al., 2018;</ref><ref type="bibr" target="#b2">Belghazi et al., 2018;</ref><ref type="bibr" target="#b15">Hjelm et al., 2019;</ref>. Since MI is computationally intractable in high-dimensional feature space, many MI estimators <ref type="bibr" target="#b2">(Belghazi et al., 2018)</ref> have been proposed to serve as lower bounds <ref type="bibr" target="#b1">(Barber &amp; Agakov, 2003;</ref><ref type="bibr" target="#b36">van den Oord et al., 2018)</ref> or upper bounds  of MI. Recently, <ref type="bibr">Kong et al. point</ref> out that the MI maximization principle of representation learning can be applied to not only computer vision but also NLP domain, and propose a unified view that recent pre-trained language models are maximizing a lower bound of MI among different segments of a word sequence.</p><p>On the other hand, deep neural networks are known to be prone to adversarial examples <ref type="bibr" target="#b14">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b30">Papernot et al., 2016;</ref><ref type="bibr" target="#b12">Eykholt et al., 2017;</ref><ref type="bibr" target="#b28">Moosavi-Dezfooli et al., 2016)</ref>, i.e., the outputs of neural networks can be arbitrarily wrong when human-imperceptible adversarial perturbations are added to the inputs. Textual adversarial attacks typically perform word-level substitution <ref type="bibr" target="#b11">(Ebrahimi et al., 2018;</ref><ref type="bibr" target="#b0">Alzantot et al., 2018;</ref><ref type="bibr" target="#b32">Ren et al., 2019)</ref> or sentence-level paraphrasing <ref type="bibr" target="#b17">(Iyyer et al., 2018;</ref><ref type="bibr" target="#b43">Zhang et al., 2019)</ref> to achieve semantic/utility preservation that seems innocuous to human, while fools NLP models. Recent studies <ref type="bibr" target="#b22">(Jin et al., 2020;</ref><ref type="bibr" target="#b42">Zang et al., 2020;</ref><ref type="bibr" target="#b29">Nie et al., 2020;</ref> further show that even large-scale pre-trained language models (LM) such as</p><p>We investigate the robustness of language models from an information theoretic perspective, and propose a novel learning framework InfoBERT, which focuses on improving the robustness of language representations by fine-tuning both local features (word-level representation) and global features (sentence-level representation) for robustness purpose. InfoBERT considers two MI-based regularizers: (i) the Information Bottleneck regularizer manages to extract approximate minimal sufficient statistics for downstream tasks, while removing excessive and noisy information that may incur adversarial attacks; (ii) the Anchored Feature regularizer carefully selects useful local stable features that are invulnerable to adversarial attacks, and maximizes the mutual information between local stable features and global features to improve the robustness of the global representation. In this paper, we provide a detailed theoretical analysis to explicate the effect of InfoBERT for robustness improvement, along with extensive empirical adversarial evaluation to validate the theory.</p><p>Our contributions are summarized as follows. (i) We propose a novel learning framework InfoBERT from the information theory perspective, aiming to effectively improve the robustness of language models. (ii) We provide a principled theoretical analysis on model robustness, and propose two MIbased regularizers to refine the local and global features, which can be applied to both standard and adversarial training for different NLP tasks. (iii) Comprehensive experimental results demonstrate that InfoBERT can substantially improve robust accuracy by a large margin without sacrificing the benign accuracy, yielding the state-of-the-art performance across multiple adversarial datasets on NLI and QA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Textual Adversarial Attacks/Defenses Most existing textual adversarial attacks focus on wordlevel adversarial manipulation. <ref type="bibr" target="#b11">Ebrahimi et al. (2018)</ref> is the first to propose a whitebox gradientbased attack to search for adversarial word/character substitution. Following work <ref type="bibr" target="#b0">(Alzantot et al., 2018;</ref><ref type="bibr" target="#b32">Ren et al., 2019;</ref><ref type="bibr" target="#b42">Zang et al., 2020;</ref><ref type="bibr" target="#b22">Jin et al., 2020)</ref> further constrains the perturbation search space and adopts Part-of-Speech checking to make NLP adversarial examples look natural to human.</p><p>To defend against textual adversarial attacks, existing work can be classified into three categories: (i) Adversarial Training is a practical method to defend against adversarial examples. Existing work either uses PGD-based attacks to generate adversarial examples in the embedding space of NLP as data augmentation <ref type="bibr" target="#b44">(Zhu et al., 2020a)</ref>, or regularizes the standard objective using virtual adversarial training . However, one drawback is that the threat model is often unknown, which renders adversarial training less effective when facing unseen attacks. (ii) Interval Bound Propagation (IBP) <ref type="bibr" target="#b10">(Dvijotham et al., 2018)</ref> is proposed as a new technique to consider the worst-case perturbation theoretically. Recent work <ref type="bibr" target="#b16">(Huang et al., 2019;</ref><ref type="bibr" target="#b20">Jia et al., 2019)</ref> has applied IBP in the NLP domain to certify the robustness of models. However, IBPbased methods rely on strong assumptions of model architecture and are difficult to adapt to recent transformer-based language models. (iii) Randomized Smoothing <ref type="bibr" target="#b8">(Cohen et al., 2019)</ref> provides a tight robustness guarantee in 2 norm by smoothing the classifier with Gaussian noise. <ref type="bibr" target="#b40">Ye et al. (2020)</ref> adapts the idea to the NLP domain, and replace the Gaussian noise with synonym words to certify the robustness as long as adversarial word substitution falls into predefined synonym sets. However, to guarantee the completeness of the synonym set is challenging.</p><p>Representation Learning MI maximization principle has been adopted by many studies on selfsupervised representation learning <ref type="bibr" target="#b36">(van den Oord et al., 2018;</ref><ref type="bibr" target="#b2">Belghazi et al., 2018;</ref><ref type="bibr" target="#b15">Hjelm et al., 2019;</ref>. <ref type="bibr">Specifically, InfoNCE (van den Oord et al., 2018)</ref> is used as the lower bound of MI, forming the problem as contrastive learning <ref type="bibr" target="#b33">(Saunshi et al., 2019;</ref><ref type="bibr" target="#b41">Yu et al., 2020)</ref>. However, <ref type="bibr" target="#b34">Tian et al. (2020)</ref> suggests that the InfoMax <ref type="bibr" target="#b25">(Linsker, 1988)</ref> principle may introduce excessive and noisy information, which could be adversarial. To generate robust representation, <ref type="bibr" target="#b45">Zhu et al. (2020b)</ref> formalizes the problem from a mutual-information perspective, which essentially performs adversarial training for worst-case perturbation, while mainly considers the continuous space in computer vision. In contrast, InfoBERT originates from an information-theoretic perspective and is compatible with both standard and adversarial training for discrete input space of language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">INFOBERT</head><p>Before diving into details, we first discuss the textual adversarial examples we consider in this paper. We mainly focus on the dominant word-level attack as the main threat model, since it achieves higher attack success and is less noticeable to human readers than other attacks. Due to the discrete nature of text input space, it is difficult to measure adversarial distortion on token level. Instead, because most word-level adversarial attacks <ref type="bibr" target="#b24">(Li et al., 2019;</ref><ref type="bibr" target="#b22">Jin et al., 2020)</ref> constrain word perturbations via the bounded magnitude in the semantic embedding space, by adapting from <ref type="bibr" target="#b18">Jacobsen et al. (2019)</ref>, we define the adversarial text examples with distortions constrained in the embedding space. Definition 3.1. ( -bounded Textual Adversarial Examples). Given a sentence x = [x 1 ; x 2 ; ...; x n ], where x i is the word at the i-th position, the -bounded adversarial sentence x = [x 1 ; x 2 ; ...; x n ] for a classifier F satisfies:</p><formula xml:id="formula_0">(1) F(x) = o(x) = o(x ) but F(x ) = o(x ), where o(·)</formula><p>is the oracle (e.g., human decision-maker); (2) ||t i − t i || 2 ≤ for i = 1, 2, ..., n, where ≥ 0 and t i is the word embedding of x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">INFORMATION BOTTLENECK AS A REGULARIZER</head><p>In this section, we first discuss the general IB implementation, and then explain how IB formulation is adapted to InfoBERT as a regularizer along with theoretical analysis to support why IB regularizer can help improve the robustness of language models. The IB principle formulates the goal of deep learning as an information-theoretic trade-off between representation compression and predictive power <ref type="bibr" target="#b35">(Tishby &amp; Zaslavsky, 2015)</ref>. Given the input source X, a deep neural net learns the internal representation T of some intermediate layer and maximizes the MI between T and label Y , so that T subject to a constraint on its complexity contains sufficient information to infer the target label Y . Finding an optimal representation T can be formulated as the maximization of the Lagrangian</p><formula xml:id="formula_1">L IB = I(Y ; T ) − βI(X; T ),<label>(1)</label></formula><p>where β &gt; 0 is a hyper-parameter to control the tradeoff, and I(Y ; T ) is defined as:</p><formula xml:id="formula_2">I(Y ; T ) = p(y, t) log p(y, t) p(y)p(t) dy dt .<label>(2)</label></formula><p>Since Eq.</p><p>(2) is intractable, we instead use the lower bound from <ref type="bibr" target="#b1">Barber &amp; Agakov (2003)</ref>:</p><formula xml:id="formula_3">I(Y ; T ) ≥ p(y, t) log q ψ (y | t) dy dt ,<label>(3)</label></formula><p>where q ψ (y|t) is the variational approximation learned by a neural network parameterized by ψ for the true distribution p(y|t). This indicates that maximizing the lower bound of the first term of IB I(Y ; T ) is equivalent to minimizing the task cross-entropy loss task = H(Y | T ).</p><p>To derive a tractable lower bound of IB, we here use an upper bound  of I(X; T )</p><formula xml:id="formula_4">I(X; T ) ≤ p(x, t) log(p(t | x)) dx dt − p(x)p(t) log(p(t | x)) dx dt .<label>(4)</label></formula><p>By combining Eq.</p><p>(3) and (4), we can maximize the tractable lower boundL IB of IB in practice by:</p><formula xml:id="formula_5">L IB = 1 N N i=1 log q ψ (y (i) | t (i) ) − β N N i=1 log(p(t (i) | x (i) )) − 1 N N j=1 log(p(t (j) | x (i) )) (5) with data samples {x (i) , y (i) } N i=1</formula><p>, where q ψ can represent any classification model (e.g., BERT), and p(t | x) can be viewed as the feature extractor f θ : X → T , where X and T are the support of the input source X and extracted feature T , respectively.</p><p>The above is a general implementation of IB objective function. In InfoBERT, we consider T as the features consisting of the local word-level features after the BERT embedding layer f θ . The following BERT self-attentive layers along with the linear classification head serve as q ψ (y|t) that predicts the target Y given representation T .</p><p>Formally, given random variables X = [X 1 ; X 2 ; ...; X n ] representing input sentences with X i (word token at i-th index), let T = [T 1 ; ...; T n ] = f θ ([X 1 ; X 2 ; ...; X n ]) = [f θ (X 1 ); f θ (X 2 ); ...; f θ (X n )] denote the random variables representing the features generated from input X via the BERT embedding layer f θ , where T i ∈ R d is the high-dimensional word-level local feature for word X i . Due to the high dimensionality d of each word feature (e.g., 1024 for BERT-large), when the sentence length n increases, the dimensionality of features T becomes too large to compute I(X; T ) in practice. Thus, we propose to maximize a localized formulation of IB L LIB defined as:</p><formula xml:id="formula_6">L LIB := I(Y ; T ) − nβ n i=1 I(X i ; T i ).<label>(6)</label></formula><p>Theorem 3.1. (Lower Bound of L IB ) Given a sequence of random variables X = [X 1 ; X 2 ; ...; X n ] and a deterministic feature extractor f θ , let T = [T 1 ; ...; T n ] = [f θ (X 1 ); f θ (X 2 ); ...; f θ (X n )]. Then the localized formulation of IB L LIB is a lower bound of L IB (Eq. (1)), i.e.,</p><formula xml:id="formula_7">I(Y ; T ) − βI(X; T ) ≥ I(Y ; T ) − nβ n i=1 I(X i ; T i ).<label>(7)</label></formula><p>Theorem 3.1 indicates that we can maximize the localized formulation of L LIB as a lower bound of IB L IB when I(X; T ) is difficult to compute. In Eq. (6), if we regard the first term (I(Y ; T )) as a task-related objective, the second term (−nβ n i=1 I(X i ; T i )) can be considered as a regularization term to constrain the complexity of representation T , thus named as Information Bottleneck regularizer. Next, we give a theoretical analysis for the adversarial robustness of IB and demonstrate why localized IB objective function can help improve the robustness to adversarial attacks.</p><p>Following Definition 3.1, let T = [T 1 ; T 2 ; ...; T n ] and T = [T 1 ; T 2 ; ...; T n ] denote the features for the benign sentence X and adversarial sentence X . The distributions of X and X are denoted by probability p(x) and q(x) with the support X and X , respectively. We assume that the feature representation T has finite support denoted by T considering the finite vocabulary size in NLP. Theorem 3.2. (Adversarial Robustness Bound) For random variables X = [X 1 ; X 2 ; ...; X n ] and</p><formula xml:id="formula_8">X = [X 1 ; X 2 ; ...; X n ], let T = [T 1 ; T 2 ; ...; T n ] = [f θ (X 1 ); f θ (X 2 ); ...; f θ (X n )] and T = [T 1 ; T 2 ; ...; T n ] = [f θ (X 1 ); f θ (X 2 ); ...; f θ (X n )]</formula><p>with finite support T , where f θ is a deterministic feature extractor. The performance gap between benign and adversarial data |I(Y ; T ) − I(Y ; T )| is bounded above by</p><formula xml:id="formula_9">|I(Y ; T ) − I(Y ; T )| ≤ B 0 + B 1 n i=1 |T |(I(X i ; T i )) 1/2 + B 2 n i=1 |T | 3/4 (I(X i ; T i )) 1/4 + B 3 n i=1 |T |(I(X i ; T i )) 1/2 + B 4 n i=1 |T | 3/4 (I(X i ; T i )) 1/4 ,<label>(8)</label></formula><p>where B 0 , B 1 , B 2 , B 3 and B 4 are constants depending on the sequence length n, and p(x).</p><p>The sketch of the proof is to express the difference of |I(Y ; T ) − I(Y ; T )| in terms of I(X i ; T i ). Specifically, Eq. (25) factorizes the difference into two summands. The first summand, the conditional entropy |H(T | Y ) − H(T | Y )|, can be bound by Eq. (42) in terms of MI between benign/adversarial input and representation I(X i ; T i ) and I(X i ; T i ). The second summand |H(T ) − H(T )| has a constant upper bound (Eq. (85)), since language models have bounded vocabulary size and embedding space, and thus have bounded entropy.</p><p>The intuition of Theorem 3.2 is to bound the adversarial performance drop |I(Y ;</p><formula xml:id="formula_10">T ) − I(Y ; T )| by I(X i ; T i ). As explained in Eq. (3), I(Y ; T ) and I(Y ; T )</formula><p>can be regarded as the model performance on benign and adversarial data. Thus, the LHS of the bound represents such a performance gap. The adversarial robustness bound of Theorem 3.2 indicates that the performance gap becomes closer when I(X i ; T i ) and I(X i ; T i ) decrease. Note that our IB regularizer in the objective function Eq. (6) achieves the same goal of minimizing I(X i ; T i ) while learning the most efficient information features, or approximate minimal sufficient statistics, for downstream tasks. Theorem 3.2 also suggests that combining adversarial training with our IB regularizer can further minimize I(X i ; T i ), leading to better robustness, which is verified in §4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ANCHORED FEATURE REGULARIZER</head><p>In addition to the IB regularizer that suppresses noisy information that may incur adversarial attacks, we propose a novel regularizer termed "Anchored Feature Regularizer", which extracts local Algorithm 1 -Local Anchored Feature Extraction. This algorithm takes in the word local features and returns the index of local anchored features. 1: Input: Word local features t, upper and lower threshold c h and c l 2: δ ← 0 // Initialize the perturbation vector δ 3: g(δ) = ∇ δ task (q ψ (t + δ), y) // Perform adversarial attack on the embedding space 4: Sort the magnitude of the gradient of the perturbation vector from</p><formula xml:id="formula_11">||g(δ) 1 || 2 , ||g(δ) 2 || 2 , ..., ||g(δ) n || 2 into ||g(δ) k1 || 2 , ||g(δ) k2 || 2 , ..., ||g(δ) kn || 2 in ascending order, where z i corresponds to its original index. 5: Return: k i , k i+1 , ..., k j , where c l ≤ i n ≤ j n ≤ c h .</formula><p>stable features and aligns them with sentence global representations, thus improving the stability and robustness of language representations.</p><p>The goal of the local anchored feature extraction is to find features that carry useful and stable information for downstream tasks. Instead of directly searching for local anchored features, we start with searching for nonrobust and unuseful features. To identify local nonrobust features, we perform adversarial attacks to detect which words are prone to changes under adversarial word substitution.</p><p>We consider these vulnerable words as features nonrobust to adversarial threats. Therefore, global robust sentence representations should rely less on these vulnerable statistical clues. On the other hand, by examining the adversarial perturbation on each local word feature, we can also identify words that are less useful for downstream tasks. For example, stopwords and punctuation usually carry limited information, and tend to have smaller adversarial perturbations than words containing more effective information. Although these unuseful features are barely changed under adversarial attacks, they contain insufficient information and should be discarded. After identifying the nonrobust and unuseful features, we treat the remaining local features in the sentences as useful stable features and align the global feature representation based on them.</p><p>During the local anchored feature extraction, we perform "virtual" adversarial attacks that generate adversarial perturbation in the embedding space, as it abstracts the general idea for existing wordlevel adversarial attacks. Formally, given an input sentence x = [x 1 ; x 2 ; ...;</p><p>x n ] with its corresponding local embedding representation t = [t 1 ; ...; t n ], where x and t are the realization of random variables X and T , we generate adversarial perturbation δ in the embedding space so that the task loss task increases. The adversarial perturbation δ is initialized to zero, and the gradient of the loss with respect to δ is calculated by g(δ) = ∇ δ task (q ψ (t+δ), y) to update δ ← ||δ|| F ≤ (ηg(δ)/||g(δ)|| F ). The above process is similar to one-step PGD with zero-initialized perturbation δ. Since we only care about the ranking of perturbation to decide on robust features, in practice we skip the update of δ to save computational cost, and simply examine the 2 norm of the gradient g(δ) i of the perturbation on each word feature t i . A feasible plan is to choose the words whose perturbation is neither too large (nonrobust features) nor too small (unuseful features), e.g., the words whose perturbation rankings are among 50% ∼ 80% of all the words. The detailed procedures are provided in Algorithm 1.</p><p>After local anchored features are extracted, we propose to align sentence global representations Z with our local anchored features T i . In practice, we can use the final-layer [CLS] embedding to represent global sentence-level feature Z. Specifically, we use the information theoretic tool to increase the mutual information I(T i ; Z) between local anchored features T i and sentence global representations Z, so that the global representations can share more robust and useful information with the local anchored features and focus less on the nonrobust and unuseful ones. By incorporating the term I(T i ; Z) into the previous objective function Eq. (6), our final objective function becomes:</p><formula xml:id="formula_12">max I(Y ; T ) − nβ n i=1 I(X i ; T i ) + α M j=1 I(T kj ; Z),<label>(9)</label></formula><p>where T kj are the local anchored features selected by Algorithm 1 and M is the number of local anchored features. An illustrative figure can be found in Appendix <ref type="figure">Figure 2</ref>.</p><p>In addition, due to the intractability of computing MI, we use InfoNCE (van den <ref type="bibr" target="#b36">Oord et al., 2018)</ref> as the lower bound of MI to approximate the last term I(T kj ; Z):</p><formula xml:id="formula_13">I (InfoNCE) (T i ; Z) := E P g ω (t i , z) − EP log t i e gω(t i ,z) ,<label>(10)</label></formula><p>where g ω (·, ·) is a score function (or critic function) approximated by a neural network, t i are the positive samples drawn from the joint distribution P of local anchored features and global representations, and t i are the negative samples drawn from the distribution of nonrobust and unuseful featuresP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we demonstrate how effective InfoBERT improves the robustness of language models over multiple NLP tasks such as NLI and QA. We evaluate InfoBERT against both strong adversarial datasets and state-of-the-art adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUP</head><p>Adversarial Datasets The following adversarial datasets and adversarial attacks are used to evaluate the robustness of InfoBERT and baselines. (I) Adversarial NLI (ANLI) <ref type="bibr" target="#b29">(Nie et al., 2020)</ref> is a large-scale NLI benchmark, collected via an iterative, adversarial, human-and-model-in-theloop procedure to attack BERT and RoBERTa. ANLI dataset is a strong adversarial dataset which can easily reduce the accuracy of BERT Large to 0%. (II) Adversarial SQuAD <ref type="bibr" target="#b19">(Jia &amp; Liang, 2017)</ref> dataset is an adversarial QA benchmark dataset generated by a set of handcrafted rules and refined by crowdsourcing. Since adversarial training data is not provided, we fine-tune RoBERTa Large on benign SQuAD training data <ref type="bibr" target="#b31">(Rajpurkar et al., 2016)</ref>  Baselines Since IBP-based methods <ref type="bibr" target="#b16">(Huang et al., 2019;</ref><ref type="bibr" target="#b20">Jia et al., 2019)</ref> cannot be applied to largescale language models yet, and the randomized-smoothing-based method <ref type="bibr" target="#b40">(Ye et al., 2020)</ref> achieves limited certified robustness, we compare InfoBERT against three competitive baselines based on adversarial training: (I) FreeLB <ref type="bibr" target="#b44">(Zhu et al., 2020a)</ref> applies adversarial training to language models during fine-tuning stage to improve generalization. In §4.2, we observe that FreeLB can boost the robustness of language models by a large margin. (II) SMART  uses adversarial training as smoothness-inducing regularization and Bregman proximal point optimization during fine-tuning, to improve the generalization and robustness of language models. (III) ALUM  performs adversarial training in both pre-training and fine-tuning stages, which achieves substantial performance gain on a wide range of NLP tasks. Due to the high computational cost of adversarial training, we compare InfoBERT to ALUM and SMART with the best results reported in the original papers.</p><p>Evaluation Metrics We use robust accuracy or robust F1 score to measure how robust the baseline models and InfoBERT are when facing adversarial data. Specifically, robust accuracy is calculated by:</p><formula xml:id="formula_14">Acc = 1 |Dadv| x ∈Dadv 1[arg max q ψ (f θ (x )) ≡ y],</formula><p>where D adv is the adversarial dataset, y is the ground-truth label, arg max selects the class with the highest logits and 1(·) is the indicator function. Similarly, robust F1 score is calculated by:</p><formula xml:id="formula_15">F1 = 1 |Dadv| x ∈Dadv v(arg max q ψ (f θ (x )), a),</formula><p>where v(·, ·) is the F1 score between the true answer a and the predicted answer arg max q ψ (f θ (x )), and arg max selects the answer with the highest probability (see <ref type="bibr" target="#b31">Rajpurkar et al. (2016)</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>To demonstrate InfoBERT is effective for different language models, we apply InfoBERT to both pretrained RoBERTa Large and BERT Large . Since InfoBERT can be applied to both standard training and adversarial training, we here use FreeLB as the adversarial training implementation. InfoBERT is fine-tuned for 2 epochs for the QA task, and 3 epochs for the NLI task. More implementation details such as α, β, c h , c l selection can be found in Appendix A.1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTAL RESULTS</head><p>Evaluation on ANLI As ANLI provides an adversarial training dataset, we evaluate models in two settings: 1) training models on benign data (MNLI <ref type="bibr" target="#b39">(Williams et al., 2018</ref><ref type="bibr">) + SNLI (Bowman et al., 2015</ref>) only, which is the case when the adversarial threat model is unknown; 2) training models on both benign and adversarial training data (SNLI+MNLI+ANLI+FeverNLI), which assumes the threat model is known in advance.</p><p>Results of the first setting are summarized in <ref type="table">Table 1</ref>. The vanilla RoBERTa and BERT models perform poorly on the adversarial dataset. In particular, vanilla BERT Large with standard training achieves the lowest robust accuracy of 26.5% among all the models. We also evaluate the robustness improvement by performing adversarial training during fine-tuning, and observe that adversarial training for language models can improve not only generalization but also robustness. In contrast, InfoBERT substantially improves robust accuracy in both standard and adversarial training. The robust accuracy of InfoBERT through standard training is even higher than the adversarial training baseline FreeLB for both RoBERTa and BERT, while the training time of InfoBERT is 1/3 ∼ 1/2 less than FreeLB. This is mainly because FreeLB requires multiple steps of PGD attacks to generate adversarial examples, while InfoBERT essentially needs only 1-step PGD attack for anchored feature selection.</p><p>Results of the second setting are provided in    <ref type="figure">Figure 1</ref>: Local anchored features contribute more to MI improvement than nonrobust/unuseful features, unveiling closer relation with robustness.</p><p>Evaluation against TextFooler InfoBERT can defend against not only human-crafted adversarial examples (e.g., ANLI) but also those generated by adversarial attacks (e.g., TextFooler). Results are summarized in <ref type="table" target="#tab_4">Table 3</ref>. We can see that InfoBERT barely affects model performance on the benign test data, and in the case of adversarial training, InfoBERT even boosts the benign test accuracy. Under the TextFooler attack, the robust accuracy of the vanilla BERT drops to 0.0% on both MNLI and SNLI datasets, while RoBERTa drops from 90% to around 20%. We observe that both adversarial training and InfoBERT with standard training can improve robust accuracy by a comparable large margin, while InfoBERT with adversarial training achieves the best performance among all models, confirming the hypothesis in Theorem 3.2 that combining adversarial training with IB regularizer can further minimize I(X i ; T i ), leading to better robustness than the vanilla one.</p><p>Evaluation on Adversarial SQuAD Previous experiments show that InfoBERT can improve model robustness for NLI tasks. Now we demonstrate that InfoBERT can also be adapted to other NLP tasks such as QA in <ref type="table" target="#tab_5">Table 4</ref>. Similar to our observation on NLI dataset, we find that InfoBERT barely hurts the performance on the benign test data, and even improves it in some cases. Moreover, InfoBERT substantially improves model robustness when presented with adversarial QA test sets (AddSent and AddOneSent). While adversarial training does help improve robustness, InfoBERT can further boost the robust performance by a larger margin. In particular, InfoBERT through standard training achieves the state-of-the-art robust F1/EM score as 78.5/72.9 compared to existing adversarial training baselines, and in the meantime requires only half the training time of adversarialtraining-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ANALYSIS OF LOCAL ANCHORED FEATURES</head><p>We conduct an ablation study to further validate that our anchored feature regularizer indeed filters out nonrobust/unuseful information. As shown in <ref type="table">Table 1</ref>   <ref type="figure">N and I N )</ref>. After adding adversarial examples into the training set and re-training the model, we find that the MI between the local features and the global features substantially increases on the adversarial test data, which accounts for the robustness improvement. We also observe that those local anchored features extracted by our anchored feature regularizer, as expected, contribute more to the MI improvement. As shown in <ref type="figure">Figure 1</ref>, the MI improvement of anchored features on adversarial test data ∆I R (red bar on the left) is higher than that of nonrobust/unuseful ∆I N (red bar on the right), thus confirming that local anchored features discovered by our anchored feature regularizer have a stronger impact on robustness than nonrobust/unuseful ones.</p><p>We conduct more ablation studies in Appendix §A.2, including analyzing the individual impact of two regularizers, the difference between global and local features for IB regularizer, hyper-parameter selection strategy and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a novel learning framework InfoBERT from an information theoretic perspective to perform robust fine-tuning over pre-trained language models. Specifically, InfoBERT consists of two novel regularizers to improve the robustness of the learned representations: (a) Information Bottleneck Regularizer, learning to extract the approximated minimal sufficient statistics and denoise the excessive spurious features, and (b) Local Anchored Feature Regularizer, which improves the robustness of global features by aligning them with local anchored features. Supported by our theoretical analysis, InfoBERT provides a principled way to improve the robustness of BERT and RoBERTa against strong adversarial attacks over a variety of NLP tasks, including NLI and QA tasks. Comprehensive experiments demonstrate that InfoBERT outperforms existing baseline methods and achieves new state of the art on different adversarial datasets. We believe this work will shed light on future research directions towards improving the robustness of representation learning for language models.   <ref type="figure">Figure 2</ref>: The complete objective function of InfoBERT, which can be decomposed into (a) standard task objective, (b) Information Bottleneck Regularizer, and (c) Local Anchored Feature Regularizer. For (b), we both theoretically and empirically demonstrate that we can improve the adversarial robustness by decreasing the mutual information of I(X i ; T i ) without affecting the benign accuracy much. For (c), we propose to align the local anchored features T kj (highlighted in Yellow) with the global feature Z by maximizing their mutual information I(T kj ; Z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 IMPLEMENTATION DETAILS</head><p>Model Details 1 BERT is a transformer <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> based model, which is unsupervised pretrained on large corpora. We use BERT Large -uncased as the baseline model, which has 24 layers, 1024 hidden units, 16 self-attention heads, and 340M parameters. RoBERTa Large shares the same architecture as BERT, but modifies key hyperparameters, removes the next-sentence pretraining objective and trains with much larger mini-batches and learning rates, which results in higher performance than BERT model on GLUE, RACE and SQuAD.</p><p>Standard Training Details For both standard and adversarial training, we fine-tune InfoBERT for 2 epochs on the QA task, and for 3 epochs on the NLI task. The best model is selected based on the performance on the development set. All fine-tuning experiments are run on Nvidia V100 GPUs. For NLI task, we set the batch size to 256, learning rate to 2 × 10 −5 , max sequence length to 128 and warm-up steps to 1000. For QA task, we set the batch size to 32, learning rate to 3 × 10 −5 and max sequence length to 384 without warm-up steps.</p><p>Adversarial Training Details 2 Adversarial training introduces hyper-parameters including adversarial learning rates, number of PGD steps, and adversarial norm. When combing adversarial training with InfoBERT, we use FreeLB as the adversarial training implementation, and set adversarial learning rate to 10 −1 or 4 * 10 −2 , adversarial steps to 3, maximal perturbation norm to 3 * 10 −1 or 2 * 10 −1 and initial random perturbation norm to 10 −1 or 0.</p><p>Information Bottleneck Regularizer Details For information bottleneck, there are different ways to model p(t | x):</p><p>1. Assume that p(t | x) is unknown. We use a neural net parameterized by q θ (t | x) to learn the conditional distribution p(t | x). We assume the distribution is a Gaussian distribution. The neural net q θ will learn the mean and variance of the Gaussian given input x and representation t. By reparameterization trick, the neural net can be backpropagated to approximate the distribution given the training samples.</p><p>2. p(t | x) is known. Since t is the representation encoded by BERT, we actually already know the distribution of p. We also denote it as q θ , where θ is the parameter of the BERT encoder f θ . If we assume the conditional distribution is a Gaussian N (t i , σ) for input x i whose mean is the BERT representation t i and variance is a fixed constant σ, the Eq.6 becomes</p><formula xml:id="formula_16">L LIB = 1 N N i=1 log q ψ (y (i) | t (i) ) − β n k=1 − c(σ)||t (i) k − t (i) k || 2 2 + 1 n N j=1 c(σ)||t j − t k || 2 2 ,<label>(11)</label></formula><p>where c(σ) is a positive constant related to σ. In practice, the sample t i from the conditional distribution Gaussian N (t i , σ) can be t i with some Gaussian noise, an adversarial examples of t i , or t i itself (assume σ = 0).</p><p>We use the second way to model p(t | x) for InfoBERT finally, as it gives higher robustness improvement than the first way empirically (shown in the following §A.2). We suspect that the main reason is because the first way needs to approximate the distribution p(t | x) via another neural net which could present some difficulty in model training.</p><p>Information Bottleneck Regularizer also introduces another parameter β to tune the trad-off between representation compression I(X i ; T i ) and predictive power I(Y ; T ). We search for the optimal β via grid search, and set β = 5 × 10 −2 for RoBERTa, β = 10 −3 for BERT on the NLI task. On the QA task, we set β = 5 × 10 −5 , which is substantially lower than β on NLI tasks, thus containing more word-level features. We think it is mainly because the QA task relies more on the word-level representation to predict the exact answer spans. More ablation results can be found in the following §A.2.</p><p>Anchored Feature Regularizer Details Anchored Feature Regularizer uses α to weigh the balance between predictive power and importance of anchored feature. We set α = 5 × 10 −3 for both NLI and QA tasks. Anchored Feature Regularizer also introduces upper and lower threshold c l and c h for anchored feature extraction. We set c h = 0.9 and c l = 0.5 for the NLI task, and set c h = 0.95 and c l = 0.75 for the QA task. The neural MI estimator used by infoNCE uses two-layer fully connected layer to estimate the MI with the intermediate layer hidden size set to 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ADDITIONAL EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 ABLATION STUDY ON INFORMATION BOTTLENECK REGULARIZER</head><p>Modeling p(t | x) As discussed in §A.1, we have two ways to model p(t | x): (i) using an auxiliary neural network to approximate the distribution; (ii) directly using the BERT encoder f θ to calculate the p(t | x). Thus we implemented these two methods and compare the robustness improvement in <ref type="table">Table 5</ref>. To eliminate other factors such as Anchored Feature Regularizer and adversarial training, we set α = 0, β = 5 × 10 −2 and conduct the following ablation experiments via standard training on standard datasets. We observe that although both modeling methods can improve the model robustness, modeling as BERT encoder gives a larger margin than the Auxiliary Net. Moreover, the second way barely sacrifices the performance on benign data, while the first way can hurt the benign accuracy a little bit. Therefore, we use the BERT Encoder f θ to model the p(t | x) in our main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Features v.s. Global Features</head><p>Information Bottlenck Regularizer improves model robustness by reducing I(X; T ). In the main paper, we use T as word-level local features. Here we consider T as sentence-level global features, and compare the robustness improvement with T as local features. To eliminate other factors such as Anchored Feature Regularizer and adversarial training, we set α = 0, β = 5 × 10 −2 and conduct the following ablation experiments via standard training.  <ref type="table">Table 5</ref>: Robust accuracy on the ANLI dataset. Here we refer "Standard Datasets" as training on the benign datasets (MNLI + SNLI) only. "Vanilla" refers to the vanilla BERT trained without Information Bottleneck Regularizer.</p><p>The experimental results are summarized in <ref type="table" target="#tab_10">Table 6</ref>. We can see that while both features can boost the model robustness, using local features yield higher robust accuracy improvement than global features, especially when adversarial training dataset is added.</p><p>Hyper-parameter Search We perform grid search to find out the optimal β so that the optimal trade-off between representation compression ("minimality") and predictive power ("sufficiency") is achieved. An example to search for the optimal β on QA dataset is shown in Fingure 3, which illustrates how β affects the F1 score on benign and adversarial datasets. We can see that from a very small β, both the robust and benign F1 scores increase, demonstrating InfoBERT can improve both robustness and generalization to some extent. When we set β = 5 × 10 −5 (log(β) = −9.9), InfoBERT achieves the best benign and adversarial accuracy. When we set a larger β to further minimize I(X i ; T i ), we observe that the benign F1 score starts to drop, indicating the increasingly compressed representation could start to hurt its predictive capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 ABLATION STUDY ON ANCHORED FEATURE REGULARIZER</head><p>Visualization of Anchored Words To explore which local anchored features are extracted, we conduct another ablation study to visualize the local anchored words. We follow the best hyperparameters of Anchored Feature Regularizer introduced in §A.1, use the best BERT model trained on benign datasets (MNLI + SNLI) only and test on the ANLI dev set. We visualize the local anchored words in <ref type="table" target="#tab_11">Table 7</ref> as follows. In the first example, we find that Anchored Features mainly focus on the important features such as quantity number "Two", the verb "playing" and objects "card"/"poker" to make a robust prediction. In the second example, the matching robust features between hypothesis and premise, such as "people", "roller" v.s. "park", "flipped upside" v.s. "ride", are aligned to infer the relationship of hypothesis and premise. These anchored feature examples confirm that Anchored Feature Regularizer is able to find out useful and stable features to improve the robustness of global representation.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 ABLATION STUDY ON DISENTANGLING TWO REGULARIZERS</head><p>To understand how two regularizers contribute to the improvement of robustness separetely, we apply two regularizers individually to both the standard training and adversarial training. We refer InfoBERT trained with IB regularizer only as "InfoBERT (IBR only)" and InfoBERT trained with Anchored Feature Regularizer only as "InfoBERT (AFR only)". "InfoBERT (Both)" is the standard setting for InfoBERT, where we incorporate both regularizers during training. For "InfoBERT (IBR only)", we set α = 0 and perform grid search to find the optimal β = 5 × 10 −2 . Similarly for "InfoBERT (AFR only)", we set β = 0 and find the optimal parameters as α = 5 × 10 −3 , c h = 0.9 and c l = 0.5.</p><p>The results are shown in <ref type="table" target="#tab_13">Table 8</ref>. We can see that both regularizers improve the robust accuracy on top of vanilla and FreeLB to a similar margin. Applying one of the regularizer can achieve similar performance of FreeLB, but the training time of InfoBERT is only 1/31/2 less than FreeLB. Moreover, after combining both regularizers, we observe that InfoBERT achieves the best robust accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.4 EXAMPLES OF ADVERSARIAL DATASETS GENERATED BY TEXTFOOLER</head><p>We show some adversarial examples generated by TextFooler in <ref type="table" target="#tab_14">Table 9</ref>. We can see most adversarial examples are of high quality and look valid to human while attacking the NLP models, thus confirming our adversarial dastasets created by TextFooler is a strong benchmark dataset to evaluate model robustness. However, as also noted in <ref type="bibr" target="#b22">Jin et al. (2020)</ref>, we observe that some adversarial examples look invalid to human For example, in the last example of <ref type="table" target="#tab_14">Table 9</ref>, TextFooler replaces "stand" with "position", losing the critical information that "girls are standing instead of kneeling" and fooling both human and NLP models. Therefore, we expect that InfoBERT should achieve better robustness when we eliminate such invalid adversarial examples during evaluation.   We first state two lemmas.</p><p>Lemma A.1. Given a sequence of random variables X 1 , X 2 , ..., X n and a deterministic function f , then ∀ i, j = 1, 2, ..., n, we have</p><formula xml:id="formula_17">I(X i ; f (X i )) ≥ I(X j ; f (X i ))<label>(12)</label></formula><p>Proof. By the definition,</p><formula xml:id="formula_18">I(X i ; f (X i )) = H(f (X i )) − H(f (X i ) | X i ) (13) I(X j ; f (X i )) = H(f (X i )) − H(f (X i ) | X j )<label>(14)</label></formula><p>Since f is a deterministic function,</p><formula xml:id="formula_19">H(f (X i ) | X i ) = 0 (15) H(f (X i ) | X j ) ≥ 0<label>(16)</label></formula><p>Therefore,</p><formula xml:id="formula_20">I(X i ; f (X i )) ≥ I(X j ; f (X i ))<label>(17)</label></formula><p>Lemma A.2. Let X = [X 1 ; X 2 ; ...; X n ] be a sequence of random variables, and T = [T 1 ; T 2 ; ...; T n ] = [f (X 1 ); f (X 2 ); ...; f (X n )] be a sequence of random variables generated by a deterministic function f . Then we have</p><formula xml:id="formula_21">I(X; T ) ≤ n n i=1 I(X i ; T i )<label>(18)</label></formula><p>Proof. Since X = [X 1 ; X 2 ; ...; X n ] and T = [T 1 ; T 2 ; ...; T n ] are language tokens with its corresponding local representations, we have I(X; T ) = I(X; T 1 , T 2 , ...,</p><formula xml:id="formula_22">T n ) = n i=1 [H(T i | T 1 , T 2 , ..., T i−1 ) − H(T i | X, T 1 , T 2 , ..., T i−1 )] (19) ≤ n i=1 [H(T i ) − H(T i | X)] = n i=1 I(X; T i ) (20) ≤ n i=1 n j=1 I(X j ; T i ) ≤ n n i=1 I(X i ; T i ),<label>(21)</label></formula><p>where the first inequality follows because conditioning reduces entropy, and the last inequality is because I(X i ; T i ) ≥ I(X j ; T i ) based on Lemma A.1.</p><p>Then we directly plug Lemma A.2 into Theorem 3.1, we have the lower bound of L IB as</p><formula xml:id="formula_23">I(Y ; T ) − βI(X; T ) ≥ I(Y ; T ) − nβ n i=1 I(X i ; T i ).<label>(22)</label></formula><p>A.3.2 PROOF OF THEOREM 3.2</p><p>We first state an easily proven lemma,</p><formula xml:id="formula_24">Lemma A.3. For any a, b ∈ [0, 1], |a log(a) − b log(b)| ≤ φ(|a − b|),<label>(23)</label></formula><p>where φ(·) : R + → R + is defined as</p><formula xml:id="formula_25">φ(x) =    0 x = 0 x log( 1 x ) 0 &lt; x &lt; 1 e 1 e x &gt; 1 e .<label>(24)</label></formula><p>It is easy to verify that φ(x) is a continuous, monotonically increasing, concave and subadditive function.</p><p>Now, we can proceed with the proof of Theorem 3.2.</p><p>Proof. We use the fact that</p><formula xml:id="formula_26">|I(Y ; T ) − I(Y ; T )| ≤ |H(T | Y ) − H(T | Y )| + |H(T ) − H(T )|<label>(25)</label></formula><p>and bound each of the summands on the right separately.</p><p>We can bound the first summand as follows: </p><formula xml:id="formula_27">|H(T | Y ) − H(T | Y )| ≤</formula><p>Since x∈X ∪X p(x | y) − q(x | y) = 0 for any y ∈ Y, we have that for any scalar a,  </p><p>where for any real-value vector a = (a 1 , ..., a n ), V (a) is defined to be proportional to the variance of elements of a:</p><formula xml:id="formula_30">V (a) = n i=1 (a i − 1 n n j=1 a j ) 2 ,<label>(37)</label></formula><p>p(t | x ∈ X ∪ X ) stands for the vector in which entries are p(t | x) with different values of x ∈ X ∪ X for a fixed t, and p(x | y) and q(x | y) are the vectors in which entries are p(x | y) and q(x | y), respectively, with different values of x ∈ X ∪ X for a fixed y.</p><p>Since</p><formula xml:id="formula_31">||p(x | y) − q(x | y)|| 2 ≤ ||p(x | y) − q(x | y)|| 1 ≤ 2,<label>(38)</label></formula><p>it follows that</p><formula xml:id="formula_32">|H(T | Y ) − H(T | Y )| ≤ y p(y) t φ 2 V (p(t | x ∈ X ∪ X ))<label>(39)</label></formula><p>Moreover, we have V (p(t | x ∈ X ∪ X ) ≤ V (p(t | x ∈ X )) + V (p(t | x ∈ X ))</p><p>≤ V (p(t | x ∈ X )) + V (p(t | x ∈ X )),</p><p>where the first inequality is because sample mean is the minimizer of the sum of the squared distances to each sample and the second inequality is due to the subadditivity of the square root function. Using the fact that φ(·) is monotonically increasing and subadditive, we get</p><formula xml:id="formula_35">|H(T | Y ) − H(T | Y )| ≤ y p(y) t φ 2 V (p(t | x ∈ X )) + y p(y) t φ 2 V (p(t | x ∈ X ))<label>(42)</label></formula><p>Now we explicate the process for establishing the bound for y p(y) t φ 2 V (p(t | x ∈ X )) and the one for y p(y) t φ 2 V (p(t | x ∈ X )) can be similarly derived.</p><p>By definition of V (·) and using Bayes' theorem p(t | x) = p(t)p(x|t) p(x) for x ∈ X , we have that</p><formula xml:id="formula_36">V (p(t | x ∈ X )) = p(t) x∈X p(x | t) p(x) − 1 |X | x ∈X p(x | t) p(x ) 2<label>(43)</label></formula><p>Denoting 1 = (1, ..., 1), we have by the triangle inequality that</p><formula xml:id="formula_37">x∈X p(x | t) p(x) − 1 |X | x ∈X p(x | t) p(x ) 2 (44) ≤ || p(x | t) p(x) − 1|| 2 + x∈X 1 − 1 |X | x ∈X p(x | t) p(x ) 2 (45) = || p(x | t) p(x) − 1|| 2 + |X | 1 − 1 |X | x ∈X p(x | t) p(x ) 2 (46) = || p(x | t) p(x) − 1|| 2 + 1 |X | |X | − x ∈X p(x | t) p(x ) 2 (47) = || p(x | t) p(x) − 1|| 2 + 1 |X | | x ∈X (1 − p(x | t) p(x ) )| (48) ≤ || p(x | t) p(x) − 1|| 2 + 1 |X | || p(x | t) p(x) − 1|| 1 (49) ≤ (1 + 1 |X | )|| p(x | t) p(x) − 1|| 1 (50) ≤ 2 min x∈X p(x) ||p(x | t) − p(x)|| 1<label>(51)</label></formula><p>From an inequality linking KL-divergence and the l 1 norm, we have that</p><formula xml:id="formula_38">||p(x | t) − p(x)|| 1 ≤ 2 log(2)D KL [p(x | t)||p(x)]<label>(52)</label></formula><p>Plugging Eq. (52) into Eq. (51) and using Eq. (43), we have the following bound:</p><formula xml:id="formula_39">V (p(t | x ∈ X )) ≤ B 2 p(t) d t ,<label>(53)</label></formula><p>where B = 4 √ 2 log <ref type="formula" target="#formula_2">(2)</ref> min x∈X p(x) and d t = D KL [p(x | t)||p(x)].</p><p>We will first proceed the proof under the assumption that Bp(t) √ d t ≤ 1 e for any t. We will later see that this condition can be discarded. If Bp(t)</p><formula xml:id="formula_40">√ d t ≤ 1 e , then t φ 2 V (p(t | x ∈ X )) (54) ≤ t Bp(t) d t log( 1 B ) + log( 1 p(t)d t ) (55) = B log( 1 B ) t p(t) d t + B t p(t) d t log( 1 p(t)d t ) (56) ≤ B log( 1 B )||p(t) d t || 1 + B|| p(t) d t || 1 ,<label>(57)</label></formula><p>where the last inequality is due to an easily proven fact that for any x &gt; 0, x log( 1 x ) ≤ √ x. We p(t) and d(t) are vectors comprising p(t) and d t with different values of t, respectively.</p><p>Using the following two inequalities:</p><formula xml:id="formula_41">||p(t) d t || 1 ≤ |T |||p(t) d t || 2 ≤ |T ||| p(t)d t || 2<label>(58)</label></formula><p>and || p(t) d t || 1 ≤ |T ||| p(t) d t || 2</p><p>= √ T ||p(t) d t || 1 ≤ |T | 3/4 || p(t)d t || 2 (60)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Benign/robust F1 score on benign/adversarial QA datasets. Models are trained on the benign SQuAD dataset with different β. Input (bold = local stable words for local anchored features.) Premise: Two woman, both sitting near a pile of poker chips, are playing cards. Hypothesis: Two woman playing poker. Premise: People are flipped upside -down on a bright yellow roller coaster. Hypothesis: People on on a ride at an amusement park.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>y p(y)|H(T | Y = y) − H(T | Y = y)| y) log p(t | y) − q(t | y) log q(t | y)| | x)[p(x | y) − q(x | y)]|),(30)wherep(x | y) = p(y | x)p(x) x p(y | x)p(x) (31) q(x | y) = p(y | x)q(x) x p(y | x)q(x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>| x)[p(x | y) − q(x | y)t | x) − a)(p(x | y) − q(x | y))| (34)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>x | y) − q(x | y)) 2 .(35)Setting a = 1 |X −X | x∈X ∪X p(t | x) we get |H(T | Y ) − H(T | Y ) ≤ y p(y) t φ V (p(t | x ∈ X ∪ X ) · ||p(x | y) − q(x | y)|| 2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>only, and test the models on both benign and adversarial test sets. (III) TextFooler<ref type="bibr" target="#b22">(Jin et al., 2020)</ref> is the state-of-the-art word-level adversarial attack method to generate adversarial examples. To create an adversarial evaluation dataset, we sampled 1, 000 examples from the test sets of SNLI and MNLI respectively, and run TextFooler against BERT Large and RoBERTa Large to obtain the adversarial text examples.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 1: Robust accuracy on the ANLI dataset. Models are trained on the benign datasets (MNLI + SNLI) only. 'A1-A3' refers to the rounds with increasing difficulty. 'ANLI' refers to A1+A2+A3.</figDesc><table><row><cell>Training</cell><cell>Model</cell><cell>Method</cell><cell></cell><cell></cell><cell>Dev</cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell>A1</cell><cell>A2</cell><cell cols="2">A3 ANLI</cell><cell>A1</cell><cell>A2</cell><cell>A3 ANLI</cell></row><row><cell>Standard</cell><cell>RoBERTa</cell><cell cols="4">Vanilla InfoBERT 47.8 31.2 31.8 49.1 26.5 27.2</cell><cell>33.8 36.6</cell><cell>49.2 27.6 24.8 47.3 31.2 31.1</cell><cell>33.2 36.2</cell></row><row><cell>Training</cell><cell>BERT</cell><cell cols="4">Vanilla InfoBERT 26.0 30.1 31.2 20.7 26.9 31.2</cell><cell>26.6 29.2</cell><cell>21.8 28.3 28.8 26.4 29.7 29.8</cell><cell>26.5 28.7</cell></row><row><cell>Adversarial</cell><cell>RoBERTa</cell><cell cols="4">FreeLB InfoBERT 48.4 29.3 31.3 50.4 28.0 28.5</cell><cell>35.2 36.0</cell><cell>48.1 30.4 26.3 50.0 30.6 29.3</cell><cell>34.4 36.2</cell></row><row><cell>Training</cell><cell>BERT</cell><cell cols="4">FreeLB InfoBERT 28.3 30.2 33.8 23.0 29.0 32.2</cell><cell>28.3 30.9</cell><cell>22.2 28.5 30.8 25.9 28.1 30.3</cell><cell>27.4 28.2</cell></row><row><cell>Training</cell><cell>Model</cell><cell>Method</cell><cell></cell><cell></cell><cell>Dev</cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell>A1</cell><cell>A2</cell><cell cols="2">A3 ANLI</cell><cell>A1</cell><cell>A2</cell><cell>A3 ANLI</cell></row><row><cell>Standard</cell><cell>RoBERTa</cell><cell cols="4">Vanilla InfoBERT 75.2 49.6 47.8 74.1 50.8 43.9</cell><cell>55.5 56.9</cell><cell>73.8 48.9 44.4 73.9 50.8 48.8</cell><cell>53.7 57.3</cell></row><row><cell>Training</cell><cell>BERT</cell><cell cols="4">Vanilla InfoBERT 59.3 48.9 45.5 58.5 46.1 45.5</cell><cell>49.8 50.9</cell><cell>57.4 48.3 43.5 60.0 46.9 44.8</cell><cell>49.3 50.2</cell></row><row><cell></cell><cell></cell><cell>FreeLB</cell><cell cols="3">75.2 47.4 45.3</cell><cell>55.3</cell><cell>73.3 50.5 46.8</cell><cell>56.2</cell></row><row><cell>Adversarial Training</cell><cell>RoBERTa</cell><cell cols="4">SMART ALUM InfoBERT 76.4 51.7 48.6 74.5 50.9 47.6 73.3 53.4 48.2</cell><cell>57.1 57.7 58.3</cell><cell>72.4 49.8 50.3 72.3 52.1 48.4 75.5 51.4 49.8</cell><cell>57.1 57.0 58.3</cell></row><row><cell></cell><cell></cell><cell>FreeLB</cell><cell cols="3">60.3 47.1 46.3</cell><cell>50.9</cell><cell>60.3 46.8 44.8</cell><cell>50.2</cell></row><row><cell></cell><cell>BERT</cell><cell>ALUM</cell><cell cols="3">62.0 48.6 48.1</cell><cell>52.6</cell><cell>61.3 45.9 44.3</cell><cell>50.1</cell></row><row><cell></cell><cell></cell><cell cols="4">InfoBERT 60.8 48.7 45.9</cell><cell>51.4</cell><cell>63.3 48.7 43.2</cell><cell>51.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Robust accuracy on the ANLI dataset. Models are trained on both adversarial and benign datasets (ANLI (training) + FeverNLI + MNLI + SNLI).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Training</cell><cell>Model</cell><cell>Method</cell><cell>SNLI</cell><cell>MNLI (m/mm)</cell><cell cols="2">adv-SNLI adv-MNLI (BERT) (BERT)</cell><cell>adv-SNLI (RoBERTa) (RoBERTa) adv-MNLI</cell></row><row><cell>Standard</cell><cell>RoBERTa</cell><cell cols="3">Vanilla InfoBERT 93.3 90.5/90.4 92.6 90.8/90.6</cell><cell>56.6 59.8</cell><cell>68.1/68.6 69.8/70.6</cell><cell>19.4 42.5</cell><cell>24.9/24.9 50.3/52.1</cell></row><row><cell>Training</cell><cell>BERT</cell><cell cols="3">Vanilla InfoBERT 91.7 86.2/86.0 91.3 86.7/86.4</cell><cell>0.0 36.7</cell><cell>0.0/0.0 43.5/46.6</cell><cell>44.9 45.4</cell><cell>57.0/57.5 57.2/58.6</cell></row><row><cell>Adversarial</cell><cell>RoBERTa</cell><cell cols="3">FreeLB InfoBERT 93.1 90.7/90.4 93.4 90.1/90.3</cell><cell>60.4 62.3</cell><cell>70.3/72.1 73.2/73.1</cell><cell>41.2 43.4</cell><cell>49.5/50.6 56.9/55.5</cell></row><row><cell>Training</cell><cell>BERT</cell><cell cols="3">FreeLB InfoBERT 92.2 87.2/87.2 92.4 86.9/86.5</cell><cell>46.6 50.8</cell><cell>60.0/60.7 61.3/62.7</cell><cell>50.5 52.6</cell><cell>64.0/62.9 65.6/67.3</cell></row></table><note>, which shows InfoBERT can further improve robust accuracy for both standard and adversarial training. Specifically, when combined with adver- sarial training, InfoBERT achieves the state-of-the-art robust accuracy of 58.3%, outperforming all existing baselines. Note that although ALUM achieves higher accuracy for BERT on the dev set, it tends to overfit on the dev set, therefore performing worse than InfoBERT on the test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: Robust accuracy on the adversarial SNLI and MNLI(-m/mm) datasets generated by</cell></row><row><cell cols="5">TextFooler based on blackbox BERT/RoBERTa (denoted in brackets of the header). Models are</cell></row><row><cell cols="5">trained on the benign datasets (MNLI+SNLI) only.</cell></row><row><cell>Training</cell><cell>Method</cell><cell>benign</cell><cell cols="2">AddSent AddOneSent</cell></row><row><cell>Standard</cell><cell>Vanilla</cell><cell cols="2">93.5/86.9 72.9/66.6</cell><cell>80.6/74.3</cell></row><row><cell>Training</cell><cell cols="3">InfoBERT 93.5/87.0 78.5/72.9</cell><cell>84.6/78.3</cell></row><row><cell>Adversarial Training</cell><cell cols="3">FreeLB ALUM InfoBERT 93.7/87.0 78.0/71.8 93.8/87.3 76.3/70.3 -75.5/69.4</cell><cell>82.3/76.2 81.4/75.9 83.6/77.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">MI Improvement after adding adv</cell></row><row><cell>0.08</cell><cell cols="4">examples in the training set</cell></row><row><cell>0.06</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>∆ ! "</cell><cell>∆ !</cell><cell>∆ $ "</cell><cell>∆ $</cell></row><row><cell cols="3">Adversarial Test Data</cell><cell cols="2">Benign Test Data</cell></row></table><note>Robust F1/EM scores based on RoBERTa Large on the adversarial SQuAD datasets (AddSent and AddOne- Sent). Models are trained on standard SQuAD 1.0 dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>R and I R ). We then calculate the MI between nonrobust/unuseful features and global features 1 M M i=1 I(T ki ; Z) on the adversarial test data and benign data as well (denoted by I</figDesc><table /><note>and 2, adding adversarial data in the training set can significantly improve model robustness. To find out what helps improve the robustness from the MI perspective, we first calculate the MI between anchored features and global features 1 M M j=1 I(T kj ; Z) on the adversarial test data and benign test data, based on the model trained without adversarial training data (denoted by I</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Robust accuracy on the ANLI dataset. Here we refer "Standard Datasets" as training on the benign datasets (MNLI + SNLI) only, and "Standard and Adversarial Datasaets" as training on the both benign and adversarial datasets (ANLI(trianing) + MNLI + SNLI + FeverNLI). "Vanilla" refers to the vanilla RoBERTa trained without Information Bottleneck Regularizer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Local anchored features extracted by Anchored Feature Regularizer.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Robust accuracy on the ANLI dataset. Models are trained on the benign datasets (MNLI + SNLI). Here we refer "IBR only" as training with Information Bottleneck Regularizer only. "AFR only" refers to InfoBERT trained with Anchored Feature Regularizer only. "Both" is the standard InfoBERT that applies two regularizers together. There is a boy in the water.Adversarial Hypothesis: There is a man in the water. Adults and children share in the looking at something, and some young ladies stand to the side.Original Hypothesis: Some children are sleeping. Adversarial Hypothesis: Some children are dreaming. Families with strollers waiting in front of a carousel.Original Hypothesis: Families have some dogs in front of a carousel. Adversarial Hypothesis: Families have some doggie in front of a carousel. Two girls are kneeling on the ground.Original Hypothesis: Two girls stand around the vending machines. Adversarial Hypothesis: Two girls position around the vending machinery.</figDesc><table><row><cell>Valid Adversarial Examples</cell></row><row><cell>Premise: A young boy is playing in the sandy water.</cell></row><row><cell>Original Hypothesis: Model Prediction: Entailment → Contradiction</cell></row><row><cell>Premise: A black and brown dog is playing with a brown and white dog .</cell></row><row><cell>Original Hypothesis: Two dogs play.</cell></row><row><cell>Adversarial Hypothesis: Two dogs gaming.</cell></row><row><cell>Model Prediction: Entailment → Neutral</cell></row><row><cell>Premise: Model Prediction: Contradiction → Neutral</cell></row><row><cell>Premise: Model Prediction: Contradiction → Entailment</cell></row><row><cell>Invalid Adversarial Examples</cell></row><row><cell>Premise:</cell></row></table><note>Input (red = Modified words, bold = original words.)Model Prediction: Contradiction → Neutral</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Adversarial Examples Generated by TextFooler for BERT Large on SNLI dataset.</figDesc><table><row><cell>A.3 PROOFS</cell></row><row><cell>A.3.1 PROOF OF THEOREM 3.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the huggingface implementation https://github.com/huggingface/transformers for BERT and RoBERTa.2  We follow the FreeLB implementations in https://github.com/zhuchen03/FreeLB.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">log(2) min x∈X q(x) .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENT</head><p>We gratefully thank the anonymous reviewers and meta-reviewers for their constructive feedback. We also thank Julia Hockenmaier, Alexander Schwing, Sanmi Koyejo, Fan Wu, Wei Wang, Pengyu Cheng, and many others for the helpful discussion. This work is partially supported by NSF grant No.1910100, DARPA QED-RML-FP-003, and the Intel RSA 2020.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Using the equality || p(t)d t || 2 = E[D <ref type="bibr">KL[p(x|t)</ref>||p(x)] ] = I(X; T ),</p><p>we reach the following bound</p><p>≤ B log( 1 B )|T | 1/2 I(X; T ) 1/2 + B|T | 3/4 I(X; T ) 1/4 .</p><p>Plug Lemma A.2 into the equation above, we have</p><p>We now show the bound is trivial if the assumption that Bp(t) √ d t ≤ 1 e does not hold. If the assumption does not hold, then there exists a t such that Bp(t)</p><p>for any t, we get that I(X; T ) ≥ 1 eB . Since |T | ≥ 1 and C ≥ 0, we get that our bound in Eq. (63) is at least</p><p>Similarly, we can establish a bound for t φ 2 V (p(t | x ∈ X )) as follows: </p><p>Now we turn to the third summand in Eq. (25), we have to bound |H(T ) − H(T )|.</p><p>Recall the definition of -bounded adversarial example. We denote the set of the benign data representation t that are within the -ball of t by Q(t ). Then for any t ∈ Q(t ), we have</p><p>for i = 1, 2, ..., n. We also denote the number of the -bounded adversarial examples around the benign representation t by c(t). Then we have the distribution of adversarial representation t as follows:</p><p>where the inequality is by log sum inequality. If we denote the C = max t c(t) which is the maximum number of -bounded textual adversarial examples given a benign representation t of a word sequence x, we have</p><p>Note that given a word sequence x of n with representation t, the number of -bounded textual adversarial examples c(t) is finite given a finite vocabulary size. Therefore, if each word has at most k candidate word perturbations, then log C ≤ n log k can be viewed as some constants depending only on n and . Now, combining Eq. (25), Eq. (74) and Eq. (85), we prove the bound in Theorem 3.2.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating natural language adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Jhang</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mani</forename><forename type="middle">B</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<editor>Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun&apos;ichi Tsujii</editor>
		<imprint>
			<biblScope unit="page" from="2890" to="2896" />
			<date type="published" when="2018" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The im algorithm: A variational approach to information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">V</forename><surname>Agakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<editor>Lluís Màrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton</editor>
		<imprint>
			<biblScope unit="page" from="632" to="642" />
			<date type="published" when="2015" />
			<publisher>The Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<editor>Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Club: A contrastive log-ratio upper bound of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weituo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>ArXiv, abs</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Certified adversarial robustness via randomized smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, volume 97 of Proceedings of Machine Learning Research</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1310" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Training verified learners with learned verifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnamurthy</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohli</surname></persName>
		</author>
		<idno>abs/1805.10265</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hotflip: White-box adversarial examples for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>Iryna Gurevych and Yusuke Miyao</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Atul Prakash, Tadayoshi Kohno, and Dawn Xiaodong Song. Robust physical-world attacks on deep learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1412.6572</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Achieving verified robustness to symbol substitutions via interval bound propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnamurthy</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<editor>Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4081" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial example generation with syntactically controlled paraphrase networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<editor>Marilyn A. Walker, Heng Ji, and Amanda Stent</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1875" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Excessive invariance causes adversarial vulnerability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<editor>Martha Palmer, Rebecca Hwa, and Sebastian Riedel</editor>
		<imprint>
			<biblScope unit="page" from="2021" to="2031" />
			<date type="published" when="2017" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Certified robustness to adversarial word substitutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerem</forename><surname>Göksel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<editor>Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4127" to="4140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SMART: robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2177" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Is BERT really robust? A strong baseline for natural language attack on text classification and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8018" to="8025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A mutual information maximization perspective of language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyprien</forename><surname>De Masson D&amp;apos;autume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Textbugger: Generating adversarial text against real-world applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NDSS. The Internet Society</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adversarial training for large neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/2004.08994</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deepfool: A simple and accurate method to fool deep neural networks. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Seyed-Mohsen Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial NLI: A new benchmark for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4885" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<editor>Jian Su, Xavier Carreras, and Kevin Duh</editor>
		<imprint>
			<biblScope unit="page" from="2383" to="2392" />
			<date type="published" when="2016" />
			<publisher>The Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generating natural language adversarial examples through probability weighted word saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Shuhuai Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>Anna Korhonen, David R. Traum, and Lluís Màrquez</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1085" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orestis</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><surname>Khandeparkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, volume 97 of Proceedings of Machine Learning Research</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5628" to="5637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">T3: Treeautoencoder constrained adversarial text generation for targeted attack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<editor>Marilyn A. Walker, Heng Ji, and Amanda Stent</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SAFER: A structure-free approach for certified robustness to adversarial word substitutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3465" to="3475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fine-tuning pretrained language model with weak supervision: A contrastive-regularized self-training approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simiao</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2010.07835</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Word-level textual adversarial attacking as combinatorial optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6066" to="6080" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">PAWS: paraphrase adversaries from word scrambling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1298" to="1308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Freelb: Enhanced adversarial training for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning adversarially robust representations via worst-case mutual information maximization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
