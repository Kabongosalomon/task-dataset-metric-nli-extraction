<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Every Pixel Counts ++: Joint Learning of Geometry and Motion with 3D Holistic Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Wei</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
						</author>
						<title level="a" type="main">Every Pixel Counts ++: Joint Learning of Geometry and Motion with 3D Holistic Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Depth Estimation</term>
					<term>Optical Flow Prediction</term>
					<term>Unsupervised Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to estimate 3D geometry in a single frame and optical flow from consecutive frames by watching unlabeled videos via deep convolutional network has made significant progress recently. Current state-of-the-art (SoTA) methods treat the two tasks independently. One typical assumption of the existing depth estimation methods is that the scenes contain no independent moving objects. while object moving could be easily modeled using optical flow. In this paper, we propose to address the two tasks as a whole, i.e. to jointly understand per-pixel 3D geometry and motion. This eliminates the need of static scene assumption and enforces the inherent geometrical consistency during the learning process, yielding significantly improved results for both tasks. We call our method as "Every Pixel Counts++" or "EPC++". Specifically, during training, given two consecutive frames from a video, we adopt three parallel networks to predict the camera motion (MotionNet), dense depth map (DepthNet), and per-pixel optical flow between two frames (OptFlowNet) respectively. The three types of information, are fed into a holistic 3D motion parser (HMP), and per-pixel 3D motion of both rigid background and moving objects are disentangled and recovered. Various loss terms are formulated to jointly supervise the three networks. An effective adaptive training strategy is proposed to achieve better performance and more efficient convergence. Comprehensive experiments were conducted on datasets with different scenes, including driving scenario (KITTI 2012 and KITTI 2015  datasets), mixed outdoor/indoor scenes (Make3D) and synthetic animation (MPI Sintel dataset). Performance on the five tasks of depth estimation, optical flow estimation, odometry, moving object segmentation and scene flow estimation shows that our approach outperforms other SoTA methods, demonstrating the effectiveness of each module of our proposed method. Code will be available at: https://github.com/chenxuluo/EPC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>E STIMATING 3D geometry (e.g. per-pixel depth) from a single image, understanding motion (e.g. relative camera pose and object motion) and optical flow between consecutive frames from a video are fundamental problems in computer vision. They enable a wide range of real-world applications such as augmented reality <ref type="bibr" target="#b0">[1]</ref>, video analysis <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and robotics navigation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. In this paper, we propose an effective learning framework that jointly estimates per-pixel depth, camera motion and optical flow, using only unlabeled videos as training data.</p><p>Our work is motivated by recent unsupervised single image depth estimation approaches <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> which train a depth estimation deep network taking unlabeled video frames as input and using supervision with view-synthesis. Their estimated depths are even better than results from those of some supervised methods <ref type="bibr" target="#b11">[12]</ref> in outdoor scenarios. Specifically, the core idea follows the rule of rigid structure from motion (SfM) <ref type="bibr" target="#b12">[13]</ref>, where the image of one view (source image) is warped to another view (target image) based on the predicted depth map of target image and their relative 3D camera motion. The photometric â€¢ P. <ref type="bibr">Wang</ref> error between the warped image and the target image is used to supervise the learning of networks. However, real world videos may contain moving objects, which are inconsistent with the rigid scene assumption used in the earlier frameworks. Zhou et al. <ref type="bibr" target="#b9">[10]</ref> try to avoid such errors by inducing an explanability mask, where both pixels from moving objects and occluded regions are ignored during training. Vijayanarasimhan et al. <ref type="bibr" target="#b13">[14]</ref> separately tackle moving objects with a multi-rigid body model by estimating k object masks and k object pivots from the motion network. This system requires placing a limitation on the number of objects, and doesn't yield better geometry estimation results than those from Zhou et al. <ref type="bibr" target="#b9">[10]</ref> or other systems <ref type="bibr" target="#b5">[6]</ref> which do not explicitly model moving objects.</p><p>Optical flow estimation methods <ref type="bibr" target="#b14">[15]</ref> do consider dense 2D pixel matching, which is able to model both rigid motion because of camera movement and non-rigid motion induced by objects in the scene. Similar as in unsupervised depth learning, one may train a flow network in an unsupervised manner through view synthesis, as proposed recently by Jason et al. <ref type="bibr" target="#b15">[16]</ref> and Ren et al. <ref type="bibr" target="#b16">[17]</ref>. Although the learned flow network yields impressive results, these systems lack understanding of the underlining 3D geometry, yielding difficulties in regularization of the predictions, e.g. in the occluded regions. Some recent works <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> leverage the benefits from the two tasks. They either failed to consider rigid/non-rigid motion, occlusion regions, or didn't enforce consistency between the depth and optical flow. (e) (f) (g) <ref type="figure">Fig. 1</ref>: (a) Two consecutive images (transparent second frame overlapped onto the first frame), (b) our estimated depth for the first frame, (c) our estimated optical flow, (d) our estimated moving object mask, (e) depth from LEGO <ref type="bibr" target="#b5">[6]</ref> for the first frame, (f) optical flow estimation from Wang et al. <ref type="bibr" target="#b6">[7]</ref>, (g) segmentation mask from EPC <ref type="bibr" target="#b7">[8]</ref>. By jointly learning all three geometrical cues, our results show significant improvement over other SoTA methods on different tasks.</p><p>In this paper, we propose an effective unsupervised/selfsupervised learning system by jointly considering the depth, camera pose and optical flow estimation via adaptive consistency. The motivation here is to better exploit the advantages of depth and optical flow. On non-occluded regions, 2D optical flow estimation is much easier and often more accurate than computing rigid flow via depth and motion. So it can be used for guiding depth and motion estimation. On the contrary, in occluded regions, there are no explicit cues for directly matching. We thus leverage depth and motion information to help optical flow estimation, as they are more reliable in this case. We call this adaptive consistency in contrast to the cross-task consistency proposed in DF-Net <ref type="bibr" target="#b19">[20]</ref>. Our pipeline consider every pixel during the learning process, yielding significant performance boost on both geometry and motion estimation over previous SoTA methods (as illustrated in <ref type="figure">Fig. 1</ref>.</p><p>We show the framework of EPC++ in <ref type="figure" target="#fig_1">Fig. 2</ref>. Given two consecutive frames (I s and I t ), we estimate forward/backward flow maps (F tâ†’s , F sâ†’t ), camera motion between the two frames (T tâ†’s ) and corresponding depth maps (D t , D s ). The three types of information are fed into a holistic motion parser (HMP), where the visibility/non-occlusion mask (V), the moving object segmentation mask (S), the per-pixel 3D motions for rigid background (M b ) and for moving object (M d ) are recovered following geometrical rules and consistency. In principle, on non-occluded pixels, the values of M d are encouraged to be close to zero in rigid regions, and to be large inside a moving object region, which yields the moving object mask. For pixels that are occluded, we use depth and camera motion to inpaint the optical flow, which shows more accurate results than using smoothness prior adopted by <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>. We adopt the above adaptive consistency principles to guide the design of losses, and learning strategies for the networks. All the operations inside the HMP are easy to compute and differentiable. Therefore, the system can be trained end-to-end, which leverage the benefits of both depth estimation and optical flow prediction.</p><p>Last but not the least, recovering depth and object motion simultaneously from a monocular video, which is dependent on the given projective camera model <ref type="bibr" target="#b20">[21]</ref>, is an ill-posed problem. In particular, from the view point of a camera, a very close object moving with the camera is equivalent to a far object keeping relatively still, yielding scale confusion for depth estimation. Similar observations are also presented in <ref type="bibr" target="#b21">[22]</ref>. Here, we address this issue by also incorporating stereo image pairs into our learning framework during training stage, resulting in a more robust system for depth, and optical flow estimation.</p><p>We conducted extensive experiments on the public KITTI 2015 <ref type="bibr" target="#b22">[23]</ref>, Make3D <ref type="bibr" target="#b23">[24]</ref> and MPI-Sintel <ref type="bibr" target="#b24">[25]</ref> dataset, and evaluated our results in multiple aspects including depth estimation, optical flow estimation, 3D scene flow estimation, camera motion and moving object segmentation. As elaborated in Sec. 4, EPC++ significantly outperforms other SoTA methods over all tasks. We will release the code of our paper upon its publication.</p><p>In summary, the contributions of this paper lie in four aspects:</p><p>â€¢ We propose an effective unsupervised/self-supervised learning framework, EPC++, to jointly learn depth, camera motion, optical flow and moving object segmentation by leveraging the consistency across different tasks. â€¢ We design a holistic motion parser (HMP) to decompose background foreground 3D motion with awareness of scene rigidity and visibility of each pixel. â€¢ We propose an adaptive learning strategy. It proves to be effective for training EPC++, which contains different coupled geometrical information. â€¢ Comprehensive experiments over five tasks are conducted to validate each component in the proposed system. Results show that EPC++ achieves SoTA performance on all the tasks on KITTI datasets (driving scene), and also generalizes well to non-driving datasets such as Make3D and MPI-Sintel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Estimating single view depth, predicting 3D motion and optical flow from images have long been central problems for computer vision. Here we summarize the most related works in various aspects without enumerating them all due to space limitation. Structure from motion and single view geometry. Geometric based methods estimate 3D from a given video with feature matching or patch matching, such as PatchMatch Stereo <ref type="bibr" target="#b25">[26]</ref>, SfM <ref type="bibr" target="#b12">[13]</ref>, SLAM <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> and DTAM <ref type="bibr" target="#b0">[1]</ref>, and are effective and efficient in many cases. When there are dynamic motions in monocular videos, there is often scale-confusion for each non-rigid movement. Thus regularization through low-rank <ref type="bibr" target="#b28">[29]</ref>, orthographic camera <ref type="bibr" target="#b29">[30]</ref>, rigidity <ref type="bibr" target="#b30">[31]</ref> or fixed number of moving objects <ref type="bibr" target="#b31">[32]</ref> are required in order to obtain an unique solution. However, those methods assumed the 2D matching are reliable, which can fail at where there is low texture, or drastic change of visual perspective etc.. More importantly, those methods can not be extended to single view reconstruction.</p><p>MotionNet Traditionally, estimating depth from a single view depends on hand-crafted features with specific and strong assumptions, such as computing vanishing point <ref type="bibr" target="#b32">[33]</ref>, using assumptions of BRDF <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, or extracting the scene layout with major plane and box representations with Manhattan world <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> etc.. These methods typically only obtain sparse geometry representations. Supervised depth estimation with CNN. Deep neural networks (DCN) developed in recent years provide stronger feature representation. Dense geometry, i.e. pixel-wise depth and normal maps, can be readily estimated from a single image <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> and trained in an end-to-end manner. The learned CNN model show significant improvement compared to other methods which were based on hand-crafted features <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Others try to improve the estimation further by appending a conditional random field (CRF) <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. However, all these supervised methods require densely labeled ground truths, which are expensive to obtain in natural environments. Unsupervised single image depth estimation. Most recently, many CNN based methods are proposed to do single view geometry estimation with supervision from stereo images or videos, yielding impressive results. Some of them relies on stereo image pairs <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, e.g. warping one image to another given known stereo baseline. Some others relies on monocular videos <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref> by incorporating 3D camera pose estimation from a motion network. However, as discussed in Sec. 1, most of these models only consider a rigid scene, where moving objects were omitted. Vijayanarasimhan et al. <ref type="bibr" target="#b13">[14]</ref> model rigid moving objects with k motion masks, while their estimated depths were negatively effected by such an explicit rigid object assumption comparing to the one without object modeling <ref type="bibr" target="#b9">[10]</ref>. Casser et al. <ref type="bibr" target="#b54">[55]</ref> use Mask R-CNN <ref type="bibr" target="#b55">[56]</ref> to generate possible moving regions and apply scale constraints on the moving objects. However, this can not be considered as a purely unsupervised or self-supervised method since it leverage supervisions from heterogeneous sources.</p><formula xml:id="formula_0">Ïœ !â†’# ! # OptFlowNet DepthNet HMP [R, t] ! Ïœ #â†’! ! M ) M * [R, t] motion large small</formula><p>Most theabove methods are solely based on photometric errors, i.e. I t (p t ) âˆ’ÃŽ t (p t ) , which use a Lambertian assumption, and are not robust in natural scenes with varing lighting conditions. To handle this problem, supervision signals based on local structural errors, such as local image gradient <ref type="bibr" target="#b10">[11]</ref>, non-local smoothness <ref type="bibr" target="#b5">[6]</ref> and structural similarity (SSIM <ref type="bibr" target="#b56">[57]</ref>) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref> were proposed and yielded more robust matching and shows additional improvement on depth estimation. Most recently, Godard et al. <ref type="bibr" target="#b21">[22]</ref> further improved the results by jointly considering stereo and monocular images with updated neural network architectures. In this work, we jointly consider the learning of optical flow network along with depth estimation and achieve SoTA performances for both depth and optical flow estimation. Optical flow estimation. Similarly, there is a historical road map for optical flow estimation from traditional dense feature matching with local patterns, such as Patch matching <ref type="bibr" target="#b14">[15]</ref>, Piecewise matching <ref type="bibr" target="#b57">[58]</ref>, to supervised learning based on convolutional neural networks (CNNs), such as FlowNet <ref type="bibr" target="#b58">[59]</ref>, SPyNet <ref type="bibr" target="#b59">[60]</ref>, and PWCNet <ref type="bibr" target="#b60">[61]</ref> etc.. These method produce significantly better performance due to deep hierarchical feature including larger while flexible context. Although these methods can be trained using synthesis datasets such as Flying Chairs <ref type="bibr" target="#b61">[62]</ref> or Sintels <ref type="bibr" target="#b24">[25]</ref>, they need high-quality labelled data of real-world scenes for good generalization, which is non-trivial to obtain <ref type="bibr" target="#b3">[4]</ref>.</p><p>The unsupervised learning of optical flow with a neural network is first introduced in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> by training CNNs with image synthesis and local flow smoothness. Most recently, in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, the authors improve the results by explicitly computing the occlusion masks where photometric errors are omitted during the training, yielding more robust results. However, these works do not have 3D scene geometry understanding, e.g. depths and camera motion from the videos. In our case, we leverage such understanding and show a significant improvement over previous SoTA results. 3D Scene flow by joint depth and optical flow estimation. Estimating 3D scene flow <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref> is a task of estimating per-pixel dense flow in 3D given a pair of images, which requires joint consideration of depths and optical flow of given consecutive frames. Traditional algorithms estimate depths from stereo images <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b66">[67]</ref>, or the given image pairs <ref type="bibr" target="#b30">[31]</ref> assuming rigid constraint, and trying to decompose the scene to piece-wise moving planes in order to finding correspondence with larger context <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>. Most recently, Behl et al. <ref type="bibr" target="#b66">[67]</ref> adopt semantic object instance segmentation and supervised stereo disparity from DispNet <ref type="bibr" target="#b69">[70]</ref> to solve large displacement of objects, yielding SoTA results on KITTI dataset.</p><p>Most recently, works in unsupervised learning have begun to consider depths and optical flow together. Yin et al. <ref type="bibr" target="#b17">[18]</ref> propose to estimate the residual flow in addition to the rigid flow, but the depth estimation did not benefit from the learning of optical flow. Ranjan et al. <ref type="bibr" target="#b18">[19]</ref> paste the optical flow from objects to the rigid flow from background and ego-motion to explain the whole scene in a competitive collaboration manner. However, rather than measuring 3D motion consistency, they divide the whole image with a selected threshold. DF-Net <ref type="bibr" target="#b19">[20]</ref> enforce consistency between rigid flow and optical flow but only in nonoccluded and static regions. In our case, we choose to model from the perspective of 3D scene flow, which is embedded in our unsupervised learning pipeline, yielding better results even with weaker backbone networks, i.e. VGG <ref type="bibr" target="#b70">[71]</ref>, demonstrating the effectiveness of EPC++. Motion segmentation. Finally, since our algorithm decomposes static background and moving objects, our approach is also related to segmentation of moving objects from a given video. Current contemporary SoTA methods are dependent on supervision from human labels by adopting CNN image features <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref> or RNN temporal modeling <ref type="bibr" target="#b73">[74]</ref>. For unsupervised video segmentation, saliency estimation based on 2D optical flow is often used to discover and track the objects <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, and long trajectories <ref type="bibr" target="#b76">[77]</ref> of the moving objects based on optical flow need to be considered. However, these approaches commonly handle non-rigid objects within a relative static background, which is out of major scope of this paper. Most recently, Barnes et al. <ref type="bibr" target="#b77">[78]</ref> show that explicitly modeling moving things with a 3D prior map can avoid visual odometry drifting. We also consider moving object segmentation, which is under an unsupervised setting with videos only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING WITH HOLISTIC 3D MOTION UNDER-STANDING</head><p>As discussed in Sec. 1, we obtain per-pixel 3D motion understanding by jointly modeling depth and optical flow, which is dependent on learning methods considering depth <ref type="bibr" target="#b9">[10]</ref> and optical flow <ref type="bibr" target="#b6">[7]</ref> independently.</p><p>In the following, we will first elaborate on the geometry relationship between the two types of information, and then discuss the details about the how we leverage the rules of 3D geometry in EPC++ learning framework (Sec. 3.1) through HMP. Finally, we clarify all our loss functions and training strategies which consider both stereo and monocular images in training, with awareness of 3D motion dissected from HMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Geometrical understanding with 3D motion</head><p>Given two images, i.e. a target view image I t and a source view image I s , suppose that D t , D s are the depth maps of I t , I s , their relative camera transformation is T tâ†’s = [R|t] âˆˆ SE(3) from I t to I s , and let optical flow from I t to I s be F tâ†’s . For one pixel p t in I t , the corresponding pixel p s in I s can be found either through camera perspective projection or with the optical flow, and they should be consistent. Formally, denote the corresponding pixel in source image I s found by optical flow as p sf and the matching pixel found by rigid transform as p st , the computation can be written as,</p><formula xml:id="formula_1">h(p st ) = Ï€(K[T tâ†’s D t (p t )K âˆ’1 h(p t ) + M * d (p t )]), p sf = p t + F tâ†’s (p t ),<label>(1)</label></formula><p>where D t (p t ) is the depth value of the target view at pixel p t , and K is the camera intrinsic matrix, h(p t ) is the homogeneous coordinate of p t . Ï€(x) convert from the homogeneous coordinates to Cartesian coordinates, i.e. x/x d where d is the vector dimension.</p><p>Here, d = 3 and the last element is the projected depth value at p s from p t , which we represent it byD s (p s ). M * d is the 3D motion of dynamic moving objects in the target camera coordinate. In this way, every pixel in I t is explained geometrically. Here, p s can be outside of the image I s , or non-visible in I s when computing optical flow, which is also evaluated in optical flow estimation of KITTI dataset 1 .</p><p>Commonly, as proposed by previous works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>, one may design CNN models for predicting D t , T tâ†’s , F tâ†’s . After computed the corresponding p t and p s , We can synthesize the target imageÃŽ t from the source image and apply photometric loss as supervision:</p><formula xml:id="formula_2">L p = pt V(p t )|I t (p t ) âˆ’ÃŽ t (p t )|.<label>(2)</label></formula><p>Where V(p t ) is the visibility mask which is 1 when p t is also visible in I s , and 0 if p t is occluded or falls out of view. Such models can be trained end-to-end.</p><p>By only considering F tâ†’s in Eq. (1), and adding flow smoothness term yields unsupervised learning of optical flow <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>. On the other hand, dropping optical flow model, and assuming there is no dynamic motion in the scene, i.e. setting M * d = 0 in Eq. (1), yields unsupervised learning of depths and motions <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>In our case, to holistically model the 3D motion, we adopt CNN models for all the three components: optical flow, depths and motion. However, dynamic motion M d and depths D s/t are two conjugate pieces of information, where there always exists a motion pattern that can exactly compensate the error caused by inaccurate depth estimation. Considering matching p t and p s based on RGB could also be noisy, this yields an illposed problem with trivial solutions that prevent stable learning. Therefore, we need to design effective learning strategies with strong regularization to provide effective supervision for all those networks, which we will describe later. Holistic 3D motion parser (HMP). In order to make the learning process feasible, we first need to distinguish between the motion from rigid background/camera motion and dynamic moving 1. http://www.cvlibs.net/datasets/kitti/eval scene flow.php?benchmark= flow objects, regions of visible and occluded, where on visible rigid regions we can rely on structure-from-motion <ref type="bibr" target="#b9">[10]</ref> for training depths and on moving regions we can find 3D object motions. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, we handle this through a HMP that takes in the provided information from three networks, i.e. DepthNet, MotionNet and OptFlowNet, and outputs the desired dissected dense motion maps of background and moving things respectively.</p><p>Formally, given depths of both images D s and D t , the learned forward/backward optical flow F tâ†’s/sâ†’t , and the relative camera pose T tâ†’s , the motion induced by rigid background M b and dynamic moving objects M d from HMP are computed as,</p><formula xml:id="formula_3">M b (p t ) = T tâ†’s Ï†(p t |D t ) âˆ’ Ï†(p t |D t ), M d (p t ) = V(p t )[Ï†(p t + F tâ†’s (p t )|D s ) âˆ’ Ï†(p t |D t ) âˆ’ M b (p t )] V(p t ) = 1( ps (1 âˆ’ |p t âˆ’ (p s + F sâ†’t )|) &gt; 0), S(p t ) = 1 âˆ’ exp{âˆ’Î± s ( M d (p t ) 2 )} (3) where Ï†(p t |D t ) = D t (p t )K âˆ’1 h(p t ) is a back projection func- tion from 2D to 3D space. Note here, different from M * d (p t ) in Eq. (1), M d (p t )</formula><p>only consider the dynamic per-pixel 3D motion at visible regions, which is easier to compute. V is the visibility mask using the occlusion estimation from optical flow F sâ†’t as presented in <ref type="bibr" target="#b6">[7]</ref>. We refer readers to their original paper for further details of the intuition and implementations. S is a soft moving object mask, which indicates the confidence of a pixel that belongs to dynamic objects. Î± s is a scaling hyper-parameter.</p><p>Here, we may further simplify the representation of</p><formula xml:id="formula_4">M d (p t ) by substituting M b (p t ) in Eq. (3)</formula><p>, and put in the back projection function of Ï†() given the formula, i.e.</p><formula xml:id="formula_5">M d (p t ) = V(p t )[Ï†(p t + F tâ†’s (p t )|D s ) âˆ’ T tâ†’s Ï†(p t |D t )] = V(p t )[D s (p sf )K âˆ’1 h(p sf ) âˆ’D s (p st )K âˆ’1 h(p st )] = V(p t )K[D s (p sf )h(p sf ) âˆ’D s (p st )h(p st )],<label>(4)</label></formula><p>Here,D s is the depth map of source image I s projected from the depth of target image I t as mentioned in Eq. (1). This will be useful for our loss design in Eq. <ref type="bibr" target="#b6">(7)</ref>. After HMP, the rigid and dynamic 3D motions are disentangled from the whole 3D motion, and a moving object mask is estimated, where we could apply various supervision accordingly based on our structural error and regularization, which drives the joint learning of depth, motion and flow networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training the networks.</head><p>In this section, we will first introduce the networks for predicting and losses we designed for unsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Network architectures.</head><p>For depth prediction D and motion estimation between two consecutive frames T , we adopt the network architecture from Yang et al. <ref type="bibr" target="#b5">[6]</ref>, which depends on a VGG based encoder and double the input resolution of that used in Zhou et al. <ref type="bibr" target="#b9">[10]</ref>, i.e. 256 Ã— 832, to acquire better ability in capturing image details. In addition, for motion prediction, we drop the decoder for their explanability mask prediction since we can directly infer the occlusion mask and moving object masks through the HMP module to avoid error matching.</p><p>For optical flow prediction F, rather than using FlowNet [59] adopted in <ref type="bibr" target="#b6">[7]</ref>, we use a light-weighted network architecture, i.e. PWC-Net <ref type="bibr" target="#b60">[61]</ref>, to learn a robust matching, which is almost 10Ã— smaller than the FlowNet <ref type="bibr" target="#b58">[59]</ref>, while producing higher matching accuracy in our unsupervised setting.</p><p>We will describe the details of all these networks in our experimental section Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Training losses.</head><p>After HMP Eq. (3), the system generates various outputs, including: 1) depth map D from a single image I, 2) relative camera motion T, 3) optical flow map F, 4) rigid background 3D motion M b , 5) dynamic 3D motion M d , 6) visibility mask V, and 7) moving object mask S. Different loss terms are also used to effectively train corresponding networks as illustrated in pipeline shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Rigid-aware structural matching. As discussed in Sec. 2, photometric matching as proposed in Eq. (2) for training flows and depths is not robust against illumination variations. In this work, in order to better capture local structures, we add additional matching cost from SSIM <ref type="bibr" target="#b56">[57]</ref>, as applied in <ref type="bibr" target="#b8">[9]</ref>. In addition, Formally, our matching cost can be written as, i.e.</p><formula xml:id="formula_6">L vs (O) = pt V * (p t , O)s(I t (p t ),ÃŽ t (p t )), where, s(I t (p t ),ÃŽ t (p t )) = (1 âˆ’ Î²)|I t (p t ) âˆ’ÃŽ t (p t )|+ Î² 1 âˆ’ SSIM(I t (p t ),ÃŽ t (p t )) 2 .<label>(5)</label></formula><p>Here, Î² is a balancing hyper-parameter which is set to be 0.85.</p><p>O represents the type of input for obtaining the matching pixels, which could be D or F as introduced in Eq. (1). V * indicates visibility mask dependent on the type of source image for synthesis. Specifically, for supervising depth, we needs to find rigid and nonoccluded regions, and we let</p><formula xml:id="formula_7">V d (p t , F) = V(p t )(1âˆ’S(p t )). For supervising optical flow, we let V f (p t , F) = V(p t ) as Eq. (1).</formula><p>We denote view synthesis loss terms for depth and optical flow as L dvs , L f vs respectively (as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>). Then, we may directly apply these losses to learn the flow, depth and motion networks.</p><p>Edge-aware local smoothness. Although the structural loss alleviates the appearance confusion of view synthesis, the matching pattern is still a very local information. Therefore, smoothness is commonly adopted for further regularizing the local matching <ref type="bibr" target="#b78">[79]</ref> to improve the results. In our experiments, we tried two types of smoothness including edge-aware smoothness from image gradient proposed by Godard <ref type="bibr" target="#b8">[9]</ref>, or smoothness with learned affinity similar to Yang et al. <ref type="bibr" target="#b5">[6]</ref>. We find that when using only photometric matching, the learned affinity provides significant improvements for final results over image gradient, but when adding structural loss (Eq. <ref type="formula" target="#formula_6">(5)</ref>), the improvements from learned affinity become marginal. From our perspective, this is mostly due to the robustness of the SSIM loss and the self-regularization from CNN. Therefore, in this work, for simplicity, we only use image gradient based edge-aware smoothness to regularize the learning of different networks, which is the same as used in <ref type="bibr" target="#b8">[9]</ref>. Formally, the spatial smoothness loss can be written as,</p><formula xml:id="formula_8">L s (O) = pt |âˆ‡ 2 O(p t )|e âˆ’Î±e|âˆ‡ 2 I(pt)| ,<label>(6)</label></formula><p>where O represents type of input. Here, we use L ds and L f s to denote the smoothness loss terms for depth and optical flow respectively. Rigid-aware 3D motion consistency. Finally, we model the consistency between depths and flows in the rigid regions based on the outputs from our HMP. Specifically, we require M d (p t ) to be small inside the rigid background regions, which can be calculated by 1 âˆ’ S. Formally, the loss functions can be written as,</p><formula xml:id="formula_9">L dmc = pt (1 âˆ’ S(p t ))|M d (p t )|, â‡” pt (1 âˆ’ S(p t ))V(p t )|D s (p sf )h(p sf ) âˆ’D s (p st )h(p st )| (7) where M d , S(p t ) is defined in Eq. (3)</formula><p>, and â‡” indicates equivalent in terms of optimization based on Eq. (4).</p><p>In practice, we found the learning could be more stable by decomposing the 3D motion consistency to 2D flow consistency and depth consistency. We hypothesize this is because the estimated depths at long distance can be much more noisy than the regions nearby (similar to the cases in supervised depth estimation <ref type="bibr" target="#b38">[39]</ref>), which induce losses difficult to minimize for the networks. Therefore, decomposing 3D motions to 2D motions and depths alleviates such difficulties. Formally, we modify the optimization of original target to a new target by separately penalizing the difference over depths D s (p sf ) and flows h(p sf ) in L mc , i.e.,</p><formula xml:id="formula_10">L dmc = L dc + L mc (8) L dc = pt V(p t )(1 âˆ’ S(p t ))(|D s (p sf ) âˆ’D s (p st )| L mc = pt V(p t )(1 âˆ’ S(p t ))|p sf âˆ’ p st |),<label>(9)</label></formula><p>where |D s (p sf )âˆ’D s (p st )| indicates the depth consistency, which is similar to the one used in <ref type="bibr" target="#b13">[14]</ref>, and |p sf âˆ’ p st | indicates flow consistency inside rigid regions, which is similar to consistency check proposed in <ref type="bibr" target="#b17">[18]</ref>. However, we argue that these consistency are made to be more effective when combining with the masks estimated in our framework. Here, we can see that the optima of L dmc is also the optima for L mc , while the former is easier to optimize and is adopted in our training losses. Flow motion consistency in occluded regions. Commonly, optical flow estimation on benchmarks, e.g. KITTI 2015 <ref type="bibr" target="#b22">[23]</ref>, also requires flow estimation for pixels inside occlusion regions V, which is not possible when solely using 2D pixel matching. Traditionally, researchers <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref> use local smoothness to "inpaint" those pixels from nearby estimated flows. Thanks to our 3D understanding, we can train those flows by requiring its geometrical consistency with our estimated depth and motion. Formally, the loss for 2D flow consistency is written as,</p><formula xml:id="formula_11">L f c = pt (1 âˆ’ V(p t ))|p sf âˆ’ p st |,<label>(10)</label></formula><p>where p sf , p st are defined in Eq. (4). We use such a loss to drive the supervision of our OptFlowNet to predicting flows only at nonvisible regions, and surprisingly, it also benefits the flows predicted at visible regions, which we think it is because well modeling of the occluded pixels helps regularization of training. Nevertheless, one possible concern of our formula in 3D motion consistency is when the occluded part is from a non-rigid movement, e.g. a car moves behind another car. To handle this problem, it requires further dissecting object instance 3D motions, which we leave to our future work, and is beyond the scope of this paper. In the datasets we experimented such as KITTI 2015, the major part of occlusion is from rigid background, which falls into our assumption. Specifically, we use the ground truth optical flow maps and moving masks from the validation images, and found 95% of occluded pixels are in rigid background.</p><p>Multi-scale penalization. Finally, in order to incorporate multiscale context for training, following <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref>, we use four scales for the outputs of D and F. In summary, our loss functional for depths and optical flow supervision from a monocular video can be written as,</p><formula xml:id="formula_12">L mono = l Î» dvs L l vs (D l ) + Î» f vs L l vs (F l ) + 2 l Î» ds L l s (D l ) + 2 l Î» f s L l s (F l ) + Î» dc L l dc + Î» mc L l mc + Î» f c L l f c<label>(11)</label></formula><p>where l indicates the level of image scale, and l = 0 indicates the one with the highest resolution. 2 l is a weighting factor for balancing the losses between different scales. Î» = [Î» dvs , Î» f vs , Î» ds , Î» f s , Î» dc , Î» mc , Î» f c ] is the set of hyperparameters balancing different losses, and we elaborate them in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.3</head><p>Adaptive stage-wise learning strategy.</p><p>In practice, we observe that jointly training all networks from scratch doesn't generates reasonable outputs. One possible reason is that many coupled geometrical cues (including parameters of all three MotionNet, OptFlowNet, DepthNet) are randomly initialized and thus generate very noise outputs at the beginning. Multiple noisy outputs (e.g. S, M b , M d ) make the learning difficult to converge. Therefore, we adopt and alternative training by adaptively adjusting the hyper-parameters, i.e. Î» and Î± s , as the training goes on, which switches on or off the learning of networks for more efficient convergence and also to serve as a better initialization for jointly learning of all networks. Formally, we adopt a stage-wise learning strategy similar to <ref type="bibr" target="#b18">[19]</ref>, which trains the framework stage by stage and start the learning of later stages after previous stages are converged. The learning algorithm is presented in Alg. 1. First, we train DeptNet/MotionNet and OptFlowNet separately and there is no consistency enforced. Then, after independent learning, we reset Î± s to be a small constant 0.01 to require the consistency over corresponding regions of the estimated depth, camera motion and optical flow. In this stage, since the networks are continuously turning better, we alternatively optimize the depth and optical flow networks through iterative training. And we adaptively apply different masks (S, 1âˆ’S, V, 1âˆ’V, where 1âˆ’S is the inverted mask of S, indicating the confidence score for the static regions and 1 âˆ’ V indicates the non-visibility score. ) in each iteration for better training of depth or optical flow networks. In our experiments, the performance of all networks are saturated after two iterations in the alternative training stage, yielding SoTA performance for all the evaluated tasks, which we will elaborate in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Stereo to solve motion confusion.</head><p>As discussed in the introduction part (Sec. 1), the reconstruction of moving objects in monocular video has projective confusion, which is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. The depth map in (b) is an example predicted with our algorithm trained with monocular samples, where the car in the front is running at the same speed and the region is estimated to be far. This is because when the depth value is estimated large, the car will stay at the same place in the warped image, yielding small photometric errors during training, as also observed in <ref type="bibr" target="#b21">[22]</ref>. Obviously, the losses of motion or smoothness Eq. (11) does not solve this issue. Therefore, we have added stereo images (which are captured at the same time but from different view points) into learning the depth network to avoid such confusion jointly with monocular videos. As shown in <ref type="figure" target="#fig_2">Fig. 3 (c)</ref>, the framework trained with stereo pairs correctly figures out the depth of the moving object regions.</p><p>Formally, when corresponding stereo image I c is additionally available for the target image I t , we treat I c as another source image, similar to I s , but with known camera pose T tâ†’c . In this case, since there is no motion factor (stereo pairs are simultaneously captured), we adopt the same loss of L s and L vs taking I c , I t as inputs for supervising the depth network. Formally, the total loss for DepthNet when having stereo images is,</p><formula xml:id="formula_13">L monostereo = L mono + l {Î» cvs L l vs (I c ) + Î» cs L l s (I c )}.<label>(12)</label></formula><p>where L(I c ) and L biâˆ’vs (I c ) indicate the corresponding losses with a visibility mask computed using stereo image I c . Here, we update steps of learning depth and motion networks in Alg. 1 by adding the loss from stereo pair with Î» cvs = 4 and Î» cs = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we firstly describe the datasets and evaluation metrics used in our experiments, and then present comprehensive evaluation of EPC++ on different tasks.</p><p>Input conv deconv Output </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>EPC++ consists of three sub-networks: DepthNet, OptFlowNet and MotionNet as described in Sec. 3. Our HMP module has no learnable parameters, thus does not increase the model size. In the following, we clarify the network architectures, corresponding training procedure and setting of hyper-parameters. Training DepthNet. We modify the DispNet <ref type="bibr" target="#b69">[70]</ref> architecture for DepthNet as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. Our dispNet is based on an encoder-decoder design with skip connections and multi-scale side outputs. The encoder consists of 14 convolutional layers with kernel size of 3 except for the first 4 conv layers with kernel size of 7, 7, 5, 5 respectively. The decoder has symmetrical architecture as the encoder, consisting of 7 conv layers and 7 deconv layers. To capture more details, both input and output scales of the DepthNet are set as 256 Ã— 832, which is twice as large as used in <ref type="bibr" target="#b9">[10]</ref>. All conv layers are followed by ReLU activation except the output layer where we apply a sigmoid activation to constrain the depth prediction within a reasonable range. In practice, output disparity range is constrained within 0-0.3. Batch normalization (BN) <ref type="bibr" target="#b79">[80]</ref> is performed on all conv layers when training with stereo videos, and is dropped when training with monocular videos for better stability and performance. This is because BN helps to reduce the scale variation between monocular and stereo images. Last, for stereo training, following <ref type="bibr" target="#b8">[9]</ref>, we ask the DepthNet to output the disparity maps of both the left and the right images for computing left-right consistency.</p><p>For training, the Adam optimizer <ref type="bibr" target="#b80">[81]</ref> is applied with Î² 1 = 0.9, Î² 2 = 0.999, learning rate of 2 Ã— 10 âˆ’4 and batch size of 4. In independent training stage, the loss balance for DepthNet are set as Î» dvs = 1.0, Î» ds = 1.0 in Eq. (11) as shown in Alg. 1 respectively. The model is trained for 180K iterations. In alternative training stage, we decrease the learning rate to 2 Ã— 10 âˆ’5 and train the network for about 10K iterations.</p><p>For training using stereo videos, the DepthNet is trained with Î» ds = 1.0, Î» dvs = 2.5 in Eq. (12), for 145,000 iterations in independent training stage and hyper-parameters and training iterations are set to be the same. Training MotionNet. The MotionNet architecture is the same as the Pose CNN in <ref type="bibr" target="#b9">[10]</ref> which outputs a 6-dimensional vector for camera motion. The training process is identical to the DepthNet since they need to be trained together. Training OptFlowNet. We use PWC-Net <ref type="bibr" target="#b60">[61]</ref> as our Opt-FlowNet. PWC-Net is based on an encoder-decoder design with intermediate layers warping CNN features for reconstruction. The batch size is set as 4 and we set Î» f vs = 1.0, Î» f s = 1. During independent training stage, the network is optimized with Adam optimizer <ref type="bibr" target="#b80">[81]</ref> with Î² 1 = 0.9, Î² 2 = 0.999, learning rate of 2 Ã— 10 âˆ’4 for 100,000 iterations from scratch. During adaptive training stages, the learning rate is decreased to 2 Ã— 10 âˆ’5 , and the network is trained with about 30K iterations. Hyper parameters. We set Î² = 0.85 in Eq. <ref type="bibr" target="#b4">(5)</ref> and Î± e = 10 in Eq. (6) following <ref type="bibr" target="#b6">[7]</ref>. For Î± s in Eq. (3), we validate it through a validation set using depth metrics within a set of {0.01, 0.05, 0.1, 0.5}. In general, there is no significant difference for the final performance, i.e.ranging from 0.144 to 0.146 for the relative absolute error. Here we pick the best one, i.e.Î± s = 0.01 as shown in Alg 1. For Î» dc , Î» mc and Î» f c for depth-flow consistency, we use similar strategy as tuning Î± s , and set to be 0.05, 0.25 and 0.005 correspondingly during the adaptive training stage for monocular setting as illustrated in Alg 1. As for stereo setting, we set Î» f c to be 0.02 with other hyper parameters remain the same. EPC++ has 38.3M parameters in total, in which DepthNet and MotionNet have 33.2M parameters and OptFlowNet has 5.1M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets and metrics</head><p>Extensive experiments were conducted on five tasks to validate the effectiveness of EPC++ in different aspects. These tasks include: depth estimation, optical flow estimation, 3D scene flow estimation, odometry and moving object segmentation. All the results are evaluated on the KITTI dataset using the corresponding standard metrics commonly used by other SoTA methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>.  </p><formula xml:id="formula_14">1 |D| d âˆˆD ||d * âˆ’d || 2 RMSE log: 1 |D| d âˆˆD ||logd * âˆ’logd || 2 Î´t: % of d âˆˆ D max( d * d , d d * ) &lt; t EPE: 1 |F | f âˆˆF f * âˆ’ f 2 F1: err &gt; 3px and err &gt; |f * | Ã— 5% ATE, terr: 1 |T | t âˆˆT t * âˆ’ t 2 rerr: 1 |T | [arccos T r(R )âˆ’1 2 âˆ’ arccos T r(R)âˆ’1 2 ] D1, D2: 1 |D| d âˆˆD |d * âˆ’d | FL: 1 |F | f âˆˆF |f * âˆ’f | pixel acc: i n ii i t i mean acc: 1 n cl i n ii t i mean IoU: 1 n cl i n ii t i + j n ji âˆ’n ii f.w. IoU: 1 k t k i t i n ii t i + j n ji âˆ’n ii</formula><p>For depth evaluation, we chose the Eigen split <ref type="bibr" target="#b11">[12]</ref> for experiments to compare with more baseline methods. The Eigen test split consists of 697 images, where the depth ground truth is obtained by projecting the Velodyne laser scanned points into the image plane. To evaluate at input image resolution, we rescale the depth predictions by bilinear interpolation. The sequence length is set to be 3 during training. For optical flow evaluation, we report performance numbers on both training and test splits of KITTI 2012 and KITTI 2015 datasets and compare with other unsupervised methods. Both training and test set contain 200 image pairs. Ground truth optical flow for training split is provided and the ground truth for test split is withheld on the official evaluation server.</p><p>For scene flow and segmentation evaluation, we evaluate on the KITTI 2015 training split, containing 200 image pairs. The scene flow ground truth is publicly available and the moving object ground truth is only provided for this split. KITTI 2015 dataset also provides an odometry data split, consisting of 9 training sequences (Seq. 00-08) and 2 test sequences (Seq. 09, Seq. 10). On average, there are 2,200 frames in one training sequence, resulting in over 20,000 training samples. Seq.09 and Seq. 10 contain about 1,200 and 1,500 frames respectively.</p><p>Make3D. Make3D dataset <ref type="bibr" target="#b23">[24]</ref> contains no videos but 534 monocular image and depth ground truth pairs. Unstructured outdoor scenes, including bush, trees, residential buildings, etc. are captured in this dataset. Same as in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b81">[82]</ref>, the evaluation is performed on the test set of 134 images. MPI-Sintel. MPI-Sintel dataset <ref type="bibr" target="#b24">[25]</ref> is obtained from an animated movie which pays special attention to realistic image effects. It contains multiple sequences including large/rapid motions. We use the "final" pass of the data to train and test our model, which consists of 1,000 image pairs. Metrics. The existing metrics of depth, optical flow, odometry, segmentation and scene flow were used for evaluation, as in previous methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b82">[83]</ref>. For depth and odometry evaluation, we adopt the code from <ref type="bibr" target="#b9">[10]</ref>. For optical flow and scene flow evaluation, we use the official toolkit provided by <ref type="bibr" target="#b3">[4]</ref>.</p><p>For foreground segmentation evaluation, we use the overall/per-class pixel accuracy and mean/frequency weighted (f.w.) IOU for binary segmentation. The definition of each metric used in our evaluation is specified in Tab. 1, in which, x * and x are ground truth and estimated results (x âˆˆ {d, f, t}). n ij is the number of pixels of class i segmented into class j. t j is the total number of pixels in class h. n cl is the total number of classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Depth evaluation</head><p>Experiment setup. The depth experiments are conducted on KITTI Eigen split <ref type="bibr" target="#b11">[12]</ref> to evaluate the performance of EPC++ and its variants. The depth ground truths are sparse maps as they come from the projected Velodyne Lidar points. Only pixels with ground truth depth values (valid Velodyne projected points) are evaluated. For monocular model, following <ref type="bibr" target="#b9">[10]</ref>, we scale the predicted depth to match the median with the groundtruth. For stereo model, we use the given intrinsic and baseline to compute the depth from estimated disparity, which is the same as <ref type="bibr" target="#b8">[9]</ref> The following evaluations are performed to present the depth performances: (1) ablation study of our approach and (2) depth performance comparison with the SoTA methods. Ablation study. We explore the effectiveness of each component of EPC++ as presented in Tab. 2. Several variant results are generated for evaluation, including:</p><p>(1) EPC++ (mono depth only): DepthNet trained with view synthesis and smoothness loss (L dvs + L ds ) on monocular sequences without visibility masks, which is already better than many SoTA methods. This is majorly due to the SSIM introduced in structural matching <ref type="bibr" target="#b8">[9]</ref>, our modified DepthNet with higher resolution inputs <ref type="bibr" target="#b5">[6]</ref>, and depth normalization as introduced in <ref type="bibr" target="#b81">[82]</ref>.</p><p>(2) EPC++ (mono depth consist): Fine-tune the trained Depth-Net with a depth consistency term as formulated with L dc = |D s (p sf ) âˆ’D s (p st )| term, which is a part of Eq. (9); we show it benefits the depth learning.</p><p>(3) EPC++ (mono joint w/ flow consist): Fine-tune the whole system with depth/flow consistency (see Eq. <ref type="formula" target="#formula_10">(9)</ref>). As we can see, joint training results in worse depth performance and also worse flow performance (see optical flow evaluation). (4) EPC++ (mono flow consist): DepthNet trained by adding flow consistency in Eq. (9), where we drop the visibility mask. We can see that the performance is worse than adding depth consistency alone since flow at non-visible parts harms the matching. It is notable that for monocular training, the left and right view frames are considered independently and thus the frameworks trained with either monocular or stereo samples leverage the same amount training data. As shown in Tab. 2, our approach (EPC++) trained with both stereo and sequential samples shows large performance boost over using only one type of training samples, proving the effectiveness of incorporating stereo into the training. With fine-tuning from HMP, comparing results of EPC++ (stereo) and EPC++ (stereo depth consist), the performance is further improved. Comparison with state-of-the-art. Following the tradition of other methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, the same crop as in <ref type="bibr" target="#b11">[12]</ref> is applied during evaluation on Eigen split. We conducted a comprehensive comparison with SoTA methods that take both monocular and stereo samples for training.</p><p>Tab. 2 shows the comparison of EPC++ and recent SoTA methods. Our approach outperforms current SoTA unsupervised methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b83">[84]</ref> on all metrics by a large margin. It is worth noting that (1) EPC++ trained with only monocular samples already outperforms <ref type="bibr" target="#b8">[9]</ref> which takes stereo pairs as input; (2) on the metrics "Sq Rel" and "RMSE", there is a large performance Thanks to the extra supervision from optical flow, our monocular results preserve the details of the occluded/de-occluded regions better, e.g. the structure of thin poles. Please note the "large depth value confusion" still happens for both monocular based methods (green circle).</p><p>boost after applying the depth-flow consistency, comparing the row "EPC++ (depth only)" and "EPC++ (mono depth consist)". The two metrics measures the square of depth prediction error, and thus are sensitive to points where the depth predictions are further away from the ground truth. Applying the depth-flow consistency eliminates some "outlier" depth predictions. When we further add stereo images for training, EPC++ achieves larger performance boost compared with its monocular counter-part. Our observation on this is that without the scale ambiguity issue in monocular training, EPC++ trained with stereo pairs benefits more from the modeling of motion segmentation and occlusions. Qualitative results of EPC++ (stereo) and EPC++ (mono) are presented in <ref type="figure" target="#fig_5">Fig.  5</ref> and <ref type="figure">Fig. 6</ref> respectively. Compared to other SoA results from <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, Our depth results preserve the details of the scene noticeably better (white circles). The green circle show in <ref type="figure">Fig. 6</ref> visualizes the motion confusion discussed in Sec. 3.3.  <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization to non-driving scenes</head><p>We also finetune our model on the MPI-Sintel Final dataset <ref type="bibr" target="#b24">[25]</ref>. As only the ground truth of training split is publicly available, we report the depth evaluation results on the training set, following the traditions of optical flow evaluation on MPI-Sintel <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b62">[63]</ref>. Two variants of EPC++ are compared: EPC++ (mono depth only) and EPC++ (mono). An improvement from 0.866 to 0.524 in AbsRel, and from 25.558 to 5.3206 in SqRel is observed when we add depth-flow consistency, S, V and adaptive training of the framework. Qualitative results are shown in upper part of <ref type="figure" target="#fig_8">Fig. 9</ref>. Please note that these results are generated by pre-trained the full EPC++ (mono) model on KITTI 2015 and finetuned on MPI-Sintel "final" pass, which only contains 1,000 frame pairs. (2) Joint training with depth: OptFlowNet is finetuned jointly with DepthNet after individually trained using L dmc . We can see that the results are worse than training with flow alone; this is because the flows from depth at rigid regions, i.e. p st in Eq. <ref type="formula" target="#formula_10">(9)</ref>, are not as accurate as those from learning OptFlowNet alone. In other words, factorized depth and camera motion in the system can introduce extra noise to 2D optical flow estimation (from 3.66 to 4.00). But we notice that the results on occluded/non-visible regions are slightly better (from 23.07 to 22.96).</p><p>(3) EPC++ all region: We fix DepthNet, but finetune OptFlowNet without using the visibility mask V. We can see the flows at rigid regions are even worse for the same reason as above, while the results at the occluded region becomes much better (from 23.07 to <ref type="bibr">16.20)</ref>. Results from variants (1)-(5) validate our assumption that the rigid flow from depth and camera motion helps the optical flow learning at the non-visible/occluded region. Comparing EPC++ vis-rigid and EPC++ non-vis for both stereo and monocular training setups, there is a large performance boost for both optical flow in occlusion regions (occ) and overall regions (all). This proves that explicitly modeling occlusion and motion mask benefits the optical flow estimation a lot. , <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b62">[63]</ref>) on "F1-bg" and "F1-all" metrics. Please note that Multi-  We compared with current unsupervised SoTA method <ref type="bibr" target="#b6">[7]</ref>. Optical flow results generated by EPC++ align better with the ground truth, especially on object boundaries (occlusion regions).</p><p>frame <ref type="bibr" target="#b85">[86]</ref> reports better performance on "F1-fg", but this method takes three frames as input to estimate the optical flow while our method only takes two. Although longer input sequence gives better estimation the movement of foreground objects, our results at full regions are still better. EPC++ (stereo) shows a further performance boost compared to monocular counterpart. This is as expected as better depth estimation provides better guidance for optical flow training.</p><p>Comparing our method with DF-Net <ref type="bibr" target="#b19">[20]</ref>, whose optical flow network is firstly pre-trained on additional SYNTHIA dataset <ref type="bibr" target="#b86">[87]</ref> and then jointly trained on KITTI dataset, we have achieved a large performance gain on both KITTI 2012 and KITTI 2015 datasets. Contrast to DF-Net, our method also models the pixels in occlusion regions and adopt an adaptive training to leverage depth information to help optical flow learning. Qualitative results are shown in <ref type="figure" target="#fig_7">Fig.8</ref>, and ours have better sharpness and smoothness of the optical flow.</p><p>Generalization to MPI-Sintel Dataset. MPI-Sintel <ref type="bibr" target="#b24">[25]</ref> is a synthetic benchmark used for optical flow evaluation. It provides very different scenes compared with KITTI. To better compare with previous works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b62">[63]</ref>, we have adopted two training setups: (1) test our model (which is trained on KITTI 2015)  <ref type="table" target="#tab_8">Table 6</ref>, we can see that the results are consistent with the ablation study on the KITTI dataset, whether tested directly or fine-tune on the MPI-Sintel dataset. This shows that our proposed training schedule can also generalize well to other scenarios. The qualitative results are shown in bottom part of <ref type="figure" target="#fig_8">Fig. 9</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Odometry estimation.</head><p>To evaluate the performance of our trained MotionNet, we use the odometry metrics as in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The same protocol as in <ref type="bibr" target="#b9">[10]</ref> is applied in our evaluation, which measures the absolute trajectory error averaged every five consecutive frames. Unlike the settings in previous works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref> which train a MotionNet using stacked five frames (as described in Sec. 3), no modifications have been made to the MotionNet, which still takes three frames as input and retrain our networks on KITTI odometry train split. We compare with several unsupervised SoTA methods on two sequences of KITTI odometry test split. To explore variants of our model, we experimented learning with monocular samples (EPC++ (mono)) and with stereo pairs (EPC++ (stereo)). As shown in <ref type="table" target="#tab_9">Table 7</ref>, our trained MotionNet shows superior performance with respect to visual SLAM methods (ORB-SLAM), and is comparable to other unsupervised learning methods with slight improvement on two test sequences. The more accurate depth and optical flow estimation from our DepthNet helps constraint the output of MotionNet, yielding better odometry results. The qualitative odometry results are shown in <ref type="figure">Fig. 10</ref>. Compared to results from SfMLearner <ref type="bibr" target="#b9">[10]</ref> or GeoNet <ref type="bibr" target="#b17">[18]</ref>, which have large offset at the end of the sequence, results from EPC++are more robust to large motion changes and closer to the ground truth </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Seq.09 Seq.10 ORB-SLAM (full) <ref type="bibr" target="#b26">[27]</ref> 0.014 Â± 0.008 0.012 Â± 0.011 ORB-SLAM (short) <ref type="bibr" target="#b26">[27]</ref> 0.064 Â± 0.141 0.064 Â± 0.130 Zhou et al. <ref type="bibr" target="#b9">[10]</ref> 0.021 Â± 0.017 0.020 Â± 0.015 DF-Net <ref type="bibr" target="#b19">[20]</ref> 0.017 Â± 0.007 0.015 Â± 0.009 Mahjourianet al. <ref type="bibr" target="#b53">[54]</ref> 0.013 Â± 0.010 0.012 Â± 0.011 GeoNet <ref type="bibr" target="#b17">[18]</ref> 0.012 Â±0.007 0.012Â±0.009 EPC++(mono) 0.013 Â± 0.007 0.012 Â± 0.008 EPC++(stereo) 0.012 Â± 0.006 0.012 Â± 0.008 trajectories.</p><p>The small quantitative performance gap leads to large qualitative performance difference because the metric only evaluates 5-frame relative errors and always assume the first frame prediction to be ground truth; thus the errors can add up in the long test sequence while the existing metrics do not take it into consideration. To better compare the odometry performance over the complete sequence, we adopted the evaluation metrics as proposed in <ref type="bibr" target="#b52">[53]</ref>. This metric evaluates the average translational and rotational errors over the full sequence and the quantitative results are shown in Tab. 8. As these metrics evaluate over the full sequence, the quantitative numbers align well with the qualitative results in <ref type="figure">Fig. 10</ref>. In summary, by applying the same MotionNet architecture as in EPC++ pipeline on KITTI odometry split, we have achieved SoTA performance on standard evaluation metric. On a metric which focuses on the long-term odometry accuracy, EPC++ outperforms the previous works by a large margin. As the ego-motion is coupled and jointly trained with depth and optical flow, the performance boost of depth and optical flow help regularize the learning of odometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Moving object segmentation</head><p>Ideally, the residual between the dynamic scene flow M d and the background scene flow M b represents the motion of foreground object. As the HMP (Eq. (3)) is capable of decomposing the foreground and background motion by leveraging the depth-flow consistency, we test the effectiveness of this decomposition by evaluating the foreground object segmentation. Experiment setup. The moving object segmentation is evaluated on the training split of the KITTI 2015 dataset. An "Object map" is provided in this dataset to distinguish the foreground and background in flow evaluation. We use this motion mask as ground truth in our segmentation evaluation. <ref type="figure" target="#fig_10">Fig. 11</ref> (second column) shows some visualizations of the segmentation ground truths. Our foreground segmentation estimation is generated by subtracting the rigid optical flow from optical flow, as indicated by S in Eq. (3). We set a threshold on M d &gt; 3 to generate a binary segmentation mask. Evaluation results. The quantitative and qualitative results are presented in Tab. 9 and <ref type="figure" target="#fig_10">Fig. 11</ref> respectively. We compare with two previous methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> that takes the non-rigid scene into consideration. Yang et al. <ref type="bibr" target="#b7">[8]</ref> explicitly models the moving object mask, and thus is directly comparable. The "explainability mask" in <ref type="bibr" target="#b9">[10]</ref> is designed to deal with both moving objects and occlusion, and here we list their performances for a more comprehensive comparison. Our generated foreground segmentation outperforms the previous methods on all metrics, and the visualization shows the motion mask aligns well with the moving object. It is worth noting that monocular EPC++, despite scale ambituity issue, already performs comparable to EPC <ref type="bibr" target="#b7">[8]</ref>, which is trained with stereo pairs. This proves the effectiveness of the modeling of segmentation mask in our pipeline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Scene flow evaluation</head><p>Experiment setup. The scene flow evaluation is performed on training split of KITTI 2015 dataset. There are 200 frames pairs (frames for t and t + 1) in the scene flow training split. The depth ground truth of the two consecutive frames and the 2D optical flow ground truth from frame t to frame t + 1 are provided. The evaluation of scene flow is performed with the KITTI benchmark evaluation toolkit 2 . As the unsupervised monocular method generates depth/disparity without absolute scale, we rescale the estimated depth by matching the median to ground truth depth for each image. Since no unsupervised methods have reported scene flow performances on KITTI 2015 dataset, we compare our model trained on monocular sequences (EPC++ (mono)) and stereo pairs (EPC++ (stereo)) with the previous results reported in <ref type="bibr" target="#b7">[8]</ref>. As shown in Tab. 10, our scene flow performance outperforms the previous SoTA method <ref type="bibr" target="#b7">[8]</ref>. Although it is not completely fair comparison, the performances of OSF <ref type="bibr" target="#b3">[4]</ref> and ISF <ref type="bibr" target="#b66">[67]</ref> are also presented in the results table. Both methods are supervised and the performances are generated by training the model on part of the training set and evaluated on the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we presented an end-to-end unsupervised learning framework, which we call every pixel counts ++ (EPC++), for jointly estimating depth, camera motion, optical flow and moving object segmentation masks. It successfully leverages the benefits of different tasks by exploiting their geometric consistency. In our framework, we proposed and adopted a depth, egomotion and optical flow consistency with explicit awareness of both motion rigidity and visibility. Thus every pixel can be explained by either rigid motion, non-rigid/object motion or occluded/non-visible regions. We proposed an adaptive training strategy to better leverage the different advantages of depth or optical flow and showed better performance than directly applying uniform across-task consistency.</p><p>We conducted comprehensive experiments to evaluate the performance of EPC++ over different datasets, and showed SoTA performance on both driving scenes (KITTI) and non-driving scenes (Make3D, MPI-Sintel) over all the related tasks. This demonstrates the effectiveness and also good generalization capability of the proposed framework. In the future, we hope to apply EPC++ to other motion videos containing deformable and articulated nonrigid objects such as the ones from MoSeg <ref type="bibr" target="#b76">[77]</ref> etc., and extend EPC++ to multiple object segmentation, which provides object part and motion understanding in an unsupervised manner. <ref type="bibr" target="#b1">2</ref>. http://www.cvlibs.net/datasets/kitti/eval scene flow.php</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The upper part of this figure shows the pipeline of our framework (EPC++). Given a a pair of consecutive frames, i.e.target image It and source image Is, the OptFlowNet is used to predict optical flow F from It to Is. The MotionNet predicts their relative camera pose Ttâ†’s. The DepthNet estimates the depth Dt from single frame. All three informations are fed into the Holistic 3D Motion Parser (HMP), which produce an segmentation mask for moving object S, occlusion mask V, 3D motion maps for rigid background M b and dynamic objects M d . The bottom part of the figure shows how different loss terms are generated from geometrical cues. Details are shown in Sec. 3.2.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Two examples of large depth confusion. Moving object in two consecutive frames (a) causes large depth value confusion for our system trained with monocular videos, as shown in (b). This issue can be resolved by incorporating stereo training samples into the system (c).Result: Trained networks for predicting D, T, and F Input : An unlabelled monocular video Define Î» = [Î» dvs , Î» f vs , Î» ds , Î» f s , Î» dc , Î» mc , Î» f c ] as loss balancing parameters in Eq. (11). Set Î± s = 0 for moving mask computation in Eq. (3). â€¢Train Depth and Motion networks till convergence with Î» = [1, 0, 1, 0, 0, 0, 0]. â€¢Train Optical flow network till convergence with Î» = [0, 1, 0, 1, 0, 0, 0]. â€¢Re-set Î± s = 0.01 for moving mask computation. while do â€¢Train Depth and Motion networks guided by optical flow till convergence with Î» = [1, 0, 1, 0, 0.05, 0.25, 0]. â€¢Train Optical flow network guided by depth flow till convergence with Î» = [0, 1, 0, 1, 0, 0, 0.005]. end Algorithm 1: Training EPC++ with monocular videos over the KITTI 2015 dataset. At each step we train the network until convergence (details in Sec. 4.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>The architecture of DepthNet. Each rectangle represents one certain layer as color coded in the legend. Number on top of the rectangles indicates the channel size of each layer (rectangle), e.g. 32 in the left indicates both convolutional layers have 32 channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>KITTI 2015. The KITTI 2015 dataset provides videos in 200 street scenes captured by stereo RGB cameras, with sparse depth ground truths captured by Velodyne laser scanner. 2D flow and 3D scene flow ground truth are generated from the ICP registration of the point cloud projection. The moving object mask is provided as a binary map to distinguish between static background and moving foreground in flow evaluation. During training, 156 stereo videos that exclude test and validation scenes are used. The monocular training sequences are constructed with three consecutive frames; left and right views are processed independently. This leads to 40,250 monocular training sequences. Stereo training pairs are constructed with left and right frame pairs, resulting in a total of 22,000 training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 5 )</head><label>5</label><figDesc>EPC++ (mono vis flow consist): DepthNet trained with depth and flow consistency as in Eq. (9), but add the computation of visibility mask V; this further improves the results. (6) EPC++ (mono): Final results from DepthNet with two iterations of adaptive depth-flow consistency training, yielding the best performance among all monocular trained methods. We also explore the use of stereo training samples in our framework, and report performances of two variants: (6) EPC (stereo depth only): DepthNet trained on stereo pairs with only L dvs + L ds . (7) EPC++ (stereo depth consist): DepthNet trained on stereo pairs with depth consistency. (8) EPC++ (stereo): Our full model trained with stereo samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 4 )</head><label>4</label><figDesc>EPC++ vis-rigid region: We fix DepthNet, and finetune Opt-FlowNet at the pixels of the visible and rigid regions, where the effect of improving at occluded region is marginal. (5) EPC++ non-vis region: We only finetune OptFlowNet with L f c and it yields improved results at all the regions of optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Visualization of optical flow results on KITTI 2015 training set images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Visualization of depth and optical flow estimation results on MPI-Sintel dataset using our model fine-tuned on MPI-Sintel. From top to bottom: input image, depth estimation result, depth ground truth, optical flow estimation result, optical flow ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>10 Fig. 10 :</head><label>1010</label><figDesc>Odometry estimation results on four sequences of KITTI 2015 dataset. The two left figures (a) and (b) are results on training sequences and the right two results on test sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 :</head><label>11</label><figDesc>Moving object segmentation results on KITTI training split. The ground truth masks are shown in blue and the red ones are our predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Evaluation metrics for our tasks. From top row to bottom row: depth, optical flow, odometry, scene flow and segmentation. |D| d âˆˆD |d * âˆ’d |/d * Sq Rel: 1 |D| d âˆˆD ||d * âˆ’d || 2 /d</figDesc><table><row><cell>Abs Rel: 1</cell></row></table><note>* RMSE:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Single view depth estimation results on the Eigen test split. Methods trained with monocular samples are presented in the upper part and those also taking stereo pairs for training are presented in the bottom part. All results are generated by models trained on KITTI data only unless specially noted. Details are in Sec. 4.3. Abs Rel Sq Rel RMSE RMSE log Î´&lt;1.25 Î´&lt;1.25 2 Î´&lt;1.25 3</figDesc><table><row><cell>Method</cell><cell>Stereo</cell><cell cols="2">Lower the better</cell><cell></cell><cell>Higher the better</cell><cell></cell></row><row><cell>Train mean</cell><cell>0.403</cell><cell>5.530</cell><cell>8.709 0.403</cell><cell>0.593</cell><cell>0.776</cell><cell>0.878</cell></row><row><cell>SfMLearner [10]</cell><cell>0.208</cell><cell>1.768</cell><cell>6.856 0.283</cell><cell>0.678</cell><cell>0.885</cell><cell>0.957</cell></row><row><cell>LEGO [6]</cell><cell>0.162</cell><cell>1.352</cell><cell>6.276 0.252</cell><cell>0.783</cell><cell>0.921</cell><cell>0.969</cell></row><row><cell>Mahjourian et al. [54]</cell><cell>0.163</cell><cell>1.240</cell><cell>6.220 0.250</cell><cell>0.762</cell><cell>0.916</cell><cell>0.968</cell></row><row><cell>DDVO [82]</cell><cell>0.151</cell><cell>1.257</cell><cell>5.583 0.228</cell><cell>0.810</cell><cell>0.936</cell><cell>0.974</cell></row><row><cell>GeoNet-ResNet(update) [18]</cell><cell>0.149</cell><cell>1.060</cell><cell>5.567 0.226</cell><cell>0.796</cell><cell>0.935</cell><cell>0.975</cell></row><row><cell>Competitive-Collaboration [19]</cell><cell>0.148</cell><cell>1.149</cell><cell>5.464 0.226</cell><cell>0.815</cell><cell>0.935</cell><cell>0.973</cell></row><row><cell>DF-Net [20] (ResNet-50)</cell><cell>0.145</cell><cell>1.290</cell><cell>5.612 0.219</cell><cell>0.811</cell><cell>0.939</cell><cell>0.977</cell></row><row><cell>EPC++ (mono depth only)</cell><cell>0.151</cell><cell>1.448</cell><cell>5.927 0.233</cell><cell>0.809</cell><cell>0.933</cell><cell>0.971</cell></row><row><cell>EPC++ (mono depth consist)</cell><cell>0.146</cell><cell>1.065</cell><cell>5.405 0.220</cell><cell>0.812</cell><cell>0.939</cell><cell>0.975</cell></row><row><cell>EPC++ (mono joint w/ flow)</cell><cell>0.156</cell><cell>1.075</cell><cell>5.711 0.229</cell><cell>0.783</cell><cell>0.931</cell><cell>0.974</cell></row><row><cell>EPC++ (mono flow consist)</cell><cell>0.148</cell><cell>1.034</cell><cell>5.546 0.223</cell><cell>0.802</cell><cell>0.938</cell><cell>0.975</cell></row><row><cell>EPC++ (mono vis flow consist)</cell><cell>0.144</cell><cell>1.042</cell><cell>5.358 0.218</cell><cell>0.813</cell><cell>0.941</cell><cell>0.976</cell></row><row><cell>EPC++ (mono)</cell><cell>0.141</cell><cell>1.029</cell><cell>5.350 0.216</cell><cell>0.816</cell><cell>0.941</cell><cell>0.976</cell></row><row><cell>UnDeepVO [53]</cell><cell>0.183</cell><cell>1.730</cell><cell>6.570 0.268</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Godard et al. [9]</cell><cell>0.148</cell><cell>1.344</cell><cell>5.927 0.247</cell><cell>0.803</cell><cell>0.922</cell><cell>0.964</cell></row><row><cell>EPC [8]</cell><cell>0.127</cell><cell>1.239</cell><cell>6.247 0.214</cell><cell>0.847</cell><cell>0.926</cell><cell>0.969</cell></row><row><cell>EPC++ (stereo depth only)</cell><cell>0.141</cell><cell>1.224</cell><cell>5.548 0.229</cell><cell>0.811</cell><cell>0.934</cell><cell>0.972</cell></row><row><cell>EPC++ (stereo depth consist)</cell><cell>0.134</cell><cell>1.063</cell><cell>5.353 0.218</cell><cell>0.826</cell><cell>0.941</cell><cell>0.975</cell></row><row><cell>EPC++ (stereo)</cell><cell>0.127</cell><cell>0.936</cell><cell>5.008 0.209</cell><cell>0.841</cell><cell>0.946</cell><cell>0.979</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Visual comparison between Godard et al.<ref type="bibr" target="#b8">[9]</ref> and EPC++ (stereo) results on KITTI frames. The depth ground truths are interpolated and all images are reshaped for better visualization. For depths, our results have preserved the details of objects noticeably better (as in white circles).</figDesc><table><row><cell>Input image</cell><cell>Depth GT</cell><cell>EPC++ (stereo)</cell><cell>Godard et al. [9]</cell></row><row><cell>Fig. 5: Input image</cell><cell>Depth GT</cell><cell>EPC++ (mono)</cell><cell>LEGO [6]</cell></row><row><cell cols="3">Fig. 6: Visual comparison between LEGO [6] and EPC++ (mono) results on KITTI test frames.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Fig. 7: Qualitative depth estimation results on Make3D. The results are generated by applying EPC++, which is trained on KITTI dataset, on Make3D test images. From top to bottom: input test image, our depth estimation result, depth ground truth.</figDesc><table><row><cell>outperforming other methods ( [9], [10]) trained with Cityscapes</cell></row><row><cell>and KITTI datasets combined, demonstrating the effectiveness</cell></row><row><cell>of our generalization capability. Qualitative results on Make3D</cell></row><row><cell>dataset are shown in</cell></row></table><note>To evaluate the general- ization ability of our model, first, we directly apply our model trained only on the KITTI dataset to the Make3D dataset ( [24], [85]), which is unseen during the training time. The comparison with other unsupervised methods is presented in Tab. 3. EPC++ trained with KITTI generalizes well to dataset with unseen scenes,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 :</head><label>3</label><figDesc>Generalization to Make3D dataset. The optical flow evaluation is performed on KITTI 2015 and KITTI 2012 datasets. For ablation study, the comparison of our full model and other variants is evaluated on the training split, which consists of 200 image pairs and the ground truth optical flow is provided. We chose the training split for ablation study as the ground truth of the test split is withheld and there is a limit of submission times per month. For our full model and comparison with the SoTA methods, we evaluated on the test split and report numbers generated by the test server.</figDesc><table><row><cell>Method</cell><cell>Training Data</cell><cell cols="4">Error Metrics Abs Rel Sq Rel RMSE RMSE log</cell></row><row><cell>Godard et al. [9]</cell><cell>CS</cell><cell cols="3">0.544 10.94 11.74</cell><cell>0.193</cell></row><row><cell>SfMLearner [10]</cell><cell>CS+K</cell><cell>0.383</cell><cell cols="2">5.32 10.47</cell><cell>0.478</cell></row><row><cell>DDVO [82]</cell><cell>K</cell><cell>0.387</cell><cell>4.72</cell><cell>8.09</cell><cell>0.204</cell></row><row><cell>Godard et al. [22]</cell><cell>K</cell><cell>0.361</cell><cell>4.17</cell><cell>7.82</cell><cell>0.175</cell></row><row><cell>EPC++ (mono depth only)</cell><cell>K</cell><cell>0.374</cell><cell>4.60</cell><cell>8.17</cell><cell>0.414</cell></row><row><cell>EPC++ (mono)</cell><cell>K</cell><cell>0.368</cell><cell>4.22</cell><cell>7.87</cell><cell>0.409</cell></row><row><cell>EPC++ (stereo depth only)</cell><cell>K</cell><cell>0.346</cell><cell>3.97</cell><cell>7.70</cell><cell>0.395</cell></row><row><cell>EPC++ (stereo)</cell><cell>K</cell><cell>0.341</cell><cell>3.86</cell><cell>7.65</cell><cell>0.392</cell></row><row><cell cols="3">4.4 Optical Flow Evaluation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Experiment setup.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Ablation study. The ablation study our model and 4 different variants is presented in Tab. 4. The model variants include: (1) Flow only: OptFlowNet trained with only view synthesis and smoothness losses L f vs + L f s .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>Ablation study of optical flow estimation on KITTI 2015 training set.</figDesc><table><row><cell>Method</cell><cell>noc</cell><cell>occ</cell><cell>all</cell></row><row><cell>Flow only</cell><cell cols="3">3.66 23.07 7.07</cell></row><row><cell>Joint training w/ depth</cell><cell cols="3">4.00 22.96 7.40</cell></row><row><cell>EPC++ all region</cell><cell cols="3">4.33 16.20 6.46</cell></row><row><cell>EPC++ vis-rigid region</cell><cell cols="3">3.97 21.79 7.17</cell></row><row><cell>EPC++ non-vis region</cell><cell cols="3">3.84 15.72 5.84</cell></row><row><cell>EPC++ (stereo) vis-rigid region</cell><cell cols="3">3.97 21.86 7.14</cell></row><row><cell>EPC++ (stereo) non-vis region</cell><cell cols="3">3.83 13.53 5.43</cell></row><row><cell cols="4">Comparison with SoTA methods. For fair comparison with</cell></row><row><cell cols="4">current SoTA optical flow methods, our OptFlowNet is evaluated</cell></row><row><cell cols="4">on both KITTI 2015, KITTI 2012 training and test splits. On test</cell></row><row><cell cols="4">split, the reported numbers are generated by official evaluation</cell></row><row><cell cols="4">servers. As shown in the Tab. 5, EPC++ (mono) outperforms all</cell></row><row><cell cols="3">current unsupervised monocular methods ( [7]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>Comparison of optical flow performances between EPC++ and current unsupervised SoTA on KITTI 2012 and KITTI 2015 datasets. The numbers reported on KITTI 2012 dataset use average EPE metric. The numbers reported on KITTI 2015 train split use EPE metric while the numbers reported on test split use the percentage of erroneous pixels (F1 score) as generated by the KITTI evaluation server. Network architectures are specified in parentheses.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">KITTI 2012</cell><cell></cell><cell cols="2">KITTI 2015</cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>#params</cell><cell>Train</cell><cell>Test</cell><cell>Train</cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>all</cell><cell>all</cell><cell>all</cell><cell>bg</cell><cell>fg</cell><cell>all</cell></row><row><cell>DSTFlow [17]</cell><cell>FlowNet-C</cell><cell>39.2M</cell><cell>10.43</cell><cell>12.40</cell><cell>16.79</cell><cell>-</cell><cell>-</cell><cell>39.00%</cell></row><row><cell>OccAwareFlow [7]</cell><cell>FlowNet-C</cell><cell>39.2M</cell><cell>3.55</cell><cell>4.20</cell><cell>8.88</cell><cell>-</cell><cell>-</cell><cell>31.20%</cell></row><row><cell>Unflow-CSS [63]</cell><cell>FlowNet-CSS</cell><cell>116.6M</cell><cell>3.29</cell><cell>-</cell><cell>8.10</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Multi-frame [86]</cell><cell>multiview PWC-Net</cell><cell></cell><cell>-</cell><cell>-</cell><cell>6.59</cell><cell>22.67%</cell><cell>24.27%</cell><cell>22.94%</cell></row><row><cell>GeoNet [18]</cell><cell>ResNet-50</cell><cell>58.5M</cell><cell>-</cell><cell>-</cell><cell>10.81</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DF-Net [20]</cell><cell>FlowNet-C</cell><cell>39.2M</cell><cell>3.54</cell><cell>4.40</cell><cell>8.98</cell><cell>-</cell><cell>-</cell><cell>25.70%</cell></row><row><cell>Competitive-Collaboration-uft [19]</cell><cell>PWC-Net</cell><cell>5.1M</cell><cell>-</cell><cell>-</cell><cell>5.66</cell><cell>-</cell><cell>-</cell><cell>25.27%</cell></row><row><cell>EPC++ (mono)</cell><cell>PWC-Net</cell><cell>5.1M</cell><cell>2.30</cell><cell>2.60</cell><cell>5.84</cell><cell>20.61%</cell><cell>26.32%</cell><cell>21.56%</cell></row><row><cell>EPC++ (stereo)</cell><cell>PWC-Net</cell><cell>5.1M</cell><cell>1.91</cell><cell>2.20</cell><cell>5.43</cell><cell>19.24%</cell><cell>26.93%</cell><cell>20.52%</cell></row><row><cell>Input images</cell><cell>Optical flow GT</cell><cell></cell><cell></cell><cell cols="2">EPC++ (mono)</cell><cell></cell><cell cols="2">Wang et al. [7]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6 :</head><label>6</label><figDesc>Optical flow performance of unsupervised methods on the MPI-Sintel final training split. We report the EPE metrics using model trained only on the KITTI dataset in the left part and present models fine-tuned on the MPI-Sintel dataset in the right part.</figDesc><table><row><cell>Method</cell><cell>Trained on KITTI</cell><cell>Fine-tuned on the MPI-Sintel</cell></row><row><cell>DSTFlow [17]</cell><cell>7.95</cell><cell>6.81</cell></row><row><cell>OccAware [7]</cell><cell>7.92</cell><cell>5.95</cell></row><row><cell>Unflow-CSS [63]</cell><cell>-</cell><cell>7.91</cell></row><row><cell>EPC++ flow only</cell><cell>7.33</cell><cell>5.90</cell></row><row><cell>EPC++ all region</cell><cell>6.95</cell><cell>6.21</cell></row><row><cell>EPC++</cell><cell>6.67</cell><cell>5.64</cell></row><row><cell cols="3">directly on MPI-Sintel data (Trained on KITTI); (2) the pre-trained</cell></row><row><cell cols="3">model (on KITTI) is further finetuned with MPI-Sintel training</cell></row><row><cell cols="3">data (Fine-tuned on MPI-Sintel). We apply the same parameters</cell></row><row><cell cols="2">and training strategy as used on KITTI.</cell><cell></cell></row><row><cell cols="2">From the results shown in</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7 :</head><label>7</label><figDesc>Odometry evaluation on two sequences of KITTI 2015 dataset. All presented results are generated by unsupervised methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8 :</head><label>8</label><figDesc>Odometry evaluation on KITTI dataset using the metric of average translation and rotation errors.</figDesc><table><row><cell>Method</cell><cell cols="4">Seq. 09 terr% rerr( â€¢ /100) terr% rerr( â€¢ /100) Seq. 10</cell></row><row><cell>Zhou et al. [10]</cell><cell>30.75</cell><cell>11.41</cell><cell>44.22</cell><cell>12.42</cell></row><row><cell>GeoNet [18]</cell><cell>39.43</cell><cell>14.30</cell><cell>28.99</cell><cell>8.85</cell></row><row><cell>EPC++ (mono)</cell><cell>8.84</cell><cell>3.34</cell><cell>8.86</cell><cell>3.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9 :</head><label>9</label><figDesc>Foreground moving object segmentation performance on KITTI 2015 dataset.</figDesc><table><row><cell>Method</cell><cell cols="4">pixel acc. mean acc. mean IoU f.w. IoU</cell></row><row><cell>Explainability mask [10]</cell><cell>0.61</cell><cell>0.54</cell><cell>0.38</cell><cell>0.64</cell></row><row><cell>EPC (stereo) [8]</cell><cell>0.89</cell><cell>0.75</cell><cell>0.52</cell><cell>0.87</cell></row><row><cell>Graphcut on residual (stereo)</cell><cell>0.76</cell><cell>0.46</cell><cell>0.40</cell><cell>0.78</cell></row><row><cell>EPC++(mono)</cell><cell>0.88</cell><cell>0.63</cell><cell>0.50</cell><cell>0.86</cell></row><row><cell>EPC++(stereo)</cell><cell>0.91</cell><cell>0.76</cell><cell>0.53</cell><cell>0.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 10 :</head><label>10</label><figDesc>Scene flow performances of different methods on KITTI 2015 training split. 8.86 4.74 5.16 17.11 6.99 6.38 20.56 8.55 ISF [67] partial yes 3.55 3.94 3.61 4.86 4.72 4.84 6.36 7.31 6.50 EPC (stereo) [8] full no 23.62 27.38 26.81 18.75 70.89 60.97 25.34 28.00 25.74 EPC++ (mono) full no 30.67 34.38 32.73 18.36 84.64 65.63 17.57 27.30 19.78 EPC++ (stereo) full no 22.76 26.63 23.84 16.37 70.39 60.32 17.58 26.89 19.64</figDesc><table><row><cell>Method</cell><cell cols="2">Test data Supervision</cell><cell>bg</cell><cell>D1 fg bg+fg bg</cell><cell>D2 fg bg+fg bg</cell><cell>FL fg bg+fg</cell></row><row><cell>OSF [4]</cell><cell>partial</cell><cell>yes</cell><cell>4.00</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DTAM: dense tracking and mapping in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatio-temporal action detection with cascade proposal and location anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vision for mobile robot navigation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="267" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lego: Learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Every pixel counts: Unsupervised geometry learning with holistic 3d motion understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop of VNAD</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of geometry from videos with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Visualsfm: A visual structure from motion system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sfm-net: Learning of structure and motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno>abs/1704.07804</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adversarial collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09806</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonrigid structure-frommotion: Estimating shape and motion with hierarchical priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="878" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01260</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV), ser. Part IV</title>
		<editor>A. Fitzgibbon et al.</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012-10" />
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Patchmatch stereo-stereo matching with slanted support windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bmvc</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lsd-slam: Large-scale direct monocular slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>SchÃ¶ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A simple prior-free method for non-rigid structure-from-motion factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="122" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Non-rigid structure from locally-rigid motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2761" to="2768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monocular dense 3d reconstruction of a complex dynamic scene from two perspective frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-body non-rigid structure-from-motion</title>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recovering surface layout from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Handbook of mathematical models in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Prados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="375" to="388" />
		</imprint>
	</monogr>
	<note>Shape from shading</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Intrinsic depth: Improving depth transfer with intrinsic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Box in the box: Joint 3d layout and object reasoning from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Match box: Indoor image matching via box-like scene estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Srajer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<idno>2017. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Discriminatively trained dense surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SURGE: surface regularized geometry estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Undeepvo: Monocular visual odometry through unsupervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="104" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<ptr target="http://lmb.informatik.uni-freiburg.de//Publications/2017/IMKDB17" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>CVPR. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>HÃ¤usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>HazÄ±rbaÅŸ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<ptr target="http://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB153" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in AAAI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unos: Unified unsupervised optical-flow and stereo-depth estimation by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Threedimensional scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Three-dimensional scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="475" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Bounding boxes, segmentations and object coordinates: How important is recognition for 3d scene flow estimation in autonomous driving scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Piecewise rigid scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1377" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A continuous optimization approach for efficient and accurate scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="757" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4083" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pixellevel matching for video object segmentation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2186" to="2195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="282" to="301" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Saliency-aware video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="20" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="8" to="12" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Driven to distraction: Self-supervised distractor learning for robust monocular visual odometry in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1894" to="1900" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>2017. 9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
