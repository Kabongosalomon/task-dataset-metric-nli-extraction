<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EDCNN: Edge enhancement-based Densely Connected Network with Compound Loss for Low-Dose CT Denoising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Liang</surname></persName>
							<email>tengfei.liang@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
							<email>twang@bjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jin</surname></persName>
							<email>yjin@bjtu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhe</forename><surname>Feng</surname></persName>
							<email>shfeng@bjtu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Li</surname></persName>
							<email>ydli@bjtu.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congyan</forename><surname>Lang</surname></persName>
							<email>cylang@bjtu.edu.cn</email>
							<affiliation key="aff5">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EDCNN: Edge enhancement-based Densely Connected Network with Compound Loss for Low-Dose CT Denoising</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Low-dose CT</term>
					<term>denoising</term>
					<term>convolutional network</term>
					<term>EDCNN</term>
					<term>edge enhancement</term>
					<term>trainable Sobel</term>
					<term>compound loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the past few decades, to reduce the risk of X-ray in computed tomography (CT), low-dose CT image denoising has attracted extensive attention from researchers, which has become an important research issue in the field of medical images. In recent years, with the rapid development of deep learning technology, many algorithms have emerged to apply convolutional neural networks to this task, achieving promising results. However, there are still some problems such as low denoising efficiency, over-smoothed result, etc. In this paper, we propose the Edge enhancement based Densely connected Convolutional Neural Network (EDCNN). In our network, we design an edge enhancement module using the proposed novel trainable Sobel convolution. Based on this module, we construct a model with dense connections to fuse the extracted edge information and realize end-to-end image denoising. Besides, when training the model, we introduce a compound loss that combines MSE loss and multi-scales perceptual loss to solve the over-smoothed problem and attain a marked improvement in image quality after denoising. Compared with the existing lowdose CT image denoising algorithms, our proposed model has a better performance in preserving details and suppressing noise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Computer tomography (CT) <ref type="bibr" target="#b0">[1]</ref> plays a very important role in modern medical diagnosis. Regarding its imaging principle, it uses the X-ray beam to scan a certain part of the human body. According to the different absorption and transmission rate of X-ray in different tissues of the human body, it detects and receives the signals passing through the human body with highly sensitive instruments. After conversion and computer processing, the tomographic image of the body to be examined can be obtained. Due to the X-ray used in this technology, the potential safety hazard in the radiation process has also caused more and more people's attention and concern [2]- <ref type="bibr" target="#b5">[5]</ref>.</p><p>When performing the CT scan, it will involve the intensity (or dose) of the used X-ray <ref type="bibr" target="#b6">[6]</ref>. As demonstrated in <ref type="bibr" target="#b7">[7]</ref>, researchers have found that the higher the dose of X-ray within a certain range, the higher the image quality of the CT image. However, patients will get more potential harm to their bodies with a greater intensity of X-ray. On the contrary, using the lower dose of radiation can reduce safety risks, but it will introduce more image noise, which brings more challenges to the doctor's later diagnosis. In this context, low-dose CT (LDCT) image denoising algorithms are proposed to solve this contradiction. The main idea <ref type="bibr" target="#b8">[8]</ref>  <ref type="bibr" target="#b9">[9]</ref> is that they firstly use CT images under the low-dose radiation as the input of the designed algorithm, and then the algorithm will output noisereduced CT images. In this way, both radiation safety and CT image quality can be considered at the same time.</p><p>In recent years, through researchers' experiments <ref type="bibr" target="#b8">[8]</ref> [10], convolutional neural network (CNN) has been shown to have good potential to solve the image denoising task and can achieve better performance than traditional methods. As for existing CNN image denoiser, researchers in this field have designed a variety of different structures of models, including fully connected convolutional neural networks (FCN) <ref type="bibr" target="#b10">[10]</ref>- <ref type="bibr" target="#b12">[12]</ref>, convolutional encoder-decoder networks with residual connections <ref type="bibr" target="#b9">[9]</ref>  <ref type="bibr" target="#b13">[13]</ref> or conveying-paths <ref type="bibr" target="#b14">[14]</ref>- <ref type="bibr" target="#b16">[16]</ref> and some network variants using 3D information <ref type="bibr" target="#b14">[14]</ref> [17], etc.</p><p>Although there have been many models and algorithms, the task of low-dose CT image denoising has not been completely solved. Existing models also face some problems such as over-smoothed results, loss of the edge, and detail information. Therefore, how to improve the low-dose CT image quality after denoising is still a key issue that needs to be resolved by researchers. In order to have better preservation of image subtle structures and details after the process of noise reduction, our paper proposes a novel CNN model, the Edge enhancement based Densely connected Convolutional Neural Network (EDCNN). The EDCNN is designed as an FCN structure, which can effectively realize the low-dose CT image denoising in the way of post-processing. And experiments show that we can get better output results by using this proposed denoiser. In general, the contributions of this paper are summarized as follows:</p><p>• Design an edge enhancement module based on the proposed trainable Sobel convolution, which can extract edge features adaptively during the optimization process. • Construct a fully convolutional neural network (EDCNN), using conveying-paths densely connection to fuse the information of input and edge features. • Introduce the compound loss used for the training stage, which integrates the MSE loss and multi-scales perceptual loss to overcome over smoothing problems.</p><p>This paper's structure is organized as follows: Section II mainly surveys the related research, including the existing models' composition and structure as well as the mainstream loss function. Section III introduces the designed EDCNN model and explains the contribution of this paper in terms of method. In section IV, we show the experimental configuration and the corresponding experimental results. In the end, section V makes a comprehensive summary of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we show the existing methods related to the low-dose CT image denoising task and discuss their implementation and performance.</p><p>Network structure: Regarding the deep learning model for low-dose CT image noise reduction, the current mainstream methods can be roughly categorized into three types: 1) Encoder-decoder: The encoder-decoder model uses a symmetrical structural design. Convolutional layers are utilized to form the encoder, carrying out the encoding of spatial information. Then the model uses the same number of deconvolutional layers to form the decoder, which generally fuses feature maps from the encoder using skip connections, such as the REDCNN <ref type="bibr" target="#b9">[9]</ref> with residual connections, the CPCE <ref type="bibr" target="#b14">[14]</ref> with conveying-paths connections, etc. LDCT images can be denoised through the entire encoder-decoder model.</p><p>2) Fully convolution network: It means that the whole network is composed of convolution layers. The output images denoised from low-dose CT images are obtained by several layers' convolution operation. As for the configuration of the convolution layer, different models have different ideas. Some models just use simple convolutional layers with kernel size set to 5 or 3, such as the denoiser in <ref type="bibr" target="#b11">[11]</ref>. The model in <ref type="bibr" target="#b16">[16]</ref> stacks convolutional layers with different dilation rate to increase the receptive field. Besides, this type of model also utilizes residual or conveying-paths connections. Our proposed EDCNN model is just designed as the FCN structure.</p><p>3) GAN-based algorithms: This type of algorithm consists of a generator and a discriminator. The generator is designed as a noise reduction network and can be used alone during the testing stage. The discriminator is used to distinguish the denoiser's output and the target high-dose CT image. They are optimized in an adversarial strategy with an alternate training process. There are some existing methods <ref type="bibr" target="#b11">[11]</ref>- <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b17">[17]</ref> that use this structure. With the further development of GAN, researchers will implement new models and perform experiments to do further exploration.</p><p>In addition to these types, there are also some algorithms that use a multi-model architecture, which uses a cascaded structure <ref type="bibr" target="#b18">[18]</ref>  <ref type="bibr" target="#b19">[19]</ref> or parallel networks <ref type="bibr" target="#b20">[20]</ref>. Our paper aims to design a single model and complete the low-dose CT image denoising task efficiently, so this type of algorithms will not be explained in detail here.</p><p>Loss function: Low dose CT image denoising is one kind of image transformation tasks, and its common loss functions used for optimization are roughly as follows:</p><p>1) Per-pixel loss: The goal of low-dose CT image denoising is to get the result close to that using high-dose radiation, so a simple idea is to set the loss function directly as the perpixel loss between the output image and the target image. In this type of loss, the Mean Square Error (MSE) loss function is commonly used [8] <ref type="bibr" target="#b9">[9]</ref>. The L1 loss function also belongs to this type. However, this kind of loss function has an obvious problem. It can not describe the structure information in the image, which is important in the denoising task. And methods trained by this type tend to output over-smoothed images <ref type="bibr" target="#b9">[9]</ref>.</p><p>2) Perceptual loss: In order to solve the spatial information dependence in image transformation tasks, <ref type="bibr" target="#b21">[21]</ref> proposes a new kind of loss function, the perceptual loss. It maps images to the feature space and calculates the similarity in this level. With regard to mappers, VGGNet <ref type="bibr" target="#b22">[22]</ref> with trained weights is often used. With this kind of loss functions, the detail information of the image can be better preserved, but there are also some problems that cannot be ignored, such as the cross-hatch artifacts introduced by this method.</p><p>3) Other loss: In GAN-based denoising algorithms, the models use adversarial loss during training, inspired by the ideas of DCGAN <ref type="bibr" target="#b23">[23]</ref>, WGAN <ref type="bibr" target="#b24">[24]</ref> and so on. These loss functions can also capture the structural information of the image to generate more realistic images. Except for these types of loss, researchers design some special forms of loss functions. For example, the MAP-NN model in <ref type="bibr" target="#b18">[18]</ref> proposes the composite loss function which contains three components including adversarial loss, mean-squared error (MSE), and edge incoherence loss. Although there are various loss functions, they are all designed to produce images of higher quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>This section presents the proposed edge enhancement-based densely connected network (EDCNN) in detail, including the edge enhancement module, overall model structure, and the loss function used for the optimization process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Edge enhancement Module</head><p>Before describing the structure of the whole model, this subsection first introduces the edge enhancement module, which directly acts on the input image.</p><p>In this module, we design the trainable Sobel convolution. As shown in <ref type="figure" target="#fig_1">Fig. 2a</ref>, different from the traditional fixed-value Sobel operator <ref type="bibr" target="#b25">[25]</ref>, a learnable parameter α is defined in the trainable Sobel operator, which is called Sobel factor by us. The value of this parameter can be adaptively adjusted during the optimization of training process, so it can extract edge information of different intensity. Besides, we define four types of operators as a group <ref type="figure" target="#fig_1">(Fig. 2a</ref>), including vertical, horizontal, and diagonal directions. Multiple groups of trainable Sobel operators can be used in this module. In the flow of this module <ref type="figure" target="#fig_1">(Fig. 2b)</ref>, firstly, it uses a certain number (a multiple of 4) of trainable Sobel operators on the input CT image, performing convolution operations to obtain a set of feature maps for extracting edge information. And then the module stacks them with the input low-dose CT Images together in the channel dimension to get the final output of this module. The goal of this module is to enrich the input information of the model at the level of data source and strengthen the effect of edge information to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overall Network Architecture</head><p>The proposed network architecture is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, which is called Edge enhancement based Densely connected Convolutional Neural Network (EDCNN). The whole model consists of an edge enhancement module and eight convolution blocks. The edge enhancement module has been explained in Section III-A. The number of trainable Sobel operators we use is 32 (8 groups of the four types).</p><p>As for the model structure after the edge enhancement module, the purpose of our design is to retain the image details in the process as much as possible. Inspired by the DenseNet <ref type="bibr" target="#b26">[26]</ref>, we design a low-dose CT denoising model with dense connection, trying to make full use of the extracted edge information and the original input. Specifically, as shown by the line in <ref type="figure" target="#fig_0">Fig. 1</ref>, we convey the output of the edge enhancement module to each convolution block through skip connection and concatenate them in the channel dimension. The inner structure of the latter convolution blocks is exactly the same except for the last layer. These blocks are composed of 1x1 and 3x3 convolution, and the number of convolutional filters is all set to 32. The number of 3x3 convolutional filters in the last layer is 1, corresponding to the output of a single channel. In each block, the point-wise convolution with 1x1 kernels is used to fuse the outputs of the previous layer and edge enhancement module, and the convolution with 3x3 kernels is used to learn features in the image as usual. Besides, to keep the output size and input size the same, feature maps in the model are padded to ensure that the spatial size does not change during the forward propagation.</p><p>In order to accelerate the convergence of the model and simplify the task of the main structure of model, we let the model directly learn the noise distribution and reconstruction information. So the output of the last convolution block is added with the original low-dose CT image to get the final noise-denoised images. In <ref type="figure" target="#fig_0">Fig. 1</ref>, the top line represents this residual connection, and the symbol, which consists of a circle and a plus sign, represents the element-wise addition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Compound Loss Function</head><p>The ultimate goal of CT image denoising is to obtain the output results similar to target images with a higher dose of radiation exposure. Assuming that I LDCT ∈ R 1×w×h represents an LDCT image with the size of w × h, and I N DCT ∈ R 1×w×h represents the target NDCT image, the denoising task can be expressed as follows:</p><formula xml:id="formula_0">F (I LDCT ) = I Output ≈ I N DCT<label>(1)</label></formula><p>where F represents the noise reduction method, and I Output denotes the output image of the denoiser. To achieve this purpose, the MSE (Eq. 2) is widely used in previous methods as the loss function. The distance between the model's output and the target image is calculated pixel by pixel. However, the loss has been verified by lots of experiments, tending to make output images over-smoothed and increase the image blur.</p><p>In order to overcome the problem, this paper introduces the compound loss function, which fuses MSE loss and multiscales perceptual loss, as shown in the following formulas:</p><formula xml:id="formula_1">L mse = 1 N N i=1 F (x i , θ) − y i 2 (2) L multi−p = 1 N S N i=1 S s=1 φs F (xi, θ) ,θ − φs yi,θ 2 (3) L compound = L mse + w p · L multi−p<label>(4)</label></formula><p>In these formulas, we use x i as the input, y i as the target, and N is the number of images. Same as above, F represents the noise reduction model with parameters θ. In Eq. 3, the symbol φ represents the model with fixed pre-trained weightŝ θ, which is used to calculate the perceptual loss. And S is the number of scales. The w p in Eq. 4 denotes the weight of the second part of the compound loss function.</p><p>Regarding the perceptual loss, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we utilize the ResNet-50 <ref type="bibr" target="#b27">[27]</ref> as the feature extractor to get the multiscale perceptual loss. Specifically, we discard the pooling layer and the fully connected layer at the end of the model, retaining only the convolution layers in the front of this model. In the beginning, we first load the model's weights trained on the ImageNet dataset <ref type="bibr" target="#b28">[28]</ref>, and then freeze these weights during training. When calculating perceptual loss value, both the denoised output and target image are sent to the extractor to do forward propagation ( <ref type="figure" target="#fig_2">Fig. 3)</ref>. We choose the feature maps after four stages of ResNet, in each of that the spatial scale of the image will be halved, representing feature spaces of different scales. Then we use the MSE to measure the similarity of these feature maps. The multi-scales perceptual loss is obtained by averaging these values.</p><p>By combining MSE and multi-scales perceptual loss, we can concern both the per-pixel similarity and the structural information of CT images. And we can adjust the hyperparameter w p to balance the two loss components (Eq. 4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>This section explains the dataset used to train and test the proposed model, the configuration of the experiment. And then we show the experimental results in this section, evaluating the noise reduction performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>In the experiment of our study, we utilize the dataset of the 2016 NIH AAPM-Mayo Clinic Low-Dose CT Grand Challenge <ref type="bibr" target="#b29">[29]</ref>, which is used by current mainstream methods in the field of low-dose CT image denoising. It contains the paired normal-dose CT (NDCT) images and synthetic quarterdose CT images (LDCT) with a size of 512x512 pixels, collected from 10 patients. So there are LDCT images for inputs of the model and NDCT images as targets, which can support the supervised training process.</p><p>As for the data preparation, we split the dataset before training, using nine patients' CT images as the training set, and the rest one patient's images as the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>The structure of the model and number of filters in each layer have been described in Section III-B, which is implemented by us based on the Pytorch framework <ref type="bibr" target="#b30">[30]</ref>. We use the default random initialization for the convolution layers in this model, and the Sobel factors of all edge enhancement modules are initialized to 1 before training. Besides, the hyperparameter w p of the compound loss function is set to 0.01.</p><p>During training, we apply a data augmentation strategy that crops patch randomly. Specifically, 4 patches with a size of 64x64 pixels will be randomly cropped from one LDCT image, and the input batch we used is taken from 32 images, which has 128 patches in total, so is the target batch of NDCT images. In the process of optimization, we utilize the AdamW optimizer <ref type="bibr" target="#b31">[31]</ref> with the default configuration. We set the learning rate to 0.001, and conduct 200 epochs of training to make the model converge. When testing the model, because of the model's fully convolutional structure, there is no limit on the size of the input image. So we let the trained model use LDCT images with the size of 512x512 pixels as the input and directly outputs the denoised results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>This subsection shows the noise reduction results of our model. For fairness, we choose the REDCNN <ref type="bibr" target="#b9">[9]</ref>, WGAN <ref type="bibr" target="#b11">[11]</ref> and CPCE <ref type="bibr" target="#b14">[14]</ref> for comparison, because of their design of the single model, which is the same as our proposed model. These models also adopt the structure of convolutional neural networks, but each of them has its characteristics. We reimplement these models, training them on the same training set. The left part of <ref type="table">Table.</ref> I shows the configuration of loss functions used by them, including our model as well.</p><p>In the noise reduction task, there are three common criteria for quantitative analysis a model, including the Peak Signal to Noise Ratio (PSNR), Structural SIMilarity (SSIM), and Root Mean Square Error (RMSE). Besides, we add a metric, VGG-P, which is the commonly used perceptual loss based on VGGNet19 <ref type="bibr" target="#b22">[22]</ref>, measuring the distance in the final convolution layer's feature space <ref type="bibr" target="#b21">[21]</ref>. As shown on the right part of <ref type="table">Table.</ref> I, all the models are tested on the split test set of the AAPM Challenge's dataset. We calculate and count the mean and standard deviation of these metrics. Through this table, we can find that the REDCNN based on MSE loss has the best performance on the metrics of PSNR and RMSE. By using perceptual loss based on VGGNet, the WGAN and CPCE have a good result on VGG-P. As for our proposed EDCNN, based on compound loss, it achieves the best or suboptimal results on every criterion, which can balance the per-pixel and structurewise performance.</p><p>Since the calculation process of PSNR and RMSE is directly related to MSE, the model trained by just using MSE as the loss function can get good results on these metrics. However, these criteria can not truly reflect the visual quality of the output image, so they can only be used as a relative reference. For comparison of denoised results, as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>, we choose a CT image with a complex structure to show the performance of these models. We can notice that there is more noise in LDCT images <ref type="figure" target="#fig_3">(Fig. 4a</ref>) than in NDCT images <ref type="figure" target="#fig_3">(Fig. 4b)</ref>. After denoising from the LDCT image, the REDCNN's output <ref type="figure" target="#fig_3">(Fig. 4c)</ref> is obviously over- The 'VGG-P' means perceptual loss based on VGGNet, and 'MS-P Loss' represents the multi-scales perceptual. PSNR, SSIM, RMSE and VGG-P are used as the metric, which are shown in the form of mean ± std. The best ones are marked in red, and the second best ones are marked in blue.</p><p>smoothed. Although it has the highest PSNR and the lowest RMSE, the visual perception of the image is not good, which has the problem of image blur and loss of structure details. The WGAN and CPCE are all based on Wasserstein GAN with perceptual loss and adversarial loss. <ref type="figure" target="#fig_3">Fig. 4d</ref> shows the denoised CT image of WGAN, which retains the structural information of the original image, but its suppression of noise is still relatively poor. Shown in <ref type="figure" target="#fig_3">Fig. 4e and Fig. 4f</ref>, the CPCE model and our EDCNN have comparable performance. The output images of them are all much similar to the target NDCT image <ref type="figure" target="#fig_3">(Fig. 4b)</ref>, preserving the subtle structure of the CT image. But from the details of the noise dots, we can still notice the difference between them. The EDCNN has better noise reduction performance than CPCE, which is also consistent with the value of the metrics in <ref type="table">Table.</ref> I. The scores are shown in the form of mean ± std (Perfect score is 5 points). The best ones are marked in red, and the second best ones are marked in blue.</p><p>In order to obtain the quantitative visual evaluation, we conduct the blind reader study. Specifically, we select 20 groups of models' denoised results in the test set with different body parts. Each group includes six CT images. The LDCT and NDCT images are used as references, and the other four images are the outputs of the above four models, which are randomly shuffled in each group. Readers are asked to score the denoised CT images on three levels, including noise reduction, structure preservation and the overall quality, with a full score of 5 points for each item. As shown in <ref type="table">Table.</ref> II, we present the statistics of the subjective scores in the form of mean ± std. The REDCNN has the best performance of noise reduction, and the GAN-based WGAN and CPCE have a high score in structure preservation. Concerning our designed EDCNN model, it consider both noise reduction and structure preservation because of the compound loss. In addition, the EDCNN gets a high score in overall image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>In this part, we compare and analyze the performance of our model under different configurations of model structure and loss function. And we discuss the validity of the final design in our proposed EDCNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Structure and Module:</head><p>To explore the effect of each component of EDCNN model, we make a decomposition experiment on the structure. First, we designed a basic model (BCNN), removing the dense connection and edge enhancement module from the structure shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, and then we add dense connection (BCNN+DC) and edge enhancement module (BCNN+DC+EM, EDCNN) in turn. In order to fully demonstrate the potential capacity of the model, all models are trained with MSE loss with the same training strategy.   <ref type="figure" target="#fig_4">5</ref> shows the curves of PSNR, testing on the test set for trained models at each epoch. We also add REDCNN as a comparison. It is worth noting that the basic model (BCNN) of our design already achieves better performance than REDCNN. And the value of PSNR will increase continuously by adding the dense connection and edge enhancement module. Besides, the edge enhancement module accelerates the convergence process of the model. In <ref type="table">Table.</ref> III, we can check the value of PSNR, SSIM, RMSE for these models. And the complete EDCNN model has the best results on these metrics. 2) Models of Perceptual Loss: As demonstrated in Section III-C, the model chosen to calculate the perceptual loss in our method is the ResNet-50. Regarding the model of perceptual loss, we compare the ResNet-50 with the VGGNet-19 that is commonly used by existing methods. In this experiment, we just train the EDCNN model by single perceptual loss. According to the previous methods, we use the last convolution layer's ouput of VGGNet-19 to calculate the loss. As for the ResNet-50 we used, we also utilize the feature maps of its last convolution layer with the same idea for comparison. Models optimized by perceptual loss tend to ouput images with some kind of texture-like noise. By observing <ref type="figure" target="#fig_6">Fig. 6</ref> carefully, we can find that the noise graininess of <ref type="figure" target="#fig_6">Fig. 6b</ref> is bigger than that of <ref type="figure" target="#fig_6">Fig. 6c</ref>. And from the visual appearance, <ref type="figure" target="#fig_6">Fig. 6c</ref> is closer to the NDCT image ( <ref type="figure" target="#fig_6">Fig. 6d</ref>). So we use the ResNet-50 model in our perceptual loss function, which has stronger feature extraction ability than the VGGNet.</p><p>3) Multi-Scales Perceptual Loss: When using perceptual loss, we need to decide which layer of feature maps to use. Here we explore the different combinations of the multi-scales perceptual loss. Specifically, we utilize the output features of four stages in ResNet-50 ( <ref type="figure" target="#fig_2">Fig. 3)</ref>. Four types of loss functions are designed, including perceptual loss with S-4, S-43, S-432, and S-4321. 'S' represents the stage and the numbers denote the number of stages used to get feature maps. The loss function will calculate MSE on these stages' extracted features (Eq. 3), and compute their average to get the final loss value. <ref type="figure" target="#fig_7">Fig. 7</ref>(a-f) show the output images, we can find that as the number of stages used increases, the 'texture' of the denoised result is closer to the NDCT image. Therefore, we decide to use the output features of four stages in ResNet-50 model to calculate our method's perceptual loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Single or Compound Loss:</head><p>During this experiment, we attain three EDCNN models trained on single MSE loss, single multi-scales perceptual loss, and compound loss respectively. They are trained in the same way, except for their loss.</p><p>As shown in <ref type="figure" target="#fig_7">Fig. 7(g-k)</ref>, we can compare the visual quality among these denoised CT images. Apparently, the result of MSE-based EDCNN model <ref type="figure" target="#fig_7">(Fig. 7h)</ref> is already oversmoothed, missing too much detail for later diagnosis, and it brings difficulties for doctors to make judgments. As for <ref type="figure" target="#fig_7">Fig. 7i</ref> and <ref type="figure" target="#fig_7">Fig. 7j</ref>, they show similar quality in detail retention, which further verifies the effectiveness of the multi-scales perceptual loss. In the meanwhile, we can notice that <ref type="figure" target="#fig_7">Fig. 7j</ref> is slightly clearer than <ref type="figure" target="#fig_7">Fig. 7i</ref>. The latter introduces some visible artifacts. EDCNN based on compound loss has better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In summary, this article presents a new denoising model with a densely connected convolutional architecture, the Edge enhancement-based Densely Connected Network (EDCNN). Through the designed edge-enhancement module based on trainable Sobel operators, the method can get richer edge information of the input image adaptively. Besides, we introduce the compound loss function, which is a weighted fusion of MSE loss and multi-scales perceptual loss. Using the wellknown mayo dataset, we make a lot of experiments and our method achieves better performance compared with previous models. In the future, we plan to further explore the multimodels structure based on our proposed EDCNN model and extend it to other image transformation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overall architecture of our proposed EDCNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The designed edge enhancement module. (a) Four types of trainable Sobel operators; (b) Process of this module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The multi-scales perceptual loss. It is based on ResNet-50, which contains 4 main stages. For more details about this model, please refer to paper<ref type="bibr" target="#b27">[27]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>The denoised results of different model. The Region of Interest (RoI) in the red box is selected and magnified in the lower left corner of images for a clearer comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>The PSNR curves in the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.</head><label></label><figDesc>Fig. 5 shows the curves of PSNR, testing on the test set for trained models at each epoch. We also add REDCNN as a comparison. It is worth noting that the basic model (BCNN) of our design already achieves better performance than REDCNN. And the value of PSNR will increase continuously by adding the dense connection and edge enhancement module. Besides, the edge enhancement module accelerates the convergence process of the model. In Table. III, we can check the value of PSNR, SSIM, RMSE for these models. And the complete EDCNN model has the best results on these metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>The denoised results of EDCNN with different perceptual loss. VGG-P and ResNet-P represent the perceptual loss based on VGGNet and ResNet respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Comparison of denoised images with different configuration of loss. (a)-(f) show different setup of multi-scales loss, and (g)-(k) compare the single loss with our compound loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Quantitative Comparison among Different Models on the AAPM Dataset</figDesc><table><row><cell>Method</cell><cell cols="4">Loss MSE Loss VGG-P Loss Adversarial Loss MS-P Loss</cell><cell>PSNR</cell><cell>SSIM</cell><cell cols="2">Metric</cell><cell>RMSE</cell><cell>VGG-P</cell></row><row><cell>LDCT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.7594±0.9675</cell><cell cols="2">0.9465±0.0113</cell><cell cols="2">0.0146±0.0016</cell><cell>0.0377±0.0055</cell></row><row><cell>REDCNN [9]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>42.3891±0.7613</cell><cell cols="2">0.9856±0.0029</cell><cell cols="2">0.0076±0.0007</cell><cell>0.0218±0.0048</cell></row><row><cell>WGAN [11]</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>38.6043±0.9492</cell><cell cols="2">0.9647±0.0078</cell><cell cols="2">0.0108±0.0013</cell><cell>0.0072±0.0019</cell></row><row><cell>CPCE [14]</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>40.8209±0.7905</cell><cell cols="2">0.9740±0.0050</cell><cell cols="2">0.0093±0.0009</cell><cell>0.0043±0.0011</cell></row><row><cell>EDCNN</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>42.0835±0.8100</cell><cell cols="2">0.9866±0.0031</cell><cell cols="2">0.0079±0.0007</cell><cell>0.0061±0.0014</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Subjective Scores on Image Quality</figDesc><table><row><cell>Method</cell><cell>Noise Reduction</cell><cell>Score Structure Preservation</cell><cell>Overall Quality</cell></row><row><cell>REDCNN</cell><cell>4.19±0.19</cell><cell>3.06±0.22</cell><cell>3.15±0.44</cell></row><row><cell>WGAN</cell><cell>2.36±0.34</cell><cell>3.58±0.17</cell><cell>3.49±0.16</cell></row><row><cell>CPCE</cell><cell>3.45±0.17</cell><cell>4.03±0.19</cell><cell>3.95±0.42</cell></row><row><cell>EDCNN</cell><cell>3.64±0.12</cell><cell>4.07±0.21</cell><cell>4.13±0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Performance Comparison on Model Structure DC represents Dense Connection, and EM represents the Edge enhancement Module. The best ones on each metric are marked in bold (mean ± std).</figDesc><table><row><cell>Method</cell><cell>PSNR</cell><cell>Metric SSIM</cell><cell>RMSE</cell></row><row><cell>LDCT</cell><cell>36.7594±0.9675</cell><cell>0.9465±0.0113</cell><cell>0.0146±0.0016</cell></row><row><cell>REDCNN</cell><cell>42.3891±0.7613</cell><cell>0.9856±0.0029</cell><cell>0.0076±0.0007</cell></row><row><cell>BCNN</cell><cell>42.6654±0.7929</cell><cell>0.9864±0.0027</cell><cell>0.0074±0.0007</cell></row><row><cell>BCNN+DC</cell><cell>42.7444±0.7684</cell><cell>0.9868±0.0025</cell><cell>0.0073±0.0007</cell></row><row><cell>BCNN+DC+EM</cell><cell>42.8128±0.7726</cell><cell>0.9870±0.0025</cell><cell>0.0073±0.0007</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work is supported by the Institute of Information Science in Beijing Jiaotong University, and we gratefully acknowledge the support from its laboratory for providing us the Titan GPUs to accomplish this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computed Tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Buzug</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-74658-4_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-74658-416" />
	</analytic>
	<monogr>
		<title level="m">Springer Handbook of Medical Technology</title>
		<editor>R. Kramme, K.-P. Hoffmann, and R. S. Pozos</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="311" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Radiation Risk to Children From Computed Tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Frush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Brent</surname></persName>
		</author>
		<ptr target="https://pediatrics.aappublications.org/content/120/3/677" />
	</analytic>
	<monogr>
		<title level="j">American Academy of Pediatrics</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="677" to="682" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Pediatrics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Radiation in medicine: Origins, risks and aspirations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elguindy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Firmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yacoub</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4355517/" />
	</analytic>
	<monogr>
		<title level="j">Global Cardiology Science &amp; Practice</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="437" to="448" />
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Radiation Dose Associated With Common Computed Tomography Examinations and the Associated Lifetime Attributable Risk of Cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith-Bindman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berrington De González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Miglioretti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Internal Medicine</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2078" to="2086" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<idno type="DOI">10.1001/archinternmed.2009.427</idno>
		<ptr target="https://doi.org/10.1001/archinternmed.2009.427" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Physician Knowledge of Radiation Exposure and Risk in Medical Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hobbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Lind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Dodd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Borgstede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American College of Radiology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="43" />
			<date type="published" when="2018-01" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computed tomography-an increasing source of radiation exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Hall</surname></persName>
		</author>
		<idno type="DOI">10.1056/NEJMra072149</idno>
		<ptr target="https://doi.org/10.1056/NEJMra072149" />
	</analytic>
	<monogr>
		<title level="j">The New England journal of medicine</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2277" to="2284" />
			<date type="published" when="2007-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Low-dose CT of the lungs: preliminary observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Naidich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gribbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Arams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Mccauley</surname></persName>
		</author>
		<idno type="DOI">https:/pubs.rsna.org/doi/abs/10.1148/radiology.175.3.2343122</idno>
		<ptr target="https://pubs.rsna.org/doi/abs/10.1148/radiology.175.3.2343122" />
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="729" to="731" />
			<date type="published" when="1990-06" />
			<publisher>Radiological Society of North America</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network using directional wavelets for low-dose X-ray CT reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">https:/aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.12344</idno>
		<ptr target="https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.12344" />
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="360" to="375" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Low-dose ct with a residual encoder-decoder convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2017.2715284</idno>
		<ptr target="http://dx.doi.org/10.1109/TMI.2017.2715284" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2524" to="2535" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low-dose ct via convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://www.osapublishing.org/boe/abstract.cfm?URI=boe-8-2-679" />
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="679" to="694" />
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Low-dose ct image denoising using a generative adversarial network with wasserstein distance and perceptual loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2018.2827462</idno>
		<ptr target="http://dx.doi.org/10.1109/TMI.2018.2827462" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1348" to="1357" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time image reconstruction for low-dose CT using deep convolutional generative adversarial networks (GANs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">https:/www.spiedigitallibrary.org/conference-proceedings-of-spie/10573/1057332/Real-time-image-reconstruction-for-low-dose-CT-using-deep/10.1117/12.2293420.short</idno>
		<ptr target="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10573/1057332/Real-time-image-reconstruction-for-low-dose-CT-using-deep/10.1117/12.2293420.short" />
	</analytic>
	<monogr>
		<title level="j">Physics of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">10573</biblScope>
			<biblScope unit="page">1057332</biblScope>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Artifact correction in low-dose dental CT imaging using Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1686" to="1696" />
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3-d convolutional encoder-decoder network for low-dose ct via transfer learning from a 2-d trained network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2018.2832217</idno>
		<ptr target="http://dx.doi.org/10.1109/TMI.2018.2832217" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1522" to="1534" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sharpness-Aware Low-Dose CT Denoising Using Conditional Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Babyn</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10278-018-0056-0</idno>
		<ptr target="https://doi.org/10.1007/s10278-018-0056-0" />
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="655" to="669" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Learning for Low-Dose CT Denoising Using Perceptual Loss and Edge Detection Layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gholizadeh-Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alirezaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Babyn</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10278-019-00274-4</idno>
		<ptr target="https://doi.org/10.1007/s10278-019-00274-4" />
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="504" to="515" />
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for noise reduction in low-dose ct</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2536" to="2545" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Competitive performance of a modularized deep neural network compared to commercial algorithms for low-dose CT image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Padole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Homayounieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Khera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nitiwarangkul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://www.nature.com/articles/s42256-019-0057-9" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="269" to="276" />
			<date type="published" when="2019-06" />
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cascaded convolutional neural networks with perceptual loss for low dose ct denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ataei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Alirezaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Babyn</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2006.14738.pdf" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Low-dose ct image denoising using parallel-clone networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2005.06724.pdf" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1603.08155.pdf" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1409.1556.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1511.06434.pdf" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Wasserstein gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1701.07875.pdf" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A 3×3 isotropic gradient operator for image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<imprint>
			<date type="published" when="1973-01" />
			<biblScope unit="page" from="271" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1608.06993.pdf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="http://www.image-net.org" />
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Low-dose CT for the detection and classification of metastatic liver lesions: Results of the 2016 Low Dose CT Grand Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Mccollough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Drees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Michalak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="339" to="352" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rk6qdGgCZ" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
