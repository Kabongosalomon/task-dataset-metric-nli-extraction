<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Invertible Conditional GANs for image editing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guim</forename><surname>Perarnau</surname></persName>
							<email>guimperarnau@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Raducanu</surname></persName>
							<email>bogdan@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Álvarez</surname></persName>
							<email>jose.alvarez@nicta.com.au</email>
							<affiliation key="aff1">
								<orgName type="department">Data61 @ CSIRO</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Invertible Conditional GANs for image editing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Adversarial Networks (GANs) have recently demonstrated to successfully approximate complex data distributions. A relevant extension of this model is conditional GANs (cGANs), where the introduction of external information allows to determine specific representations of the generated images. In this work, we evaluate encoders to inverse the mapping of a cGAN, i.e., mapping a real image into a latent space and a conditional representation. This allows, for example, to reconstruct and modify real images of faces conditioning on arbitrary attributes. Additionally, we evaluate the design of cGANs. The combination of an encoder with a cGAN, which we call Invertible cGAN (IcGAN), enables to re-generate real images with deterministic complex modifications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image editing can be performed at different levels of complexity and abstraction. Common operations consist in simply applying a filter to an image to, for example, augment the contrast or convert to grayscale. These, however, are low-complex operations that do not necessarily require to comprehend the scene or object that the image is representing. On the other hand, if one would want to modify the attributes of a face (e.g. add a smile, change the hair color or even the gender), this is a more complex and challenging modification to perform. In this case, in order to obtain realistic results, a skilled human with an image edition software would often be required.</p><p>A solution to automatically perform these non-trivial operations relies on generative models. Natural image generation has been a strong research topic for many years, but it has not been until 2015 that promising results have been achieved with deep learning techniques combined with generative modeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Generative Adversarial Networks (GANs) <ref type="bibr" target="#b2">[3]</ref> is one of the state-of-the-art approaches for image generation. GANs are especially interesting as they are directly optimized towards generating the most plausible and realistic data, as opposed to other models (e.g. Variational Autoencoders <ref type="bibr" target="#b3">[4]</ref>), which focus on an image reconstruction loss. Additionally, GANs are able to explicitly control generated images features with a conditional extension, conditional GANs (cGANs). However, the GAN framework lacks an inference mechanism, i.e., finding the latent representation of an input image, which is a necessary step for being able to reconstruct and modify real images.</p><p>In order to overcome this limitation, in this paper we introduce Invertible Conditional GANs (IcGANs) for complex image editing as the union of an encoder used jointly with a cGAN. This model allows to map real images into a high-feature space (encoder) and perform meaningful modifications on them (cGAN). As a result, we can explicitly control the attributes of a real image <ref type="figure" target="#fig_0">(Figure 1</ref>), which could be potentially useful in several applications, be it creative processes, data augmentation or face profiling. The summary of contributions of our work is the following:</p><p>• Proposing IcGANs, composed of two crucial parts: an encoder and a cGAN. We apply this model to MNIST <ref type="bibr" target="#b4">[5]</ref> and CelebA <ref type="bibr" target="#b5">[6]</ref> datasets, which allows performing meaningful and realistic editing operations on them by arbitrarily changing the conditional information y. • Introducing an encoder in the conditional GAN framework to compress a real image x into a latent representation z and conditional vector y. We consider several designs and training procedures to leverage the performance obtained from available conditional information. • Evaluating and refining cGANs through conditional position and conditional sampling to enhance the quality of generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>There are different approaches for generative models. Among them, there are two promising ones that are recently pushing the state-of-the-art with highly plausible generated images.</p><p>The first one is Variational Autoencoders (VAE) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, which impose a prior representation space z (e.g. normal distribution) in order to regularize and constrain the model to sample from it. However, VAEs main limitation is the pixel-wise reconstruction error used as a loss function, which causes the output images to look blurry. The second approach is Generative Adversarial Nets (GANs). Originally proposed by Goodfellow et al. <ref type="bibr" target="#b2">[3]</ref>, GANs have been improved with a deeper architecture (DCGAN) by Radford et al. <ref type="bibr" target="#b1">[2]</ref>. The latest advances introduced several techniques that improve the overall performance for training GANs <ref type="bibr" target="#b8">[9]</ref> and an unsupervised approach to disentangle feature representations <ref type="bibr" target="#b9">[10]</ref>. Additionally, the most advanced and recent work on cGANs trains a model to generate realistic images from text descriptions and landmarks <ref type="bibr" target="#b10">[11]</ref>.</p><p>Our work is considered in the content of the GAN framework. The baseline will be the work of Radford's et al. (DCGANs) <ref type="bibr" target="#b1">[2]</ref>, which we will add a conditional extension. The difference of our approach to prior work is that we also propose an encoder (Invertible cGAN) with which we can, given an input image x, to obtain its representation as a latent variable z and a conditional vector y. Then, we can modify z and y to re-generate the original image with complex variations. Dumoulin et al. <ref type="bibr" target="#b11">[12]</ref> and Donahue et al. <ref type="bibr" target="#b12">[13]</ref> also proposed an encoder in GANs, but in a non-conditional and jointly trained setting. Additionally, Makhzani et al. <ref type="bibr" target="#b13">[14]</ref> and Larsen et al. <ref type="bibr" target="#b14">[15]</ref> proposed a similar idea to this paper by combining a VAE and a GAN with promising results.</p><p>Reed et al. <ref type="bibr" target="#b15">[16]</ref> implemented an encoder in a similar fashion to our approach. This paper builds alongside their work in a complementary manner. In our case, we analyze more deeply the encoder by including conditional information encoding and testing different architectures and training approaches. Also, we evaluate unexplored design decisions for building a cGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background: Generative Adversarial Networks</head><p>A GAN is composed of two neural networks, a generator G and a discriminator D. Both networks are iteratively trained competing against each other in a minimax game. The generator aims to approximate the underlying unknown data distribution p data to fool the discriminator, whilst the discriminator is focused on being able to tell which samples are real or generated. On convergence, we want p data = p g , where p g is the generator distribution.</p><p>More formally, considering the function v(θ g , θ d ), where θ g and θ d are the parameters of the generator G and discriminator D respectively, we can formulate GAN training as optimizing</p><formula xml:id="formula_0">min g max d v(θ g , θ d ) = E x∼p data [log D(x)] + E z∼pz [log(1 − D(G(z)))],<label>(1)</label></formula><p>where z is a vector noise sampled from a known simple distribution p z (e.g. normal).</p><p>GAN framework can be extended with conditional GANs (cGANs) <ref type="bibr" target="#b16">[17]</ref>. They are quite similar to vanilla (non-conditional) GANs, the only difference is that, in this case, we have extra information y (e.g. class labels, attribute information) for a given real sample x. Conditional information strictly depends on real samples, but we can model a density model p y in order to sample generated labels y for generated data x . Then, Equation 1 can be reformulated for the cGAN extension as</p><formula xml:id="formula_1">min g max d v(θ g , θ d ) = E x,y∼p data [log D(x, y)] + E z∼pz,y ∼py [log(1 − D(G(z, y ), y ))].<label>(2)</label></formula><p>Once a cGAN is trained, it allows us to generate samples using two level of variations: constrained and unconstrained. Constrained variations are modeled with y as it directly correlates with features of the data that are explicitly correlated with y and the data itself. Then, all the other variations of the data not modeled by y (unconstrained variations) are encoded in z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Invertible Conditional GANs</head><p>We introduce Invertible Conditional GANs (IcGANs), which are composed of a cGAN and an encoder. Even though encoders have recently been introduced into the GAN framework <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>, we are the first ones to include and leverage the conditional information y into the design of the encoding process. In section 4.1 we explain how and why an encoder is included in the GAN framework for a conditional setting. In section 4.2, we introduce our approach to refine cGANs on two aspects: conditional position and conditional sampling. The model architecture is described in section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoder</head><p>A generator x = G(z, y ) from a GAN framework does not have the capability to map a real image x to its latent representation z. To overcome this problem, we can train an encoder/inference network E that approximately inverses this mapping (z, y) = E(x). This inversion would allow us to have a latent representation z from a real image x and, then, we would be able to explore the latent space by interpolating or adding variations on it, which would result in variations on the generated image x . If combined with a cGAN, once the latent representation z has been obtained, explicitly controlled variations can be added to an input image via conditional information y (e.g. generate a certain digit in MNIST or specify face attributes on a face dataset). We call this combination Invertible cGAN, as now the mapping can be inverted: (z, y) = E(x) and x = G(z, y), where x is an input image and x its reconstruction. See <ref type="figure" target="#fig_1">Figure 2</ref> for an example on how a trained IcGAN is used.</p><p>Our approach consists of training an encoder E once the cGAN has been trained, as similarly considered by Reed et al <ref type="bibr" target="#b15">[16]</ref>. In our case, however, the encoder E is composed of two sub-encoders: E z , which encodes an image to z, and E y , which encodes an image to y. To train E z we use the generator to create a dataset of generated images x and their latent vectors z, and then minimize a squared reconstruction loss L ez (Eq. 3). For E y , we initially used generated images x and their conditional information y for training. However, we found that generated images tend to be noisier than real ones and, in this specific case, we could improve E y by directly training with real images and labels from the dataset p data (Eq. 4).</p><formula xml:id="formula_2">L ez = E z∼pz,y ∼py z − E z (G(z, y )) 2 2 (3) L ey = E x,y∼p data y − E y (x) 2 2<label>(4)</label></formula><p>Although E z and E y might seem completely independent, we can adopt different strategies to make them interact and leverage the conditional information (for an evaluation of them, see section 5.3):</p><p>• SNG: One single encoder with shared layers and two outputs. That is, E z and E y are embedded in a single encoder. Recently, Dumoulin et al. <ref type="bibr" target="#b11">[12]</ref> and Donahue et al. <ref type="bibr" target="#b12">[13]</ref> proposed different approaches on how to train an encoder in the GAN framework. One of the most interesting approaches consists in jointly training the encoder with both the discriminator and the generator. Although this approach is promising, our work has been completely independent of these articles and focuses on another direction, since we consider the encoder in a conditional setting. Consequently, we implemented our aforementioned approach which performs nearly equally <ref type="bibr" target="#b12">[13]</ref> to their strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Conditional GAN</head><p>We consider two main design decisions concerning cGANs. The first one is to find the optimal conditional position y on the generator and discriminator, which, to our knowledge, has not been previously addressed. Secondly, we discuss the best approach to sample conditional information for the generator.</p><p>Conditional position In the cGAN, the conditional information vector y needs to be introduced in both the generator and the discriminator. In the generator, y ∼ p data and z ∼ p z (where p z = N (0, 1)) are always concatenated in the filter dimension at the input level <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. As for the discriminator, different authors insert y in different parts of the model <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. We expect that the earlier y is positioned in the model the better since the model is allowed to have more learning interactions with y. Experiments regarding the optimal y position will be detailed in section 5.2.</p><p>Conditional sampling There are two types of conditional information, y and y . The first one is trivially sampled from (x, y) ∼ p data and is used for training the discriminator D(x, y) with a real image x and its associated label y. The second one is sampled from y ∼ p y and serves as input to the generator G(z, y ) along with a latent vector z ∼ p z to generate an image x , and it can be sampled using different approaches:</p><p>• Kernel density estimation: also known as Parzen window estimation, it consists in randomly sampling from a kernel (e.g. Gaussian kernel with a cross-validated σ). • Direct interpolation: interpolate between label vectors y from the training set <ref type="bibr" target="#b15">[16]</ref>. The reasoning behind this approach is that interpolations can belong to the label distribution p y . • Sampling from the training set y ∼ p y , p y = p data : Use directly the real labels y from the training set p data . As Gauthier <ref type="bibr" target="#b17">[18]</ref> pointed out, unlike the previous two approaches, this method could overfit the model by using the conditional information to reproduce the images of the training set. However, this is only likely to occur if the conditional information is, to some extent, unique for each image. In the case where the attributes of an image are binary, one attribute vector y could describe a varied and large enough subset of images, preventing the model from overfitting given y. Kernel density estimation and direct interpolation are, at the end, two different ways to interpolate on p y . Nevertheless, interpolation is mostly suitable when the attribute information y is composed of real vectors y ∈ R n , not binary ones. It is not the case of the binary conditional information of the datasets used in this paper (see section 5.1 for dataset information). Directly interpolating binary vectors would not create plausible conditional information, as an interpolated vector y ∈ R n would not belong to p y ∈ {0, 1} n nor p data ∈ {0, 1} n . Using a kernel density estimation would not make sense either, as all the binary labels would fall in the corners of a hypercube. Therefore, we will directly sample y from p data .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model architecture</head><p>Conditional GAN The work of this paper is based on the Torch implementation of the DCGAN 1 <ref type="bibr" target="#b1">[2]</ref>. We use the recommended configuration for the DCGAN, which trains with the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> (β 1 = 0.5, β 2 = 0.999, = 10 −8 ) with a learning rate of 0.0002 and a mini-batch size of 64 (samples drawn independently at each update step) during 25 epochs. The output image size used as a baseline is 64 × 64. Also, we train the cGAN with the matching-aware discriminator method from Reed et al. <ref type="bibr" target="#b15">[16]</ref>. In <ref type="figure" target="#fig_2">Figure 3</ref> we show an overview architecture of both generator and discriminator for the cGAN. For a more detailed description of the model see <ref type="table" target="#tab_0">Table 1</ref>. Encoder For simplicity, we show the architecture of the IND encoders <ref type="table" target="#tab_1">(Table 2)</ref>, as they are the ones that give the best performance. Batch Normalization and non-linear activation functions are removed from the last layer to guarantee that the output distribution is similar to p z = N (0, 1). Additionally, after trying different configurations, we have replaced the last two convolutional layers with two fully connected layers at the end of the encoder, which yields a lower error. The training configuration (Adam optimizer, batch size, etc) is the same as the one used for the cGAN model. We use two image datasets of different complexity and variation, MNIST <ref type="bibr" target="#b4">[5]</ref> and CelebFaces Attributes (CelebA) <ref type="bibr" target="#b5">[6]</ref>. MNIST is a digit dataset of grayscale images composed of 60,000 training images and 10,000 test images. Each sample is a 28 × 28 centered image labeled with the class of the digit (0 to 9). CelebA is a dataset composed of 202,599 face colored images and 40 attribute binary vectors. We use the aligned and cropped version and scale the images down to 64 × 64. We also use the official train and test partitions, 182K for training and 20K for testing. Of the original 40 attributes, we filter those that do not have a clear visual impact on the generated images, which leaves a total of 18 attributes. We will evaluate the quality of generated samples of both datasets. However, a quantitative evaluation will be performed on CelebA only, as it is considerably more complex than MNIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluating the conditional GAN</head><p>The goals of this experiment are two. First, we evaluate the general performance of the cGAN with an attribute predictor network (Anet) on CelebA dataset. Second, we test the impact of adding y in different layers of the cGAN (section 4.2, conditional position).</p><p>We use an Anet 2 as a way to make a quantitative evaluation in a similar manner as Salimans et al.</p><p>Inception model <ref type="bibr" target="#b8">[9]</ref>, as the output given by this Anet (i.e., which attributes are detected on a generated sample) is a good indicator of the generator ability to model them. In other words, if the predicted Anet attributes y are closer to the original attributes y used to generate an image x , we expect that the generator has successfully learned the capability to generate new images considering the semantic meaning of the attributes. Therefore, we use the generator G to create images x conditioned on attribute vectors y ∼ p data (i.e. x = G(z, y)), and make the Anet predict them. Using the Anet output, we build a confusion matrix for each attribute and compute the mean accuracy and F1-Score to test the model and its inserted optimal position of y in both generator and discriminator. In <ref type="table" target="#tab_2">Table 3</ref> we can see how cGANs have successfully learned to generate the visual representations of the conditional attributes with an overall accuracy of ∼ 86%. The best accuracy is achieved by inserting y in the first convolutional layer of the discriminator and at the input level for the generator. Thus, we are going to use this configuration for the IcGAN. Both accuracy and F1-Score are similar as long as y is not inserted in the last convolutional layers, in which case the performance considerably drops, especially in the generator. Then, these results reinforce our initial intuition of y being added at an early stage of the model to allow learning interactions with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluating the encoder</head><p>In this experiment, we prioritize the visual quality of reconstructed samples as an evaluation criterion. Among the different encoder configurations of section 4.1, IND and IND-COND yield a similar qualitative performance, being IND slightly superior. A comparison of these different configurations is shown in <ref type="figure" target="#fig_3">Figure 4a</ref> and in <ref type="figure" target="#fig_3">Figure 4b</ref> we focus on IND reconstructed samples. On another level, the fact that the generator is able, via an encoder, to reconstruct unseen images from the test set shows that the cGAN is generalizing and suggests that it does not suffer from overfitting, i.e., it is not just memorizing and reproducing training samples. Additionally, we compare the different encoder configurations in a quantitative manner by using the minimal squared reconstruction loss L e as a criterion. Each encoder is trained minimizing L e with respect to latent representations z (L ez ) or conditional information y (L ey ). Then, we quantitatively evaluate different model architectures using L e as a metric on a test set of 150K CelebA generated images. We find that the encoder that yields the lowest L e is also IND (0.429), followed closely by IND-CND (0.432), and being SNG the worst case (0.500).</p><p>Furthermore, we can see an interesting property of minimizing a loss based on the latent space instead of a pixel-wise image reconstruction: reconstructed images tend to accurately keep high-level features of an input image (e.g. how a face generally looks) in detriment to more local details such as the exact position of the hair, eyes or face. Consequently, a latent space based encoder is invariant to these local details, making it an interesting approach for encoding purposes. For example, notice how the reconstructions in the last row of CelebA samples in <ref type="figure" target="#fig_3">Figure 4b</ref> fill the occluded part of the face by a hand. Another advantage with respect to element-wise encoders such as VAE is that GAN based reconstructions do not look blurry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluating the IcGAN</head><p>In order to test that the model is able to correctly encode and re-generate a real image by preserving its main attributes, we take real samples from MNIST and CelebA test sets and reconstruct them with modifications on the conditional information y. The result of this procedure is shown in <ref type="figure">Figure 5</ref>, where we show a subset of 9 of the 18 for CelebA attributes for image clarity. We can see that, in MNIST, we are able to get the hand-written style of real unseen digits and replicate these style on all the other digits. On the other hand, in CelebA we can see how reconstructed faces generally match the specified attribute. Additionally, we noticed that faces with uncommon conditions (e.g., looking away from the camera, face not centered) were the most likely to be noisy. Furthermore, attributes such as mustache often fail to be generated especially on women samples, which might indicate that the generator is limited to some unusual attribute combinations.</p><p>Manipulating the latent space The latent feature representation z and conditional information y learned by the generator can be further explored beyond encoding real images or randomly sampling z. In order to do so, we linearly interpolate both z and y with pairs of reconstructed images from the CelebA test set <ref type="figure">(Figure 6a</ref>). All the interpolated faces are plausible and the transition between faces is smooth, demonstrating that the IcGAN learned manifold is also consistent between interpolations. Then, this is also a good indicator that the model is generalizing the face representation properly, as it is not directly memorizing training samples.</p><p>In addition, we perform in <ref type="figure">Figure 6b</ref> an attribute transfer between pairs of faces. We infer the latent representation z and attribute information y of two real faces from the test set, swap y between those faces and re-generate them. As we previously noticed, the results suggest that z encodes pose, illumination and background information, while y tends to represent unique features of the face.  <ref type="figure">Figure 5</ref>: The result of applying an IcGAN to a set of real images from MNIST (a) and CelebA (b). A real image is encoded into a latent representation z and conditional information y, and then decoded into a new image. We fix z for every row and modify y for each column to obtain variations.</p><p>(a) (b) <ref type="figure">Figure 6</ref>: Different ways of exploring the latent space. In (a) we take two real images and linearly interpolate both z and y to obtain a gradual transformation from one face to another. In (b) we take two real images, reconstruct them and swap the attribute information y between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We introduce an encoder in a conditional setting within the GAN framework, a model which we call Invertible Conditional GANs (IcGANs). It solves the problem of GANs lacking the ability to infer real samples to a latent representation z, while also allowing to explicitly control complex attributes of generated samples with conditional information y. We also refine the performance of cGANS by testing the optimal position in which the conditional information y is inserted in the model. We have found that for the generator, y should be added at the input level, whereas the discriminator works best when y is at the first layer. Additionally, we evaluate several ways to training an encoder. Training two independent encoders -one for encoding z and another for encoding y -has proven to be the best option in our experiments. The results obtained with a complex face dataset, CelebA, are satisfactory and promising.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example of how the IcGAN reconstructs and applies complex variations on a real image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Scheme of a trained IcGAN, composed of an encoder (IND approach) and a cGAN generator. We encode a real image x into a latent representation z and attribute information y, and then apply variations on it to generate a new modified image x .• IND: Two independent encoders. E z and E y are trained separately.• IND-COND: Two encoders, where E z is conditioned on the output of encoder E y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of the generator (a) and discriminator (b) of our cGAN model. The generator G takes as input both z and y. In the discriminator, y is concatenated in the first convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a) Comparison of different encoder configurations, where IND yields the most faithful reconstructions. (b) Reconstructed samples from MNIST and CelebA using IND configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Detailed generator and discriminator architecture</figDesc><table><row><cell></cell><cell></cell><cell>Generator</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Discriminator</cell></row><row><cell>Operation</cell><cell cols="5">Kernel Stride Filters BN Activation Operation</cell><cell cols="3">Kernel Stride Filters BN Activation</cell></row><row><cell>Concatenation</cell><cell cols="4">Concatenate z and y on 1st dimension</cell><cell cols="2">Convolution 4 × 4</cell><cell>2 × 2</cell><cell>64</cell><cell>No Leaky ReLU</cell></row><row><cell cols="2">Full convolution 4 × 4</cell><cell>2 × 2</cell><cell>512</cell><cell>Yes ReLU</cell><cell cols="4">Concatenation Replicate y and concatenate to 1st conv. layer</cell></row><row><cell cols="2">Full convolution 4 × 4</cell><cell>2 × 2</cell><cell>256</cell><cell>Yes ReLU</cell><cell cols="2">Convolution 4 × 4</cell><cell>2 × 2</cell><cell>128</cell><cell>Yes Leaky ReLU</cell></row><row><cell cols="2">Full convolution 4 × 4</cell><cell>2 × 2</cell><cell>128</cell><cell>Yes ReLU</cell><cell cols="2">Convolution 4 × 4</cell><cell>2 × 2</cell><cell>256</cell><cell>Yes Leaky ReLU</cell></row><row><cell cols="2">Full convolution 4 × 4</cell><cell>2 × 2</cell><cell>64</cell><cell>Yes ReLU</cell><cell cols="2">Convolution 4 × 4</cell><cell>2 × 2</cell><cell>512</cell><cell>Yes Leaky ReLU</cell></row><row><cell cols="2">Full convolution 4 × 4</cell><cell>2 × 2</cell><cell>3</cell><cell>No Tanh</cell><cell cols="2">Convolution 4 × 4</cell><cell>1 × 1</cell><cell>1</cell><cell>No Sigmoid</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Encoder IND architecture. Last two layers have different sizes depending on the encoder (z for E z or y for E y ). n y represents the size of y.</figDesc><table><row><cell>Operation</cell><cell cols="3">Kernel Stride Filters</cell><cell>BN Activation</cell></row><row><cell>Convolution</cell><cell>5 × 5</cell><cell>2 × 2</cell><cell>32</cell><cell>Yes ReLU</cell></row><row><cell>Convolution</cell><cell>5 × 5</cell><cell>2 × 2</cell><cell>64</cell><cell>Yes ReLU</cell></row><row><cell>Convolution</cell><cell>5 × 5</cell><cell>2 × 2</cell><cell>128</cell><cell>Yes ReLU</cell></row><row><cell>Convolution</cell><cell>5 × 5</cell><cell>2 × 2</cell><cell>256</cell><cell>Yes ReLU</cell></row><row><cell cols="2">Fully connected -</cell><cell>-</cell><cell cols="2">z: 4096, y: 512 Yes ReLU</cell></row><row><cell cols="2">Fully connected -</cell><cell>-</cell><cell>z: 100, y: n y</cell><cell>No None</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative cGAN evaluation depending on y inserted position. The first row shows the results obtained with real CelebA images as an indication that Anet predictions are subject to error.</figDesc><table><row><cell></cell><cell></cell><cell>Discriminator</cell><cell></cell><cell>Generator</cell></row><row><cell>Model</cell><cell cols="4">Mean accuracy Mean F1-Score Mean accuracy Mean F1-Score</cell></row><row><cell>CelebA test set</cell><cell>92.78%</cell><cell>71.47%</cell><cell>92.78%</cell><cell>71.47%</cell></row><row><cell>y inserted on input</cell><cell>85.74%</cell><cell>49.63%</cell><cell>89.83%</cell><cell>59.69%</cell></row><row><cell cols="2">y inserted on layer 1 86.01%</cell><cell>52.42%</cell><cell>87.16%</cell><cell>52.40%</cell></row><row><cell cols="2">y inserted on layer 2 84.90%</cell><cell>50.00%</cell><cell>82.49%</cell><cell>52.36%</cell></row><row><cell cols="2">y inserted on layer 3 85.96%</cell><cell>52.38%</cell><cell>82.49%</cell><cell>38.01%</cell></row><row><cell cols="2">y inserted on layer 4 77.61%</cell><cell>19.49%</cell><cell>73.90%</cell><cell>4.03%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Torch code for DCGAN model available at https://github.com/soumith/dcgan.torch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The architecture of the Anet is the same as Ey fromTable 2.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work is funded by the Projects TIN2013-41751-P of the Spanish Ministry of Science and the CHIST ERA project PCIN-2015-226.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DRAW: A Recurrent Neural Network For Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6114" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1406.5298" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
		<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606.03498" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606.03657" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarially Learned Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606.00704" />
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1605.09782</idno>
		<ptr target="http://arxiv.org/abs/1605.09782" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.05644" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1512.09300" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1605.05396" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno>abs/1411.1784</idno>
		<ptr target="http://arxiv.org/abs/1411.1784" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets for convolutional face generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
		<ptr target="http://cs231n.stanford.edu/reports/jgauthie_final_report.pdf" />
	</analytic>
	<monogr>
		<title level="m">Class project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://www.arxiv.org/pdf/1412.6980.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
