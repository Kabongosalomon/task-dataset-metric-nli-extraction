<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Gómez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Meoni</surname></persName>
						</author>
						<title level="a" type="main">MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep learning</term>
					<term>multispectral image classifica- tion</term>
					<term>scene classification</term>
					<term>semi-supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised learning techniques are at the center of many tasks in remote sensing. Unfortunately, these methods, especially recent deep learning methods, often require large amounts of labeled data for training. Even though satellites acquire large amounts of data, labeling the data is often tedious, expensive and requires expert knowledge. Hence, improved methods that require fewer labeled samples are needed. We present MSMatch, the first semi-supervised learning approach competitive with supervised methods on scene classification on the EuroSAT benchmark dataset. We test both RGB and multispectral images and perform various ablation studies to identify the critical parts of the model. The trained neural network achieves state-of-the-art results on EuroSAT with an accuracy that is between 1.98% and 19.76% better than previous methods depending on the number of labeled training examples. With just five labeled examples per class we reach 94.53% and 95.86% accuracy on the EuroSAT RGB and multispectral datasets, respectively. With 50 labels per class we reach 97.62% and 98.23% accuracy. Our results show that MSMatch is capable of greatly reducing the requirements for labeled data. It translates well to multispectral data and should enable various applications that are currently infeasible due to a lack of labeled data. We provide the source code of MSMatch online to enable easy reproduction and quick adoption.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE last decade has seen a momentous increase in the availability of remote sensing data, thus enhancing the effectiveness of image processing and analysis methods using deep learning <ref type="bibr" target="#b0">[1]</ref>. The former is driven by continuously decreasing launch costs, especially for so-called Smallsats (&lt; 500kg). As Wekerle et al. <ref type="bibr" target="#b1">[2]</ref> describe, less than 40 Smallsats were launched per year between 2000 and 2012 but over a hundred in 2013 and almost 200 in 2014. Since then the numbers have been increasing with over 300 launches in 2018 and 2017 <ref type="bibr" target="#b2">[3]</ref>. Many of these are imaging satellites serving either commercial purposes <ref type="bibr" target="#b3">[4]</ref> or related to earth observation programs, such as the European Space Agencies' Copernicus program <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>. This has led to an increase in the availability of large datasets. Concurrently, image processing and analysis have improved dramatically with the advent of deep learning methods <ref type="bibr" target="#b5">[6]</ref>. As a consequence, there is a large corpus of research describing successful applications of deep learning methods to remote sensing data <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>. However, training deep neural P. <ref type="bibr">Gómez</ref>  networks usually requires large amounts of labeled samples, where the expected solution has been manually annotated by experts <ref type="bibr" target="#b10">[11]</ref>. This is in particular tedious for imaging modalities such as radar data or multispectral (MS) imaging data, which is not as easily labeled by humans as, e.g., RGB imaging data. One way to alleviate these issues is the application of socalled semi-supervised learning (SSL) techniques. These aim to train machine learning methods, e.g. neural networks, while providing only a small set of labeled training samples and a typically larger corpus of unlabeled training samples. Although this has also garnered a lot of attention in the remote sensing community <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b15">[16]</ref>, some recent advances in SSL have not yet found their way to the remote sensing community. In the last two years, the state-of-the-art in SSL has advanced significantly 1 to a point, where the proposed methods are virtually competitive with fully supervised approaches <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Thus, adapting the new SSL approaches to remote sensing bears the promise to save large amounts of time and cost that would be required for manual labeling.</p><p>In this work, we propose MSMatch, a novel approach that combines these advances <ref type="bibr" target="#b18">[19]</ref> together with recent neural network architectures (so-called EfficientNets <ref type="bibr" target="#b19">[20]</ref>) for the problem of land scene classification, i.e. correctly identifying land use or land cover of satellite or airborne images. This is an active research problem with a broad range of research focusing on it <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. We compare with previous methods on the EuroSAT benchmark dataset <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> collected by the Sentinel-2A satellite. The dataset also includes MS data, which is commonly used for tasks related to vegetation mapping <ref type="bibr" target="#b7">[8]</ref>.</p><p>In summary, the main contributions of this work are:</p><p>• First SSL approach that is competitive with supervised methods on scene classification with EuroSAT • MSMatch can reach 94.53% and 95.86% accuracy respectively on EuroSAT RGB and MS with only five labels per class, greatly reducing the need for labeled data. • Adapting recent advances in neural network architectures and SSL approaches to remote sensing, extending their applicability to MS data • Analysis of critical components of the proposed pipeline by performing ablation studies and identification of heterogeneous properties in the classes in EuroSAT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The need for a large amount of labeled training data is one of the most significant bottlenecks in bringing deep learning approaches to practical applications <ref type="bibr" target="#b8">[9]</ref>. For satellite imaging data this problem is particularly aggravated as satellites have greatly varying sensors and applications, which makes a transfer between a model trained on data from one application or sensor to another challenging <ref type="bibr" target="#b25">[26]</ref>. Hence, SSL is of particular interest in remote sensing. In the following, we describe relevant works that applied SSL to scene classification problems. Further, we point out the works that led to significant improvements in SSL in the broader machine learning community in the last years.</p><p>A. Semi-supervised learning applied to remote sensing There are several datasets that have been established as benchmark datasets for scene classification. Aside from the aforementioned EuroSAT dataset, the most commonly used ones are the UC Merced Land Use Dataset (UCM) <ref type="bibr" target="#b26">[27]</ref> and the Aerial Image Dataset (AID) <ref type="bibr" target="#b27">[28]</ref>. Both, UCM and AID, use aerial imaging data and provide, respectively, 2100 and 10000 images for a classification of 21 and 30 classes. Aside from the SSL works mentioned, there is also a multitude of studies using supervised methods for these datasets (e.g. <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>). In terms of SSL approaches, there have been several interesting approaches: Guo et al. <ref type="bibr" target="#b31">[32]</ref> trained a generative adversarial network (GAN) that performs particularly well with few labels on UCM and on EuroSAT. Han et al. <ref type="bibr" target="#b32">[33]</ref> used self-labeling to achieve even better results on UCM using comparatively more labels. They report similarly good results with comparatively large label counts (10% of the whole data) on AID. Dai et al. <ref type="bibr" target="#b33">[34]</ref> used ensemble learning and residual networks with much fewer labels on UCM and on AID. A self-supervised learning paradigm was suggested by Tao et al. <ref type="bibr" target="#b15">[16]</ref> for AID and EuroSAT. Another GAN-based approach has been suggested in the work of Roy et al. <ref type="bibr" target="#b14">[15]</ref>, who applied it to EuroSAT, but other approaches have already accomplished better results, such as the work by Zhang and Yang <ref type="bibr" target="#b13">[14]</ref>, who utilized the EuroSAT MS data -however with 300 labels per class. Yamashkin et al. <ref type="bibr" target="#b34">[35]</ref> also suggested an SSL approach where they extended dataset, but their results are not competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recent advances in semi-supervised learning</head><p>Semi-supervised learning is a field that has been gaining a lot of interest in recent years. However, the last two years have seen several methods being published that led to unprecedented results and, for the first time, achieved results that are competitive with supervised methods trained on significantly more data. For example, the accuracy on the popular CIFAR-10 dataset <ref type="bibr" target="#b35">[36]</ref> for training with just 250 labels has improved from 47% to 95% 2 from 2016 to 2020. Many of these improvements rely on smart data augmentation strategies such as RandAugment <ref type="bibr" target="#b36">[37]</ref> or AutoAugment <ref type="bibr" target="#b37">[38]</ref>. The most significant improvements were made in 2019 in two works describing the so-called MixMatch <ref type="bibr" target="#b16">[17]</ref> method and Unsupervised Data Augmentation <ref type="bibr" target="#b38">[39]</ref>. The former improved the state-of-the-art by over 20% and the latter pushed the accuracy above 90%. In a series of follow-up works <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b39">[40]</ref> results were further improved until the current state-of-art method was introduced in 2020. Utilizing the ideas of pseudo-labeling and consistency regularization by Bachman et al. <ref type="bibr" target="#b40">[41]</ref>, FixMatch <ref type="bibr" target="#b18">[19]</ref> achieved state-of-the-art result on four benchmark datasets including almost 95% accuracy on CIFAR-10 with 250 labels. This is comparable to the performance of a supervised approach for the utilized network architecture. Furthermore, using just four labels per class, they still achieved 89% accuracy. To the authors' knowledge, none of the mentioned works have found their way into the remote sensing community yet. Hence, this work aims to build on these recent advances to achieve stateof-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>This section will introduce the utilized dataset, network architecture, SSL method and setup of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. EuroSAT Dataset</head><p>Some of the most commonly utilized benchmarks for SSL in remote sensing are UCM, AID and EuroSAT. Given the comparatively large images, the computational burden of the heavy image augmentation and training the model is larger for UCM and AID. Further, they are built from aerial data and only provide 2100 and 10000 RGB images, respectively. Hence, we focused instead on the EuroSAT dataset, which consists of 27000 64 × 64 pixel images in RGB and 13band MS format. The MS bands are between 443 nm and 2190 nm, the spatial resolution is up to 10 meters per pixel depending on the band. Note that especially the infrared bands are well-suited to vegetation identification. The data stem from the Sentinel-2A satellite and are split into ten classes, such as River, Forest, Permanent Crop and similar ones. Some examples of images from the EuroSAT dataset can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>. The data were obtained from the authors' GitHub respository 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EfficientNet</head><p>EfficientNets <ref type="bibr" target="#b19">[20]</ref> have become the go-to neural network architecture for many applications. They achieve state-ofart results, especially in terms of efficiency as EfficientNets use comparatively few parameters in relation to achieved performance. The architecture was conceived using neural architecture search <ref type="bibr" target="#b41">[42]</ref>, a method where the neural network architecture itself is optimized. Tan and Le propose several versions of EfficientNets called B0 to B7 with increasing numbers of parameters and performance. Thereby, it is possible to choose a suitable trade-off that keeps memory and computational requirements manageable at a sufficient model complexity. Note, however, that EfficientNets have not seen broad adoption in the remote sensing community, yet. None of the mentioned prior SSL works utilized them. And, to the authors' knowledge, EfficientNets have also not been used in combination with the FixMatch approach. We utilize them for MSMatch given their excellent performance and low memory footprint. We relied on an open-source implementation utilizing PyTorch 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. FixMatch</head><p>There are two central ideas behind the effectiveness of the FixMatch approach, pseudo-labeling and consistency regularization <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Pseudo-labeling refers to practice of using the model (or another model) to automatically label otherwise unlabeled data. The second idea, consistency regularization, refers to the concept that the model should predict the same output for similar inputs. FixMatch training consists of a supervised and unsupervised loss. While the supervised loss is a common crossentropy loss, the unsupervised loss incorporates both pseudo-labeling and consistency regularization. This is achieved by creating two different augmentations of the same image, a so-called weak and a strong one. As depicted in <ref type="figure" target="#fig_1">Figure 2</ref>, the weakly augmented image is used to create a pseudo-label for the image. Consistency regularization is then employed by computing a crossentropy loss between a pseudo-label on the weakly augmented image and the model's classification of the strongly augmented image. The total loss from the supervised loss L s and unsupervised loss L u is then obtained as L = L s + L u . For the implementation of FixMatch we adapted an opensource PyTorch implementation 5 . To the authors' knowledge this is also the first work applying FixMatch to MS images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Augmentation</head><p>Data augmentation is frequently used to help neural networks generalize better to unseen data and to increase the richness of the utilized training data <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. During the FixMatch training process the augmentation is critical to ensure that the little amount of available labeled data is exploited optimally using the weak augmentations, and to aid  generalization to unseen data using the strong augmentations. For the weak augmentation, we only utilized horizontal flips and image translations by up to 12.5%. Note, that Kurakin et al. <ref type="bibr" target="#b18">[19]</ref> describe in their work that, e.g., they tried to harness stronger augmentations for the labeled data but experienced training divergence. We encountered similar issues utilizing, e.g., image crops. For the strong augmentation of the RGB images, several methods from the Python library Pillow were applied. For the strong augmentation of the MS data, a slightly reduced set (due to a lack of implementations for more than three image channels) were applied utilizing the albumentations Python module <ref type="bibr" target="#b42">[43]</ref>. For details, we refer to the source code of MSMatch mentioned in the next paragraph. Exemplary weak and strong augmentations of RGB EuroSAT images are depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training</head><p>All models were trained for three different random seeds on NVIDIA RTX 2080 TI graphics cards using PyTorch 1.7. The training utilized a stochastic gradient descent optimizer with a Nesterov momentum of 0.9 and different weight decay amounts. A learning rate of 0.03 was used and reduced as in the original FixMatch paper with cosine annealing. Training batch size was 32, with one batch containing that many images and additionally seven times as many unlabeled ones. The training was run for a total of 500 epochs with 1000 iterations each, after which all investigated models had converged. All images were normalized to the mean and standard deviation of the EuroSAT dataset. The test sets were stratified. The test sets for each seed contained 10% of the data and hence 2700 images. For a supervised baseline the unsupervised loss L u was set to 0 to allow a fair and direct comparison. With this setup, training of one model requires up to 48 hours on a single GPU. We provide the code for this work open source online 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>We report results for both, the RGB and MS, versions of EuroSAT. Aside from a detailed comparison with previous research depicted in <ref type="table" target="#tab_1">Table I</ref> we also investigated the impact of the weight decay strength in MSMatch as well as the number of parameters of the utilized EfficientNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Number of Labels</head><p>The main factor for comparing SSL approaches is the number of labeled training examples used for training. We tested a large range of different amounts ranging from 50 (five per class) to 3000 (300 per class) labels to ensure comparability with prior research. The rest of the (unlabeled) data are used for the unsupervised part of the training. As seen in <ref type="table" target="#tab_1">Table I</ref>, MSMatch outperforms all prior research by large margins for all tested amounts of available labels. The greatly enhanced accuracy per training sample is especially prominent for the cases with just 50 and 100 labels (five and ten per class). In these cases MSMatch improves on previous methods between 16% and 20%. Using 1000 (100 per class) labels, which was the most popular amount in prior research, it improves the state-of-the-art by 7%. The last three rows in <ref type="table" target="#tab_1">Table I</ref> showcase the impact of using MS data and difference if, instead of using our SSL approach, we train an EfficientNet using only the labeled samples and no unlabeled samples. Notably, the MS data improves results even further. The proposed method is hence successfully adapted to MS data. Note, that results on supervised baseline, i.e. training without any unlabeled data, are clearly worse. This demonstrates the effect of the proposed SSL framework. Even for 3000 labels, the SSL method performs significantly better than the baseline. Additionally, <ref type="figure">Figure 3</ref> shows F1 scores for all classes and amount of training labels. Notably, some classes, such as PermanentCrop, seem to require more samples to reach optimal results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyperparameters</head><p>Two further factors were investigated in detail. One factor that was found to be particularly critical by Kurakin et al. <ref type="bibr" target="#b18">[19]</ref> is the strength of the weight decay, which penalizes large weights in the neural network. They found 5.0 · 10 −4 to be optimal in most cases. The other one is model size, which is a decisive factor for model performance and can be varied using the different versions (B0 to B7) of the EfficientNet architecture. Note that only B0 to B3 fit into the available GPU</p><formula xml:id="formula_0">)6FRUH $QQXDO&amp;URS )RUHVW +HUEDFHRXV9HJHWDWLRQ +LJKZD\ ,QGXVWULDO 3DVWXUH 3HUPDQHQW&amp;URS</formula><p>5HVLGHQWLDO 5LYHU 6HD/DNH &amp;ODVV RIODEHOV <ref type="figure">Fig. 3</ref>. Classification F1 scores for models trained with a different amount of labeled samples. Classes are stratified, RGB data was used. Notably, some classes, such as PermanentCrop, seem to require more samples than others, such as SeaLake, to reach good scores. Results are averaged over three seeds. memory with the utilized training settings and, thus, no larger models were compared. Note that results between <ref type="table" target="#tab_1">Tables I-III</ref> were run on different machines which led to slightly different random seeds and mean values. Results for different weight decay values are displayed in <ref type="table" target="#tab_1">Table II</ref>. In our experiments we found that slightly larger weight decay values were beneficial than originally proposed by Kurakin et al. <ref type="bibr" target="#b18">[19]</ref>. The highest accuracy was obtained with a weight decay of 7.5 · 10 −4 which lead to an accuracy 96.63.</p><p>Detailed results for the model size are given in <ref type="table" target="#tab_1">Table III</ref>. We found the EfficientNet-B2 with 9.2 million parameters to provide the best trade-off of performance and model size with an accuracy of 96.91 %. However, performance gains from a larger number of parameters are limited and smaller than standard deviation among random seeds. Thus, choosing a smaller model when optimizing for efficiency can also be reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>Overall, MSMatch outperforms all previously published SSL works tested on EuroSAT. Even with a low number of labels (five per class), it is able to achieve a high accuracy at  <ref type="table" target="#tab_1">Table III</ref>) it is noteworthy that improvements from a larger number of parameters are only marginal. Possibly, the reason is that the investigated classification problem features just ten classes. The larger models may be overparameterized for the problem. However, it is also conceivable that the proposed training procedure performs better on smaller models. This will require further investigation in the future. Another interesting factor is the varying performance depending on the class as depicted in <ref type="figure">Figure 3</ref>. Clearly, some classes require more labeled training data than others, which hints at a possible improvement to the described method. The number of labeled samples could be adjusted for each class in relation to the model's performance on it. For example, after observing the worse performance on the class PermanentCrop when training with 50 labels in an operational scenario, this insight could be used to selectively label more data from such an underperforming class. In practice, the lower performance for these classes might also hint at an underrepresentation of some necessary features in the supplied training data. Kurakin et al. <ref type="bibr" target="#b18">[19]</ref> also observed a strong impacted of the selected samples on the performance. In <ref type="figure" target="#fig_2">Figure 4</ref> we display some saliency maps (using guided backpropagation <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>), where a model trained on 50 labels misclassified the examples whereas a 3000-label model succeeded. Note that the 3000-label model clearly relies on specific contour features in the image that the 50-label model does not recognize. One element that may warrant more detailed examination in the future is the type of augmentations utilized for the strong augmentation. Kurakin et al. <ref type="bibr" target="#b18">[19]</ref> described some performance impact of the utilized augmentation method. Due to the computational demands of running a large number of test runs the authors were unable to investigate this in more detail. However, it would warrant further investigation, especially in regards to the interplay with MS data.</p><p>VI. CONCLUSION This works presents MSMatch, a novel SSL approach that is able to vastly improve the state-of-the-art on the EuroSAT dataset compared to previous works. Depending on number of labels, it improves accuracy by between 1.47% and 18.43% compared to previous works. More importantly, it showcases that an accuracy of 95.85% is obtainable with just five labels per class, which bears the promise to make MSMatch applicable to scenarios where a lack of labeled data previously inhibited training neural networks for the task. The method also translates well to MS data, which is, however, harder to process given a lack of GPU-based data augmentation frameworks. Future research will aim to test more datasets such as UCM or AID, which are computationally more demanding, especially in terms of GPU memory. Adapting MSMatch to a segmentation problem is also conceivable given suitable augmentation methods and might be of interest to broaden the range of possible applications even further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>4 https://github.com/lukemelas/EfficientNet-PyTorch (Accessed 11.03.2021) 5 https://github.com/LeeDoYup/FixMatch-pytorch (Accessed 11.03.2021) Examples of weak and strong augmentation applied to different EuroSAT images. In this picture, strong augmentation effects applied to the class River are AutoContrast, Color, and Solarize; the class Permanent Crop is augmented through Sharpness, ShearX,and Equalize; finally, the class Industrial is augmented through Posterize, Rotate, and Brightness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The modified FixMatch pipeline to produce the unsupervised loss Lu(ỹ,ŷ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Guided backpropagation saliency maps for a model trained on 50 and 3000 labels, respectively. RGB data was used. Images are examples where the 50-label model misclassified the images while the 3000-label model succeeded. The saliency map clearly displays the features that the 3000-label model utilized which were missed by the 50-label one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and Gabriele Meoni are with the Advanced Concepts Team, European Space Agency, Noordwijk, The Netherlands e-mail: pablo.gomez@esa.int, gabriele.meoni@esa.int Manuscript received ...; revised ....</figDesc><table /><note>* Equal contribution</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ACCURACY</head><label>I</label><figDesc>RESULTS COMPARISON ON EUROSAT IN PERCENT. WORKS USING MS DATA ARE MARKED WITH AN ASTERISK. MSMATCH OUTPERFORMS ALL OTHER METHODS ON EUROSAT. THE BEST RESULTS PER LABEL AMOUNT ARE BOLD.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Number of labels</cell><cell></cell></row><row><cell>Work</cell><cell></cell><cell>50</cell><cell>100</cell><cell>500</cell><cell cols="3">1000 2000 3000</cell></row><row><cell cols="2">Guo et al. 2020 [32]</cell><cell>-</cell><cell cols="2">76.79 -</cell><cell cols="3">88.72 90.66 -</cell></row><row><cell cols="2">Roy et al. 2018 [15]</cell><cell>-</cell><cell cols="2">68.60 -</cell><cell cols="3">86.10 89.00 -</cell></row><row><cell cols="2">Tao et al. 2020 [16]</cell><cell cols="2">76.10 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Zhang &amp; Yang 2020* [14] -</cell><cell cols="4">80.12 89.01 91.11 -</cell><cell>96.67</cell></row><row><cell cols="2">Supervised Baseline</cell><cell cols="6">40.75 54.63 77.99 87.41 91.74 93.94</cell></row><row><cell>Ours (RGB)</cell><cell></cell><cell cols="6">94.53 96.04 97.62 97.63 98.07 98.14</cell></row><row><cell>Ours (MS)*</cell><cell></cell><cell cols="6">95.86 96.63 98.23 98.33 98.47 98.65</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">COMPARISON OF DIFFERENT WEIGHT DECAY VALUES IN TERMS OF</cell></row><row><cell cols="8">ACCURACY IN PERCENT. ALL RUNS USED AN EFFICIENTNET-B2 AND 250</cell></row><row><cell></cell><cell cols="5">LABELS. THE BEST RESULT IS BOLD.</cell><cell></cell><cell></cell></row><row><cell>Weight</cell><cell>5.0 ·</cell><cell>7.5 ·</cell><cell>1.0 ·</cell><cell>2.5 ·</cell><cell>5.0 ·</cell><cell>7.5 ·</cell><cell>1.0 ·</cell></row><row><cell>decay</cell><cell>10 −5</cell><cell>10 −5</cell><cell>10 −4</cell><cell>10 −4</cell><cell>10 −4</cell><cell>10 −4</cell><cell>10 −3</cell></row><row><cell>Accuracy</cell><cell>94.63 ±0.79</cell><cell>93.49 ±1.95</cell><cell>95.53 ±1.78</cell><cell>95.21 ±1.88</cell><cell>95.26 ±3.24</cell><cell>96.63 ±0.34</cell><cell>96.53 ±0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF EFFICIENTNETS IN TERMS OF ACCURACY AND PARAMETERS. ALL MODELS WERE TRAINED ON 250 LABELS WITH A WEIGHT DECAY OF 7.5 · 10 −4 . THE BEST RESULT IS BOLD. 78±0.04 96.85±0.46 96.85±0.56 96.91±0.27 94.53% or 95.86% for RGB and MS data, respectively. This makes the approach applicable in scenarios where only very limited labeled data is available. The superior performance using MS data both highlights the potential of utilizing such data as well as the suitability of the proposed method for it. Compared to the original FixMatch approach, we find that slightly higher weight decay values benefit the training. Kurakin et al. [19] described this for the CIFAR-100 dataset, which features 100 classes. However, EuroSAT, as the other datasets investigated by Kurakin et al., only features ten classes. The benefit of the higher weight decay in our experiments may also be related to the different model choices, as Kurakin et al. relied on a different network architecture. In terms of the comparison of model parameters in the network architecture (see</figDesc><table><row><cell>Model</cell><cell>B0</cell><cell>B1</cell><cell>B2</cell><cell>B3</cell></row><row><cell># of Model Parameters</cell><cell>5.3M</cell><cell>7.8M</cell><cell>9.2M</cell><cell>12M</cell></row><row><cell>Accuracy [%]</cell><cell>96.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">see https://paperswithcode.com/task/semi-supervised-image-classification (Accessed 17.03.2021)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">see https://paperswithcode.com/sota/semi-supervised-image-classification-on-cifar-6 (Accessed 11.03.2021)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/phelber/EuroSAT (Accessed 11.03.2021)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/gomezzz/MSMatch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing: A comprehensive review and list of resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Status and trends of smallsats and their launch vehicles-an up-to-date review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wekerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pessoa Filho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E V L D</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Trabasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Aeros. Tech. Manag</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="286" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Are smallsats entering the maturity stage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Puteaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najjar</surname></persName>
		</author>
		<ptr target="https://spacenews.com/analysis-are-smallsats-entering-the-maturity-stage/" />
	</analytic>
	<monogr>
		<title level="m">SpaceNews</title>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Earth-observing companies push for more-advanced science satellites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Popkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">545</biblScope>
			<biblScope unit="issue">7655</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Esa&apos;s earth observation strategy and copernicus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aschbacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Satellite earth observations and their impact on society and policy</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="81" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sidike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Nasrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Van Esesn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A S</forename><surname>Awwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01164</idno>
		<title level="m">The history began from alexnet: A comprehensive survey on deep learning approaches</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">42609</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep feature extraction and classification of hyperspectral images based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6232" to="6251" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing applications: A meta-analysis and review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="166" to="177" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning for remote sensing data: A technical tutorial on the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hyperspectral images classification with gabor filtering and convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2355" to="2359" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A semi-supervised convolutional neural network for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="839" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning using pseudo labels for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1259" to="1270" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised multi-spectral land cover classification with multi-attention and adaptive kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1881" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic-fusion gans for semi-supervised satellite image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification with self-supervised paradigm under limited labeled samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neur. Inf. Proc. Sys. (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neur. Inf. Proc. Sys. (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Remote sensing image scene classification meets deep learning: Challenges, methods, benchmarks, and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.01094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey on deep learning-driven remote sensing image scene understanding: Scene classification, scene retrieval and scene-guided object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2110</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Introducing eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="204" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2217" to="2226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards the use of artificial intelligence on the edge in space systems: Challenges and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Furano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dunne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moloney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferlet-Cavrois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavoularis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Psarakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-O</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Aerosp. Electron. Syst. Mag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="44" to="56" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems</title>
		<meeting>the 18th SIGSPATIAL international conference on advances in geographic information systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aid: A benchmark data set for performance evaluation of aerial scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3965" to="3981" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A full convolutional network based on densenet for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Biosci. Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3345" to="3367" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scale-free convolutional neural network for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="6916" to="6928" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional neural network for remote-sensing scene classification: Transfer learning analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pires De Lima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marfurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gan-based semisupervised scene classification of remote sensing image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A semi-supervised generative framework with deep learning features for high-resolution remote sensing image scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="23" to="43" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semisupervised scene classification for remote sensing images: A method based on convolutional neural networks and ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="869" to="873" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving the efficiency of deep learning methods in remote sensing data analysis: Geosystem approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Yamashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Yamashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Zanozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Radovanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Barmin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="179" to="516" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Realmix: Towards realistic semi-supervised deep learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beltramelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08766</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning with pseudoensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neur. Inf. Proc. Sys. (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3365" to="3373" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Albumentations: fast and flexible image augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pytorch cnn visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ozbulak</surname></persName>
		</author>
		<ptr target="https://github.com/utkuozbulak/pytorch-cnn-visualizations" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">He received his M.Sc. in computer science from the Technical University Munich in 2015. Research topics of interest to him range from machine learning and inverse problems to numerical methods and high performance computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phd</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESA&apos;s</title>
		<imprint/>
	</monogr>
	<note>Advanced Concepts Team at ESTEC, Noordwijk. He received his PhD from the Friedrich-Alexander-Universität Erlangen-Nürnberg in 2019 (supervisor Prof. Döllinger)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">PhD, received the Laurea degree in electronic engineering from the University of Pisa in 2016 and the Ph.D. degree in information engineering in 2020. During his Ph.D., he developed skills in digital and embedded systems design, digital signal processing, and artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Meoni</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Since 2020 he is a research fellow in the ESA Advanced Concept Team. His research interests include machine learning. embedded systems and edge computing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
