<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">No more meta-parameter tuning in unsupervised sparse feature learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-02-24">24 Feb 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
							<email>adriana.romero@ub.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Departament de Matemàtica Aplicada i Anàlisi</orgName>
								<orgName type="institution">Universitat de Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petia</forename><surname>Radeva</surname></persName>
							<email>petia.ivanova@ub.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Departament de Matemàtica Aplicada i Anàlisi</orgName>
								<orgName type="institution">Universitat de Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">CGATTA@CVC.UAB.ES Centre de Visió per Computador</orgName>
								<address>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">No more meta-parameter tuning in unsupervised sparse feature learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-02-24">24 Feb 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm, which exploits a new way of optimizing for sparsity. Experiments on STL-10 show that the method presents state-of-the-art performance and provides discriminative features that generalize well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Significant effort has been devoted to handcraft appropriate feature representations of data in several fields. In tasks such as image classification and object recognition, unsupervised learned features have shown to compete well or even outperform manually designed ones <ref type="bibr" target="#b24">(Ranzato et al., 2006;</ref><ref type="bibr" target="#b28">Yang et al., 2009;</ref>. Unsupervised feature learning has also shown to be helpful in greedy layerwise pre-training of deep architectures <ref type="bibr" target="#b10">(Hinton et al., 2006;</ref><ref type="bibr" target="#b1">Bengio et al., 2006;</ref><ref type="bibr" target="#b15">Larochelle et al., 2009;</ref><ref type="bibr" target="#b6">Erhan et al., 2010)</ref>.</p><p>In <ref type="bibr" target="#b0">(Bengio, 2009)</ref>, the author claims that potentially interesting research involves pre-training algorithms, which " <ref type="bibr">[...]</ref> would be proficient at extracting good features but involving an easier optimization problem." In addition to that, one of the main criticisms to state-of-the-art methods is that they require a significant amount of metaparameters <ref type="bibr" target="#b2">(Bengio et al., 2013)</ref>. As stated in <ref type="bibr" target="#b26">(Snoek et al., 2012)</ref>, the tuning of these meta-parameters is a laborious task that requires expert knowledge, rules of thumb or extensive search and, whose setting can vary for dif-ferent tasks. Therefore, there is great interest for metaparameter free methods  and automatic approaches to optimize the performance of learning algorithms <ref type="bibr" target="#b26">(Snoek et al., 2012)</ref>.</p><p>Nevertheless, little effort has been devoted to address this problem (see <ref type="table">Table 1</ref> for a comparison of meta-parameters required by unsupervised feature learning methods). To the best of our knowledge, work in this direction includes ICA  and sparse filtering . Although ICA provides good results at object recognition tasks <ref type="bibr" target="#b16">(Le et al., 2011;</ref><ref type="bibr" target="#b20">Ngiam et al., 2011)</ref>, the method scales poorly to large datasets and high input dimensionality.</p><p>Computational complexity is also a major drawback of many state-of-the-art methods. ICA requires an expensive orthogonalization to be computed at each iteration. Sparse coding has an expensive inference, which requires a prohibitive iterative optimization. Significant amount of work has been done in order to overcome this limitation <ref type="bibr" target="#b18">(Lee et al., 2006;</ref><ref type="bibr" target="#b13">Kavukcuoglu et al., 2010)</ref>. Predictive Sparse Decomposition (PSD) <ref type="bibr" target="#b13">(Kavukcuoglu et al., 2010)</ref> is a successful variant of sparse coding, which uses a predictor to approximate the sparse representation and solves the sparse coding computationally expensive encoding step.</p><p>In this paper, we aim to solve some of the above-mentioned problems. We propose a meta-parameter free, off-theshelf, simple and fast approach, which exploits a new way of optimizing for a sparsity, without explicitly modeling the data distribution. The method iteratively builds an ideally sparse target and optimizes the dictionary by minimizing the error between the system output and the ideally sparse target. Defining sparsity concepts in terms of expected output allows to exploit a new strategy in unsupervised training. <ref type="table">Table 1</ref>. Meta-parameters to tune of state-of-the-art unsupervised feature learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Meta-parameters to tune</head><p>Sparse RBM <ref type="bibr" target="#b10">(Hinton et al., 2006;</ref><ref type="bibr" target="#b19">Lee et al., 2008)</ref> weight decay, sparseness constant, sparsity penalty, momentum Sparse auto-encoders <ref type="bibr" target="#b24">(Ranzato et al., 2006)</ref> weight decay, sparseness constant, sparsity penalty Sparse Coding <ref type="bibr" target="#b21">(Olshausen &amp; Field, 1997)</ref> sparsity penalty RICA <ref type="bibr" target="#b16">(Le et al., 2011)</ref> reconstruction penalty PSD <ref type="bibr" target="#b13">(Kavukcuoglu et al., 2010)</ref> sparsity penalty, prediction penalty OMP-k <ref type="bibr" target="#b22">(Pati et al., 1993;</ref><ref type="bibr" target="#b3">Blumensath &amp; Davies, 2007;</ref> k (non-zero elements) ICA  -Sparse Filtering It is worth stressing that many optimization strategies can be used to minimize the above-mentioned error and that parameters of these optimization techniques must not be considered as belonging to our approach.</p><p>Experiments on STL-10 dataset show that the method outperforms state-of-the-art methods in single layer image classification, providing discriminative features that generalize well.</p><p>Linear feature extraction methods combined with sparse coding encodings are among best performers on object recognition datasets. The importance of properly combining training/encoding and encoding/pooling strategies has been argued in  and <ref type="bibr" target="#b29">(Zeiler &amp; Fergus, 2013)</ref> respectively. Since the goal of this paper is to propose a new method for unsupervised feature learning, dealing with all the possible combinations of encoding and pooling could mask the benefits of the method that we propose. However, for the sake of fair comparison with the state-of-the-art, we test the method with sparse coding and soft-threshold encodings combined with sum pooling, following the experimental pipeline of ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">State-of-the-art</head><p>Commonly used algorithms for unsupervised feature learning include Restricted Boltzmann Machines (RBM) <ref type="bibr" target="#b10">(Hinton et al., 2006)</ref>, auto-encoders <ref type="bibr" target="#b1">(Bengio et al., 2006)</ref>, sparse coding <ref type="bibr" target="#b23">(Raina et al., 2007)</ref> and hybrids such as PSD <ref type="bibr" target="#b13">(Kavukcuoglu et al., 2010)</ref>. Many other methods such as ICA , Reconstruction ICA (RICA) <ref type="bibr" target="#b16">(Le et al., 2011)</ref>, Sparse Filtering  and methods related to vector quantization such as Orthogonal Matching Pursuit (OMP-k) <ref type="bibr" target="#b22">(Pati et al., 1993;</ref><ref type="bibr" target="#b3">Blumensath &amp; Davies, 2007;</ref> have also been used in the literature to extract unsupervised feature representations. These algorithms could be divided into two categories: explicitly modeling or not the input distribution. Sparse auto-encoders <ref type="bibr" target="#b24">(Ranzato et al., 2006)</ref>, sparse RBM <ref type="bibr" target="#b10">(Hinton et al., 2006;</ref><ref type="bibr" target="#b19">Lee et al., 2008;</ref><ref type="bibr" target="#b9">Hinton, 2010;</ref><ref type="bibr" target="#b8">Goh et al., 2012)</ref>, sparse coding <ref type="bibr" target="#b21">(Olshausen &amp; Field, 1997)</ref>, PSD <ref type="bibr" target="#b13">(Kavukcuoglu et al., 2010)</ref>, OMP-k <ref type="bibr" target="#b22">(Pati et al., 1993;</ref><ref type="bibr" target="#b3">Blumensath &amp; Davies, 2007;</ref> and Reconstruction ICA (RICA) <ref type="bibr" target="#b16">(Le et al., 2011</ref>) explicitly model the data distribution by minimizing the reconstruction error. Although learning a good approximation of the data distribution may be desirable, approaches such as sparse filtering  show that this seems not so important if the goal is to have a discriminative sparse system. Sparse filtering does not attempt to explicitly model the input distribution but focuses on the properties of the output distribution instead.</p><p>Sparsity is among the desirable properties of a good output representation <ref type="bibr" target="#b7">(Field, 1994;</ref><ref type="bibr" target="#b21">Olshausen &amp; Field, 1997;</ref><ref type="bibr" target="#b24">Ranzato et al., 2006;</ref><ref type="bibr" target="#b19">Lee et al., 2008;</ref><ref type="bibr" target="#b16">Le et al., 2011;</ref><ref type="bibr" target="#b20">Ngiam et al., 2011;</ref><ref type="bibr" target="#b2">Bengio et al., 2013)</ref>. Sparse features consist of a large amount of outputs, which respond rarely and provide high responses when they do respond. Sparsity can be described in terms of population sparsity and lifetime sparsity <ref type="bibr" target="#b27">(Willmore &amp; Tolhurst, 2001)</ref>. Both lifetime and population sparsity are important properties of the output distribution. On one hand, lifetime sparsity plays an important role in preventing bad solutions such as numerous dead outputs. There seems to be a consensus to overcome such degenerate solutions, which is to ensure similar statistics among outputs <ref type="bibr" target="#b7">(Field, 1994;</ref><ref type="bibr" target="#b27">Willmore &amp; Tolhurst, 2001;</ref><ref type="bibr" target="#b24">Ranzato et al., 2006;</ref><ref type="bibr" target="#b20">Ngiam et al., 2011)</ref>. On the other hand, population sparsity helps providing a simple interpretation of the input data such as the ones found in early visual areas. To the best of our knowledge, the definition of population sparsity remains ambiguous.</p><p>State-of-the-art methods optimize either for one or both sparsity forms in their objective function. The great majority seeks sparsity using the L 1 penalty and does not optimize for an explicit level of sparsity in their outputs. Sparse auto-encoders optimize for a target activation allowing to deal with lifetime sparsity; nevertheless, the target activation requires tuning and does not explicitly control the level of population sparsity. OMP-k defines the level of population sparsity by setting k to the maximum expected number of non-zero elements per output code, whereas the methods in <ref type="bibr" target="#b21">(Olshausen &amp; Field, 1997;</ref><ref type="bibr" target="#b24">Ranzato et al., 2006;</ref><ref type="bibr" target="#b19">Lee et al., 2008;</ref><ref type="bibr" target="#b16">Le et al., 2011;</ref><ref type="bibr" target="#b20">Ngiam et al., 2011)</ref> do not explicitly define the proportion of outputs expected to be active at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we describe how the proposed method learns a sparse feature representation of the data in terms of population and lifetime sparsity. The method iteratively builds an ideally sparse target and optimizes the dictionary by minimizing the error between the system output and the ideally sparse target. Subsection 3.1 highlights the algorithm to enforce lifetime and population sparsity in the ideally sparse target. Subsection 3.2 provides implementation details on the system and optimization strategies used to minimize the error between the system output and the ideally sparse target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Enforcing Population and Lifetime Sparsity by defining an ideal target</head><p>We define population and lifetime sparsity as properties of an ideal sparse output. Given N training samples and an output of dimensionality N h , we define the first property of the output as: The rationale of our approach is to appropriately generate an ideal output target that fulfils properties <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula">(2)</ref>, and then learn the parameters of the system by minimizing the L 2 error between the output target and the output generated by the system during training. In this way, we seek a system optimized for both population and lifetime sparsity in an explicit way.</p><p>The key component of our approach is how to define the ideal output target based on the above-mentioned properties. However, to ensure that the optimization of the system parameters converges, we add a third property:</p><p>3. Minimal Perturbation: The ideal output target should be defined as the best approximation of the system output by means of L 2 error fulfilling properties (1) &amp; (2).</p><p>Creating the output target that ensures the above-mentioned properties is analogous to solving an assignment problem. The Hungarian method <ref type="bibr" target="#b14">(Kuhn, 1955</ref>) is a combinatorial optimization algorithm, which solves the assignment problem. However, its computational cost O((N N h ) 3/2 ) is prohibitive. Therefore, in the next section we propose a simple and fast O(N N h ) algorithm to generate the ideal output target, which ensures sparsity properties <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula">(2)</ref> and provides an approximate solution for minimal perturbation property (3).</p><p>3.1.1. IDEAL TARGET GENERATION: THE ENFORCING POPULATION AND LIFETIME SPARSITY (EPLS) ALGORITHM Let us assume that we have a system, which produces a row output vector h. We use the notation h j to refer to one element of h. We define an output matrix H composed of N b output vectors of dimensionality N h , such that N b ≤ N . Likewise, we define an ideal target output matrix T of the same size. Algorithm 1 details the EPLS algorithm to generate the ideal target T from H. For the sake of simplicity, every step of the algorithm where the subscript j appears must be applied ∀j ∈ {1, 2, . . . , N h }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 EPLS</head><p>Require: H, a, N Ensure: T, a 1: T = 0 2: for n = 1 → N b do 3:</p><p>hj = Hn,j 4: k = arg maxj (hj − aj ) 5:</p><p>T n,k = 1 6: a k = a k + N h N + ǫ 7: end for 8: Remap T to active/inactive values of the corresponding function.</p><p>Starting with no activation in T (line 1), the algorithm proceeds as follows. A row vector h from H is processed at each iteration (line 3). The crucial step is performed in line 4: the output k that has to be activated in the n th row of T is selected as the one that has the maximal activation value h j minus the inhibitor a j . The inhibitor a j can be seen as an accumulator that "counts" the number of times an output j has been selected, increasing its inhibition progressively by N h /N until reaching maximal inhibition. This prevents the selection of an output that has already been activated N/N h times. The rationale behind the equation in line 4 is that, while selecting the maximal responses in the matrix H, we have to take care to distribute them evenly among all outputs (in order to ensure Strong Lifetime Sparsity). Using this strategy, it can be demonstrated that the resulting matrix T perfectly fulfills properties <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula">(2)</ref>. In line 5, the algorithm activates the k th element of n th row of the target matrix T. By activating the "relative" maximum, we approximate property (3). Finally, the inhibitor a is updated in line 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">System and Optimization strategies</head><p>Let us assume that we have a system parameterized by Γ = {W, b}, with activation function f , which takes as input a data vector d and produces an output vector h = f (d, Γ). We use the same notation as in Section 3 and define a data matrix D composed of N rows and N d columns, where N d is the input dimensionality.</p><p>To compare our training strategy to previous well known systems, we tested our algorithm using</p><formula xml:id="formula_0">H = f (DW + b) ,<label>(1)</label></formula><p>where f is a logistic non-linearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">OPTIMIZATION STRATEGY</head><p>The system might be trained by means of an off-the-shelf mini-batch Stochastic Gradient Descent (SGD) method with adaptive learning rates such as variance-based SGD (vSGD) <ref type="bibr" target="#b25">(Schaul et al., 2013)</ref>. Algorithm 2 details the latter training process. The mini-batch size N b can be set to any value, in all the experiments we have set N b = N h . Starting with Γ set to small random numbers as in (LeCun et al., 1998) (line 1), at each epoch we shuffle the samples of the training set (line 3), reset the EPLS inhibitor a to a flat activation (line 4) and process all mini-batches. For each mini-batch b, samples D (b) are selected (line 6). Then, the output H (b) is computed (line 7) and the EPLS is invoked to compute T (b) and update a (line 8). After that, the gradient of the error is computed (line 9) and the learning rate η is estimated as in <ref type="bibr" target="#b25">(Schaul et al., 2013</ref>) (line 10). The system parameters are then updated to minimize the L 2 error E (b) = ||H (b) −T (b) || 2 2 (line 11). Finally, the bases W in Γ are limited to have unit norm to avoid degenerate solutions (line 13). This procedure is repeated until a stop condition is met; in our experiments, the training stops when the rel-ative decrement error between epochs is small (&lt; 10 −6 ). When updating the system parameters, we assume that T does not depend on Γ, thus ∂T ∂Γ = 0; we carried out experiments that show that this approximation does not significantly influence the gradient descent convergence nor the quality of the minimization. Moreover, this assumption makes the algorithm faster, since we remove the need of computing the numerical partial derivatives of T.</p><p>The mini-batch vSGD allows to scale the algorithm easily, especially with respect to the number of samples N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Standard EPLS training</head><p>Require: D Ensure: Γ 1: Γ = small random values 2: repeat 3: Shuffle D randomly 4: a = flat activation 5:</p><formula xml:id="formula_1">for b = 1 → ⌊N/N b ⌋ do 6: Select mini-batch samples D (b) 7: H (b) = f (D (b) , Γ) 8: (T (b) , a) = EP LS(H (b) , a, N ) 9: G = ∇Γ||H (b) − T (b) || 2 2</formula><p>10: Estimate learning rate η as in <ref type="bibr" target="#b25">(Schaul et al., 2013)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Γ = Γ − ηG 12: end for 13:</p><p>Limit the bases W in Γ to have unit norm 14: until stop condition verified</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The performance of training and encoding strategies in single layer networks has been extensively analyzed in the literature <ref type="bibr" target="#b20">Ngiam et al., 2011)</ref> on STL-10 1 dataset. STL-10 dataset consists of 96x96 pixels color images belonging to 10 different classes. The dataset is divided into a large unlabeled training set containing 100K images and smaller labeled training and test sets, containing 5000 and 8000 images, respectively. It has to be considered that in STL-10, the primary challenge is to make use of the unlabeled data (100K images), which is 100 times bigger than the labeled data used to train the classifier (1000 images per fold). In this case, the supervised training must strongly rely on the ability of the unsupervised method to learn discriminative features. Moreover, since the unlabeled dataset contains other types of animals (bears, rabbits, etc.) and vehicles <ref type="bibr">(trains, buses, etc.)</ref> in addition to the ones in the labeled set, the unsupervised method should be able to generalize well.</p><p>To validate our method, we follow the experimental pipeline of . We first extract random patches and normalize them for local brightness and con-  trast. Note that EPLS does not require any whitening of the input data, since it decorrelates the data during the training by means of the imposed strong sparsity properties of the output target. Then, we apply the system to retrieve sparse features of patches covering the input image, pool them into 4 quadrants and finally train a L 2 SVM for classification purposes. We tune the SVM parameter using 5fold cross-validation. As in , we use a receptive field of 10x10 pixels and a stride of 1. The number of outputs is set to N h = 1600 for fair comparison with the other state-of-the-art methods. We also provide the results of our method with sign split (N h = 1600x2, using W and −W for encoding as in ) and using the sparse coding (SC) encoder, which  found to be the best when small number of labeled data is available. For this encoder, we searched over the same set of parameter values as , i.e., λ = {0.5, 0.75, 1.0, 1.25, 1.5}. The parameter λ is tuned to consider the use of sparse coding as encoder after the training and, thus, does not belong to the method that we propose. <ref type="table" target="#tab_1">Table 2</ref> summarizes the results obtained on this dataset compared to other state-of-the-art methods. When pairing each training method with its associated natural encoding, EPLS outperforms all the other methods. When pairing the training methods with sparse coding, EPLS outperforms the state-of-the-art best performer in single layer networks as well, achieving 61.0% (0.58%) accuracy. More-over, the standard deviation of the folds is lower than the one provided by OMP-1 with sparse coding encoding. Results are even more impressive if we compare them to metaparameter free algorithms. <ref type="figure" target="#fig_0">Figure 1</ref> shows a subset of 100 randomly selected bases learned by our method, 10x10 pixel receptive field and a system of N h = 1600 outputs. As shown in the figure, the method learns not only common bases such as oriented edges/ridges in many directions and colors but also corner detectors, tri-banded colored filters, center surrounds and Laplacian of Gaussians among others. This suggests that enforcing lifetime sparsity helps the system to learn a set of complex, rich and diversified bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Computational complexity</head><p>The EPLS algorithm requires the computation of T, which has O(N N h ) cost, and therefore scales linearly on both N and N h . Since we can use vSGD for optimization, the method scales linearly on N given a fixed number of epochs. Finally, applying the activation function, the cost of computing the derivative is linear with N d , since we use a closed form for ∂E ∂Γ . The memory complexity is related to the mini-batch size N b . Consequently, the method can scale gracefully to very large datasets: theoretically, it requires to store in memory the mini-batch input data D <ref type="bibr">(b)</ref> </p><formula xml:id="formula_2">(N b N d elements), out- put H (b) (N b N h elements), target T (b) (N b N<label>h</label></formula><p>elements) and the system parameters to optimize Γ (N h (N d + 1) elements); a total amount of N h (N d + 1) + N b (N d + 2N h ) elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Our results show that simultaneously enforcing both population and lifetime sparsity helps in learning discriminative dictionaries, which reflect in better performance, especially when compared to meta-parameter free methods <ref type="bibr" target="#b16">Le et al., 2011)</ref>. Experiments suggest that our algorithm is able to extract features that generalize well on unseen data. When comparing the performance STL-10 dataset, our algorithm outperforms state-of-the-art best performers. Results suggest that our algorithm helps the classifier in generalizing with a few training examples (1% of the dataset), gaining 2% accuracy w.r.t. the stateof-the art best performer (OMP-1 paired with sparse coding) with a lower standard deviation across folds, suggesting more robustness to variations in the training folds.</p><p>It is important to highlight that OMP-1 can be seen as a special case of our algorithm, where the activation function is |DW| and lifetime sparsity is not taken into account in the optimization process (potentially leading to dead out-puts). Our algorithm has several advantages over OMP-1:</p><p>(1) It can use any activation function; (2) by enforcing lifetime sparsity it does not suffer of the dead output problem, thus not requiring ad-hoc tricks to avoid it; (3) it does not require whitening, which can be a problem if the input dimensionality is large <ref type="bibr" target="#b16">(Le et al., 2011)</ref>.</p><p>With our proposal, we advance in the meta-parameter free line of ICA  and sparse filtering . It is clear that the advantage of sparse filtering over ICA comes from removing the orthogonality constraint, and imposing some sort of "competition" between outputs, which also permits overcomplete representations. Following this spirit, our algorithm imposes an even more strict form of competition to prevent dead outputs by means of Strong Lifetime Sparsity and confirms the trend of  that data reconstruction seems not so important if the goal is to have a discriminative sparse system.</p><p>Last and most importantly, it is worth highlighting five interesting properties of the EPLS algorithm. First, the method is meta-parameter free, which highly simplifies the training process for practitioners, especially when used as a greedy pre-training method in deep architectures. Second, the method is fast and scales linearly with the number of training samples and the input/output dimensionalities. Third, EPLS is easy to implement. We implemented the EPLS in Algorithm 1 in less than 50 lines of C code. The mini-batch vSGD is a general purpose optimizer; our Matlab implementation of vSGD plus the EPLS mex source will be publicly available after publication. Fourth, the proposed learning strategy is not limited to perceptrons. Fifth, there is an interest in the literature in avoiding redundancy in the image representation by using the algorithms in a convolutional fashion <ref type="bibr" target="#b13">(Kavukcuoglu et al., 2010)</ref>. For this purpose, the EPLS can be slightly modified to apply the procedure to a whole image at once and consider the minibatch size to be the image divided into patches. This aspect is not considered in the paper and is left for future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we introduced the Enforcing Population and Lifetime Sparsity method. The algorithm provides a metaparameter free, off-the-shelf, simple and computationally efficient approach for unsupervised sparse feature learning. It seeks both lifetime and population sparsity in an explicit way in order to learn discriminative features, thus preventing dead outputs.</p><p>Results show that the method significantly outperforms all state-of-the-art methods on STL-10 dataset with lower standard deviation across folds, suggesting more robust-ness across training sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Random subset of bases learned by EPLS, a receptive field of 10 pixels and N h = 1600 (better seen in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The output vectors must be composed solely of active and inactive units (no intermediate values between two fixed scalars are allowed) and all outputs must activate for an equal number of inputs. Activation is exactly distributed among the N h outputs.Our Strong Lifetime Sparsity definition is a more strict requirement than the high dispersal concept introduced in, since they only require that "the mean squared activations of each feature (output)[...]  should be roughly the same for all features (outputs)". While high dispersal attempts to diversify the learned bases, it does not guarantee the output distribution, in the lifetime sense, to be composed of only a few activations. Furthermore, our definition ensures the absence of dead outputs.</figDesc><table><row><cell>Given our definition of Strong Lifetime Sparsity, the popu-</cell></row><row><cell>lation sparsity must require that, for each training sample,</cell></row><row><cell>only one output element is active:</cell></row><row><cell>2. Strong Population Sparsity: For each training sam-</cell></row><row><cell>ple only one output must be active.</cell></row></table><note>1. Strong Lifetime Sparsity:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy on STL-10.</figDesc><table><row><cell>Algorithm</cell><cell></cell><cell>Accuracy</cell></row><row><cell cols="2">Single-Layer with meta-parameters</cell><cell></cell></row><row><cell cols="2">RICA (Le et al., 2011) (1600/Natural)</cell><cell>52.9%</cell></row><row><cell>OMP-1 (1600/Natural)</cell><cell></cell><cell>51.8% (0.47%)</cell></row><row><cell cols="2">OMP-1 (whitening, 1600/Natural)</cell><cell>53.1% (0.52%)</cell></row><row><cell cols="2">OMP-1 (whitening, 1600x2/Natural)</cell><cell>54.5% (0.66%)</cell></row><row><cell cols="2">OMP-1 (whitening, 1600x2/SC)</cell><cell>59.0% (0.80%)</cell></row><row><cell cols="2">Single-Layer without meta-parameters</cell><cell></cell></row><row><cell>Raw pixels</cell><cell></cell><cell>31.8% (0.62%)</cell></row><row><cell cols="2">ICA (whitening, Complete/Natural)</cell><cell>48.0% (1.47%)</cell></row><row><cell>K-means-tri (whitening, 1600)</cell><cell></cell><cell>51.5% (1.73%)</cell></row><row><cell cols="2">Sparse Filtering (1600/Natural)</cell><cell>53.5% (0.53%)</cell></row><row><cell>Natural</cell><cell>Natural</cell><cell>SC</cell></row><row><cell>(1600)</cell><cell>(1600x2)</cell><cell>(1600x2)</cell></row><row><cell cols="3">EPLS 56.6% (0.66%) 56.9% (0.50%) 61.0% (0.58%)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.stanford.edu/∼acoates/stl10/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the difference between orthogonal matching pursuit and orthogonal least squares. Unpublished manuscript</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blumensath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
		<ptr target="http://www.see.ed.ac.uk/˜tblumens/papers/BDOMPvsOLS07.pdf" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An analysis of singlelayer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIS-TATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The importance of encoding versus training with sparse coding and vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="921" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What is the goal of sensory coding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="559" to="601" />
			<date type="published" when="1994-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised and supervised visual codes with restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlin</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joo-Hwee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="298" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Practical Guide to Training Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Independent component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Wiley Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning convolutional feature hierachies for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>-Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring strategies for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ICA with reconstruction cost for efficient overcomplete feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse deep belief net model for visual area v2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ekanadham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sparse filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1125" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sparse coding with an overcomplete basis set: a strategy employed by v1? Vision Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rezaifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Krishnaprasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACSSC</title>
		<imprint>
			<date type="published" when="1993-11" />
			<biblScope unit="page" from="40" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-taught learning: Transfer learning from unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Honglak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energybased model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">No More Pesky Learning Rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2960" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Characterizing the sparseness of neural codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Willmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Tolhurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="255" to="270" />
			<date type="published" when="2001-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1301.3557</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
