<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SetVAE: Learning Hierarchical Composition for Generative Modeling of Set-Structured Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Kim</surname></persName>
							<email>jinwoo-kim@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Yoo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><forename type="middle">Lee</forename><surname>Seunghoon</surname></persName>
							<email>seunghoon.hong@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Kaist</surname></persName>
						</author>
						<title level="a" type="main">SetVAE: Learning Hierarchical Composition for Generative Modeling of Set-Structured Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative modeling of set-structured data, such as point clouds, requires reasoning over local and global structures at various scales. However, adopting multi-scale frameworks for ordinary sequential data to a set-structured data is nontrivial as it should be invariant to the permutation of its elements. In this paper, we propose SetVAE, a hierarchical variational autoencoder for sets. Motivated by recent progress in set encoding, we build SetVAE upon attentive modules that first partition the set and project the partition back to the original cardinality. Exploiting this module, our hierarchical VAE learns latent variables at multiple scales, capturing coarse-to-fine dependency of the set elements while achieving permutation invariance. We evaluate our model on point cloud generation task and achieve competitive performance to the prior arts with substantially smaller model capacity. We qualitatively demonstrate that our model generalizes to unseen set sizes and learns interesting subset relations without supervision. Our implementation is available at https://github.com/ jw9730/setvae.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There have been increasing demands in machine learning for handling set-structured data (i.e., a group of unordered instances). Examples of set-structured data include object bounding boxes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>, point clouds <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>, support sets in the meta-learning <ref type="bibr" target="#b5">[6]</ref>, etc. While initial research mainly focused on building neural network architectures to encode sets <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17]</ref>, generative models for sets have recently grown popular <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>A generative model for set-structured data should verify the two essential requirements: (i) exchangeability, meaning that a probability of a set instance is invariant to its elements' ordering, and (ii) handling variable cardinality, meaning that a model should flexibly process sets with variable cardinalities. These requirements pose a unique chal-* Equal contribution  <ref type="figure">Figure 1</ref>: Color-coded attention learned by SetVAE encoder for three data instances of ShapeNet Airplane <ref type="bibr" target="#b2">[3]</ref>. Level 1 shows attention at the most coarse scale. Level 2 and 3 show attention at more fine-scales. lenge in set generative modeling, as they prevent the adaptation of standard generative models for sequences or images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27]</ref>. For instance, typical operations in these models, such as convolution or recurrent operations, exploit implicit ordering of elements (e.g., adjacency), thus breaking the exchangeability. Several works circumvented this issue by imposing heuristic ordering <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22]</ref>. However, when applied to set-structured data, any ordering assumed by a model imposes an unnecessary inductive bias that might harm the generalization ability.</p><p>There are several existing works satisfying these requirements. Edwards et al., <ref type="bibr" target="#b3">[4]</ref> proposed a simple generative model encoding sets into latent variables, while other approaches build upon various generative models, such as generative adversarial networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>, flow-based models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13]</ref>, and energy-based models <ref type="bibr" target="#b30">[31]</ref>. All these works define valid generative models for set-structured data, but with some limitations. To achieve exchangeability, many approaches process set elements independently <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>, <ref type="table">Table 1</ref>: Summary of several set generative frameworks available to date. Our SetVAE jointly achieves desirable properties, with the advantages of the VAE framework combined with our novel contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Exchangeability Variable cardinality</head><p>Inter-element dependency Hierachical latent structure l-GAN <ref type="bibr" target="#b0">[1]</ref> × × × PC-GAN <ref type="bibr" target="#b17">[18]</ref> × × PointFlow <ref type="bibr" target="#b29">[30]</ref> × × EBP <ref type="bibr" target="#b30">[31]</ref> × SetVAE (ours) limiting the models in reflecting the interactions between the elements during generation. Some approaches take the inter-element dependency into account <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>, but have an upper bound on the number of elements <ref type="bibr" target="#b24">[25]</ref>, or less scalable due to heavy computations <ref type="bibr" target="#b30">[31]</ref>. More importantly, existing models are less effective in capturing subset structures in sets presumably because they represent a set with a single-level representation. For sets containing multiple sub-objects or parts, it would be beneficial to allow a model to have structured latent representations such as one obtained via hierarchical latent variables.</p><p>In this paper, we propose SetVAE, a novel hierarchical variational autoencoder (VAE) for sets. SetVAE models interaction between set elements by adopting attention-based Set Transformers <ref type="bibr" target="#b16">[17]</ref> into the VAE framework, and extends it to a hierarchy of latent variables <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref> to account for flexible subset structures. By organizing latent variables at each level as a latent set of fixed cardinality, SetVAE is able to learn hierarchical multi-scale features that decompose a set data in a coarse-to-fine manner <ref type="figure">(Figure 1</ref>) while achieving exchangeability and handling variable cardinality. In addition, composing latent variables invariant to input's cardinality allows our model to generalize to arbitrary cardinality unseen during training.</p><p>The contributions of this paper are as follows:</p><p>• We propose SetVAE, a novel hierarchical VAE for sets with exchangeability and varying cardinality. To the best of our knowledge, SetVAE is the first VAE successfully applied for sets with arbitrary cardinality. SetVAE has a number of desirable properties compared to previous works, as summarized in <ref type="table">Table 1</ref>.</p><p>• Equipped with novel Attentive Bottleneck Layers (ABLs), SetVAE is able to model the coarse-to-fine dependency across the arbitrary number of set elements using a hierarchy of latent variables.</p><p>• We conduct quantitative and qualitative evaluations of SetVAE on generative modeling of point cloud in various datasets, and demonstrate better or competitive performance in generation quality with less number of parameters than the previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Permutation-Equivariant Set Generation</head><p>Denote a set as x = {x i } n i=1 ∈ X n , where n is the cardinality of the set and X represents the domain of each element x i ∈ R d . In this paper, we represent x as a matrix x = [x 1 , ..., x n ] T ∈ R n×d . Note that any operation on a set should be invariant to the elementwise permutation and satisfy the two constraints of permutation invariance and permutation equivariance.</p><formula xml:id="formula_0">Definition 1. A function f : X n → Y is permutation in- variant iff for any permutation π(·), f (π(x)) = f (x).</formula><p>Definition 2. A function f : X n → Y n is permutation equivariant iff for any permutation π(·), f (π(x)) = π(f (x)).</p><p>In the context of generative modeling, the notion of permutation invariance translates into exchangeability, requiring a joint distribution of the elements invariant with respect to the permutation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3. A distribution for a set of random variables</head><formula xml:id="formula_1">x = {x i } n i=1</formula><p>is exchangeable if for any permutation π, p(x) = p(π(x)).</p><p>An easy way to achieve exchangeability is to assume each element to be i.i.d. and process a set of initial elements</p><formula xml:id="formula_2">z (0) = {z (0) i } n i=1 independently sampled from p(z (0) i )</formula><p>with an elementwise function f elem to get the x:</p><formula xml:id="formula_3">x = {x i } n i=1 where x i = f elem (z (0) i )<label>(1)</label></formula><p>However, assuming elementwise independence poses a limit in modeling interactions between set elements. An alternative direction is to process z (0) with a permutationequivariant function f equiv to get the x:</p><formula xml:id="formula_4">x = {x i } n i=1 = f equiv ({z (0) i } n i=1 ).<label>(2)</label></formula><p>We refer to this approach as the permutation-equivariant generative framework. As the likelihood of x does not depend on the order of its elements (because elements of z (0) are i.i.d.), this approach achieves exchangeability. </p><formula xml:id="formula_5">Multihead FF q k v Q' V Q (a) MAB MAB Q V MAB Q V x I x' h (b) ISAB</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Permutation-Equivariant Set Encoding</head><p>To design permutation-equivariant operations over a set, Set Transformer <ref type="bibr" target="#b16">[17]</ref> provides attentive modules that model pairwise interaction between set elements while preserving invariance or equivariance. This section introduces two essential modules in the Set Transformer.</p><p>First, Multihead Attention Block (MAB) takes the query and value sets, Q ∈ R nq×d and V ∈ R nv×d , respectively, and performs the following transformation ( <ref type="figure" target="#fig_1">Figure 2a</ref>):</p><formula xml:id="formula_6">MAB(Q, V ) = LN(a + FF(a)) ∈ R nq×d ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_7">a = LN(Q + Multihead(Q, V, V )) ∈ R nq×d ,<label>(4)</label></formula><p>where FF denotes elementwise feedforward layer, Multihead denotes multi-head attention <ref type="bibr" target="#b28">[29]</ref>, and LN denotes layer normalization <ref type="bibr" target="#b16">[17]</ref>. Note that the output of Eq. <ref type="formula" target="#formula_6">(3)</ref> is permutation equivariant to Q and permutation invariant to V . Based on MAB, Induced Set Attention Block (ISAB) processes the input set x ∈ R n×d using a smaller set of inducing points I ∈ R m×d (m &lt; n) by ( <ref type="figure" target="#fig_1">Figure 2b)</ref>:</p><formula xml:id="formula_8">ISAB m (x) = MAB(x, h) ∈ R n×d ,<label>(5)</label></formula><p>where h = MAB(I, x) ∈ R m×d .</p><p>The ISAB first transforms the input set x into h by attending from I. The resulting h is a permutation invariant projection of x to a lower cardinality m. Then, x again attends to h to produce the output of n elements. As a result, ISAB is permutation equivariant to x.</p><formula xml:id="formula_10">Property 1. In ISAB m (x), h is permutation invariant to x. Property 2. ISAB m (x) is permutation equivariant to x.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Variational Autoencoders for Sets</head><p>The previous section suggests that there are two essential requirements for VAE for set-structured data: it should be able to model the likelihood of sets (i) in arbitrary cardinality and (ii) invariant to the permutation (i.e., exchangeable). This section introduces our SetVAE objective that satisfies the first requirement while achieving the second requirement is discussed in Section 4.</p><p>The objective of VAE <ref type="bibr" target="#b14">[15]</ref> is to learn a generative model p θ (x, z) = p θ (z)p θ (x|z) for data x and latent variables z. Since the true posterior is unknown, we approximate it using the inference model q φ (z|x) and optimize the variational lower bound (ELBO) of the marginal likelihood p(x):</p><formula xml:id="formula_11">L VAE = E q φ (z|x) [log p θ (x|z)] − KL (q φ (z|x)||p θ (z)) . (7)</formula><p>Vanilla SetVAE When our data is a set x = {x i } n i=1 , Eq. <ref type="formula">(7)</ref> should be modified such that it can incorporate the set of arbitrary cardinality n 1 . To this end, we propose to decompose the latent variable z into the two independent variables as z = {z (0) , z (1) }. We define</p><formula xml:id="formula_12">z (0) = {z (0) i } n i=1</formula><p>to be a set of initial elements, whose cardinality is always the same as a data x. Then we model the generative process as transforming z (0) into a set x conditioned on the z <ref type="bibr" target="#b0">(1)</ref> .</p><p>Given the independence assumption, the prior is factorized by p(z) = p(z (0) )p(z <ref type="bibr" target="#b0">(1)</ref> ). The prior on initial set p(z (0) ) is further decomposed into the cardinality and element-wise distributions as:</p><formula xml:id="formula_13">p(z (0) ) = p(n) n i=1 p(z (0) i ).<label>(8)</label></formula><p>We model p(n) using the empirical distribution of the training data cardinality. We find that the choice of the prior p(z (0) i ) is critical to the performance, and discuss its implementation in Section 4.2.</p><p>Similar to the prior, the approximate posterior is defined as q(z|x) = q(z (0) |x)q(z (1) |x) and decomposed into:</p><formula xml:id="formula_14">q(z (0) |x) = q(n|x) n i=1 q(z (0) i |x)<label>(9)</label></formula><p>We define q(n|x) = δ(n) as a delta function with n = |x|, and set q(z <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>. The resulting ELBO can be written as</p><formula xml:id="formula_15">(0) i |x) = p(z (0) i ) similar to</formula><formula xml:id="formula_16">L SVAE = E q(z|x) [log p(x|z)] − KL(q(z (0) |x)||p(z (0) )) − KL(q(z (1) |x)||p(z (1) )).<label>(10)</label></formula><p>In the supplementary file, we show that the first KL divergence in Eq. (10) is a constant and can be ignored in the optimization. During inference, we sample z (0) by first sampling the cardinality n ∼ p(n) then the n initial elements independently from the prior p(z</p><formula xml:id="formula_17">(0) i ).</formula><p>Hierarchical SetVAE To allow our model to learn a more expressive latent structure of the data, we can extend the vanilla SetVAE using hierarchical latent variables. Specifically, we extend the plain latent variable z <ref type="bibr" target="#b0">(1)</ref> into L disjoint groups {z <ref type="bibr" target="#b0">(1)</ref> , ..., z (L) }, and introduce a top-down hierarchical dependency between z (l) and {z (0) , ..., z (l−1) } for every l &gt; 1. This leads to the modification in the prior and approximate posterior to</p><formula xml:id="formula_18">p(z) = p(z (0) )p(z (1) ) l&gt;1 p(z (l) |z (&lt;l) )<label>(11)</label></formula><formula xml:id="formula_19">q(z|x) = q(z (0) |x)q(z (1) |x) l&gt;1 q(z (l) |z (&lt;l) , x). (12)</formula><p>Applying Eq. (11) and <ref type="bibr" target="#b11">(12)</ref> to Eq. (10), we can derive the ELBO as</p><formula xml:id="formula_20">L HSVAE = E q(z|x) [log p(x|z)] − KL(q(z (0) |x)||p(z (0) )) − KL(q(z (1) |x)||p(z (1) )) − L l=2 E q(z (&lt;l) |x) KL(q(z (l) |z (&lt;l) , x)||p(z (l) |z (&lt;l) ). (13)</formula><p>Hierarchical prior and posterior To model the prior and approximate posterior in Eq. (11) and (12) with top-down latent dependency, we employ the bidirectional inference in <ref type="bibr" target="#b22">[23]</ref>. We outline the formulations here and elaborate on the computations in Section 4.</p><p>Each conditional p(z (l) |z (&lt;l) ) in the prior is modeled by the factorized Gaussian, whose parameters are dependent on the latent variables of the upper hierarchy z (&lt;l) :</p><formula xml:id="formula_21">p(z (l) |z (&lt;l) ) = N µ l (z (&lt;l) ), σ l (z (&lt;l) ) .<label>(14)</label></formula><p>Similarly, each conditional in the approximate posterior q(z (l) |z (&lt;l) , x) is also modeled by the factorized Gaussian. We use the residual parameterization in <ref type="bibr" target="#b25">[26]</ref> which predicts the parameters of the Gaussian using the displacement and scaling factors (∆µ,∆σ) conditioned on z (&lt;l) and x:</p><formula xml:id="formula_22">q(z (l) |z (&lt;l) , x) = N (µ l (z (&lt;l) ) + ∆µ l (z (&lt;l) , x), σ l (z (&lt;l) ) · ∆σ l (z (&lt;l) , x)). (15)</formula><p>Invariance and equivariance We assume that the decoding distribution p(x|z (0) , z (1:L) ) is equivariant to the permutation of z (0) and invariant to the permutation of z (1:L) since such model induces an exchangeable model:</p><formula xml:id="formula_23">p(π(x)) = p(π(x)|π(z (0) ), z (1:L) )p(π(z (0) ))p(z (1:L) )dz = p(x|z (0) , z (1:L) )p(z (0) )p(z (1:L) )dz = p(x). (16)</formula><p>We further assume that the approximate posterior distributions q(z (l) |z (&lt;l) , x) are invariant to the permutation of x. In the following section, we describe how we implement the encoder and decoder satisfying these criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SetVAE Framework</head><p>We present the overall framework of the proposed Set-VAE. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates an overview. SetVAE is based on the bidirectional inference <ref type="bibr" target="#b22">[23]</ref>, which is composed of the bottom-up encoder and top-down generator sharing the same dependency structure. In this framework, the inference network forms the approximate posterior by merging bottom-up information from data with the top-down information from the generative prior. We construct the encoder using a stack of ISABs in Section 2.2, and treat each of the projected set h as a deterministic encoding of data.</p><p>Our generator is composed of a stack of special layers called Attentive Bottleneck Layer (ABL), which extends the ISAB in Section 2.2 with the stochastic interaction with the latent variable. Specifically, ABL processes a set at each layer of the generator as follows:</p><formula xml:id="formula_24">ABL m (x) = MAB(x, FF(z)) ∈ R n×d (17) with h = MAB(I, x) ∈ R m×d ,<label>(18)</label></formula><p>where FF denotes a feed-forward layer, and the latent variable z is derived from the projection h. For generation <ref type="figure" target="#fig_2">(Figure 3a)</ref>, we sample z from the prior in Eq. (14) by,</p><formula xml:id="formula_25">z ∼ N (µ, σ) where µ, σ = FF(h).<label>(19)</label></formula><p>For inference <ref type="figure" target="#fig_2">(Figure 3b</ref>), we sample z from the posterior in Eq. (15) by,</p><formula xml:id="formula_26">z ∼ N (µ + ∆µ, σ · ∆σ) where ∆µ, ∆σ = FF(h + h enc ),<label>(20)</label></formula><p>where h enc is obtained from the corresponding ISAB layer of the bottom-up encoder. Following <ref type="bibr" target="#b22">[23]</ref>, we share the parameters between the generative and inference networks. A detailed illustration of ABL is in the supplementary file.</p><p>To generate a set, we first sample the initial elements z (0) and the latent variable z (1) from the prior p(z (0) ) and p(z (1) ), respectively. Given these inputs, the generator iteratively samples the subsequent latent variables z (l) from the prior p(z (l) |z (&lt;l) ) one by one at each layer of ABL, while processing the set conditioned on the sampled latent variable via Eq. <ref type="bibr" target="#b16">(17)</ref>. The data x is then decoded elementwise from the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analysis</head><p>Modeling Exchangeable Likelihood The architecture of SetVAE satisfies the invariance and equivariance criteria in Section 3. This is, in part, achieved by producing latent variables from the projected sets of bottom-up ISAB and topdown ABL. As the projected sets are permutation invariant to input (Section 2), the latent variables z (1:L) provide an invariant representation of the data. Furthermore, due to Learning Coarse-to-Fine Dependency In SetVAE, both the ISAB and ABL project the input set x of cardinality n to the projected set h of cardinality m via multi-head attention (Eq. <ref type="formula" target="#formula_9">(6)</ref> and <ref type="formula" target="#formula_3">(18)</ref>). In the case of m &lt; n, this projection functions as a bottleneck to the cardinality. This allows the model to encode some features of x into the h and discover interesting subset dependencies across the set elements. Denoting m l as the bottleneck cardinality at layer l <ref type="figure" target="#fig_2">(Figure 3</ref>), we set m l &lt; m l+1 to induce the model to discover coarseto-fine dependency of the set, such as object parts. Such bottleneck also effectively reduces network size, allowing our model to perform competitive or better than the prior arts with less than 50% of their parameters. This coarse-tofine structure is a unique feature of SetVAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>This section discusses the implementation of SetVAE. We leave comprehensive details on the supplementary file.</p><p>Multi-Modal Prior. Although a unimodal Gaussian is a typical choice for the initial element distribution p(z (0) i ) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref>, we find that the model converges significantly faster when we employ the multi-modal prior. We use a mixture of Gaussians (MoG) with K components:</p><formula xml:id="formula_27">p(z (0) i ) = K k=1 π k N (z (0) i ; µ (0) k , σ (0) k ).<label>(21)</label></formula><p>Likelihood. For the likelihood p θ (x|z), we may consider a Gaussian distribution centered at the reconstruction. In the case of point sets, we design the likelihood by</p><formula xml:id="formula_28">L recon (x) = − log p θ (x|z) = 1 2 d(x,x) + const,<label>(22)</label></formula><p>where d(x,x) is the optimal matching distance defined as</p><formula xml:id="formula_29">d(x,x) = min π i x i −x π(i) 2 2 .<label>(23)</label></formula><p>In other words, we measure the likelihood with the Gaussian at optimally permuted x, and thus maximizing this likelihood is equivalent to minimizing the optimal matching distance between the data and the reconstruction. Unfortunately, directly maximizing this likelihood requires O(n 3 ) computation due to the matching. Instead, we choose the Chamfer Distance (CD) as a proxy reconstruction loss,</p><formula xml:id="formula_30">L recon (x) = CD(x,x) = i min j x i −x j 2 2 + j min i x i −x j 2 2 .<label>(24)</label></formula><p>The CD may not admit a direct interpretation as a negative log-likelihood of p θ (x|z), but shares the optimum with the matching distance having a proper interpretation. By employing the CD for the reconstruction loss, we learn the VAE with a surrogate for the likelihood p θ (x|z). CD requires O(n 2 ) computation time, so is scalable to the moderately large sets. Note also that the CD should be scaled appropriately to match the likelihood induced by optimal matching distance. We implicitly account for this by applying weights to KL divergence in our final objective function:</p><formula xml:id="formula_31">L HSVAE (x) = L recon (x) + βL KL (x),<label>(25)</label></formula><p>where L KL (x) is the KL divergence in Eq. (13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Set generative modeling. SetVAE is closely related to recent works on permutation-equivariant set prediction <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20]</ref>. Closest to our approach is the autoencoding TSPN <ref type="bibr" target="#b15">[16]</ref> that uses a stack of ISABs <ref type="bibr" target="#b16">[17]</ref> to predict a set from randomly initialized elements. However, TSPN does not allow sampling, as it uses a pooling-based deterministic set encoding (FSPool) <ref type="bibr" target="#b33">[34]</ref> for reconstruction. SetVAE instead discards FSPool and access projected sets in ISAB directly, which allows an efficient variational inference and a direct extension to hierarchical multi-scale latent. Our approach differs from previous generative models treating each element i.i.d. and processing a random initial set with an elementwise function <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b12">13]</ref>. Notably, PointFlow <ref type="bibr" target="#b29">[30]</ref> uses a continuous normalizing flow (CNF) to process a 3D Gaussian point cloud into an object. However, assuming elementwise independence could pose a limit in modeling complex element interactions. Also, CNF requires the invertibility of the generative model, which could further limit its expressiveness. SetVAE resolves this problem by adopting permutation equivariant ISAB that models inter-element interactions via attention, and a hierarchical VAE framework with flexible latent dependency.</p><p>Contrary to previous works specifically designed for a certain type of set-structured data (e.g., point cloud <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30]</ref>), we emphasize that SetVAE can be trivially applied to arbitrary set-structured data. We demonstrate this by applying SetVAE to the generation of a scene layout represented by a set of object bounding boxes.</p><p>Hierarchical VAE. Our model is built upon the prior works on hierarchical VAEs for images <ref type="bibr" target="#b10">[11]</ref>, such as Ladder-VAE <ref type="bibr" target="#b22">[23]</ref>, IAF-VAE <ref type="bibr" target="#b13">[14]</ref>, and NVAE <ref type="bibr" target="#b25">[26]</ref>. To model long-range pixel correlations in images, these models organize latent variables at each hierarchy as images while gradually increasing their resolution via upsampling. However, the requirement for permutation equivariance has prevented applying multi-scale approaches to sets. ABLs in SetVAE solve this problem by defining latent variables in the projected scales of each hierarchy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental Setup</head><p>Dataset We examine SetVAE using ShapeNet <ref type="bibr" target="#b2">[3]</ref>, Set-MNIST <ref type="bibr" target="#b32">[33]</ref>, and Set-MultiMNIST <ref type="bibr" target="#b4">[5]</ref> datasets. For ShapeNet, we follow the prior work using 2048 points sampled uniformly from the mesh surface <ref type="bibr" target="#b29">[30]</ref>. For Set-MNIST, we binarized the images in MNIST and scaled the coordinates to [0, 1] <ref type="bibr" target="#b15">[16]</ref>. Similarly, we build Set-MultiMNIST using 64 × 64 images of MultiMNIST <ref type="bibr" target="#b4">[5]</ref> with two digits randomly located without overlap. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparison to Other Methods</head><p>We compare SetVAE with the state-of-the-art generative models for point clouds including l-GAN <ref type="bibr" target="#b0">[1]</ref>, PC-GAN <ref type="bibr" target="#b17">[18]</ref>, and PointFlow <ref type="bibr" target="#b29">[30]</ref>. Following these works, we train our model for each category of airplane, chair, and car. <ref type="table" target="#tab_0">Table 2</ref> summarizes the evaluation result. SetVAE achieves better or competitive performance to the prior arts using a much smaller number of parameters (8% to 45% of competitors). Notably, SetVAE often outperforms Point-Flow with a substantial margin in terms of minimum matching distance (MMD) and has better or comparable coverage (COV) and 1-NNA. Lower MMD indicates that SetVAE generates high-fidelity samples, and high COV and low 1-NNA indicate that SetVAE generates diverse samples covering various modes in data. Together, the results indicate that SetVAE generates realistic, high-quality point sets. Notably, we find that SetVAE trained with CD (Eq. (24)) generalizes well to EMD-based metrics than l-GAN.</p><p>We also observe that SetVAE is significantly faster than</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-MNIST Airplane</head><p>Chair Encoder Attention Generator Attention <ref type="figure">Figure 7</ref>: Attention visualization at a selected layer. Each point is color-coded by its assignment based on attention.</p><p>PointFlow in both training (56× speedup; 0.20s vs. 11.2s) and testing (68× speedup; 0.052s vs. 3.52s) 2 . It is because PointFlow requires a costly ODE solver for both training and inference, and has much more parameters. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates qualitative comparisons. Compared to PointFlow, we observe that SetVAE generates sharper details, especially in small object parts such as wings and engines of an airplane or wheels of a car. We conjecture that this is because our model generates samples considering inter-element dependency while capturing shapes in various granularities via a hierarchical latent structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Internal Analysis</head><p>Cardinality disentanglement Ideally, a generative model for sets should be able to disentangle the cardinality of a set from the rest of the generative factors (e.g., structure, style). SetVAE partly achieves this by decomposing the latent variables into the initial set z (0) and the hierarchical latent variables z (1:L) . To validate this, we generate samples by changing the initial set's cardinality while fixing the rest. <ref type="figure">Figure 5</ref> illustrates the result. We observe that SetVAE generates samples having consistent global structure with a varying number of elements. Surprisingly, it even generalizes well to the cardinalities not seen during training. For instance, the model generalizes to any cardinality between 100 and 10,000, although it is trained with only 2048 points in ShapeNet and less than 250 points in Set-MNIST. It shows that SetVAE disentangles the cardinality from the factors characterizing an object.</p><p>We compare SetVAE to PointFlow in extremely high cardinality setting (100k points) in <ref type="figure">Figure 6</ref>. Although Point-Flow innately disentangles cardinality by modeling each element independently, we observe that it tends to generate noisy, blurry boundaries in large cardinality settings. In contrast, SetVAE retains the sharpness of the structure even for extreme cardinality, presumably because it considers inter-element dependency in the generation process.  Discovering coarse-to-fine dependency SetVAE discovers interesting subset structures via hierarchical bottlenecks in ISAB and ABL. To demonstrate this, we visualize the encoder attention (Eq. (6)) and generator attention (Eq. (18)) in <ref type="figure">Figure 7</ref>, where each point is color-coded based on its hard assignment to one of m inducing points <ref type="bibr" target="#b2">3</ref> . We observe that SetVAE attends to semantically interesting parts consistently across samples, such as wings and engines of an airplane, legs and backs of a chair, and even different instances of multiple digits. <ref type="figure" target="#fig_5">Figure 8</ref> illustrates the point-wise attention across levels. We observe that the top-level tends to capture the coarse and symmetric structures, such as wings and wheels, which are further decomposed into much finer granularity in the subsequent levels. We conjecture that this coarse-to-fine subset dependency helps the model to generate accurate structure in various granularity from global structure to local details.</p><p>Interestingly, we find that the hierarchical structure of SetVAE sometimes leads to the disentanglement of generative factors across layers. To demonstrate this, we train two classifiers in Set-MultiMNIST, one for digit class and the other for their positions. We train the classifiers using latent variables at each generator layer as an input, and measure the accuracy at each layer. In <ref type="figure" target="#fig_6">Figure 9</ref>, the latent variables at the lower layers tend to contribute more in locating the digits, while higher layers contribute to generating shape. <ref type="bibr" target="#b2">3</ref> For illustrative purposes, we present results from a selected head.  Ablation study <ref type="table" target="#tab_1">Table 3</ref> summarizes the ablation study of SetVAE (see the supplementary file for qualitative results and evaluation detail). We consider two baselines: Vanilla SetVAE using a global latent variable z (1) (Section 3), and hierarchical SetVAE with unimodal prior (Section 4.2). The Vanilla SetVAE performs much worse than our full model. We conjecture that a single latent variable is not expressive enough to encode complex variations in Mul-tiMNIST, such as identity and position of multiple digits. We also find that multi-modal prior stabilizes the training of attention and guides the model to better local optima.</p><p>Extension to categorical bounding boxes SetVAE provides a solid exchangeability guarantee over sets, thus applicable to any set-structured data. To demonstrate this, we trained SetVAE on categorical bounding boxes in indoor scenes of the SUN-RGBD dataset <ref type="bibr" target="#b23">[24]</ref>. As shown in <ref type="figure" target="#fig_7">Figure 10</ref>, SetVAE generates plausible layouts, modeling a complicated distribution of discrete semantic categories and continuous spatial instantiation of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We introduced SetVAE, a novel hierarchical VAE for sets of varying cardinality. Introducing a novel bottleneck equivariant layer that learns subset representations, SetVAE performs hierarchical subset reasoning to encode and generate sets in a coarse-to-fine manner. As a result, SetVAE generates high-quality, diverse sets with reduced parameters. We also showed that SetVAE achieves cardinality disentanglement and generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>In this section, we discuss detailed derivations and descriptions of SetVAE presented in Section 3 and Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. KL Divergence of Initial Set Distribution</head><p>This section provides proof that the KL divergence between the approximate posterior and the prior of the initial elements in Eq. <ref type="formula" target="#formula_3">(10)</ref> and <ref type="formula" target="#formula_3">(13)</ref> is a constant.</p><p>Following the definition in Eq. (8) and Eq. (9), we decompose the prior as p(z (0) ) = p(n)p(z (0) |n) and the approximate posterior as q(z (0) |x) = δ(n)q(z (0) |n, x), where δ(n) is defined as a delta function centered at n = |x|. Here, the conditionals are given by</p><formula xml:id="formula_32">p(z (0) |n) = n i=1 p(z (0) i ),<label>(26)</label></formula><formula xml:id="formula_33">q(z (0) |n, x) = n i=1 q(z (0) i |x).<label>(27)</label></formula><p>As described in the main text, we set the elementwise distributions identical, p(z</p><formula xml:id="formula_34">(0) i ) = q(z (0) i |x)</formula><p>. This renders the conditionals equal,</p><formula xml:id="formula_35">p(z (0) |n) = q(z (0) |n, x).<label>(28)</label></formula><p>Then, the KL divergence between the approximate posterior and the prior in Eq. <ref type="formula" target="#formula_3">(10)</ref>  </p><p>where the second equality comes from the Eq. <ref type="bibr" target="#b27">(28)</ref>. From the definition of KL divergence, we can rewrite Eq. (30) as</p><formula xml:id="formula_37">KL(q(z (0) |x)||p(z (0) )) = E δ(n)p(z (0) |n) log δ(n)p(z (0) |n) p(n)p(z (0) |n) (31) = E δ(n)p(z (0) |n) log δ(n) p(n) ,<label>(32)</label></formula><p>As the logarithm in Eq. (32) does not depend on z (0) , we can take it out from the expectation over p(z (0) |n) as follows:</p><formula xml:id="formula_38">KL(q(z (0) |x)||p(z (0) )) = E δ(n) E p(z (0) |n) log δ(n) p(n) (33) = E δ(n) log δ(n) p(n) ,<label>(34)</label></formula><p>which can be rewritten as <ref type="figure">Figure 11</ref>: The detailed structure of Attentive Bottleneck Layer during sampling (for generation) and inference (for reconstruction).</p><formula xml:id="formula_39">E δ(n) [log δ(n) − log p(n)].<label>(35)</label></formula><formula xml:id="formula_40">MAB x I h V Q x' MAB V Q FF z (a) Generation MAB x I h V Q x' MAB V Q FF FF h enc z (b) Inference</formula><p>The expectation over the delta function δ(n) is simply an evaluation at n = |x|. As δ is defined over a discrete random variable n, its probability mass at the center |x| equals 1. Therefore, log δ(n) at n = |x| reduces to log 1 = 0, and we obtain KL(q(z (0) |x)||p(z (0) )) = − log p(|x|).</p><p>(36)</p><p>As discussed in the main text, we model p(n) using the empirical distribution of data cardinality. Thus, p(|x|) only depends on data distribution, and −KL(q(z (0) |x)||p(z (0) )) in Eq. (10) can be omitted from optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Implementation of SetVAE</head><p>Attentive Bottleneck Layer. In <ref type="figure">Figure 11</ref>, we provide the detailed structure of Attentive Bottleneck Layer (ABL) that composes the top-down generator of SetVAE (Section 4). We share the parameters in ABL for generation and inference, which is known to be effective in stabilizing the training of hierarchical VAE <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref>. During generation <ref type="figure">(Figure 11a</ref>), z is sampled from a Gaussian prior N (µ, σ) (Eq. <ref type="formula" target="#formula_3">(19)</ref>). To predict µ and σ from h, we use an elementwise fully-connected (FC) layer, of which parameters are shared across elements of h. During inference, we sample the latent variables from the approximate posterior N (µ + ∆µ, σ · ∆σ), where the correction factors ∆µ, ∆σ are predicted from the bottom-up encoding h enc by an additional FC layer. Note that the FC for predicting µ, σ is shared for generation and inference, but the FC that predicts ∆µ, ∆σ is used only for inference.</p><p>Slot Attention in ISAB and ABL. SetVAE discovers subset representation via projection attention in ISAB (Eq. (6)) and ABL (Eq. (18)). However, with a basic attention mechanism, the projection attention may ignore some parts of input by simply not attending to them. To prevent this, in both ISAB and ABL, we change the projection attention to Slot Attention <ref type="bibr" target="#b20">[21]</ref>.</p><p>Specifically, plain projection attention 4 (Eq. (4)) treats input x ∈ R n×d as key (K) and value (V ), and uses a set of inducing points I ∈ R m×d as query (Q). First, it obtains the attention score matrix as follows:</p><formula xml:id="formula_41">A = QK T √ d ∈ R m×n .<label>(37)</label></formula><p>Each row index of A denotes an inducing point, and each column index denotes an input element. Then, the value set V is aggregated using A. With Softmax axis=d (·) denoting softmax normalization along d-th axis, the plain attention normalizes each row of A, as follows:</p><formula xml:id="formula_42">Att(Q, K, V ) = W V ∈ R m×d ,<label>(38)</label></formula><p>where W = Softmax axis=2 (A) ∈ R m×n .</p><p>As a result, an input element can get zero attention if every query suppresses it. To prevent this, Slot Attention normalizes each column of A and computes weighted mean:</p><formula xml:id="formula_44">SlotAtt(Q, K, V ) = W V,<label>(40)</label></formula><p>where</p><formula xml:id="formula_45">W ij = A ij n l=1 A il for A = Softmax axis=1 (A).<label>(41)</label></formula><p>As attention coefficients across a row sum up to 1 after softmax, slot attention guarantees that an input element is not ignored by every inducing point. With the adaptation of Slot Attention, we observe that inducing points often attend to distinct subsets of the input to produce h, as illustrated in the <ref type="figure">Figure 7</ref> and <ref type="figure" target="#fig_5">Figure 8</ref> of the main text. This is similar to the observation of <ref type="bibr" target="#b20">[21]</ref> that the competition across queries encouraged segmented representations (slots) of objects from a multi-object image. A difference is that unlike in <ref type="bibr" target="#b20">[21]</ref> where the queries are noise vectors, we design the query set as a learnable parameter I. Also, we do not introduce any refinement steps to the projected set h to avoid the complication of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Details</head><p>This section discusses the detailed descriptions and additional results of experiments in Section 6 in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. ShapeNet Evaluation Metrics</head><p>We provide descriptions of evaluation metrics used in the ShapeNet experiment (Section 6 in the main paper). We measure standard metrics including coverage (COV), minimum matching distance (MMD), and 1-nearest neighbor accuracy (1-NNA) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref>. Following recent literature <ref type="bibr" target="#b12">[13]</ref>, we omit Jensen-Shannon Divergence (JSD) <ref type="bibr" target="#b0">[1]</ref> because it does not assess the fidelity of each point cloud. To measure the similarity D(x, y) between point clouds x and y, we use Chamfer Distance (CD) (Eq. (24)) and Earth Mover's Distance (EMD), where the EMD is defined as:</p><formula xml:id="formula_46">EMD(x, y) = min π i x i − y π(i) 2 .<label>(42)</label></formula><p>Let S g be the set of generated point clouds and S r be the set of reference point clouds with |S r | = |S g |.</p><p>Coverage (COV) measures the percentage of reference point clouds that is a nearest neighbor of at least one generated point cloud, computed as follows:</p><formula xml:id="formula_47">COV(S g , S r ) = |{argmin y∈Sr D(x, y)|x ∈ S g }| |S r | . (43)</formula><p>Minimum Matching Distance (MMD) measures the average distance from each reference point cloud to its nearest neighbor in the generated point clouds:</p><formula xml:id="formula_48">MMD(S g , S r ) = 1 |S r | y∈Sr min x∈Sg D(x, y).<label>(44)</label></formula><p>1-Nearest Neighbor Accuracy (1-NNA) assesses whether two distributions are identical. Let S −x = S r ∪ S g − {x} and N x be the nearest neighbor of x in S −x . With 1(·) an indicator function:</p><formula xml:id="formula_49">1-NNA(S g , S r ) = x∈Sg 1(N x ∈ S g ) + y∈Sr 1(N y ∈ S r ) |S g | + |S r | .<label>(45)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Hierarchical Disentanglement</head><p>This section describes an evaluation protocol used in <ref type="figure" target="#fig_6">Figure 9</ref> in the main paper. To investigate the latent representations learned at each level, we employed Linear Discriminant Analysis (LDA) as simple layer-wise classifiers. The classifiers take the latent variable at each layer z l , ∀l ∈ [1, L] as an input, and predict the identity and position of two digits (in 4 × 4 quantized grid) respectively in Set-MultiMNIST dataset. To this end, we first train the Set-VAE in the training set of Set-MultiMNIST. Then we train the LDA classifiers using the validation dataset, where the input latent variables are sampled from the posterior distribution of SetVAE (Eq. (12)). We report the training accuracy of the classifiers at each layer in <ref type="figure" target="#fig_6">Figure 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Ablation study</head><p>In this section, we provide details of the ablation study presented in <ref type="table" target="#tab_1">Table 3</ref> of the main text.</p><p>Baseline As baselines, we use a SetVAE with unimodal Gaussian prior over the initial set elements, and a nonhierarchical, Vanilla SetVAE presented in Section 3.</p><p>To implement a SetVAE with unimodal prior, we only change the initial element distribution p(z (0) i ) from MoG (Eq. (21)) to a multivariate Gaussian with a diagonal covariance matrix N (µ (0) , σ (0) ) with learnable µ (0) and σ (0) . This approach is adopted in several previous works in permutation-equivariant set generation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>To implement a Vanilla SetVAE, we employ a bottomup encoder same to our full model and make the following changes to the top-down generator. As illustrated in <ref type="figure" target="#fig_1">Figure 12</ref>, we remove the subset relation in the generator by fixing the latent cardinality to 1 and employing a global prior N (µ 1 , σ 1 ) with µ 1 , σ 1 ∈ R 1×d for all ABL. To compute permutation-invariant h enc ∈ R 1×d , we aggregate every elements of h from all levels of encoder network by average pooling. During inference, h enc is provided to every ABL in the top-down generator.</p><p>Evaluation metric For the ablation study of SetVAE on the Set-MultiMNIST dataset, we measure the generation quality in image space by rendering each set instance to 64 × 64 binary image based on the occurrence of a point in a pixel bin. To measure the generation performance, we compute Frechet Inception Distance (FID) score <ref type="bibr" target="#b8">[9]</ref> using the output of the penultimate layer of a VGG11 network trained from scratch for MultiMNIST image classification into 100 labels (00-99). Given the channel-wise mean µ g , µ r and covariance matrix Σ g , Σ r of generated and reference set of images respectively, we compute FID as follows:</p><formula xml:id="formula_50">d 2 = µ g − µ r 2 + Tr(Σ g + Σ r − 2 Σ g Σ r ).<label>(46)</label></formula><p>To train the VGG11 network, we replace the first conv layer to take single-channel inputs, and use the same Mul-tiMNIST train set as in SetVAE. We use the SGD optimizer with Nesterov momentum, with learning rate 0.01, momentum 0.9, and L2 regularization weight 5e-3 to prevent overfitting. We train the network for 10 epochs using batch size 128 so that the training top-1 accuracy exceeds 95%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Qualitative Results</head><p>Ablation study This section provides additional results of the ablation study, which corresponds to the <ref type="table" target="#tab_1">Table 3</ref> of the main paper. We compare the SetVAE with two baselines: SetVAE with a unimodal prior and the one using a single global latent variable (i.e., Vanilla SetVAE). <ref type="figure" target="#fig_2">Figure 13</ref> shows the training loss curves of SetVAE and the unimodal prior baseline on the Set-MultiMNIST dataset. We observe that training of the unimodal baseline is unstable compared to SetVAE which uses a 4-component MoG. We conjecture that a flexible initial set distribution provides a cue for the generator to learn stable subset representations.</p><p>In <ref type="figure" target="#fig_3">Figure 14</ref>, we visualize samples from SetVAE and the two baselines. As the training of unimodal SetVAE was un- <ref type="figure">Figure 15</ref>: Color-coded mixture assignments on output sets. stable, we provide the results from a checkpoint before the training loss diverges (third row of <ref type="figure" target="#fig_3">Figure 14</ref>). The Vanilla SetVAE without hierarchy (second row of <ref type="figure" target="#fig_3">Figure 14</ref>) focuses on modeling the left digit only and fails to assign a balanced number of points. This failure implies that multilevel subset reasoning in the generative process is essential in faithfully modeling complex set data such as Set-MultiMNIST.</p><p>Role of mixture initial set Although the multi-modal prior is not a typical choice, we emphasize that it marginally adds complexity to the model since it only introduces the additional learnable mixture parameters (π</p><formula xml:id="formula_51">(0) k , µ (0) k , σ (0) k ).</formula><p>Despite the simplicity, we observe that mixture prior is effective in stabilizing training, especially when there are clearly separating modes in data such as in the Set-MultiMNIST dataset <ref type="figure">(Fig. 15</ref>). Still, the choice of the initial prior is flexible and orthogonal to our contribution. <ref type="figure">Figure 16</ref> presents the generated samples from SetVAE on ShapeNet, Set-MNIST and Set-MultiMNIST datasets, extending the results in <ref type="figure" target="#fig_3">Figure 4</ref> of the main text. As illustrated in the figure, SetVAE generates point sets with high diversity while capturing thin and sharp details (e.g. engines of an airplane and legs of a chair, etc.). <ref type="figure">Figure 17</ref> presents the additional results of <ref type="figure">Figure 5</ref> in the main paper, which illustrates samples generated by increasing the cardinality of the initial set z (0) while fixing the hierarchical latent variables z (1:L) . As illustrated in the figure, SetVAE is able to disentangle the cardinality of a set from the rest of its generative factors, and is able to generalize to unseen cardinality while preserving the disentanglement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ShapeNet results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cardinality disentanglement</head><p>Notably, SetVAE can retain the disentanglement and generalize even to a high cardinality (100k) as well. <ref type="figure" target="#fig_5">Figure 18</ref> presents the comparison to PointFlow with varying cardinality, which extends the results of the <ref type="figure">Figure 6</ref> in the main paper. Unlike PointFlow that exhibits degradation and blurring of fine details, SetVAE retains the fine structure of the generated set even for extreme cardinality.</p><p>Coarse-to-fine dependency In <ref type="figure" target="#fig_6">Figure 19</ref> and <ref type="figure" target="#fig_1">Figure 20</ref>, we provide additional visualization of encoder and generator attention, extending the <ref type="figure">Figure 7</ref> and <ref type="figure" target="#fig_5">Figure 8</ref> in the main text. We observe that SetVAE learns to attend to a subset of points consistently across examples. Notably, these subsets often have a bilaterally symmetric structure or correspond to semantic parts. For example, in the top level of the encoder (rows marked level 1 in <ref type="figure" target="#fig_6">Figure 19</ref>), the subsets include wings of an airplane, legs &amp; back of a chair, or wheels &amp; rear wing of a car (colored red).</p><p>Furthermore, SetVAE extends the subset modeling to multiple levels with a top-down increase in latent cardinality. This allows SetVAE to encode or generate the structure of a set in various granularities, ranging from global structure to fine details. Each column in <ref type="figure" target="#fig_6">Figure 19</ref> and <ref type="figure" target="#fig_1">Figure 20</ref> illustrates the relations. For example, in level 3 of <ref type="figure" target="#fig_6">Figure 19</ref>, the bottom-up encoder partitions an airplane into fine-grained parts such as an engine, a tip of the tail wing, etc. Then, going bottom-up to level 1, the encoder composes them to fuselage and symmetric pair of wings. As for the top-down generator in <ref type="figure" target="#fig_1">Figure 20</ref>, it starts in level 1 by composing an airplane via the coarsely defined body and wings. Going top-down to level 3, the generator descends into fine-grained subsets like an engine and tail wing. <ref type="table" target="#tab_2">Table 4</ref> provides the network architecture and hyperparameters of SetVAE. In the table, FC(d, f ) denotes a fullyconnected layer with output dimension d and nonlinearity f . ISAB m (d, h) denotes an ISAB m with m inducing points, hidden dimension d, and h heads (in Section 2.2). MoG K (d) denotes a mixture of Gaussian (in Eq. (21)) with K components and dimension d. ABL m (d, d z , h) denotes an ABL m with m inducing points, hidden dimension d, latent dimension d z , and h heads (in Section 4). All MABs used in ISAB and ABL uses fully-connected layer with bias as FF layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Architecture and Hyperparameters</head><p>In <ref type="table">Table 5</ref>, we provide detailed training hyperparameters. For all experiments, we used Adam optimizer with first and second momentum parameters 0.9 and 0.999, respectively and decayed the learning rate linearly to zero after 50% of the training schedule. Following <ref type="bibr" target="#b25">[26]</ref>, we linearly annealed β from 0 to 1 during the first 2000 epochs for ShapeNet datasets, 40 epochs for Set-MNIST, and 50 epochs for Set-MultiMNIST dataset.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of Multihead Attention Block (MAB) and Induced Set Attention Block (ISAB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The hierarchical SetVAE. N prior denotes the prior (Eq.<ref type="bibr" target="#b18">(19)</ref>) and N post denotes the posterior (Eq.(20)). permutation equivariance of ISAB, the top-down stack of ABLs produce an output equivariant to the initial set z (0) . This renders the decoding distribution p(x|z (0) , z (1:L) ) permutation equivariant to z (0) . Consequently, the decoder of SetVAE induces an exchangeable model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Examples of randomly generated point sets from SetVAE (ours) and PointFlow in ShapeNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Samples from SetVAE for different cardinalities. At each row, the hierarchical latent variables are fixed and the initial set is re-sampled with different cardinality. Samples from SetVAE and PointFlow in a high cardinality setting. Only the initial sets are re-sampled. Evaluation Metrics For evaluation in ShapeNet, we compare the standard metrics including Minimum Matching Distance (MMD), Coverage (COV), and 1-Nearest Neighbor Accuracy (1-NNA), where the similarity between point clouds are computed with Chamfer Distance (CD) (Eq. (24)), and Earth Mover's Distance (EMD) based on optimal matching. The details are in the supplementary file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of encoder attention across multiple layers. IP notes the number of inducing points at each level. See the supplementary file for more results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Layer-wise classification results using the hierarchical latent variables in Set-MultiMNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Generation results from SetVAE trained on SUN-RGBD dataset. Zoom-in for a better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>is written asKL(q(z (0) |x)||p(z (0) )) = KL(δ(n)q(z (0) |n, x)||p(n)p(z (0) |n))(29)= KL(δ(n)p(z (0) |n)||p(n)p(z (0) |n)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Structure of Vanilla SetVAE without hierarchical priors and subset reasoning in generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Training loss curves from SetVAE with multimodal and unimodal initial set trained on Set-MultiMNIST dataset. Samples from SetVAE and its ablated version trained on Set-MultiMNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 :Figure 17 :Figure 18 :Figure 19 :Figure 20 :</head><label>1617181920</label><figDesc>Additional examples of generated point clouds from SetVAE. Additional examples demonstrating cardinality generalization of SetVAE. More examples in high-cardinality setting, compared with PointFlow. More examples of color-coded encoder attention. More examples of color-coded generator attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Comparison against the state-of-the-art generative models. ↑: the higher the better. ↓: the lower the better. The best scores are highlighted in bold. MMD-CD is scaled by 10 3 , and MMD-EMD by 10 2 .</figDesc><table><row><cell></cell><cell></cell><cell cols="2"># Parameters (M)</cell><cell cols="2">MMD(↓)</cell><cell cols="2">COV(%,↑)</cell><cell cols="2">1-NNA(%,↓)</cell></row><row><cell>Category</cell><cell>Model</cell><cell>Full</cell><cell>Gen</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell><cell>CD</cell><cell>EMD</cell></row><row><cell></cell><cell>l-GAN (CD)[30]</cell><cell>1.97</cell><cell>1.71</cell><cell>0.239</cell><cell>4.27</cell><cell>43.21</cell><cell>21.23</cell><cell>86.30</cell><cell>97.28</cell></row><row><cell></cell><cell>l-GAN (EMD)[30]</cell><cell>1.97</cell><cell>1.71</cell><cell>0.269</cell><cell>3.29</cell><cell>47.90</cell><cell>50.62</cell><cell>87.65</cell><cell>85.68</cell></row><row><cell></cell><cell>PC-GAN[30]</cell><cell>9.14</cell><cell>1.52</cell><cell>0.287</cell><cell>3.57</cell><cell>36.46</cell><cell>40.94</cell><cell>94.35</cell><cell>92.32</cell></row><row><cell>Airplane</cell><cell>PointFlow[30]</cell><cell>1.61</cell><cell>1.06</cell><cell>0.217</cell><cell>3.24</cell><cell>46.91</cell><cell>48.40</cell><cell>75.68</cell><cell>75.06</cell></row><row><cell></cell><cell>SetVAE (Ours)</cell><cell>0.75</cell><cell>0.39</cell><cell>0.199</cell><cell>3.07</cell><cell>43.45</cell><cell>44.93</cell><cell>75.31</cell><cell>77.65</cell></row><row><cell></cell><cell>Training set</cell><cell>-</cell><cell>-</cell><cell>0.226</cell><cell>3.08</cell><cell>42.72</cell><cell>49.14</cell><cell>70.62</cell><cell>67.53</cell></row><row><cell></cell><cell>l-GAN (CD)[30]</cell><cell>1.97</cell><cell>1.71</cell><cell>2.46</cell><cell>8.91</cell><cell>41.39</cell><cell>25.68</cell><cell>64.43</cell><cell>85.27</cell></row><row><cell></cell><cell>l-GAN (EMD)[30]</cell><cell>1.97</cell><cell>1.71</cell><cell>2.61</cell><cell>7.85</cell><cell>40.79</cell><cell>41.69</cell><cell>64.73</cell><cell>65.56</cell></row><row><cell></cell><cell>PC-GAN[30]</cell><cell>9.14</cell><cell>1.52</cell><cell>2.75</cell><cell>8.20</cell><cell>36.50</cell><cell>38.98</cell><cell>76.03</cell><cell>78.37</cell></row><row><cell>Chair</cell><cell>PointFlow[30]</cell><cell>1.61</cell><cell>1.06</cell><cell>2.42</cell><cell>7.87</cell><cell>46.83</cell><cell>46.98</cell><cell>60.88</cell><cell>59.89</cell></row><row><cell></cell><cell>SetVAE (Ours)</cell><cell>0.75</cell><cell>0.39</cell><cell>2.55</cell><cell>7.82</cell><cell>46.98</cell><cell>45.01</cell><cell>58.76</cell><cell>61.48</cell></row><row><cell></cell><cell>Training set</cell><cell>-</cell><cell>-</cell><cell>1.92</cell><cell>7.38</cell><cell>57.25</cell><cell>55.44</cell><cell>59.67</cell><cell>58.46</cell></row><row><cell></cell><cell>l-GAN (CD)[30]</cell><cell>1.97</cell><cell>1.71</cell><cell>1.55</cell><cell>6.25</cell><cell>38.64</cell><cell>18.47</cell><cell>63.07</cell><cell>88.07</cell></row><row><cell></cell><cell>l-GAN (EMD)[30]</cell><cell>1.97</cell><cell>1.71</cell><cell>1.48</cell><cell>5.43</cell><cell>39.20</cell><cell>39.77</cell><cell>69.74</cell><cell>68.32</cell></row><row><cell></cell><cell>PC-GAN[30]</cell><cell>9.14</cell><cell>1.52</cell><cell>1.12</cell><cell>5.83</cell><cell>23.56</cell><cell>30.29</cell><cell>92.19</cell><cell>90.87</cell></row><row><cell>Car</cell><cell>PointFlow[30]</cell><cell>1.61</cell><cell>1.06</cell><cell>0.91</cell><cell>5.22</cell><cell>44.03</cell><cell>46.59</cell><cell>60.65</cell><cell>62.36</cell></row><row><cell></cell><cell>SetVAE (Ours)</cell><cell>0.75</cell><cell>0.39</cell><cell>0.88</cell><cell>5.05</cell><cell>48.58</cell><cell>44.60</cell><cell>59.66</cell><cell>63.35</cell></row><row><cell></cell><cell>Training set</cell><cell>-</cell><cell>-</cell><cell>1.03</cell><cell>5.33</cell><cell>48.30</cell><cell>51.42</cell><cell>57.39</cell><cell>53.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Ablation study performed on Set-MultiMNIST dataset using FID scores for 64 × 64 rendered images.</figDesc><table><row><cell>Model</cell><cell>FID(↓)</cell></row><row><cell>SetVAE (Ours)</cell><cell>1047</cell></row><row><cell>Non-hierarchical</cell><cell>1470</cell></row><row><cell>Unimodal prior</cell><cell>1252</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Detailed network architectures used in our experiments.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Without loss of generality, we use n to denote the cardinality of a set but assume that the training data is composed of sets in various size.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Measured on a single GTX 1080ti with a batch size of 16.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For simplicity, we explain with single-head attention instead of MultiHead.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards a neural statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-object representation learning with iterative variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphaël</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Structured vaes: Composing probabilistic graphical models and variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">B</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sandeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joun Yeop</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Soo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<title level="m">Probabilistic framework for normalizing flow on manifolds. CoRR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Conditional set generation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Point cloud GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GRAINS: generative recursive autoencoders for indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Gadi Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owais</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao (richard)</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exchangeable neural ODE for set modeling. NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haidong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junier</forename><forename type="middle">B</forename><surname>Oliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Objectcentric learning with slot attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast and flexible indoor scene synthesis via deep convolutional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Casper Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Søren Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Kosiorek 2. Generative adversarial set transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stelzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointflow: 3d point cloud generation with continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Energy-based processes for exchangeable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep set prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam Prügel-</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fspool: Learning set representations with featurewise sort pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam Prügel-</forename><surname>Bennett</surname></persName>
		</author>
		<idno>ICLR, 2020. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<title level="m">ShapeNet Set-MNIST Set-MultiMNIST Encoder Generator Encoder Generator Encoder Generator Input: FC(64, −) Initial set: MoG 4 (32) Input: FC(64, −) Initial set: MoG 4 (32) Input: FC(64, −) Initial set: MoG</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>ISAB 32 (64, 4) ABL 1 (64, 16, 4) ISAB 32 (64, 4) ABL 2 (64, 16, 4) ISAB 32 (64, 4) ABL 2 (64, 16, 4) ISAB 16 (64, 4) ABL 1 (64, 16, 4) ISAB 16 (64, 4) ABL 4 (64, 16, 4) ISAB 16 (64, 4) ABL 4 (64, 16, 4) ISAB 8 (64, 4) ABL 2 (64, 16, 4) ISAB 8 (64, 4) ABL 8 (64, 16, 4) ISAB 8 (64, 4) ABL 8 (64, 16, 4) ISAB 4 (64, 4) ABL 4 (64, 16, 4) ISAB 4 (64, 4) ABL 16 (64, 16, 4) ISAB 4 (64, 4) ABL 16 (64, 16, 4) ISAB 2 (64, 4) ABL 8 (64, 16, 4) ISAB 2 (64, 4) ABL 32 (64, 16, 4) ISAB 2 (64, 4) ABL 32 (64, 16, 4) ISAB 1 (64, 4) ABL 16 (64, 16, 4) Output: FC(2, tanh) Output: FC(2, tanh) ISAB 1 (64, 4) ABL 32 (64, 16, 4) (Output + 1)/2 (Output + 1</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detailed training hyperparameters used in our experiments</title>
		<idno>Output: FC(3</idno>
	</analytic>
	<monogr>
		<title level="j">ShapeNet Set-MNIST Set-MultiMNIST</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>rate 1e-3, linear decay to zero after half of training β (Eq. (25)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
