<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Graph-Level Representation for Drug Discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Laboratory of CAD&amp;CG</orgName>
								<orgName type="institution">Technology Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<email>dengcai@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Laboratory of CAD&amp;CG</orgName>
								<orgName type="institution">Technology Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
							<email>xiaofeihe@cad.zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Laboratory of CAD&amp;CG</orgName>
								<orgName type="institution">Technology Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Graph-Level Representation for Drug Discovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicating macroscopic influences of drugs on human body, like efficacy and toxicity, is a central problem of smallmolecule based drug discovery. Molecules can be represented as an undirected graph, and we can utilize graph convolution networks to predication molecular properties. However, graph convolutional networks and other graph neural networks all focus on learning node-level representation rather than graph-level representation. Previous works simply sum all feature vectors for all nodes in the graph to obtain the graph feature vector for drug predication. In this paper, we introduce a dummy super node that is connected with all nodes in the graph by a directed edge as the representation of the graph and modify the graph operation to help the dummy super node learn graph-level feature. Thus, we can handle graph-level classification and regression in the same way as node-level classification and regression. In addition, we apply focal loss to address class imbalance in drug datasets. The experiments on MoleculeNet show that our method can effectively improve the performance of molecular properties predication.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Reducing the high attrition rates in drug development continues to be a key challenge for the pharmaceutical industry. When biological researches yield evidence that a particular molecule could modulate essential pathways to achieve therapeutic activity, the discovered molecule often fails as a potential drug for a number of reasons including toxicity, low activity, and low solubility <ref type="bibr" target="#b24">(Waring et al. 2015)</ref>. The primary problem of small-molecule based drug discovery is finding analogue molecules with increased efficacy and reduced safe risks to the patient to optimize the candidate molecule. Machine learning is a powerful tool of drugs virtual screening that can help drug developer eliminate unqualified candidate molecules quickly.</p><p>A molecule can be of arbitrary size and shape. However, most machine learning methods can only handle inputs of a fixed size. A wide-used conventional proposal is to utilize hand-crafted feature like ECFP <ref type="bibr" target="#b17">(Rogers and Hahn 2010)</ref>, Coulomb Matrix <ref type="bibr" target="#b19">(Rupp et al. 2012)</ref> as input and feed it into conventional classifier like random forest and multilayer perceptron. In the last few years, convolutional neural network clearly surpassed the conventional methods that use hand-crafted feature and SVM in object classification tasks <ref type="bibr" target="#b8">(Krizhevsky, Sutskever, and Hinton 2012)</ref>. It indicates that end-to-end learning with reasonable differentiable feature extractor may surpass the conventional two stage classifier. Following this idea, <ref type="bibr" target="#b4">(Duvenaud et al. 2015</ref>) model a molecule as a graph, the nodes of which stand for atoms and the edges of which represent chemical bonds linking some of the atoms together, and propose graph convolutional network which directly takes the graph as input to learn the representations of molecules. Previous studies show that graph convolutional network broadly offers the best performance in most of the datasets in MoleculeNet <ref type="bibr" target="#b25">(Wu et al. 2017)</ref>.</p><p>Molecular properties of drugs are almost macroscopic influences on human body, like efficacy and toxicity. In fact, unlike common graph-based application like web classification, citation network and knowledge graph, predicating drugs properties is graph-level classification and regression rather than node-level classification and regression. However, present graph neural networks, including graph convolutional network <ref type="bibr" target="#b4">(Duvenaud et al. 2015)</ref>, similar graph networks that take the original molecule as input <ref type="bibr" target="#b0">(Altae-Tran et al. 2017;</ref><ref type="bibr" target="#b26">Yao et al. 2017;</ref><ref type="bibr" target="#b21">Sch√ºtt et al. 2017;</ref>, and other general graph neural networks designed for all types of graph <ref type="bibr" target="#b10">(Li et al. 2015;</ref><ref type="bibr" target="#b1">Bruna et al. 2013;</ref><ref type="bibr" target="#b3">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b7">Kipf and Welling 2016;</ref><ref type="bibr" target="#b14">Niepert, Ahmed, and Kutzkov 2016)</ref>, all focus on learning node-level representation rather than graphlevel representation. Previous works <ref type="bibr" target="#b0">(Altae-Tran et al. 2017;</ref><ref type="bibr" target="#b25">Wu et al. 2017</ref>) simply sums all feature vectors for all nodes in the graph to obtain the graph representation for molecule properties classification and regression.</p><p>To learn better graph-level representation and handle graph-level classification and regression, we introduce a dummy super node that is connected with all nodes in the graph by a directed edge, as the representation of the graph. Thus, we can handle graph-level classification and regression in the same way as node-level classification and regression. The idea of taking the feature of one specific node as the feature of the graph has been proposed before <ref type="bibr" target="#b20">(Scarselli et al. 2009</ref>), but earlier work simply assign the first atom in the atom-bond description as the specific node. In our work, the dummy super node is initialized to zero and get updated through graph convolutional networks simultaneously as the genuine nodes do. We modify the graph operations(Altae-Tran et al. 2017) and apply node-level batch normalization to learn better features. We take the feature of the dummy super node as the feature of the graph and feed it into classifier.</p><p>Another problem of molecular properties predication is that the datasets of molecules are often unbalanced. That is, the positive samples may be only a small part of the total samples. Take HIV dataset(hiv ) as instance, there are only 1487 chemical compounds that are HIV active in the dataset of totally 41913 chemical compounds. Previous works <ref type="bibr" target="#b0">(Altae-Tran et al. 2017;</ref><ref type="bibr" target="#b25">Wu et al. 2017</ref>) did not pay much attention to the fact of unbalanced data. To address the unbalanced data, we adopt focal loss <ref type="bibr" target="#b11">(Lin et al. 2017)</ref> and show that it can effectively improve the classification performance in unbalanced molecular dataset.</p><p>We evaluated our model on several datasets of toxity, biological activities and solubility in MoleculeNet, including Tox21 ToxCast, PCBA, MUV, HIV, FreeSolv. The results show that our method could effectively improve the performance on molecular properties predication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Our work draws inspiration both from the field of molecular machine learning and graph neural network. In what follows, we provide a brief overview of previous works in both fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Molecule Mchine Learning</head><p>Encoding molecules into fixed-length strings or vectors is a core challenge for molecular machine learning. <ref type="bibr" target="#b19">(Rupp et al. 2012)</ref> introduce Coulomb Matrix as feature of molecules and apply a machine learning model to predict atomization energies of a diverse set of organic molecules. <ref type="bibr" target="#b17">(Rogers and Hahn 2010)</ref> propose Extended-connectivity fingerprints (ECFP), a novel class of topological fingerprints for molecular characterization. <ref type="bibr" target="#b4">(Duvenaud et al. 2015)</ref> propose graph convolutional network that operates directly on graph, which could learn better representations than conventional method like ECFP. (Altae-Tran et al. 2017) combine graph convolutional network with residual LSTM embedding for one-shot learning on drug discovery. <ref type="bibr" target="#b26">(Yao et al. 2017;</ref><ref type="bibr" target="#b21">Sch√ºtt et al. 2017</ref>) apply neural network to predicate the molecular energy and reduced the predication error to 1 kcal/mol.  propose atomic neural network to predicate the binding free energy of a subset of protein-ligand complexes found in the PDBBind dataset. <ref type="bibr" target="#b15">(Ramsundar et al. 2015)</ref> applied Massively multitask neural architectures to synthesize information from many distinct biological sources. <ref type="bibr" target="#b25">(Wu et al. 2017)</ref> introduce MoleculeNet, a large scale benchmark for molecular machine learning, which contains multiple public datasets, and establish metrics for evaluation and high quality open-source implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph neural network</head><p>Graph-based Neural networks have previously introduced in(Gori, <ref type="bibr" target="#b5">Monfardini, and Scarselli 2005;</ref><ref type="bibr" target="#b20">Scarselli et al. 2009</ref>) as a form of recurrent neural network. <ref type="bibr" target="#b10">(Li et al. 2015)</ref> modify the graph neural network by using gated recurrent units and modern optimization techniques and then extend to output sequences. Spectral graph convolutional neural networks are introduced by <ref type="bibr" target="#b1">(Bruna et al. 2013</ref>) and later extended by <ref type="bibr" target="#b3">(Defferrard, Bresson, and Vandergheynst 2016)</ref> with fast localized convolutions. <ref type="bibr" target="#b7">(Kipf and Welling 2016)</ref> introduced a number of simplifications to spectral graph convolutinal neural network and improve scalibility and classification performance in large-scale networks. <ref type="bibr" target="#b2">(Cao, Lu, and Xu 2016</ref>) propose a novel model for learning graph representations, which generates a low-dimensional vector representation for each vertex by capturing the graph structural information. <ref type="bibr" target="#b14">(Niepert, Ahmed, and Kutzkov 2016)</ref> propose a framework for learning convolutional neural networks for arbitrary graphs and applied it to molecule classification.</p><p>All above work focus on learning node-level representation in graph rather than graph-level representation. However, predicating molecular properties is in fact a problem of graph-level classification and regression. In this paper, we introduce a dummy super node that is connected to all other nodes by a directed edge to learn graph-level representation rather than simply using the sum of the vectors of all nodes as the graph-level representation <ref type="bibr" target="#b4">(Duvenaud et al. 2015;</ref><ref type="bibr" target="#b0">Altae-Tran et al. 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Graph-Level Representation</head><p>For most macroscopic molecular properties predication tasks, like efficacy and toxicity, we could neglect the detailed edge information and take a molecule as an undirected graph <ref type="bibr" target="#b25">(Wu et al. 2017)</ref>. We apply graph convolutional networks <ref type="bibr" target="#b4">(Duvenaud et al. 2015)</ref> to learn representation of the atoms(nodes) in the molecule. To learn the representation of the molecule(graph), we introduce a dummy super node that is connected with all nodes in the graph by a directed edge. We modify the graph operation to help the dummy super node learn graph-level feature, and utilize neural network as a classifier for graph-level properties predication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolutional Network</head><p>A molecule could be modeled as a graph, the nodes of which stand for atoms and the edges of which represent chemical bonds linking some of the atoms together. In graph convolution network, for a specific node, we separately feed it and its neighbours into two dense layer, and calculate the sum as the new features of the node. The weights are shared between dense layers that operate on different node with the same degree. Formally, for a specific node v, that have to-</p><formula xml:id="formula_0">tally d neighbours n i (1 ‚â§ i ‚â§ d), the new feature of the node v of graph convolution operation is formulated as v = W d self v + 1‚â§i‚â§d W d nb n i + b d<label>(1)</label></formula><p>where W d self is the weight for self node, W d nb is the weight for neighbour node, boldsymbolb d is the bias. These weights vary from different degree d of the specific node, for that the nodes with different number of neighbours are affected by neighbours in different ways. Since the output of graph convolution layer is also a graph, we could continually apply graph convolution layers to the graph. Thus, the receptive field of the network would be larger, and the nodes S and all nodes are fed into dense layer and summed Max over v and its neighbors <ref type="figure">Figure 1</ref>: Graphical representation of the major graph operation. The dark blue node represents the specific node v, the light blue nodes represent the neighbours of v, the dotted node represents the dummy super node S. The orange and yellow arrows represent dense layer with different weights. The red arrows represent max polling, the black arrow represents identical mapping. The dummy super node is connected with all other nodes by a directed edge. The directed edge indicates that the dummy super node learns feature from genuine nodes but does not affect the feature of genuine nodes. Note that, for GraphConv and GraphPool, the operation is shown for a single node v, however, these operations are performed on all nodes in graph simultaneously in fact.</p><p>would be affected by further neighbours. The left thumbnail in figure illustrates the graph convolution operation. The yellow arrows indicate the dense layer with weight W d nb for neighbours, representing the neighbours' effect on the specific node. The orange arrows indicate the dense layer with weight W d self for the specific node self. In analogy to pooling layers in convolutional neural networks, Graph Pooling is introduced by (Altae-Tran et al. 2017). Graph Polling is a simple operation that returns the maximum activation across the node and its neighbours. Simple illustration is shown in <ref type="figure">Figure .</ref> With graph pooling, we could easily enlarge the receptive field without adding extra weights. However, graph pooling does not change the size of the graph as the pooling of CNN does to the features of images.</p><p>Although there is no real convolution operation in graph convolutional network, graph convolutional network is design for similar reason to convolutional neural network, the 'local geometry' and reasonable weight sharing. Convolutional networks <ref type="bibr" target="#b9">(LeCun et al. 1998</ref>) apply convolutions to exploit the translation equivariance of image features. And the feature learned form CNN is translation equivariant to some extent. Graph convolutional network also focus on learning local feature, through sharing weight with dense layers operate on nodes with the same number of degree. Thus, the feature learned by graph convolutional is determined by local neighbours rather the global position of nodes. Such features are equivariant to permutation of atomic group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dummy Super Node</head><p>Learning graph-level is a central problem of molecule classification and regression. Modern graph convolution networks often contain two graph convolution layers and two graph polling layers, the receptive field of the such model is too small, especially compared with the size of most drug compounds. Simply summing the node-level features can not enlarge the receptive of the model in a real sense. A naive way to enlarge the receptive field is simply applying more graph convolutional layers. However, such method makes model more likely to overfit and perform worse in validation and test subset for the lack of enough data in drug datasets.</p><p>In order to learn better graph-level representations, we introduce a dummy super node that is connected with all nodes in the graph by a directed edge, as the representation of the graph. Since the dummy super node is directly connected with all nodes in graph, it can easily learn global feature through one graph convolution layer. The directed edge pointed to the dummy super node from other genuine nodes, indicates that the dummy super node could learn features from all other genuine nodes, while none of the genuine nodes would be affected by the dummy super node. Consequently, the dummy super node could learn the global feature while the genuine nodes keep learning local features that is equivariant to permutation of atomic group. Since the feature of molecules is likely to be more complex than that of atoms, we could use a longer vector as the feature of the dummy super node.</p><p>Formally, we could formulate the new feature of the dummy super node S of graph convolution layer as</p><formula xml:id="formula_1">S = W self S + 1‚â§i‚â§N W nb v i + b<label>(2)</label></formula><p>where W self is the weight for the dummy super node, W nb is the weight for genuine node, N is the number of nodes, v i is the feature of ith node, b is the bias. The dummy super node S is initialized to zero and get updated through graph convolutional networks simultaneously as the genuine nodes do.</p><p>GC: Graph Convolution GP: Graph Pooling BN: Node-Level Batch Normalization GC+RELU+BN GP GC+RELU+BN GP GC+RELU+BN Two-Layer Classifier <ref type="figure">Figure 2</ref>: Our Network Structure. We apply three graph convolution blocks(GC + RELU + BN) and two graph pooling layers, then we feed the feature of the dummy super node to a two-layer classifier. The batch normalization used here is node-level batch normalization. The third graph convolution block only calculates the feature of the dummy super node, for the new feature of other nodes would not be fed into following classifier.</p><p>The new features of genuine nodes are not affected by the dummy super node, and keep the same as Equation 1. Graph pooling is only applied to genuine nodes, and the dummy super node is not regarded as neighbours of other nodes when graph pooling is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Structure</head><p>In addition to graph operations, we also apply node-level batch normalization, relu activation, tanh activation and dense layer in the model. For some datasets with imbalance data, we replace common cross entropy loss with focal loss. The whole network structure is shown in <ref type="figure">Figure .</ref> We add the dummy super node and its edges to the original graph before sending it to the network. In our model, We apply three graph convolution blocks(GC + RELU + BN) and two graph pooling layers, then we feed the feature of the dummy super node to a two-layer classifier. The batch normalization used there is node-level batch normalization that would be discussed later. Only the feature of the dummy super node is calculated in the third graph convolution block, for the new feature of other nodes would not be fed into following classifier.</p><p>Unlike the ResNet, we place relu activation before batch normalization, for the experiment shows that it is hard to train when we place relu activation behind. We argue that the molecule feature is different from that of images. The noise in images usually does not affect the image label, while that in molecules not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node-Level Batch Normalization</head><p>Since the number of atoms vary from molecules, standard batch normalization <ref type="bibr" target="#b6">(Ioffe and Szegedy 2015)</ref> can not be applied to graph convolution network directly. We extend standard batch normalization to node-level batch normalization. We normalize the feature of each node, make it zero mean and unit variance. Since the dummy super node is also a node, we simply apply identical normalization to the dummy super node as well.</p><p>Focal Loss Data in drug datasets is often unbalanced. Take HIV dataset as instance, there are only 1487 chemical compounds that are HIV active in the dataset of totally 41913 chemical compounds. However, previous work <ref type="bibr" target="#b0">(Altae-Tran et al. 2017;</ref><ref type="bibr" target="#b25">Wu et al. 2017)</ref> do not pay much attention to the fact of unbalanced data in Drug dataset.</p><p>Downsample is a simple method to tackle with unbalanced data. However, the number of data in drug datasets is always not large for the difficulty of acquiring data and get them labeled, and simple downsample may result in serious overfitting of neural network for the lack of sufficient data. In addition, Boosting is an effective resample method that may address imbalance data. But it's not easy to integrate boosting with deepchem <ref type="bibr" target="#b25">(Wu et al. 2017)</ref>, the modern chemical machine learning framework.</p><p>Focal Loss is an elegant and brief proposal that introduced by <ref type="bibr" target="#b11">(Lin et al. 2017)</ref> to address class imbalance. It shows that focal loss could effectively improve the performance of onestage detector in object detection. In fact, Focal Loss is a kind of reshaped cross entropy loss that the weighs of wellclassified examples are reduced. Formally, Focal Loss is defined as F L(p) = ‚àí(y(1 ‚àí p) Œ≥ log p + (1 ‚àí y)p Œ≥ log(1 ‚àí p)).</p><p>where y ‚àà {0, 1} specifies the ground-truth class, p ‚àà [0, 1] is the model's estimated probability for the class with label y = 1, Œ≥ is tunable focusing parameter. When Œ≥ = 0, focal loss is equivalent to cross entropy loss, and as Œ≥ is increased the effect of the modulating factor is likewise increased. Focal loss focuses on training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the classifier during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>We evaluate our model on several datasets of MoleculeNet, ranging from drug activity, toxity, solvation. Our code is released in Github 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoleculeNet</head><p>MoleculeNet is a dataset collection built upon multiple public databases, covering various levels of molecular properties, ranging from atomic-level properties to macroscopic influences on human body.</p><p>For chemical data, random splitting datasets into train/valid/test collections that is widely used in machine learning, is often not correct <ref type="bibr" target="#b22">(Sheridan 2013)</ref>. Consequently, MoleculeNet implements multiple different splittings(Index Split, Random Split, Scaffold Split) for each dataset. Scaffold split attempts to separate structurally molecules in the training/validation/test sets, thus the scaffold split offers a greater challenge and demands a higher level of generalization ability for learning algorithms than index split and random split.</p><p>For quite a few datasets in MoleculeNet, the number of the positive samples and the number of negative samples is not balanced. Thus the accuracy metrics widely used in machine learning classification tasks are not suitable here. In Molecu-leNet, classification tasks are evaluated by area under curve (AUC) of the receiver operating characteristic (ROC) curve, and regression tasks are evaluated by squared Pearson correlation coefficient (R2).</p><p>We pick following datasets(Tox21, ToxCast, HIV, MUV, PCBA, FreeSolv) on macroscopic chemical influences on human body from MoleculeNet, and evaluate our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tox21</head><p>The Toxicology in the 21st Century (Tox21) initiative created a public database measuring toxicity of compounds, which has been used in the 2014 Tox21 Data Challenge(tox ). Tox21 contains qualitative toxicity measurements for 8014 compounds on 12 different targets, including stress response pathways and nuclear receptors.</p><p>ToxCast ToxCast is another data collection (from the same initiative as Tox21) providing toxicology data for a large library of compounds based on in virtual highthroughput screening <ref type="bibr" target="#b16">(Richard et al. 2016</ref>). The processed collection in MoleculeNet contains qualitative results of over 600 experiments on 8615 compounds.</p><p>MUV The MUV dataset(Rohrer and Baumann 2009) contains 17 challenging tasks for around 90,000 compounds and is specifically designed for virtual screening techniques. The positives examples in these datasets are selected to be structurally distinct from one another.</p><p>PCBA PubChem BioAssay (PCBA) is a database that consists of biological activities of small molecules generated by high-throughput screening <ref type="bibr" target="#b23">(Wang et al. 2011)</ref>. The processed collection in MoleculeNet is a subset that contains 128 bioassays measured over 400,000 compounds.</p><p>HIV The HIV dataset was introduced by the Drug Therapeutics Program (DTP) AIDS Antiviral Screen, which tested the ability to inhibit HIV replication for 41,913 compounds(hiv ). Screening results were evaluated and placed into three categories: confirmed inactive (CI), confirmed active (CA) and confirmed moderately active (CM). In MoleculeNet, the latter two labels are combined, making it a classification task between inactive (CI) and active (CA and CM).</p><p>FreeSolv The Free Solvation Database (FreeSolv) provides experimental and calculated hydration free energy of small molecules in water . Free-Solv contains 643 compounds, a subset of which are also used in the SAMPL blind prediction challenge ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings and Performance Comparisons</head><p>We apply the identical network to all above datasets, except for the output channel number of the last layer that is determined by the number of tasks. For different datasets, we only tune the hyperparameter of training. We compare our model with logistic regression with ECFP feature as input and standard graph convolution network. To have a fair comparison, we reimplement graph convolution network, and most of the performance of our implements are much better than that reported by <ref type="bibr" target="#b25">(Wu et al. 2017)</ref>.</p><p>Classification Tasks For classification tasks(Tox21, Tox-Cast, MUV, PCBA), we utilize the area under curve (AUC) of the ROC curve as metric(larger is better). The results are shown in <ref type="table" target="#tab_1">Table 2</ref>, <ref type="table" target="#tab_2">Table 3</ref>, <ref type="table" target="#tab_3">Table 5 and Table 4</ref>. Our model generally surpasses standard graph convolution network and conventional logistic regression in both small datasets on toxity(Tox21, ToxCast) and larger datasets on bioactivities(MUV, PCBA). It indicates that our method could generally improve the performance of graph convolution network. Compared with standard graph convolution network, in average, our model achieves an improvement of about 1.5% in the test subset of the four datasets.</p><p>Regression Tasks For regression tasks(FreeSolv), we utilize squared Pearson correlation coefficient(R 2 ), RMSE, MAE as metrics. The experimental results are shown in Table 1. We only report the performance of graph convolution network and our model here. Our model has a better performance in general. We argue that our model failed to surpass graph convolution network in validation subset under random splitting for stochastic factors introduced by splitting     be predicated in FreeSolv has been widely used as a test of computational chemistry methods. With energy values ranging from -25.5 to 3.4kcal/mol in the FreeSolv dataset, the RMSE of ab-initio calculations results reach up to 1.5kcal/mol . Our methods clearly outperform ab-initio calculations when we utilize index split and random split. However, our method fails to outperform ab-initio calculations when the dataset is split by scaffold split that separate molecules structurally. We argue that it is the insufficient data that results in weak generalization ability of our model. When fed with enough data, our model may overall surpass classic ab-initio calculations.</p><p>imbalance Class We apply focal loss in HIV dataset that have unbalanced class. The experimental results of <ref type="table" target="#tab_5">Table  6</ref> shown that focal loss could further improve the performance. It is interesting that our model overfits quickly in train dataset when we apply focal loss. We argue that focal loss helps our model handle hard example and fit better in train dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we point out that molecular properties predication demand graph-level representation. However, most of the previous works on graph neural network only focus on learning node-level representation. Such representation is clearly not sufficient for molecular properties predication. In order to learn graph-level representation, we propose the dummy super node that is connected with all nodes in the graph by a directed edge and modify the graph operation to help the dummy super node learn graph-level feature. Thus, we can handle graph-level classification and regression in the same way as node-level classification and regression. The experiment on MoleculeNet shows that our method could generally improve the performance of graph convolution network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>v</head><label></label><figDesc>and its neighbors are fed into dense layer and summed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The Performance in FreeSolv dataset. R 2 is squared Pearson correlation coefficient(larger is better). Since R 2 used by<ref type="bibr" target="#b25">(Wu et al. 2017)</ref> can not perfectly reflect the performance of the model, we provide with RMSE and MAE(kcal/mol) as well.</figDesc><table><row><cell>Model</cell><cell>Split Method</cell><cell>R 2</cell><cell cols="2">Valid RMSE MAE</cell><cell>R 2</cell><cell cols="2">Test RMSE MAE</cell></row><row><cell></cell><cell>Index</cell><cell>0.935</cell><cell>0.909</cell><cell cols="2">0.703 0.941</cell><cell>0.963</cell><cell>0.738</cell></row><row><cell>GraphConv</cell><cell>Random</cell><cell>0.928</cell><cell>0.982</cell><cell cols="2">0.644 0.895</cell><cell>1.228</cell><cell>0.803</cell></row><row><cell></cell><cell>Scaffold</cell><cell>0.883</cell><cell>2.115</cell><cell cols="2">1.555 0.709</cell><cell>2.067</cell><cell>1.535</cell></row><row><cell></cell><cell>Index</cell><cell>0.952</cell><cell>0.787</cell><cell cols="2">0.566 0.945</cell><cell>0.933</cell><cell>0.598</cell></row><row><cell>Our</cell><cell>Random</cell><cell>0.933</cell><cell>1.010</cell><cell cols="2">0.652 0.910</cell><cell>1.112</cell><cell>0.659</cell></row><row><cell></cell><cell>Scaffold</cell><cell>0.884</cell><cell>2.076</cell><cell cols="2">1.404 0.746</cell><cell>1.939</cell><cell>1.415</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The area under curve (AUC) of the ROC curve of various models in Tox21 dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">Split Method Train Valid</cell><cell>Test</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.903 0.704 0.738</cell></row><row><cell>ECFP+LR</cell><cell>Random</cell><cell cols="2">0.901 0.742 0.755</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.905 0.651 0.697</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.945 0.829 0.820</cell></row><row><cell>GraphConv</cell><cell>Random</cell><cell cols="2">0.938 0.833 0.846</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.955 0.778 0.752</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.965 0.839 0.848</cell></row><row><cell>Our</cell><cell>Random</cell><cell cols="2">0.964 0.842 0.854</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.971 0.788 0.759</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The area under curve (AUC) of the ROC curve of various models in ToxCast dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">Split Method Train Valid</cell><cell>Test</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.727 0.578 0.464</cell></row><row><cell>ECFP+LR</cell><cell>Random</cell><cell cols="2">0.713 0.538 0.557</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.717 0.496 0.496</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.904 0.723 0.708</cell></row><row><cell>GraphConv</cell><cell>Random</cell><cell cols="2">0.901 0.734 0.754</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.914 0.662 0.640</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.927 0.747 0.734</cell></row><row><cell>Our</cell><cell>Random</cell><cell cols="2">0.924 0.746 0.768</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.929 0.696 0.657</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The area under curve (AUC) of the ROC curve of various models in PCBA dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">Split Method Train Valid</cell><cell>Test</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.809 0.776 0.781</cell></row><row><cell>ECFP+LR</cell><cell>Random</cell><cell cols="2">0.808 0.772 0.773</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.811 0.746 0.757</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.895 0.855 0.851</cell></row><row><cell>GraphConv</cell><cell>Random</cell><cell cols="2">0.896 0.854 0.855</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.900 0.829 0.829</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.904 0.869 0.864</cell></row><row><cell>Our</cell><cell>Random</cell><cell cols="2">0.899 0.863 0.867</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.907 0.847 0.845</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The area under curve (AUC) of the ROC curve of various models in MUV dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">Split Method Train Valid</cell><cell>Test</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.960 0.773 0.717</cell></row><row><cell>ECFP+LR</cell><cell>Random</cell><cell cols="2">0.954 0.780 0.740</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.956 0.702 0.712</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.951 0.816 0.792</cell></row><row><cell>GraphConv</cell><cell>Random</cell><cell cols="2">0.949 0.787 0.836</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.979 0.779 0.735</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.982 0.816 0.795</cell></row><row><cell>Our</cell><cell>Random</cell><cell cols="2">0.989 0.800 0.845</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.990 0.816 0.762</cell></row><row><cell cols="4">method and too few numbers of compounds in FreeSolv.</cell></row><row><cell cols="4">It is interesting that how our method competes with clas-</cell></row><row><cell cols="4">sic ab-initio calculations. Hydration free energy that should</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The area under curve (AUC) of the ROC curve of various models in HIV dataset. With focal loss(Œ≥ = 2), our method achieves further improvement.</figDesc><table><row><cell>Model</cell><cell cols="2">Split Method Train Valid</cell><cell>Test</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.864 0.739 0.741</cell></row><row><cell>ECFP+LR</cell><cell>Random</cell><cell cols="2">0.860 0.806 0.809</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.858 0.798 0.738</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.945 0.779 0.728</cell></row><row><cell>GraphConv</cell><cell>Random</cell><cell cols="2">0.939 0.835 0.822</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.938 0.795 0.769</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.973 0.789 0.737</cell></row><row><cell>Our</cell><cell>Random</cell><cell cols="2">0.951 0.842 0.830</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.967 0.813 0.763</cell></row><row><cell></cell><cell>Index</cell><cell cols="2">0.993 0.793 0.749</cell></row><row><cell>Our + Focal Loss</cell><cell>Random</cell><cell cols="2">0.993 0.843 0.851</cell></row><row><cell></cell><cell>Scaffold</cell><cell cols="2">0.992 0.816 0.776</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/microljy/graph_level_ drug_discovery</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Low data drug discovery with one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Altae-Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Atomic convolutional networks for predicting proteinligand binding affinity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10603</idno>
		<ptr target="https://wiki.nci.nih.gov/display/NCIDTPdata/" />
	</analytic>
	<monogr>
		<title level="m">Neural Networks, 2005. IJCNN&apos;05. Proceedings. 2005 IEEE International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A new model for learning in graph domains</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Freesolv: a database of experimental and calculated hydration free energies, with input files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Mobley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Guthrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">711</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Blind prediction of solvation free energies from the sampl4 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Mobley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Wymer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Guthrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput Aided Mol Des</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="135" to="150" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Konerding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02072</idno>
		<title level="m">Massively multitask networks for drug discovery</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toxcast chemical landscape: paving the road to 21st century toxicology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Judson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Houck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Grulke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Volarath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Thillainadarajah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rathman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Wambaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical research in toxicology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1225" to="1251" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Maximum unbiased validation (muv) data sets for virtual screening based on pubchem bioactivity data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Rohrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="184" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast and accurate modeling of molecular atomization energies with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">58301</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch√ºtt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Time-split cross-validation as a method for estimating the goodness of prospective prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Sheridan</surname></persName>
		</author>
		<ptr target="https://tripod.nih.gov/tox21/challenge/" />
	</analytic>
	<monogr>
		<title level="m">Tox21 data challenge</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="783" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pubchem&apos;s bioassay database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">O</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karapetyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dracheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Shoemaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="400" to="412" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An analysis of the attrition of drug candidates from four major pharmaceutical companies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Waring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arrowsmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Leeson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pairaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Pennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews. Drug discovery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">475</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Moleculenet: A benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00564</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Intrinsic bond energies from a bonds-in-molecules neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Herr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Parkhill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physical Chemistry Letters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
