<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADVERSARIAL DEEP EMBEDDED CLUSTERING: ON A BETTER TRADE-OFF BETWEEN FEATURE RANDOMNESS AND FEATURE DRIFT A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-27">September 27, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nairouz</forename><surname>Mrabah</surname></persName>
							<email>mrabah.nairouz@courrier.uqam.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Quebec at Montreal Montreal</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Bouguessa</surname></persName>
							<email>bouguessa.mohamed@uqam.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Quebec at Montreal Montreal</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riadh</forename><surname>Ksantini</surname></persName>
							<email>ksantini@uwindsor.ca</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Windsor Windsor</orgName>
								<address>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ADVERSARIAL DEEP EMBEDDED CLUSTERING: ON A BETTER TRADE-OFF BETWEEN FEATURE RANDOMNESS AND FEATURE DRIFT A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-27">September 27, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised Learning</term>
					<term>Deep Learning</term>
					<term>Clustering</term>
					<term>Autoencoders</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clustering using deep autoencoders has been thoroughly investigated in recent years. Current approaches rely on simultaneously learning embedded features and clustering the data points in the latent space. Although numerous deep clustering approaches outperform the shallow models in achieving favorable results on several high-semantic datasets, a critical weakness of such models has been overlooked. In the absence of concrete supervisory signals, the embedded clustering objective function may distort the latent space by learning from unreliable pseudo-labels. Thus, the network can learn non-representative features, which in turn undermines the discriminative ability, yielding worse pseudo-labels. In order to alleviate the effect of random discriminative features, modern autoencoder-based clustering papers propose to use the reconstruction loss for pretraining and as a regularizer during the clustering phase. Nevertheless, a clustering-reconstruction trade-off can cause the Feature Drift phenomena. In this paper, we propose ADEC (Adversarial Deep Embedded Clustering) a novel autoencoder-based clustering model, which addresses a dual problem, namely, Feature Randomness and Feature Drift, using adversarial training. We empirically demonstrate the suitability of our model on handling these problems using benchmark real datasets. Experimental results validate that our model outperforms state-of-the-art autoencoder-based clustering methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Meanwhile, Generative Adversarial Network (GAN) <ref type="bibr" target="#b20">[21]</ref> has shown great promise in learning complex natural data distributions. It allows to synthesize out-of-sample data points. Besides, it has been shown that GAN can obtain images with impressive visual quality <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b5">6]</ref>. Apart from being a successful generative model, the adversarial training strategy has inspired several modern achievements on unsupervised representation learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10]</ref>. Although GAN does not come with an encoder out-of-the-box, some recent papers have suggested to extend the classical GAN framework to permit data encoding in a latent space, where the semantic factors of variations and similarities are better-emphasized <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b56">57]</ref>. Nevertheless, it is still unclear to what extent the features learned, based on a deep generative model, can be useful for downstream discriminative tasks (e.g., classification and clustering).</p><p>To address the aforementioned problems, we propose Adversarial Deep Embedded Clustering (ADEC). Our framework consists of eliminating the strong competition between embedded clustering and reconstruction without incurring a Feature Randomness cost. This is done by getting the strong competition outside of a single network, while relying on a discriminator, in order to make sure that the embedded representations preserve the intrinsic data characteristics. Optimizing every objective function in a separate network , based on adversarial training, allows to reach a better trade-off between Feature Randomness and Feature Drift. Experimentation on real benchmark datasets shows the superiority of our framework, in terms of accuracy (ACC) and normalized mutual information (NMI). In a nutshell, the key contributions of this paper are:</p><p>• Devising a new pretraing framework based on adversarially constrained interpolation and data transformation.</p><p>• Overcoming the clustering-reconstruction compromise based on an adversarial training strategy.</p><p>• Enhancing state-of-the-art autoencoder-based clustering by alleviating Feature Randomness and Feature Drift.</p><p>• Outperforming modern clustering models in terms of ACC and NMI on real benchmark datasets.</p><p>The rest of this paper has the following organization: Section 2 is devoted to related work. Section 3 presents an analysis of the trade-off between Feature Randomness and Feature Drift. In section 4, we present our methodology for tackling the Feature Randomness and Feature Drift problems. In Section 5, we show our experimental results. Finally, section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>To demonstrate the merit of our proposed framework, we provide a critical review of mainstream approaches related to our work. ADEC comes under the realm of deep clustering strategies. Furthermore, it is deemed to be part of the concerted effort to combine GANs and autoencoders. To this end, we shall review the existing deep clustering approaches and the unifying techniques of GANs and autoencoders. We should also review DEC <ref type="bibr" target="#b63">[64]</ref> and IDEC <ref type="bibr" target="#b21">[22]</ref> since they constitute our principal baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Clustering</head><p>In deep unsupervised learning, typically, there are two conceivable options to make up for the absence of supervisory signals. The first option consists of contriving a pretext task that encourages to learn general-purpose features. It is better known as self-supervision. For this case, labels are extracted from the input data. The intuition behind self-supervision is that the pretext task can not be solved efficiently without gaining a semantically high-level grasp of the input data. The obtained features can be used to outsource downstream tasks, such as, classification. There exists a wide variety of pretext tasks. Among them, the vanilla reconstruction, the denoising loss <ref type="bibr" target="#b59">[60]</ref>, the variational loss <ref type="bibr" target="#b33">[34]</ref>, the adversarial loss <ref type="bibr" target="#b20">[21]</ref>, predicting the location of image patches <ref type="bibr" target="#b15">[16]</ref>, predicting the permutations of a "jigsaw puzzle" <ref type="bibr" target="#b45">[46]</ref>, predicting unpainted image patches <ref type="bibr" target="#b46">[47]</ref>, and predicting image colorization <ref type="bibr" target="#b69">[70]</ref>. The second option consists of contriving a pseudo supervisory signal. Similar to self-supervision, the labeling is available within the data. However, for pseudo-supervision, labels are predicted. Therefore, they are not 100% correct. In this paper, L s denotes a self-supervised loss and L p denotes a pseudo-supervised loss.</p><p>A possible categorization of deep clustering methods can be imputed to the used loss functions and the way they are combined. Based on that, the existing models fall into three main categories. In <ref type="figure" target="#fig_0">Figure 1</ref>, the framework of each category is illustrated. Within the actual context, the pseudo-supervised loss stands for the embedded clustering objective function, which can be any one of the typical clustering objective functions, such as, Gaussian Mixture Model (GMM) or k-means. As regards the self-supervised cost, reconstruction is commonly selected.</p><p>For the first category, the clustering is directly performed using a pseudo-supervised loss. However, the self-acquired labels are unreliable due to their hypothetical aspect. This can mislead the data grouping by learning non-representative features, which in turn deteriorates the discriminative ability of the model. Concisely, the main weakness of the methods affiliated with this category is Feature Randomness. As part of this category, Yang et al. proposed JULE <ref type="bibr" target="#b65">[66]</ref>, a deep recurrent framework that allows to perform agglomerative clustering and feature learning alongside with a unified triplet loss. The whole process is optimized end-to-end. However, one among the prominent downsides of JULE is the run-time overhead due to the recurrent framework. Chang et al. proposed DAC <ref type="bibr" target="#b8">[9]</ref>, a framework that enables to cluster the data based on pairwise constraints. DAC has curriculum learning strategy, where only high-confidence training samples are selected. In another line of research, Hu et al. proposed IMSAT <ref type="bibr" target="#b26">[27]</ref>, which consists of maximizing the mutual information between discrete predictions and their associated data points. The loss function of IMSAT is regularized by a self-augmented training term that allows to penalize the discrepancy between initial data and their geometrically transformed ones. DeepCluster <ref type="bibr" target="#b7">[8]</ref> is another framework intimately tied to this category. It alternates between two basic steps. First, the latent representations are clustered by k-means. Then, the obtained clustering assignments are fed to a convolutional neural network as supervisory signals to learn better features. It was mainly applied to large-scale datasets.</p><p>As for the second category, the network is pretrained using a self-supervised cost function. Then, the obtained latent features are finetuned by retraining using pseudo labels. Compared with the random initialization, pretraining with a self-supervised loss leads to improved initial features. Nevertheless, there is no correction mechanism to attenuate the noisy labels harm. Hence, Feature Randomness is a strongly remaining concern for this category. As part of this category, DEC <ref type="bibr" target="#b63">[64]</ref> is the first deep clustering framework to follow a pretraining-finetuning strategy.</p><p>The third category consists of pretraining using a self-supervised cost function similar to the second category. However, their finetuning phases are different. In fact, the third category regularizes the pseudo-supervised objective function with a self-supervised one. The advantage of such a strategy is that it offers a mechanism to reduce Feature Randomness. However, combining pseudo-supervision and self-supervision can lead to a strong competition between them. In order to balance the two cost functions, a hyperparameter is required. To give an example, Guo et al. proposed IDEC <ref type="bibr" target="#b21">[22]</ref> and Dizaji et al. proposed DEPICT <ref type="bibr" target="#b14">[15]</ref>. Both models can be considered as extensions to DEC. They regularize the clustering loss with reconstruction during the finetuning phase. Therefore, the decoder is maintained throughout the whole training process. The main difference between them is that DEPICT utilizes a convolutional architecture, whereas IDEC leverages a fully-connected autoencoder. Apart from that, Yang et al. proposed DCN <ref type="bibr" target="#b64">[65]</ref>. Compared with IDEC and DEPICT, DCN optimizes a latent k-means objective function. VaDE <ref type="bibr" target="#b31">[32]</ref> is another model from this category. Its variational auto-encoding architecture allows to impose a GMM latent distribution. Thanks to the reparameterization trick, VaDE can be optimized using backpropagation. In addition to clustering, VaDE can generate data samples. Even so, it is subjected to elevated computational complexity similar to all the other variational frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Embedding Clustering (DEC)</head><p>DEC <ref type="bibr" target="#b63">[64]</ref> has two phases. The pretraining phase allows to learn low-dimensional embedded representations by training the autoencoder with vanilla reconstruction. Then, comes the clustering phase. First, the decoder is discarded. After that, the encoder is trained to jointly optimize the embedded representations and the clustering centers. For every training iteration, a soft clustering assignment q ij (1) is computed based on the Student's t-distribution. q ij represents an assessment of the between the embedded data point z i and the center µ j .</p><formula xml:id="formula_0">q ij = (1 + zi−µj 2 α ) − α+1 2 j (1 + zi−µ j 2 α ) − α+1 2 .<label>(1)</label></formula><p>The DEC loss function <ref type="formula" target="#formula_1">(2)</ref> is the Kullback Leibler divergence between the soft clustering assignment q ij and an auxiliary target distribution p ij <ref type="bibr" target="#b2">(3)</ref>.</p><formula xml:id="formula_1">L DEC = KL(P ||Q) = i j p ij log( p ij q ij ),<label>(2)</label></formula><formula xml:id="formula_2">p ij = q 2 ij /f j j q 2 ij /f j .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Improved Deep Embedded Clustering (IDEC)</head><p>IDEC <ref type="bibr" target="#b21">[22]</ref> has the same pretraining phase as DEC. The main difference between them is that IDEC is finetuned to minimize joint embedded clustering and reconstruction as described by <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_3">L IDEC = L r + γL DEC .<label>(4)</label></formula><p>L r stands for the reconstruction and (γ &gt; 0) is in charge of balancing the two costs. The key idea of IDEC is to block the clustering loss from corrupting the feature space. However, we argue that combining embedded clustering and vanilla reconstruction gives birth to a strong competition between them (i.e., Feature Drift).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Combining Autoencoders with GANs</head><p>Interpolating data samples from the prior distribution, in the latent space of the generator, leads to realistic and semantically explainable variations <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b5">6]</ref>. As a consequence of GAN effectiveness in capturing the semantic factors of variations, many researchers have studied the inverse mapping problem (i.e., projecting the data back in the embedded space) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b56">57]</ref>. As coupled with the generator, an encoder can potentially learn to produce latent high-semantic features from the initial data distribution. This can bring an important advancement in solving inverse problems (e.g., image inpainting and noise removal) and downstream discrimination tasks. Two of the most seminal contributions on combining the power of GANs with Autoencoders, are BiGAN <ref type="bibr" target="#b16">[17]</ref> and AAE <ref type="bibr" target="#b42">[43]</ref>.</p><p>Although we use the same architectural components (i.e., Encoder, Decoder and Discriminator), our framework differs from the previous mentioned ones in several glaring aspects. First, our discriminator operates in the data space. In contrast, the critic of AAE processes samples from the latent space, while BiGAN framework concatenates a sample from the data space with its projection in the embedded space, before feeding it to the discriminator. Second, in BiGAN, the encoder and decoder can not directly communicate with each other. Therefore, the objective function of this approach does not have an explicit reconstruction cost. However, AAE and our model explicitly minimize the cycle cost. Unlike AAE, where both the encoder and decoder are trained to perform reconstruction, our encoder weights are frozen, while optimizing with respect to the cycle loss, in order to avoid drifting the features learned using the clustering loss. Moreover, in BiGAN and AAE, the encoder and decoder are trained jointly in competition with the discriminator network. However, in our framework, each network is trained separately. Furthermore, AAE and BiGAN are standard generative models. So, in order to allow sampling, they enforce the aggregated posterior to match an arbitrary prior. However, in our case, we do not impose any hypothetical prior distribution and the adversarial training strategy is introduced to tackle problems related to embedded clustering, that is, Features Randomness and Feature Drift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Trade-Off Between Feature Randomness and Feature Drift</head><p>In this section, we propose a mathematical formalism to characterize Feature Randomness and Feature Drift. We explain the identified problems and we shed light on the trade-off between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Randomness</head><p>For a pseudo-supervised loss, the used labels are predicted based on a presumptive similarity metric. It then follows that part of the pseudo-labels mismatch the real ones. Zhang et al. <ref type="bibr" target="#b68">[69]</ref> showed empirically that standard deep neural networks can easily and perfectly fit completely random labels without any considerable time overhead, using the same hyperparameters and architecture as used for training with correct labels. This result suggests that a neural network has sufficient capacity to memorize the whole dataset even when there is little or no correlation between the training samples and their corresponding labels.</p><p>We call Feature Randomness the training of a neural network using pseudo-labels. Put it differently, Feature Randomness takes place when a significant portion of the true labels are substituted by random ones. At every iteration of a neural network optimization process, Feature Randomness can be characterized by ∆ F R <ref type="bibr" target="#b4">(5)</ref>. ∆ F R is the cosine of the angle between the gradient of the unsupervised loss and the gradient of the real supervised loss w.r.t the network parameters w. y true denotes the true labels (100% correct) and y pseudo denotes the pseudo-labels (partially correct).</p><formula xml:id="formula_4">∆ F R = cos( ∂L(x, y true , w) ∂w , ∂L(x, y pseudo , w) ∂w ).<label>(5)</label></formula><p>Training with pseudo-labels deteriorates the generalization capacity of a neural network. In fact, it enforces learning features that emphasize similarities between uncorrelated data points. In order to reduce the harm of Feature Randomness, a possible solution consists of adjusting the gradient of the pseudo-supervised loss L p by another vector. The gradient of L s is a candidate to be that vector for several reasons. First, it is well known that minimizing L s generates reasonable general-purpose features. Second, the self-acquired labels for L s are 100% correct. Hence, minimizing L s does not contribute to Feature Randomness. Besides, self-supervision can be used to integrate relevant prior-knowledge. In <ref type="figure" target="#fig_4">Figure 2</ref>, we illustrate the role of the self-supervised objective function in adjusting the gradient of the pseudo-supervised one. y pretext denotes the pretext-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Adjusting the gradient of the pseudo-supervised loss by the gradient of a self-supervised loss to better approximate the gradient of the true supervised loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Drift</head><p>In the context of multi-objective optimization, two objective functions are said to be conflicting, if optimizing one of them in value degrade the other one. In such a case, the optimum should be computed, while taking into consideration the trade-offs between the competing objective functions. A solution is called non-dominated when there are multiple optima that jointly optimize the objective functions. These optima are considered equally valid in the absence of extra subjective information.</p><p>In the case of deep learning, we call Feature Drift the optimization of a neural network's loss function whose secondary component (regularizer) strongly competes with the main one. This phenomenon can lead to a failure of the global learning process. The features learned based on the main cost function can be easily drifted by updating with respect to the secondary loss. To better understand this problem, <ref type="figure" target="#fig_1">Figure 3</ref> shows a simplistic illustration of Feature Drift. In <ref type="figure" target="#fig_1">Figure  3</ref>.a, a linear combination of two strongly competing vectors − → A 1 and − → B 1 is pulling the ball. In <ref type="figure" target="#fig_1">Figure 3</ref>.b, the ball is pulled by another couple of forces − → A 2 and − → B 2 , which are less conflicting. In both cases, the pulling forces are adjusted using a balancing positive coefficient γ. So, the resultant vector is equal to − → A 1 + γ − → B 1 for the first case and − → A 2 + γ − → B 2 for the second case. In this example, we consider that the pulling forces</p><formula xml:id="formula_5">− → A 1 , − → A 2 , − → B 1 ,</formula><p>and − → B 2 are constants and the γ coefficient is variable. After applying the resultant vector, the object is supposed to reach a position P (γ). In both figures, the colored area (delimited by the competing vectors) represents the field of possible positions after applying the resultant vector. The target solution lies within the green field. It is reasonable to predict that the smaller the area of the field, the easier to reach the target solution. For two strongly competing vectors, we observe that the variation of γ dramatically affects the reached position. Therefore, the choice of γ is quite critical for this case. Whereas, for the second case, where the competition is less ardent, the variation of γ has a less important impact on the reached position. Hence, the choice of γ is less crucial than in the first case. We conclude that the importance of a balancing coefficient depends on the level of competition. Most importantly, we observe that the level of competition can be assessed by the cosine of the angle between the two competing vectors. Hence, at every training iteration, we can characterize Feature Drift as following:</p><formula xml:id="formula_6">∆ F D = cos( ∂L p (x, y pseudo , w) ∂w , ∂L s (x, y pretext , w) ∂w ).<label>(6)</label></formula><p>Where ∂Lp ∂w and ∂Ls ∂w are the gradient of the pseudo-supervised loss and the gradient of the self-supervised loss, respectively. When the strongly contending vectors are not balanced meticulously, the desired solution would not be reached even after multiple iterations. Besides, it is of great importance to make unsupervised learning methods less reliant on unpredictable and dataset-specific parameters. The example presented by <ref type="figure" target="#fig_1">Figure 3</ref> was selected for its simplicity, since it is difficult to visualize the gradient vectors in a high-dimensional space.</p><p>(a) Strong competition between A1 and B1.</p><p>(b) Weak competition between A2 and B2. Several modern deep clustering models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32]</ref> jointly perform reconstruction and embedded clustering. In this work, we show empirically that combining them leads to a very strong competition between their gradients. Under specific conditions, we provide a mathematical proof of this hypothesis. For this reason, we consider the problem of clustering a dataset DS = {x i ∈ R n } N i=1 and X the matrix whose raw vectors are x i ∈ DS. We presume that the number of clusters K = 2. Our operators include a linear encoder E A : R n → R d , which maps samples from the data space to the latent space and a linear decoder G B performing an inverse mapping. The matrices A and B hold the learnable parameters of the encoder and decoder, respectively. We define the vector z i = E A (x i ) = A . x i as the projection of the data point x i in the latent space andx i = G B (z i ) = B . z i is the reconstructed representation of x i . We denoteẑ i = A . B . z i . Furthermore, we constrain A to the set of semi-orthogonal matrices. Thus, <ref type="bibr">K|]</ref>, C j represents the cluster j and N j is the number of points in C j . The center of the embedded points is denoted byz</p><formula xml:id="formula_7">A T . A = I d , where I d is the identity matrix. Each cluster is associated with a centroid µ j = 1 Nj i∈Cj z i in the embedded space R d , where j ∈ [|1,</formula><formula xml:id="formula_8">= 1 N N i=1 z i . We define, L r = N i=1 (x i −x i ) 2 ,</formula><p>as the reconstruction loss, and</p><formula xml:id="formula_9">L k = K j=1 i∈Cj (z i − µ j ) 2 , as the k-means loss in the latent space. Let d(C l1 , C l2 ) = i∈C l 1 j∈C l 2 (z i − z j ) 2</formula><p>, be the average distance between two clusters C l1 and C l2 . If l 1 is equal to l 2 , this distance is called within-cluster distance, and defined by C l1 and C l2 . Otherwise, it is called between-cluster distance, and defined by C l1 and C l2 . Theorem 1. Under the specific conditions described above, the loss function L DCN = L k + γL r can be expressed as following:</p><formula xml:id="formula_10">L DCN = (1 + γ)J 1 − 1 2 J 2 + γJ 3 ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_11">J 1 = 1 N d(C 1 , C 2 ) + 1 2N d(C 1 , C 1 ) + 1 2N d(C 2 , C 2 ), J 2 = N 1 N 2 N 2 d(C 1 , C 2 ) N 1 N 2 − d(C 1 , C 1 ) N 2 1 − d(C 2 , C 2 ) N 2 2 , J 3 = N i=1 (ẑ i −z) 2 − 2(z i −z) T (ẑ i −z).</formula><p>The proof of Theorem 1 is provided in Appendix A. This theorem shows the implicit competition between a typical clustering loss (k-means) and the reconstruction one. Intuitively, minimizing the clustering loss has two objectives. First, it allows to emphasize the similarities between data points within the same cluster. Second, it enables to stress the variations between data points from different clusters. However, minimizing the reconstruction loss aims to preserve all the similarities and variances between every couple of data points, whether or not they belong to the same cluster. Using Theorem 1, we can notice that minimizing the first term of J 2 leads to the maximization of between-cluster distances, which force the clusters to be separable. Added to that, minimizing the second and third terms of J 2 minimizes within-cluster variances, which pushes the clusters to be as compact as possible. However, minimizing J 1 implies minimizing both between-cluster distances and within-cluster variances.</p><p>In a general case, increasing γ significantly causes the self-supervised loss to easily win the competition. Thus, any discriminative feature learned in the direction of the pseudo-supervised loss's gradient can be easily drifted by the gradient of the self-supervised loss. On the other side, decreasing γ significantly leads to Feature Randomness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adversarial Deep Embedded Clustering</head><p>In this section, we describe our proposed framework ADEC. Our model is designed to address Feature Randomness and Feature Drift. To this end, we consider the problem of clustering a dataset</p><formula xml:id="formula_12">X = {x i ∈ R n } N i=1 into K clusters. Each cluster is associated with a centroid µ j in the embedded space R d , where j ∈ [|1, K|].</formula><p>Our operators include a deep non-linear encoder E φ : R n → R d , which maps samples from the data space to the latent space and a deep non-linear decoder G θ performing an inverse mapping. φ and θ represent the learnable parameters of the encoder and decoder, respectively. We define the vector z i = E φ (x i ) to be the projection of a data point x i into the latent space and</p><formula xml:id="formula_13">x i = G θ (z i ) is the reconstructed representation of x i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pretraining phase</head><p>Following state-of-the-art autoencoder-based clustering approaches, we pretrain the encoder and decoder. In the context of deep clustering, pseudo-labels are the cluster representatives (i.e., embedded centers). It is important to start the clustering phase with latent features that reflect the data distribution. Otherwise, it would be impossible to extract meaningful pseudo-labels. Training a neural network using embedded centers extracted from completely random latent representations, leads to excessive Feature Randomness (due to the large number of unreliable pseudo-labels). It is well-known that self-supervision allows to learn reliable general-purpose features by solving a pretext task. Therefore, the pretraining phase should consist of minimizing a self-supervised loss.</p><p>Previously proposed algorithms, such as, DEC and IDEC rely on a stacked denoising self-encoding strategy <ref type="bibr" target="#b61">[62]</ref> for initializing the training weights φ and θ. In our case, we opted for pretraining the autoencoder using vanilla reconstruction loss regularized by an adversarially constrained interpolation <ref type="bibr" target="#b2">[3]</ref> and data augmentation (e.g., slight random rotation and translation of the input samples) <ref type="bibr" target="#b22">[23]</ref>. These techniques are backed up by results showing an important enhancement in learning unsupervised representations for downstream tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>. When pretraining the model, a real number α ∈ [0, 1] is randomly sampled to computex α , such that,</p><formula xml:id="formula_14">x α = G θ (αE φ (x 1 ) + (1 − α)E φ (x 2 ))</formula><p>is the reconstruction of a data point interpolated from the latent representations of x 1 and x 2 . The framework of ACAI <ref type="bibr" target="#b2">[3]</ref> simulates a game competition between two adversarial networks. The autoencoder is trained to generate interpolated points. While the critic C ψ , which is a neural network parameterized by ψ, enables to regress the interpolation parameter α in (9), the autoencoder aims to fool the critic into considering the generated interpolants as real samples (i.e., outputting α = 0) in <ref type="bibr" target="#b7">(8)</ref>. The second term in (9) allows the critic to identify non-interpolated inputs. The coefficient γ, in <ref type="formula" target="#formula_16">(9)</ref>, is randomly selected from [0, 1] at every iteration and λ, in <ref type="formula" target="#formula_15">(8)</ref>, is responsible for balancing the reconstruction and the regularization. For the sake of simplification, we assume that x stands for the data samples after carrying out the random transformations (rotation and translation). The full framework of our pretraining phase is illustrated in <ref type="figure">Figure 4</ref>. To the best of our knowledge, we are the first to propose such a pretraining strategy in the context of deep clustering. <ref type="figure">Figure 4</ref>: The pretraining phase of ADEC.</p><formula xml:id="formula_15">L E,G (φ(t), θ(t)) = ||x −x|| 2 2 + λ||C ψ (x α )|| 2 2 ,<label>(8)</label></formula><formula xml:id="formula_16">L C (ψ(t)) = ||C ψ (x α ) − α|| 2 2 + ||C ψ (γx + (1 − γ)x)|| 2 2 .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Clustering phase</head><p>For this phase, on top of the pretrained encoder and decoder, we need one additional network. More precisely, we introduce a Discriminator D ω : R n → [0, 1]. Similar to the standard GAN framework, the Discriminator allows to identify real data samples from the fake ones. This network is parameterized with ω. Based on our experimental results, the feature learned by minimizing the embedded clustering objective function can be easily drifted, while minimizing the reconstruction loss. To inhibit this implicit strong competition from taking place, our strategy aims at transforming within-network competition to a different between-networks one. Therefore, each network is trained independently from the other ones to avoid the drifting effect.</p><p>Training the encoder is the main step in our framework. The clustering loss in <ref type="formula" target="#formula_0">(10)</ref> is inspired by DEC <ref type="bibr" target="#b63">[64]</ref>. It refines the clusters by gradually stressing high confidence assignments. It is worth to note that our methodology can be applied using a different embedded clustering loss. The choice of the DEC cost can be explained by its simplicity and popularity in the deep clustering community. Unlike DEC, we add a regularization term (second part of <ref type="formula" target="#formula_0">(10)</ref>). Our regularization allows to penalize generating embedded features, which could not be decoded into realistic data points. This constraint is verified by the discriminator, leading to rejecting discriminative features, which corrupt the clustering space. Hence, we argue that minimizing (10) enables to reduce Feature Randomness.</p><formula xml:id="formula_17">L E (φ(t)) = KL(P ||Q) + E x∼p(x) [log(1 − D ω (G θ (E φ (x)))].<label>(10)</label></formula><p>As shown by equation <ref type="formula" target="#formula_0">(10)</ref>, our model does not require a balancing hyperparameter. Unlike IDEC, where the balancing hyperparameter is critical and hard-to-tune due to the strong trade-off, in our case, the clustering and regularization terms do not reflect any explicit competition. We would provide an experimental study on hyperparameter tuning to validate the aforesaid hypothesis.</p><p>Unlike DEC, where the decoder is discarded straight away, this network plays a pivotal role in our case. It can be seen as a monitor. It allows to investigate the variations of the embedded representations induced by training the encoder.</p><p>Hence, we argue that the decoder should be trained as well to catch-up with the encoder updates. However, training the decoder similar to IDEC would drift the discriminative features learned by the encoder. We propose to restrain the backpropagation of the reconstruction loss to the decoder layers as shown by equation <ref type="bibr" target="#b10">(11)</ref>. We argue that such a strategy helps in reducing Feature Drift.</p><formula xml:id="formula_18">L G (θ(t)) = E x∼p(x) [ x − G θ (E φ (x)) 2 2 ].<label>(11)</label></formula><p>Unlike DEC and IDEC, we introduce a discriminator as an additional architectural component. As exhibited by equation <ref type="formula" target="#formula_0">(12)</ref>, the discriminator is supposed to differentiate real data points from those generated randomly. Similar to the decoder, the discriminator should be sufficiently trained before updating the encoder weights at every clustering iteration. An illustration of the clustering phase is given by <ref type="figure" target="#fig_2">Figure 5</ref>. At the end of the training process, we observe that the output images are smoother. Added to that, they do not represent a pure reconstruction anymore even if the decoder is trained for a huge number of iterations. This suggests that the encoder learned to destroy non-discriminative information. Another interesting observation is that the decoder maps images from the same class to the same blurry output image. This observation suggests that the encoder has learned to collapse within-class variances. Such characteristics of our model are inconsistent with IDEC pure reconstruction as illustrated by <ref type="figure" target="#fig_3">Figure 6</ref>. </p><formula xml:id="formula_19">V D (ω(t)) = E x∼p(x) [log(D ω (x))] + E x∼p(x) [log(1 − D ω (G θ (E φ (x)))].<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization</head><p>ADEC has five kind of learnable parameters φ, µ, θ, ω and ψ. All of them are updated using mini-batch SGD and backpropagation.L G ,Ṽ D andL E are, respectively, the stochastic mini-batch approximation of L G , V D and L E . The gradients are computed following Theorem 2 and Theorem 3. Refer to Appendix B and C for the proofs. Theorem 2. The gradient of the loss function L E (φ(t)) w.r.t the encoded data points z i is calculated by <ref type="bibr" target="#b12">(13)</ref>, where (JG θ ) T denotes the transpose Jacobian matrix of G θ and D ω is the gradient of D ω .</p><formula xml:id="formula_20">∂L E (φ(t)) ∂z i = 2 K j=1 (1 + z i − µ j ) 2 ) −1 (p ij − q ij )(z i − µ j ) + (JG θ (z i )) T . D ω (G θ (z i )) 1 − D ω (G θ (z i )) .<label>(13)</label></formula><p>Theorem 3. The gradient of L E (φ(t)) w.r.t. to the cluster center µ j is computed following <ref type="bibr" target="#b13">(14)</ref>.</p><formula xml:id="formula_21">∂L E (φ(t)) ∂µ j = −2 K j=1 (1 + z i − µ j ) 2 ) −1 (p ij − q ij )(z i − µ j ).<label>(14)</label></formula><p>For the clustering phase, we run the optimization for M axIter batch iterations or until the clustering assignment variation between two consecutive clustering iterations is lower than tol%. We found empirically that the decoder needs to be trained for a greater number of iterations compared to the other networks, otherwise it would cause instability. Thus, we alternate between training the {Decoder G θ , Encoder E φ , Discriminator D ω } for M number of iterations and training the {Decoder G θ } alone also for M auxiliary iterations. The target distribution P , which is computed based on the predicted clustering assignment distribution Q, constitutes the support for computing the pseudo-labels. P is updated every T iterations based on equations <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_2">(3)</ref>. In practice, we refrain from bringing modifications on P at every single step to avoid instability. The predicted label y pred (i) for a data point x i is calculated based on the following equation:</p><formula xml:id="formula_22">y pred (i) = argmax j (q ij ).<label>(15)</label></formula><p>Our proposed algorithm is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>An extensive experimental protocol is conducted to validate the suitability of ADEC in tackling Feature Randomness and Feature Drift. In order to perform this, we need to specify the scope of our experiments and the required experimental configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Scope of experiments</head><p>Deep Clustering models differ from each other in five substantial aspects. Each one of these aspects has been proved to have a significant impact on the clustering quality. The first factor is the used architecture. Some studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref> rely on sophisticated architectures (e.g., ResNet32, AlexNet and VGG) to cluster very large datasets. Other studies <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b21">22]</ref> leverage fairly sized architectures to cluster sizeable datasets. Likewise, in this paper, we opted for the same architecture used by <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b64">65]</ref>. The second factor is the integrated prior knowledge (e.g. invariance of images' labels to small linear transformations and symmetries). For instance, previous works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23]</ref> have proved that data augmentation based on prior knowledge leads to better clustering results. Inspired by these studies, we apply a similar data augmentation technique for image datasets. The third factor of quality is the learning dynamics. This was the focus of the following studies: (1) deep over-clustering <ref type="bibr" target="#b30">[31]</ref>, which offers two sub-heads, one for grouping the data in more clusters than required, and the second for clustering according to the ground truth number of clusters; (2) deep adaptive clustering <ref type="bibr" target="#b8">[9]</ref>, which clusters the easy samples first and then gradually supply the learning model with more difficult ones; and (3) clustering with a dynamic loss function <ref type="bibr" target="#b43">[44]</ref>, which gradually change the cost function according to the clustered samples. Unlike these papers, ADEC does not rely on any specific learning dynamics. Finally, the fourth and fifth factors consist in choosing the self-supervised and pseudo-supervised losses, respectively. Jabi et al. <ref type="bibr" target="#b28">[29]</ref> proved that, under mild conditions, several pseudo-supervised objective functions are equivalent to each other.</p><p>All the previous deep clustering studies revolve around the five mentioned axes. The modification of any one of these factors is deemed to improve or worsen the effectiveness and efficiency of the studied deep clustering model. The  <ref type="formula" target="#formula_15">(8)</ref> and <ref type="bibr" target="#b8">(9)</ref>. Pretrain the discriminator by maximizing <ref type="bibr" target="#b11">(12)</ref>. Initialize the embedded centroids µ using k-means. test ← T rue, j ← 0 . for i=0 to M axIter do if (i mod T == 0) then Update Q and P using <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_2">(3)</ref>. Save last predicted labels: y pred_old ← y pred . Compute the new predicted labels y pred using <ref type="bibr" target="#b14">(15)</ref>.</p><formula xml:id="formula_23">if ( 1 N N i=1 (y pred = y pred_old ) &lt; tol) then End training. end if end if if (test == T rue) then θ ← θ − ϑ θLG . j ← j + 1. if (j &gt; M ) then test ← F alse, j ← 0. end if else φ ← φ − ϑ φLE (L E is computed using (10)). θ ← θ − ϑ θLG (L G is computed using (11)). ω ← ω + ϑ ωṼD (Ṽ D is computed using (12)). µ ← µ − ϑ µLE . j ← j + 1. if (j &gt; M ) then test ← T rue, j ← 0. end if end if end for</formula><p>following experimental protocol aims to show that the trade-off between Feature Randomness and Feature Drift, which was neglected by previous studies, is influential in designing deep clustering models. For this reason, our experiments should include a comparison, where all the other factors of quality are kept identical between ADEC and its baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>All experiments are conducted on a server with 4 Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz, 32 GO RAM and a NVIDIA TESLA K80 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Datasets</head><p>We evaluate our approach on six benchmark datasets:</p><p>• MNIST-full <ref type="bibr" target="#b37">[38]</ref>: a 10 classes database of 70, 000 grayscale handwritten digit images of size 28 × 28 each.</p><p>• MNIST-test: a subset of 10, 000 images of the MNIST-full dataset.</p><p>• USPS <ref type="bibr" target="#b27">[28]</ref>: a 10 classes database of 9, 298 grayscale digit images of size 16 × 16 each.</p><p>• Fashion-MNIST [63]: a 10 classes database of 70, 000 grayscale images of size 28 × 28 each.</p><p>• REUTERS-10K <ref type="bibr" target="#b38">[39]</ref>: a 4 classes database (corporate/industrial, government/social, markets and economics) of 10, 000 articles. The 2, 000 most frequent words in all articles are selected. Then, for each article, we compute the TF-IDF features using the selected dictionary.</p><p>• Mice Protein <ref type="bibr" target="#b24">[25]</ref>: an 8 classes database of 1, 080 mice samples. The features of this database consists of the expression levels of 77 proteins.</p><p>All datasets are normalized before being fed to the clustering models, thereby the norm of each data point 1 n x i 2 2 is approximately equal to 1. For fully-connected models, we flatten the input data if its dimension is greater than one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Baselines</head><p>In order to show the effectiveness of the proposed model, ADEC is compared against classical clustering algorithms, subspace clustering algorithms, manifold clustering algorithms, and state-of-the-art deep clustering algorithms. The classical clustering baselines include k-means <ref type="bibr" target="#b41">[42]</ref>, Gaussian mixture models (GMM) <ref type="bibr" target="#b3">[4]</ref>, Least Squares Non-negative Matrix Factorization (LSNMF) <ref type="bibr" target="#b39">[40]</ref> and agglomerative clustering (AC) <ref type="bibr" target="#b29">[30]</ref>. The subspace clustering methods include Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit (SSC-OMP) <ref type="bibr" target="#b67">[68]</ref> and Scalable Elastic Net Subspace Clustering (EnSC) <ref type="bibr" target="#b66">[67]</ref>. The other subspace clustering baselines are not efficient enough to deal with 70,000 samples and therefore they are left out. The manifold clustering approaches include normalized-cut spectral clustering (SC) <ref type="bibr" target="#b53">[54]</ref> and Kernel (RBF) k-means <ref type="bibr" target="#b51">[52]</ref>. Finally, the deep clustering algorithms include DeepCluster <ref type="bibr" target="#b7">[8]</ref>, JULE <ref type="bibr" target="#b65">[66]</ref>, SR-k-means <ref type="bibr" target="#b28">[29]</ref>, DEC <ref type="bibr" target="#b63">[64]</ref>, IDEC <ref type="bibr" target="#b21">[22]</ref>, DCN <ref type="bibr" target="#b64">[65]</ref>, VaDE <ref type="bibr" target="#b31">[32]</ref> and DEPICT <ref type="bibr" target="#b14">[15]</ref>. Our baselines also cover clustering the embedded data of an autoencoder using k-means and FINCH <ref type="bibr" target="#b50">[51]</ref> denoted, respectively, by (AE+k-means) and (AE+FINCH). As a side note, all the fully-connected baselines share the same architecture with ADEC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Evaluation Metrics</head><p>We adopt the metrics ACC <ref type="bibr" target="#b6">[7]</ref>, NMI <ref type="bibr" target="#b54">[55]</ref>, ∆ F R and ∆ F D for assessing the clustering quality. The first two metrics are widely used to compare deep clustering methods. The third and fourth metrics are among the contributions of this work. ACC and NMI lie within the range [0, 1] and ∆ F R and ∆ F D lie within the range [−1 <ref type="bibr">, 1]</ref>. Higher values are better. As shown by <ref type="bibr" target="#b15">(16)</ref> and <ref type="formula" target="#formula_0">(17)</ref>, ACC and NMI depend on y pred and y true , where y pred is a vector representing the predicted labels and y true is the ground-truth labels vector.</p><formula xml:id="formula_24">ACC = max T ( N i=1 1 {y true (i) = T (y pred (i))} N ).<label>(16)</label></formula><p>T is selected from the set of all possible permutations mapping the predicted clusters to the ground-truth categories. The best matching can be found using the Hungarian Algorithm <ref type="bibr" target="#b34">[35]</ref>.</p><p>N M I(y pred , y true ) = I(y true , y pred ) .</p><p>H denotes the entropy and I is the mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Implementation</head><p>The encoder has eight layers of dimensions n -500 -500 -2000 -10. Apart from the bottleneck layer, all the other ones are activated by ReLu <ref type="bibr" target="#b40">[41]</ref>. The decoder is an inverse mapping of the encoding layers 10 -2000 -500 -500 -n with ReLu activations except for the last layer. We pretrain the autoencoder in competition with a critic for 13 × 10 4 iterations to perform data reconstruction constrained by an adversarially constrained interpolation. The learning weights are optimized using Adam <ref type="bibr" target="#b32">[33]</ref> with a learning rate equal to 0.0001. β 1 , β 2 , and (i.e., hyperparameters specific to Adam) have the respective values 0.9, 0.999, and 10 −8 . According to ACAI <ref type="bibr" target="#b2">[3]</ref> paper, λ and α are set equal to 0.5 and 1, respectively. For the clustering stage, the encoder, decoder, and discriminator are trained alternatively for M axIter = 10 5 . The training is stopped before reaching the final iteration if the convergence criterion is met. This criterion is parameterized by a threshold tol = 0.001. We update the encoder, decoder and discriminator weights using SGD with a learning rate ϑ = 0.001 and momentum 0.9. All backpropagation updates are performed on random batches of size 256 for both stages (i.e., pretraining and clustering). ADEC is implemented using Python and Tensorflow <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Our experimental protocol has three parts. In the first part, our model is compared with state-of-the-art clustering algorithms. In the second part, we analyse the ability of our model to tackle Feature Randomness and Feature Drift. In the last part, some qualitative results are exhibited. Before showing our results, we establish some useful notations.  In all the following experiments: -indicates OUT_OF_MEMORY, denotes the unsuitability of the algorithm to process one-dimensional data, ‡ indicates that the pretraining phase does not support Data Transform and Adversarially Constrained Interpolation, † indicates that the pretraining phase does not support Data Transform, * indicates that the evaluated methods share the same pretraining weights, the same architecture, the same learning dynamics and the same clustering loss with ADEC. <ref type="table" target="#tab_1">Table 1</ref> illustrates the evaluation of several clustering approaches, including our proposed method, in terms of ACC and NMI. All baselines methods are tuned according to their default settings. First of all, we observe that state-of-the art subspace clustering algorithms, such as, SSC-OMP and EnSC are generally not suitable for clustering datasets with semantic similarities (e.g., images, text, sounds). In fact, subspace clustering presumes the data to lie in a union of low-dimensional linear subspaces. However, this assumption does not hold for datasets with clusters lieing near non-linearly shaped manifolds <ref type="bibr" target="#b49">[50]</ref>. Secondly, we observe that the manifold clustering approaches have better ACC and NMI values than the classical approaches on some datasets. In fact, for the manifold category, the selection of the non-linear transform is largely empirical. Particularly, no kernel space is sufficiently well-suited to effectively cluster any dataset. Thirdly, in most cases, we can see that deep clustering models outperform all the other approaches by a huge margin. This observation confirms the suitability of deep clustering when it comes to clustering high-dimensional datasets. Finally, comparing among the deep clustering approaches, we can observe that our method provides the best results on every dataset. In terms of ACC and NMI, ADEC outperforms its state-of-the-art counterpart DEPICT by 2% and 5%, respectively. Worthy of note that DEPICT is the convolutional version of DEC with some minor modifications. In order to understand the outperformance of our approach, we need to conduct further experiments.  For a fair comparison with state-of-the-art deep clustering approaches, the baselines need to be reimplemented in a way to neutralize factors, which are out of this article scope. The new reimplemented models share the same deep clustering factors (i.e., architecture, learning dynamics, integrated prior knowledge and clustering loss) with ADEC. From <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref>, we can notice a considerable improvement in terms of ACC and NMI for the modified version of DEC and IDEC, comparatively to the original ones. More specifically, DEC* outperforms vanilla DEC by a huge margin. Similarly, IDEC* surpasses its standard counterpart significantly. The huge gap between the ordinary pretrained models (i.e., based on a simple reconstruction) and the modified ones, demonstrates the effectiveness of combining Adversarially Constrained Interpolation and Data Transformation, as a pretraining strategy. Furthermore, as we can see from <ref type="table" target="#tab_2">Table 2</ref>, ADEC* outperforms DEC* and IDEC*. This result suggests that ADEC offers a better trade-off between Feature Randomness and Feature Drift. This hypothesis will be further supported in the subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Comparing state-of-the art approaches</head><p>In <ref type="table" target="#tab_3">Table 3</ref>, we report the execution time of different deep clustering methods. The comparison is limited to deep clustering models. Henceforth, we exclude all the other clustering categories due to their less competitive results as just demonstrated by the previous comparison in <ref type="table" target="#tab_1">Table 1</ref>. As we can see in <ref type="table" target="#tab_3">Table 3</ref>, the run-time of ADEC is significantly higher than the execution times of DEC, IDEC, DCN and DeepCluster on all datasets. As it stands, these methods are more efficient than ADEC. However, we can also observe that the execution time of our method is on par with the execution times of DEPICT, SR-k-means and JULE. Interestingly, our algorithm is way faster than VaDE on all datasets.</p><p>For fairness of comparison and in order to better assess the efficiency of our method, we run our algorithm against the modified versions of DEC and IDEC (same as the previous experiment). Based on <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref>, we can see that the run-time of DEC* and IDEC* are significantly higher than the execution times of vanilla DEC and vanilla IDEC, respectively. Therefore, we can conclude that DEC* and IDEC* are less efficient than DEC and IDEC, respectively. This conclusion can be explained by the long pretraining phase. Hence, the gain achieved by pretraining with an Adversarially Constrained Interpolation comes at the cost of a higher execution time. Another observation, DEC* and IDEC* are slightly faster than ADEC*. This is expected and it can be imputed to the adversarial training of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Feature Randomness and Feature Drift</head><p>In this section, our conducted experiments aim to show the ability of ADEC to reach a better trade-off between Feature Randomness and Feature Drift. Therefore, we perform an ablation of our adversarial mechanism. Instead of this mechanism, we regularize the clustering loss with vanilla reconstruction. The obtained model is IDEC*. Then, we compare ADEC with IDEC* in terms of ∆ F R and ∆ F D . As mentioned earlier, both models share the same optimizer, the same pretraining phase and the same embedded clustering loss. The only difference between them is the regularization technique.</p><p>The first experiment examines the impact of our adversarial mechanism in reducing Feature Randomness. In this section, we show results for the MNIST dataset. Such results are representative of the general behavior of our approach and the same conclusion can be drawn on the other datasets. In <ref type="figure" target="#fig_6">Figure 7</ref>, we draw the evolution of ∆ F R for ADEC and IDEC* during training on MNIST. Based on this figure, we observe that the average values of ∆ F R for ADEC is considerably higher than the one for IDEC*. A higher ∆ F R value means that the gradient of ADEC is a better approximation to the supervised gradient. Hence, this experiment confirms that our adversarial regularization is more suitable for alleviating Feature Randomness than vanilla reconstruction.  The second experiment examined the impact of our adversarial mechanism in reducing Feature Drift. In <ref type="figure" target="#fig_8">Figure 8</ref>, we draw the evolution of ∆ F D for ADEC and IDEC* during training on MNIST. Based on this figure, we observe that the values of ∆ F D for IDEC* are always negative. This result confirms the strong competition between the gradient of the embedded clustering loss and the gradient of the reconstruction loss. Added to that, we observe that the average values of ∆ F D for ADEC is considerably higher than the one for IDEC*. A higher ∆ F D value indicates that the competition between the embedded clustering gradient and the reconstruction gradient is stronger than the competition between the embedded clustering gradient and the adversarial gradient. Hence, this experiment confirms that our adversarial regularization is more suitable for alleviating Feature Drift than vanilla reconstruction.  The third experiment examined the impact of Feature Drift on the learning curves. In <ref type="figure" target="#fig_9">Figure 9</ref>, we draw the learning curves of ADEC and IDEC*, in terms of ACC and NMI, during training on MNIST. Based on this figure, we observe that the learning curves of ADEC are not only above the learning curves of IDEC*, but also smoother. A zoom in to the learning curves of IDEC*, as illustrated by <ref type="figure" target="#fig_0">Figures 11 and 12</ref>, shows noticeable fluctuations. However, zooming in to the learning curves of ADEC shows a smooth increase in both metrics. The observed fluctuations for IDEC* can be explained by the competition between the reconstruction and the embedded clustering.   The fourth experiment examined the impact of Feature Drift on the sensitivity of the balancing hyperparameter γ.</p><p>In <ref type="figure" target="#fig_0">Figure 10</ref>, we draw the learning curves of IDEC*, for different values of γ, during training on MNIST. In our experiments, γ is selected from the following set 10 −3 , 10 −2 , 10 −1 , 1, 10, 10 2 , 10 3 . Based on the obtained results, we observe that only one value of the set (γ = 0.01) yields an acceptable learning curve. All the other values make the learning curve drop significantly. Hence, we can conclude that IDEC* is very sensitive to the choice of γ. This result can be explained by Feature Drift (the strong competition between the gradient of the self-supervised loss and the gradient of the pseudo-supervised loss). However, in our case, ADEC does not require any balancing hyperparameter.  In <ref type="figure" target="#fig_0">Figure 13</ref>, the discriminative ability of ADEC is illustrated by projecting the data in a 2D latent subspace for different datasets. From this figure, we can see that the projected embedded data points are grouped in well-separated clusters. <ref type="figure" target="#fig_0">Figure 14</ref> illustrates the top 10 high-confidence images from each cluster for two datasets, namely, MNIST and Fashion MNIST. In this figure, images are inserted in decreasing order from left to right according to their distance to their associated clustering centers. Every single row represents a different cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this article, we have proposed an Adversarial Deep Embedded Clustering algorithm. Our method enables to regularize the clustering loss in a way to alleviate Feature Randomness. To overcome Feature Drift, the strong clustering-reconstruction trade-off have been Eliminated. Empirical results have showed that ADEC outperforms state-of-the-art clustering methods in terms of ACC and NMI. Furthermore, our experimental studies have validated that ADEC offers a better trade-off between Feature Drift and Feature Randomness. For ADEC, similar to the most relevant deep clustering models, self-supervision and pseudo-supervision are combined linearly. It is very interesting to study other possible combinations and to find theoretical justifications. Besides, it is worthy to extend the scope of this work by using a more sophisticated architecture (e.g., ResNet32, AlexNet and VGG) to process higher semantic datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of theorem 1</head><p>We start by computing:</p><formula xml:id="formula_26">N i=1 (z i −z) 2 = 1 N 2 N i=1   N j=1 (z i − z j )   2 , = 1 N 2 N i=1 N j=1 (z i − z j ) 2 + 1 N 2 N i=1 j =j (z i − z j ) T (z i − z j ), = d(C 1 , C 1 ) N 2 + d(C 2 , C 2 ) N 2 + 2 d(C 1 , C 2 ) N 2 + 1 N 2 N i=1 j =j (z i − z j ) T (z i − z j ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>And since</head><formula xml:id="formula_27">N i=1 j =j (z i − z j ) T (z i − z j ) = i =j,i =j ,j =j (z j − z i ) 2 + (z j − z i ) 2 − (z j − z j ) 2 , = (N − 2) N i=1 N j=1 (z j − z i ) 2 , = (N − 2) d(C 1 , C 1 ) + (N − 2) d(C 2 , C 2 ) + 2 (N − 2) d(C 1 , C 2 ). Therefore N i=1 (z i −z) 2 = 1 N d(C 1 , C 2 ) + 1 2N d(C 1 , C 1 ) + 1 2N d(C 2 , C 2 ), = J 1 .</formula><p>The L r function can be written as: </p><formula xml:id="formula_28">L r = N i=1 (x i −x i ) T (x i −x i</formula><formula xml:id="formula_29">= N i=1 (z i −ẑ i ) T (z i −ẑ i ), = N i=1 (z i −z +z −ẑ i ) 2 , = N i=1 (z i −z) 2 + N i=1 (ẑ i −z) 2 − 2(z i −z) T (ẑ i −z), = N i=1 (z i −z) 2 + J 3 , = J 1 + J 3 .</formula><p>According to <ref type="bibr" target="#b13">[14]</ref>, the L k function can be written as :</p><formula xml:id="formula_30">L k = K k=1 i,j∈C k (z j − z i ) 2 , = J 1 − 1 2 J 2 .</formula><p>So we obtain</p><formula xml:id="formula_31">L DCN = L k + γL r , = (1 + γ)J 1 − 1 2 J 2 + γJ 3 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of theorem 2</head><p>The loss function L E (φ(t)) can be written as:</p><formula xml:id="formula_32">L E (φ(t)) = N i=1 K j=1 p ij log( p ij q ij ) + N i=1 log(1 − D ω (G θ (z i ))), = N i=1 K j=1 p ij log(p ij ) − p ij log(q ij ) + N i=1 log(1 − D ω (G θ (z i ))), ∂L E (φ(t)) ∂z i =   K j=1 ∂p ij log(p ij ) ∂z i − ∂p ij log(q ij ) ∂z i   + ∂log(1 − D ω (G θ (z i ))) ∂z i , =   K j=1 −p ij ∂log(q ij ) ∂z i   + ∂log(1 − D ω (G θ (z i ))) ∂z i .</formula><p>Then, we compute ∂log(1−Dω(G θ (zi))) ∂zi and ∂log(qij ) ∂zi separately.</p><formula xml:id="formula_33">∂log(1 − D ω (G θ (z i ))) ∂z i = − (JG θ (z i )) T . D ω (G θ (z i )) 1 − D ω (G θ (z i )) . ∂log(q ij ) ∂z i = ∂log( (1+ zi−µj 2 ) −1 j (1+ zi−µ j 2 ) −1 ) ∂z i , = ∂log((1 + z i − µ j 2 ) −1 ) ∂z i − ∂log( j (1 + z i − µ j 2 ) −1 ) ∂z i , = − 2(z i − µ j ) 1 + z i − µ j ) 2 + 2 j (z i − µ j )(1 + z i − µ j 2 ) −2 j (1 + z i − µ j 2 ) −1 , = − 2(z i − µ j )(1 + z i − µ j ) 2 ) −2 q ij j (1 + z i − µ j 2 ) −1 + 2 j (z i − µ j )(1 + z i − µ j 2 ) −2 j (1 + z i − µ j 2 ) −1 .</formula><p>After substitutions, we have:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The different deep clustering categories. The self-supervised loss L s enforces reasonable general-purpose features and the pseudo-supervised loss L p is used for clustering the embedded data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparing the impact of strong and weak competition in reaching a target position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The clustering phase of ADEC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>First row: MNIST input images; Second row: Output images from IDEC; Third row: Output images from ADEC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 2 [</head><label>2</label><figDesc>H(y true ) + H(y pred )]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) ADEC.(b) IDEC*.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>∆ F R during training on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>∆ F D during training on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>ACC and NMI during training on MNIST.Figure 10: Sensitivity analysis for γ during training on MNIST. (a) ADEC. (b) IDEC*.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>ACC during training on MNIST. (a) ADEC. (b) IDEC*.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>NMI during training on MNIST-test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>2D embedding subspace visualization to show the discriminative ability of ADEC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>(a) MNIST.(b) Fashion MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Each row shows the top 10 high-confidence images from each cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>), = tr((X − BAX)(X − BAX) T ), = tr(A T A(X − BAX)(X − BAX) T ), = tr(A(X − BAX)(X − BAX) T A T ), = tr((AX − ABAX)(AX − ABAX) T ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Minibatch stochastic gradient descent training of Adversarial Deep Embedded Clustering. Input: Input data: X, Number of clusters: K, Learning rate: ϑ, Convergence threshold: tol, Maximum iterations: M axIter, Auxiliary iterations: M , Distribution update interval: T . Output: Encoder weights: φ, Decoder weights: θ, Discriminator weights: ω, Embedded centroids: µ.</figDesc><table /><note>Pretrain the autoencoder by minimizing</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the clustering performances in terms of ACC and NMI. The different clustering categories are separated by double horizontal lines. Best method in bold, second best emphasized.</figDesc><table><row><cell>Method</cell><cell cols="2">MNIST-full</cell><cell cols="2">MNIST-test</cell><cell cols="2">USPS</cell><cell cols="2">Fashion-MNIST</cell><cell cols="2">REUTERS-10K</cell><cell cols="2">Mice Protein</cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell></row><row><cell>k-means</cell><cell cols="7">0.532 0.500 0.546 0.501 0.668 0.627 0.474</cell><cell>0.512</cell><cell>0.522</cell><cell>0.313</cell><cell>0.342</cell><cell>0.252</cell></row><row><cell>GMM</cell><cell cols="7">0.433 0.366 0.540 0.493 0.551 0.530 0.556</cell><cell>0.557</cell><cell>0.402</cell><cell>0.375</cell><cell>0.139</cell><cell>1.00</cell></row><row><cell>LSNMF</cell><cell cols="7">0.540 0.455 0.550 0.463 0.575 0.551 0.549</cell><cell>0.523</cell><cell>0.596</cell><cell>0.361</cell><cell>0.497</cell><cell>0.506</cell></row><row><cell>AC</cell><cell cols="7">0.621 0.682 0.695 0.711 0.683 0.725 0.500</cell><cell>0.564</cell><cell>0.526</cell><cell>0.365</cell><cell>0.294</cell><cell>0.211</cell></row><row><cell>SSC-OMP</cell><cell cols="7">0.309 0.315 0.413 0.450 0.477 0.503 0.100</cell><cell>0.007</cell><cell>0.402</cell><cell>0.008</cell><cell>0.152</cell><cell>0.078</cell></row><row><cell>EnSC</cell><cell cols="7">0.111 0.014 0.603 0.591 0.610 0.684 0.629</cell><cell>0.636</cell><cell>0.401</cell><cell>0.014</cell><cell>0.434</cell><cell>0.347</cell></row><row><cell>SC</cell><cell cols="7">0.656 0.731 0.660 0.704 0.649 0.794 0.508</cell><cell>0.575</cell><cell>0.402</cell><cell>0.375</cell><cell>0.298</cell><cell>0.268</cell></row><row><cell>RBF k-means</cell><cell>-</cell><cell>-</cell><cell cols="4">0.560 0.523 0.629 0.631</cell><cell>-</cell><cell>-</cell><cell>0.499</cell><cell>0.288</cell><cell>0.363</cell><cell>0.269</cell></row><row><cell>AE + k-means</cell><cell cols="7">0.807 0.730 0.702 0.617 0.720 0.698 0.585</cell><cell>0.614</cell><cell>0.695</cell><cell>0.475</cell><cell>0.238</cell><cell>0.131</cell></row><row><cell>AE + FINCH</cell><cell>-</cell><cell>-</cell><cell cols="4">0.709 0.754 0.704 0.788</cell><cell>-</cell><cell>-</cell><cell>0.241</cell><cell>0.414</cell><cell>0.157</cell><cell>0.083</cell></row><row><cell>DeepCluster</cell><cell cols="7">0.797 0.661 0.854 0.713 0.562 0.540 0.542</cell><cell>0.510</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DCN</cell><cell cols="7">0.830 0.810 0.802 0.786 0.688 0.683 0.501</cell><cell>0.558</cell><cell>0.422</cell><cell>0.109</cell><cell>0.197</cell><cell>0.051</cell></row><row><cell>DEC</cell><cell cols="7">0.863 0.834 0.856 0.830 0.762 0.767 0.518</cell><cell>0.546</cell><cell>0.814</cell><cell>0.598</cell><cell>0.184</cell><cell>0.026</cell></row><row><cell>IDEC</cell><cell cols="7">0.881 0.867 0.846 0.802 0.761 0.785 0.529</cell><cell>0.557</cell><cell>0.790</cell><cell>0.550</cell><cell>0.196</cell><cell>0.037</cell></row><row><cell>SR-k-means</cell><cell cols="7">0.939 0.866 0.863 0.873 0.901 0.912 0.507</cell><cell>0.548</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VaDE</cell><cell cols="7">0.945 0.876 0.287 0.287 0.566 0.512 0.578</cell><cell>0.630</cell><cell>0.793</cell><cell>0.521</cell><cell>0.139</cell><cell>1.00</cell></row><row><cell>JULE</cell><cell cols="7">0.964 0.913 0.961 0.915 0.950 0.913 0.563</cell><cell>0.608</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DEPICT</cell><cell cols="7">0.965 0.917 0.963 0.915 0.899 0.906 0.392</cell><cell>0.392</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ADEC</cell><cell cols="7">0.986 0.961 0.985 0.957 0.981 0.948 0.586</cell><cell>0.662</cell><cell cols="4">0.821  ‡ 0.605  ‡ 0.500  † 0.604  †</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the clustering performances of DEC*, IDEC*, and ADEC in terms of ACC and NMI. Best method in bold, second best emphasized.</figDesc><table><row><cell>Method</cell><cell cols="2">MNIST-full</cell><cell cols="2">MNIST-test</cell><cell cols="2">USPS</cell><cell cols="2">Fashion-MNIST</cell><cell cols="2">REUTERS-10K</cell><cell cols="2">Mice Protein</cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell></row><row><cell>DEC*</cell><cell cols="7">0.971 0.929 0.968 0.920 0.963 0.910 0.575</cell><cell>0.589</cell><cell cols="4">0.814  ‡ 0.598  ‡ 0.267  † 0.158  †</cell></row><row><cell>IDEC*</cell><cell cols="7">0.982 0.952 0.978 0.944 0.980 0.946 0.575</cell><cell>0.631</cell><cell cols="4">0.790  ‡ 0.550  ‡ 0.188  † 0.033  †</cell></row><row><cell>ADEC</cell><cell cols="7">0.986 0.961 0.985 0.957 0.981 0.948 0.586</cell><cell>0.662</cell><cell cols="4">0.821  ‡ 0.605  ‡ 0.500  † 0.604  †</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the execution times (in seconds) of different deep clustering approaches.</figDesc><table><row><cell>Method</cell><cell cols="2">MNIST-full MNIST-test</cell><cell>USPS</cell><cell cols="3">Fashion-MNIST REUTERS-10K Mice Protein</cell></row><row><cell>DeepCluster</cell><cell>1,375</cell><cell>74</cell><cell>64</cell><cell>1,250</cell><cell>-</cell><cell>-</cell></row><row><cell>DCN</cell><cell>640</cell><cell>55</cell><cell>49</cell><cell>732</cell><cell>279</cell><cell>40</cell></row><row><cell>DEC</cell><cell>693</cell><cell>58</cell><cell>53</cell><cell>2,384</cell><cell>105</cell><cell>22</cell></row><row><cell>IDEC</cell><cell>890</cell><cell>349</cell><cell>110</cell><cell>857</cell><cell>97</cell><cell>150</cell></row><row><cell>SR-k-means</cell><cell>14,872</cell><cell>1,657</cell><cell>1,655</cell><cell>4,551</cell><cell>-</cell><cell>-</cell></row><row><cell>VaDE</cell><cell>123,000</cell><cell>15,000</cell><cell>13,000</cell><cell>120,000</cell><cell>105</cell><cell>15</cell></row><row><cell>JULE</cell><cell>12,500</cell><cell>3,247</cell><cell>2,540</cell><cell>13,100</cell><cell>-</cell><cell>-</cell></row><row><cell>DEPICT</cell><cell>9,561</cell><cell>2,320</cell><cell>1,778</cell><cell>8,581</cell><cell>-</cell><cell>-</cell></row><row><cell>ADEC</cell><cell>10,735</cell><cell>10,013</cell><cell>8,445</cell><cell>10,502</cell><cell>669  ‡</cell><cell>1,047  †</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the execution times (in seconds) of DEC*, IDEC* and ADEC.</figDesc><table><row><cell>Method</cell><cell cols="6">MNIST-full MNIST-test USPS Fashion-MNIST REUTERS-10K Mice Protein</cell></row><row><cell>DEC*</cell><cell>9,667</cell><cell>9,092</cell><cell>7,692</cell><cell>10,840</cell><cell>53  ‡</cell><cell>639  †</cell></row><row><cell>IDEC*</cell><cell>9,556</cell><cell>9,160</cell><cell>7,693</cell><cell>9,623</cell><cell>55  ‡</cell><cell>646  †</cell></row><row><cell>ADEC</cell><cell>10,735</cell><cell>10,013</cell><cell>8,445</cell><cell>10,502</cell><cell>669  ‡</cell><cell>1,047  †</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A PREPRINT -SEPTEMBER 27, 2019</p><p>C Proof of theorem 3 µ j and −z i play symmetric roles in the first part of L E (φ(t)), and the regularization part does not depend on µ j . Therefore,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analysis of multivariate social science data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiona</forename><surname>Bartholomew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Steele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irini</forename><surname>Galbraith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moustaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding and improving interpolation in autoencoders via an adversarial regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining projected clusters in high-dimensional spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Bouguessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengrui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="507" to="522" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Locally consistent concept factorization for document clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="902" to="913" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shiming Xiang, and Chunhong Pan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5879" to="5887" />
		</imprint>
	</monogr>
	<note>Deep adaptive image clustering</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mean shift, mode seeking, and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizong</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="790" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Trevor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kernel k-means: spectral clustering and normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="551" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">K-means clustering via principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Kamran Ghasedi Dizaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Herandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5747" to="5756" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hessian eigenmaps: Locally linear embedding techniques for highdimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carrie</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5591" to="5596" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kdd</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI-17)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1753" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep embedded clustering with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="550" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Associative deep clustering: Training a classification network with no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Plapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elie</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the German Conference on Pattern Recognition (GCPR)</title>
		<meeting>the German Conference on Pattern Recognition (GCPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-organizing feature maps identify proteins critical to learning in a mouse model of down syndrome</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Higuera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Katheleen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof J</forename><surname>Gardiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">129126</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cnn-based joint clustering and representation learning with feature drift compensation for large-scale image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="421" to="429" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1558" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Jabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Mitiche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Ben Ayed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04246</idno>
		<title level="m">Deep clustering: On the link between discriminative models and k-means</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data clustering: 50 years beyond k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="651" to="666" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Invariant information distillation for unsupervised image segmentation and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06653</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Variational deep embedding: An unsupervised and generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuxi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huachun</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI-17)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1965" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">3d data management: Controlling data volume, velocity and variety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Laney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>META group research note</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep learning. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Projected gradient methods for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2756" to="2779" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep clustering with a dynamic autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nairouz</forename><surname>Mrabah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riadh</forename><surname>Naimul Mefraz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ksantini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07752</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Nonlinear dimensionality reduction by locally linear embedding. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient parameter-free clustering using first neighbor relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saquib</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Sohil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01449</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Deep continuous clustering. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">107</biblScope>
		</imprint>
	</monogr>
	<note>Departmental Papers (CIS)</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cluster ensembles-a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">A global geometric framework for nonlinear dimensionality reduction. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Joshua B Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Wasserstein auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schoelkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Discrete choice methods with simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kenneth E Train</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Deep discriminative latent space for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Tzoreff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoni</forename><surname>Choukroun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10795</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Oracle based active set algorithm for scalable elastic net subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3928" to="3937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Scalable sparse subspace clustering by orthogonal matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3918" to="3927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Low-rank sparse subspace for spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
