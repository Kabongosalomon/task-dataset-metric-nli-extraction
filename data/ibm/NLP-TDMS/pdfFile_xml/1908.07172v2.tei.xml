<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human Mesh Recovery from Monocular Images via a Skeleton-disentangled Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<email>yusun@stu.hit.edu.cnyun.ye@intel.comliuwu@live.cnwpgao</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Human Mesh Recovery from Monocular Images via a Skeleton-disentangled Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe an end-to-end method for recovering 3D human body mesh from single images and monocular videos. Different from the existing methods try to obtain all the complex 3D pose, shape, and camera parameters from one coupling feature, we propose a skeleton-disentangling based framework, which divides this task into multi-level spatial and temporal granularity in a decoupling manner. In spatial, we propose an effective and pluggable "disentangling the skeleton from the details" (DSD) module. It reduces the complexity and decouples the skeleton, which lays a good foundation for temporal modeling. In temporal, the selfattention based temporal convolution network is proposed to efficiently exploit the short and long-term temporal cues. Furthermore, an unsupervised adversarial training strategy, temporal shuffles and order recovery, is designed to promote the learning of motion dynamics. The proposed method outperforms the state-of-the-art 3D human mesh recovery methods by 15.4% MPJPE and 23.8% PA-MPJPE on Human3.6M. State-of-the-art results are also achieved on the 3D pose in the wild (3DPW) dataset without any fine-tuning. Especially, ablation studies demonstrate that skeleton-disentangled representation is crucial for better temporal modeling and generalization. The code is released at https://github.com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Different from traditional 3D pose estimation that usually predicts the location of 14/17 skeleton joints <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>, 3D human body mesh recovery from the monocular images is a more complex task, which tries to estimate the more detailed 3D shape and joint angles. In detail, it needs to estimate more than 85 parameters, which controls 6890 vertices <ref type="bibr" target="#b21">[22]</ref> that form the surface of 3D body mesh. More- <ref type="figure">Figure 1</ref>. Human 3D mesh recovery from monocular video based on skeleton disentangling and temporal coherence. over, the information loss from the 3D scene to a 2D image, inherent ambiguity, and complex changes in human body shape and pose further increase the complexity of this task. Therefore, although the 3D body mesh recovery is important in computer vision, motion/event analysis <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19]</ref>, and virtual try-on <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref>, it is still a frontier challenge. In this paper, we try to solve this problem via a skeleton-disentangled representation in multi-level spatial and temporal granularity.</p><p>Most of the existing methods try to recover the human 3D mesh from single images. Previous multi-stage approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> first extract human body information (e.g. 2D pose and segmentation) and then estimate the 3D model parameters from them, which is typically not optimal. Recently, HMR <ref type="bibr" target="#b14">[15]</ref> provides an end-to-end solution to learn a mapping from image pixels directly to model parameters, which shows a significant performance advantage over the two-stage methods. However, as shown in <ref type="figure">Figure 1</ref>, severe coupling problem makes the prediction unstable. In HMR, 3D pose, body shape, and camera parameters are derived directly from the same feature vector using the fully connected layers. Without any decoupling measurements, high complexity makes features of different targets tightly coupled. Moreover, existing datasets with 3D an-notations are collected in a constrained environment with limited motion and shape patterns. Therefore, many previous methods have employed 2D in-the-wild pose datasets, like MPII <ref type="bibr" target="#b1">[2]</ref>, to learn richer poses. It may be sufficient for 3D pose estimation. While, for 3D mesh recovery, 2D pose is far from enough for recovering the complex human 3D shape and pose details. Lacking supervision makes the predictions of the details vulnerable, which further exacerbates the coupling problem. The coupling strategy makes the model trained on these datasets cannot well generalize to the complex environments and various human states.</p><p>To solve this problem, we propose a lightweight and pluggable DSD module to decouple different factors in an end-to-end manner. The main idea of DSD module is to disentangle the skeleton from the 3D mesh details with bilinear transformation. Firstly, information of the pose skeleton and the rest details (e.g., body shape, detailed pose information) are extracted independently. Furthermore, the bilinear transformation is employed to aggregate two pieces of information while keeping their decoupling in the new feature space. Finally, the network is trained end-to-end to keep the global optimal. In the evaluations, we demonstrate that the DSD module outperforms the state-of-the-art methods <ref type="bibr" target="#b15">[16]</ref> by 13% PA-MPJPE on Human3.6M dataset. Moreover, the evaluations also demonstrate that the proposed portable DSD module can be easily plugged into other 2D/3D pose estimation network for recovering 3D human mesh.</p><p>Recovering human 3D mesh from single images may suffer from the inherent ambiguity as multiple 3D poses and shapes can be mapped to the same situation in a 2D image. To tackle this problem, we propose a self-attention temporal network (SATN) for efficiently optimizing the coherence between predictions of adjacent frames. SATN is the combination of self-attention module and TCN. TCN is employed for its efficient parallel computation and excellent short-term modeling ability. However, the inherently hierarchical structure of convolution layers limits the representation learning of the long-term sequence. More specifically, if the distance of two frames is larger than the kernel size of a single convolution layer, the model requires a stack of convolution layers to construct a long-term connection to relate them. As a result, the associations of the entire sequence cannot be established until the upper layer, which is inefficient. Therefore, we need a more efficient network to establish associations as early as possible. Based on this thought, self-attention is employed to associate the temporal features before TCN. Valuable connections among all frames can be established within just one self-attention layer. In this manner, associations can be efficiently established at the bottom of the temporal network, which greatly helps TCN efficiently learning the short and long-term temporal coherence.</p><p>Given a video clip centered at frame t, SATN is devel-oped to estimate human 3D mesh of frame t. Correspondingly, the supervision is only for the frame t. It is hard to determine whether SATN has learned the long-term temporal correlation. In other words, we lack supervision for guiding the temporal representation learning. Therefore, we propose an unsupervised adversarial training strategy for learning the temporal correlation from the order of video frames. In detail, frames of a video clip are first shuffled and then resorted using the self-attention and sequence sorting module. By recovering the correct temporal order of the motion sequence in the video, a strong supervision signal is generated for learning the motion dynamics. Besides, considering that the motion orders are reversible and the adjacent poses are similar, the target of sequence sorting is well designed to meet these properties. Compared with the DSD network, we report an additional 4.2%/7.3% improvements brought by our temporal model, in terms of PA-MPJPE on Human3.6M and 3DPW respectively. The proposed approach outperform the stateof-the-art methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37]</ref> that predict 3D mesh by 15.4% MPJPE and 23.8% PA-MPJPE on Human3.6M. Besides, state-of-the-art results are also achieved on 3DPW without any fine-tuning. Especially, using features reorganized by DSD module for temporal modeling relatively improves 11.1% and 27.5% PA-MPJPE on 3DPW and Human3.6M respectively. It demonstrates that skeleton-disentangled representation is critical for better temporal motion modeling and generalization.</p><p>In summary, the following contributions are made:</p><p>1. An effective and portable DSD module is proposed to disentangle the skeleton from the rest part of the human 3D mesh. It reduces the complexity and lays a good foundation for better temporal modeling and ge.</p><p>2. Self-attention temporal network is proposed to learn the short and long-term temporal coherence of 3D human body efficiently.</p><p>3. An unsupervised adversarial training strategy is proposed to guide the representation learning of motion dynamics in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Our entire framework is trained in an end-to-end manner. We outperform previous approaches that output 3D meshes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref> in terms of 3D joint error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recovering 3D human mesh from single images. Most of the existing approaches formulated this problem as recovering the parameters of a statistical human body model, SMPL <ref type="bibr" target="#b21">[22]</ref>. Recently proposed ConvNet-based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36]</ref> have shown impressive performance. They can be split into two categories: two stages, direct estimation.</p><p>The two-stage methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17]</ref> first predict intermediate results, like human parsing <ref type="bibr" target="#b25">[26]</ref>, and then predict the SMPL parameters from them. For instance, Pavlakos et al. <ref type="bibr" target="#b26">[27]</ref> developed two individual networks to infer pose and shape from silhouettes and keypoint locations separately. The two-stage methods are robust to the domain shift, but throw away the important details of human body. Some other methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref> directly estimate SMPL parameters from images in an end-to-end manner. In particular, HMR <ref type="bibr" target="#b14">[15]</ref> designed a discriminator to distinguish the authenticity of predicted SMPL parameters. In this generative adversarial manner, HMR could be trained with only 2D pose annotations. HMR greatly outperformed all two-stage methods. It may suggest that the end-to-end and non-disruptive feature extraction process is important in this task. Besides, Tung et al. <ref type="bibr" target="#b31">[32]</ref> developed various supervision approaches, like 2D re-projection of joints and segmentation, to better utilize all available annotations. However, as we introduced before, these methods suffer from severe coupling problem.</p><p>To tackle the coupling problem, we propose a DSD module to disentangle the skeleton from the rest details. It reduces network complexity and improves the accuracy of 3D pose recovery. In this process, the detailed information of 2D images is well preserved. The proposed DSD module based single-frame network outperforms all these methods.</p><p>Recovering 3D pose from monocular video. We focus on 3D pose estimation from monocular video using deep networks, which is the most similar to ours. Various networks are developed to exploit temporal information. For example, TP-Net <ref type="bibr" target="#b4">[5]</ref> and Martinez et al. <ref type="bibr" target="#b23">[24]</ref> trained a fully connected (FC) network to exploit the temporal coherence from the 3D/2D pose sequence. Similarly, Hossain et al. <ref type="bibr" target="#b12">[13]</ref> employed LSTM to predict 3D poses from 2D poses in a sequence-to-sequence manner. Considering the computation efficiency of RNN-based methods is limited, temporal convolution network (TCN) <ref type="bibr" target="#b2">[3]</ref> is proposed and widely used for temporal modeling. For example, Pavllo et al. <ref type="bibr" target="#b27">[28]</ref> employed the TCN to predict 3D pose from a consecutive 2D pose sequence. They also conduct backprojection of estimated poses from 3D to 2D for supervision using the labels generated by the state-of-the-art 2D pose detector. Currently, as we state before, TCN-based methods still lack temporal supervision. We propose an adversarial training strategy to use the temporal order as supervision.</p><p>Recovering 3D mesh from monocular video. Few methods are proposed to recover human 3D mesh from monocular video. Generally, we can split the existing methods into two categories: optimization-based and CNNbased. For example, SFV <ref type="bibr" target="#b28">[29]</ref> first predicts human 3D meshes of each frame by CNN and then smooths the single-frame results via traditional optimization-based postprocessing. However, compared with CNN, the efficiency of optimization-based methods is limited. Most recently, HMR-video <ref type="bibr" target="#b15">[16]</ref> employed the TCN to exploit the temporal information. They proposed to learn the motion dynamic by predicting the actions before and after the current frame. The proposed method outperforms their methods on both 3DPW and Human3.6M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The proposed method is to estimate the 3D human mesh parameters Θ from single images and monocular videos. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the proposed framework has two parts: a) spatially, extracting features of each frame using the proposed DSD module (red) and b) temporally, learning the short and long-term temporal coherence of adjacent frames.</p><p>For single images, the most straightforward method is to estimate Θ directly from features extracted using Resnet-50 <ref type="bibr" target="#b11">[12]</ref>. We follow this simple pipeline and insert a DSD module before the final fully connected layer. The proposed DSD module can re-organize the coupled features and disentangle skeleton from the rest details in feature space.</p><p>For monocular videos, we propose a temporal network (SATN) to estimate Θ from a tuple of features (light green) re-organized by DSD module. More specifically, from n (n = 5 in <ref type="figure" target="#fig_0">Figure 2</ref>) frame features centered at frame t, SATN is built to predict the Θ t . SATN consists of selfattention module (blue) and TCN (gray).</p><p>Besides, an adversarial training strategy (gold) is designed to help the self-attention module learning motion dynamics in the video. In the middle of <ref type="figure" target="#fig_0">Figure 2</ref>), features of adjacent frames are shuffled. Self-attention module along with the sequence sorting module is trained to recover the correct temporal order. This strategy is performed in a multi-task learning manner. The correct order of frame features is recovered after the self-attention module. Features in correct temporal order are sent to TCN for predicting the human 3D mesh Θ t as usual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Human Body Representation</head><p>A parametric statistical 3D human body model, SMPL, is employed to encode the 3D mesh into low-dimensional parameters. SMPL disentangles the shape and pose of a human body. It establishes an efficient mapping M (β, θ; Φ) : R |θ|×|β| → R 3×6890 from shape β and pose θ to a triangulated mesh with 6890 vertices, where Φ represents the statistical prior of human body. The shape parameter β ∈ R 10 is the linear combination weights of 10 basic shape. The pose parameter θ ∈ R 3×23 represents relative 3D rotation of 23 joints in axis-angle representation. To unify the pose formats of different datasets, 14 common joints of LSP <ref type="bibr" target="#b13">[14]</ref> are selected. A linear regressor P 3d is developed to derive these 14 joints from 6890 vertices of human body mesh. The linear combination operation of this regressor guarantees that joints location is differentiable with respect to shape β and pose θ parameters.</p><p>A weak-perspective camera model is employed in this task to establish the mapping from 3D space to 2D image plane, in convenience of supervising 3D mesh with 2D pose labels. Finally, a 85 dimensional vector Θ = {θ, β, R, t, s} is adopted to represent a 3D human body in camera coordinate, where R ∈ R 3 is the global rotation in axis-angle representation, t ∈ R 2 and s ∈ R represents translation and scale in image plane, respectively. The pro-</p><formula xml:id="formula_0">jection of M (β, θ; Φ) is x = sΠ(RM (β, θ; Φ)) + t,<label>(1)</label></formula><p>where Π is an orthographic projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">DSD Module</head><p>First of all, we need to extract the 3D mesh information from each frame. As we introduced before, the existing single-frame methods suffer from severe coupling problem. To overcome this problem, we propose the DSD module to disentangle the skeleton from the rest part of the human 3D mesh, including body 3D shape and the detailed poses like the orientation of head, hands, and feet. The DSD module brings two main advantages: a) decoupling reduces the complexity by meticulously task dividing; and b) skeletondisentangled representation is a better foundation for exploring the temporal motion dynamics.</p><p>As shown in the red part of <ref type="figure" target="#fig_0">Figure 2</ref>, a two-branch structure is designed to extract the skeleton and the rest detailed features separately. We follow <ref type="bibr" target="#b30">[31]</ref> to estimate joint coordinates of the 2D/3D skeleton with minor modifications. Three de-convolution layers are stacked on top of the backbone and followed by DIR (differential integral regression) to convert the normalized heatmaps H into 2D/3D joint coordinates. The DIR(H) is the integration of all location indices p in the heatmaps, weighted by their probabilities. For instance, the 3D coordinate values of the k-th joint J k could be derived from the k-th 3D heatmap H k by</p><formula xml:id="formula_1">J k = D pz=1 H py=1 W px=1 pH k (p),<label>(2)</label></formula><p>where D, H, and W are depth, height and width of the k-th heatmap H k respectively. For 2D heatmaps, D = 1.</p><p>After independent feature extraction, we need to find a proper way to aggregate them. For the two-factor problems, bilinear transformation is well-known for its strong ability of decoupling the style-content factors <ref type="bibr" target="#b8">[9]</ref>, such as the identity and head orientation in face recognition; or accent and word class in speech recognition. Therefore, we employ it to disentangle the skeleton from the rest details. Given the 2D skeleton coordinates x s ∈ R N ×28 and corresponding detailed features x d ∈ R N ×512 , their bilinear transforma-</p><formula xml:id="formula_2">tion y ∈ R N ×512 is y = x s Ax d T ,<label>(3)</label></formula><p>where A ∈ R 512×28×512 is the learnable weights. Moreover, with high modularity and the simple design, it is convenient to transplant DSD module to other networks. For instance, DSD module could be easily plugged into the existing 2D/3D pose estimation network for predicting 3D human body meshes. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, two fully connected layers, and a bilinear transformation layer are all we need for this conversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self-attention Temporal Network</head><p>Secondly, we propose a self-attention temporal network (SATN) for learning long and short-term temporal coherence in the video. Given skeleton-disentangled features from DSD module, SATN predicts smoother Θ in temporal. TCN performs well in modeling the short-term patterns and efficient parallel computation. But due to its inherent hierarchical structure, it is inefficient to model the long-term correlation in the video. To solve this problem, we propose to build the TCN on top of the self-attention module. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, given a tuple of frame features x f ∈ R N ×512 centered at t , self-attention module is employed to relate different frames and establish the long-term sequence representations. Then, the outputs of the self-attention module are fed into the TCN for predicting the human 3D mesh Θ t .</p><p>In particular, the positional encoding is added to each input feature vector for injecting information about their absolute position in sequence. Multi-head attention (MHA) <ref type="bibr" target="#b32">[33]</ref> is adopted to accomplish the self-attention mechanism, which is a variance of typical scaled dot-product attention (SDPA) <ref type="bibr" target="#b32">[33]</ref>. In this work, we employ 8 heads for MHA. In detail, x f is first linearly projected to {x i f ∈ R N ×64 , i = 1, ..., 8} via 8 fully connected (FC) layers. Then SDPA is performed on each {x i f } in parallel. The outputs are concatenated and linearly projected to y f ∈ R N ×512 using a FC layer. The SDPA is derived as</p><formula xml:id="formula_3">SDP A(x f ) = sof tmax( x f x f T √ d )x f ,<label>(4)</label></formula><p>where d=512 is the dimension of inputs and serves as the scaling factor. In <ref type="figure" target="#fig_2">Equation 4</ref>, the output is the weight sum of x f . The weight matrix represents the relation between each two frames. The paths length between different frames is reduced to constants. With the assistance of the self-attention module, the bottom layers of TCN could reach to the information of entire sequence. The receptive field is relatively expanded. Short and long-term temporal coherence can be learned more efficiently. Although we predict from multi-frame features in temporal, the supervision is still in single-frame level. We lack the temporal supervision for the long-term representation learning. To tackle this problem, we propose an unsupervised adversarial training strategy. Similar to recent methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18]</ref>, we also use the temporal order of frames as the supervision for the representation learning. However, there are some differences in the formulation of the problem. Most of the previous methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b7">8]</ref> formulated the problem as the binary classification that verifies the correct/incorrect temporal order. Lee et al. <ref type="bibr" target="#b17">[18]</ref> developed it to predict n!/2 combinations for each n-tuple of frames. In this work, we further develop it to directly estimate the original position indices of the shuffled frames for richer supervision. Besides, two adaptive changes of loss function are made to meet the special properties of the motion sequence.</p><p>As illustrated in the <ref type="figure" target="#fig_0">Figure 2</ref>, frame features are first shuffled before input. As before, the input is added up with the positional coding, which stays unchanged to avoid disclosure of the order information. After going through the self-attention module, the outputs are sent to the TCN and the sequence sorting module separately. Before fed into TCN, the shuffled order is retrieved to avoid the interference with the normal representation learning of the TCN. The sequence sorting module predicts the position indices of the correct order. Specifically, for each tuple of 9 shuffled frames, we need to predict 9 indices, indicating their positions in the original sequence. Besides, two adaptive modifications of sorting target are made. Firstly, considering the reasonable orders of some actions (e.g., standing up/sitting down) are reversible, the minor loss between the predicted and the forward/backward ground truth orders will be chosen. Secondly, sometimes, the differences between adjacent frames are too small to be properly preserved after singleframe convolution encoding. In this situation, the context order of adjacent frames becomes ambiguous. But the typical one-hot label will make equal punishments on this kind of failures without considering the adjacent ambiguity. To tackle this problem, we replace the hard one-hot label with the soft Gaussian-like label for proper supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Functions</head><p>As we mentioned in Section 1, full 3D annotations of 2D image available are limited to the experimental environment. Models trained on these data cannot generalize well to the in-the-wild images. To make full use of existing 2D/3D data, the estimated 3D human body parameters are optimized with</p><formula xml:id="formula_4">L Θ = w pm L pm + w 3d L 3d + w 2d L 2d + w r L r<label>(5)</label></formula><p>where w pm , w 3d , w 2d , w r are weights of these loss items.</p><p>For images with motion capture, L pm is employed to supervise the the body parameter θ, acquired by moshing <ref type="bibr" target="#b20">[21]</ref>, with L2 loss directly. In particular, inspired by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>, angular pose θ ∈ Θ is converted from rotation vectors to </p><p>and</p><formula xml:id="formula_6">J 2d = sΠ(RJ 3d ) + t,<label>(7)</label></formula><p>where M represents a mapping from shape β and pose θ to a 3D human body mesh, P 3d represents a linear mapping from a 3D mesh to 3D joints coordinates and Π is an orthographic projection, as we defined in Section 3.1. Besides, it is important to establish rational constraints of joint angles and body shape, especially for learning from images with only 2D pose annotations. Therefore, we follow <ref type="bibr" target="#b14">[15]</ref> and employ a discriminator to provide rationality loss L r using Mocap data <ref type="bibr" target="#b0">[1]</ref>. Besides, L 2DJ is employed to supervise the 2D skeleton coordinates x s in the DSD module using L1 loss. The sorting results of sequence sorting module are supervised with L2 loss of L s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Human3.6M is the only dataset with 3D annotations we used to train. It contains videos of multiple actors performing 17 activities, which are captured in a controlled environment. We downsample all videos from 50fps to 10fps for removing the redundancy. The proposed method is evaluated on common protocol following <ref type="bibr" target="#b14">[15]</ref> for a more comprehensive comparison. In details, we train on 5 subjects (S1, S5, S6, S7, S8) and test on subject S9 and S11. We report both the mean per joint position error (MPJPE) and Procrustes Aligned MPJPE (PA-MPJPE), which is MPJPE after rigid alignment of predicted pose with ground truth, in millimeters. Except for the common MPJPE and PA-MPJPE, mean per joint velocity and acceleration error (MPJVE/MPJAE), in mm/s and mm/s 2 respectively, are adopted to evaluate the smoothness and stability of predictions over time. 3DPW <ref type="bibr" target="#b33">[34]</ref> is a recent challenge dataset that contains 60 video sequences (24 train, 24 test, 12 validation) of richer activities, such as climbing, golfing, relaxing on the beach, etc. They leverage video and IMU to obtain accurate 3D pose despite the complexity of scenes. For a fair comparison, none of the approaches get trained on 3DPW. We report PA-MPJPE on all splits (train/test/val) of this dataset. 2D in-the-wild datasets. We use MPII <ref type="bibr" target="#b1">[2]</ref>, LSP <ref type="bibr" target="#b13">[14]</ref>, AICH <ref type="bibr" target="#b34">[35]</ref>, and Penn Action <ref type="bibr" target="#b37">[38]</ref>, which are 2D pose datasets without 3D annotations, to train our single-frame DSD network for better generalization. We follow <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref> to use 14 LSP joints as the skeleton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implement Details</head><p>Architecture: The framework has spatial and temporal parts. In spatial, features are extracted using ResNet-50, pre-trained on MPII, and further decoupled using DSD module. In temporal, features of video clips are gathered and put into SATN, which compose of self-attention module and TCN. The self-attention module contains 2 Transformer <ref type="bibr" target="#b32">[33]</ref> blocks. TCN follows the design of <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3]</ref> but only contains 2 convolution blocks. Since the Human3.6M is down-sampled, we take a receptive filed of 9 frames for the TCN, equivalent to 45 frames in the original video. The sequence sorting module consists of 3 convolution blocks followed by 3 FC layers.</p><p>Training details: We train both single-frame DSD network and SATN for 40 epochs. Considering the efficiency of training, we pre-compute the single-frame features and train SATN with them directly. The SGD is adopted as the optimizer with momentum=0.9. The learning rate and batch size are set to 1e-4 and 16 respectively. The weights of loss items are set as w pm = 20, w r = 0.6, w 2d = 10, w 3d = 60. Besides, considering the domain gap between the train and the test set, we discard the hyper-parameters of the batch normalization layers during the evaluation. Besides, to enhance the stability of occlusion, we enlarge the scale augmentation to generate some samples with half body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons to State-of-the-art Approaches</head><p>The results in <ref type="table">Table 1</ref> shows that the proposed methods achieve the state-of-the-art results on 3DPW. Again, all approaches are directly tested without any fine-tuning and Human3.6M is the only 3D train set. It demonstrates that the proposed methods perform well in generalization. The most similar method to ours is HMR-video <ref type="bibr" target="#b15">[16]</ref>, which also learns motion dynamic with TCN for temporal op-  <ref type="table">Table 3</ref>. Ablation study of the components in SATN.</p><p>timization. We compared with their two evaluation settings, HMR-video and HMR-video-L. The main difference between two settings is that HMR-video is trained with datasets of similar scale with ours, while HMR-video-L is trained with their internet dataset which is nearly 20x larger than ours. Our approach outperforms HMR-video in terms of PA-MPJPE by 13.2% on 3DPW test set. On all splits of 3DPW, we evaluate HMR-video-L model they released. As shown in <ref type="table">Table 1</ref>, the proposed method superior to HMRvideo-L on all splits. State-of-the-art results are also achieved on Hu-man3.6M. In <ref type="table">Table 2</ref>, the proposed method outperforms the HMR-video and HMR-video-L by 26.6% and 23.8% in terms of PA-MPJPE respectively. Note that, in <ref type="table">Table 1</ref>, TP-Net and Simple-baseline, also trained on Human3.6M, perform extremely well on Human3.6M, while showing poor generalization ability on 3DPW. Besides, the proposed method greatly outperforms SMPLify on 3DPW, which indicates that the improvements are not brought by the different 3D pose representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation of the DSD module</head><p>1) The ablation study of the DSD module. Two baseline methods are compared in <ref type="table">Table 2</ref> for evaluating the DSD module. Direct is to directly estimate the Θ without DSD module. Concat is to replace the bilinear transformation layer of DSD module with the concatenation operation. Compared with Direct, <ref type="table">Table 2</ref> shows that Concat performs even worse in PA-MPJPE, but adding DSD module brings significant improvement (by 45.3% MPJPE and 29.4% PA-MPJPE). The results demonstrate that decoupling skeleton improves the accuracy of 3D pose recovery. Next, we will validate that the skeleton is sufficiently disentangled from the rest details by the DSD module.</p><p>2) Decoupling effectiveness. We set up a visual experiment to compare the decoupling effectiveness of DSD module and Concat. For verifying whether the skeleton is sufficiently decoupled, we deliberately replace the estimated skeleton with the randomly selected one. The left part of <ref type="figure" target="#fig_2">Figure 4</ref> with gray human mesh shows the different responses from DSD module (the upper two rows) and Concat (the lower two rows) to this replacement. As we can see, the body postures of 3D mesh predicted by the DSD module are correctly changed together with the replaced skeleton, while the outputs from Concat are completely messed up. The experimental results show that the skeleton is successfully disentangled from the rest details. Likewise, a similar conclusion can also be drawn from the right part of <ref type="figure" target="#fig_2">Figure 4</ref> with blue human mesh, which is the results when we replace the detailed features and keep skeleton unchanged.</p><p>3) Stabilily. In addition, <ref type="table" target="#tab_1">Table 4</ref>  6.9 6.8 6.9 6.5 6.5 6.3 6.4 6.3 6.5 6.7 6.8 6.8 <ref type="table">Table 5</ref>. MPJAE(mm/s 2 ): Acceleration error over the poses of the predicted 3D human meshes.</p><p>improves stability with smoother predictions. 4) Comparisons to the state-of-the-art single-frame methods. In <ref type="table">Table 2</ref>, we compare proposed DSD module with HMR and STN. We tightly follow the same evaluation protocol and use the same backbone, ResNet-50, as stated in their articles. Compared with HMR, adding DSD module reduces the error by 31.6% MPJPE and 23.7% PA-MPJPE, which further proves the effectiveness of DSD module. We also compare to STN <ref type="bibr" target="#b36">[37]</ref>, which is the most recent proposed method that also predicts angular pose and outputs 3D meshes. In <ref type="table">Table 2</ref>, DSD network outperforms the STN by 14% MPJPE and 27.9% PA-MPJPE. Some qualitative results of DSD module are illustrated in <ref type="figure" target="#fig_1">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluation of SATN</head><p>1) The ablation study of SATN. <ref type="table">Table 2</ref> shows that compared with the single-frame DSD network, adding SATN brings an additional 4.2%/7.3% improvement in terms of MPJPE/PA-MPJPE on Human3.6M. In addition, <ref type="table">Table 1</ref> shows that adding SATN brings a higher improvement (7.3% PA-MPJPE) on 3DPW. Besides, except for reducing the error of 3D pose recovery, as shown in <ref type="table" target="#tab_1">Table 4</ref> and 5, SATN significantly improves the smoothness of the predictions.</p><p>2) The ablation study of components in SATN. <ref type="table">Table 3</ref> shows that performance gets steadily improved by adding the proposed components, including the self-attention module and the adversarial training strategy. Note that making temporal modeling simply with the TCN is not enough. Compared with the results of the single-frame DSD network in <ref type="table">Table 2</ref>, <ref type="table">Table 3</ref> shows that DSD+TCN leads to the degradation of performance. Similar conclusions can also be drawn from HMR-video <ref type="bibr" target="#b15">[16]</ref>. As shown in <ref type="table">Table 3</ref>, involving self-attention module greatly improves the performance of the temporal network and reverses the degradation. This phenomenon demonstrates that it is vital for TCN to establish associations of the entire input sequence as early as possible and self-attention module performs well in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Role of the DSD Module for SATN</head><p>The results in <ref type="table">Table 3</ref> shows that skeleton-disentangled representation is of great importance for effective temporal modeling. Backbone+TCN in <ref type="table">Table 3</ref> is to train the TCN with features from the backbone directly without DSD module. As shown in <ref type="table">Table 2</ref>, the results of Back-bone+TCN are comparative to the HMR-video <ref type="bibr" target="#b15">[16]</ref>, which has similar architecture. By contrast, involving DSD module (DSD+TCN in <ref type="table">Table 3</ref>) brings a 11.1% improvement in terms of PA-MPJPE. Besides, when we replace TCN with SATN, the performances get further improvement. Especially, in <ref type="table">Table 3</ref>, the combination of DSD and SATN (DSD+TCN+Self-attention+adversarial training) causes a magic reaction and outperforms all backbone-based methods by 26.6% MPJPE and 27.5% PA-MPJPE. However, PA-MPJPE of Backbone+TCN and Backbone+SATN in <ref type="table">Table 3</ref> are nearly equal, which indicates that skeleton-disentangled representation is essential for motion modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose an end-to-end framework for recovering 3D human mesh from single images and monocular videos via a skeleton-disentangled representation. Decoupling skeleton reduces the complexity and the 3D pose error. The proposed DSD module could be regarded as an efficient bridge between 2D/3D pose estimation and 3D mesh recovery. Moreover, SATN is well designed to explore long and short-term temporal coherence. Massive evaluations demonstrate that we provide an attractive and efficient baseline method for related problems, including human motion analysis, 3D virtual try-on and multi-person 3D meshes recovery.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed skeleton-disentangling based self-attention temporal network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Some qualitative results of DSD network.3 × 3 rotation matrices via the Rodrigues formula for stable training. For images with 2D/3D pose annotations, L 2d and L 3d are employed to supervise the skeleton pose J 2d /J 3d of estimated body mesh using L1/L2 loss respectively. J 3d and J 2d are derived from the predicted Θ = {θ, β, R, t, s} by J 3d = P 3d (M (β, θ; Φ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Decoupling effectiveness comparison between DSD module (the two rows upper) and Concat(the two rows lower) by replacing the skeleton (shown in the left part) or the detailed features (shown in the right part).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>and 5 show their MPJVE and MPJAE. Note that Concat has much larger MPJVE and MPJAE than the rest, even including Direct. It indicates that the predictions derived from the concatenated features are more unstable. By contrast, the DSD module Method Dir. Disc. Eat Greet Phone Pose Purch. Sit SitD. Smoke Photo Wait Walk WalkD. WalkT. Avg.↓ Direct 14.5 15.6 14.9 15.6 15.4 15.0 15.9 15.4 16.1 15.6 15.7 15.7 15.8 16.0 16.1 16.1 Concat 20.0 20.9 19.4 19.7 19.2 18.8 19.5 19.1 20.1 19.4 19.7 19.6 19.7 20.4 20.4 20.5 DSD 12.3 12.8 12.2 12.6 12.4 12.1 12.1 11.8 12.3 12.0 12.0 12.0 12.2 12.MPJVE(mm/s): Velocity error over the poses of the predicted 3D human meshes. Method Dir. Disc. Eat Greet Phone Pose Purch. Sit SitD. Smoke Photo Wait Walk WalkD. WalkT. Avg.↓</figDesc><table><row><cell>4</cell><cell>12.5 12.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">CMU graphics lab motion capture database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards multi-pose guided virtual try-on network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flow-navigated warping gan for video virtual try-on</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning bilinear models for two-factor problems in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognizing an action using its name: A knowledge-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">T-c3d: Temporal convolutional 3d network for realtime action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate estimation of human body orientation from rgb-d sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on cybernetics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mosh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A progressive search paradigm for the internet of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision, 3DV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11742</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sfv: Reinforcement learning of physical skills from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of pose embeddings from spatiotemporal relations in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Sumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Dencker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ai challenger: A large-scale dataset for going deeper in image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06475</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Monoperfcap: Human performance capture from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Yoshiyasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryusuke</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ko</forename><surname>Ayusawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Murai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11328</idno>
		<title level="m">Skeleton transformer networks: 3d human pose and skinned mesh from single rgb image</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
