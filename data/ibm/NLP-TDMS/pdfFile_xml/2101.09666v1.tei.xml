<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAD-CAM GUIDED CHANNEL-SPATIAL ATTENTION MODULE FOR FINE-GRAINED VISUAL CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
						</author>
						<title level="a" type="main">GRAD-CAM GUIDED CHANNEL-SPATIAL ATTENTION MODULE FOR FINE-GRAINED VISUAL CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Fine-grained visual classification</term>
					<term>gradient-weighted class activation mapping</term>
					<term>channel-spatial attention mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained visual classification (FGVC) is becoming an important research field, due to its wide applications and the rapid development of computer vision technologies. The current state-of-the-art (SOTA) methods in the FGVC usually employ attention mechanisms to first capture the semantic parts and then discover their subtle differences between distinct classes. The channel-spatial attention mechanisms, which focus on the discriminative channels and regions simultaneously, have significantly improved the classification performance. However, the existing attention modules are poorly guided since part-based detectors in the FGVC depend on the network learning ability without the supervision of part annotations. As obtaining such part annotations is laborintensive, some visual localization and explanation methods, such as gradient-weighted class activation mapping (Grad-CAM), can be utilized for supervising the attention mechanism. We propose a Grad-CAM guided channel-spatial attention module for the FGVC, which employs the Grad-CAM to supervise and constrain the attention weights by generating the coarse localization maps. To demonstrate the effectiveness of the proposed method, we conduct comprehensive experiments on three popular FGVC datasets, including CUB-200-2011, Stanford Cars, and FGVC-Aircraft datasets. The proposed method outperforms the SOTA attention modules in the FGVC task. In addition, visualizations of feature maps also demonstrate the superiority of the proposed method against the SOTA approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Fine-grained visual classification (FGVC) aims to distinguish fine-grained classes under the same coarse class labels, e.g., birds <ref type="bibr" target="#b0">[1]</ref>, airplanes <ref type="bibr" target="#b1">[2]</ref>, cars <ref type="bibr" target="#b2">[3]</ref>fi and flowers <ref type="bibr" target="#b3">[4]</ref>, etc. The main challenge of the FGVC task is the tiny inter-class difference along with significant intra-class diversity. For example, it is difficult to distinguish a redhead woodpecker from a pileated woodpecker and a downy woodpecker caused by highly similar sub-categories, but with the adjustment of poses, scales, and rotations, the redhead woodpecker can be guide CNN block attention <ref type="figure">Fig. 1</ref>. Motivation of the Grad-CAM guided channel-spatial attention module. The blue part is the general pipeline of the previous attention mechanisms. The yellow line is our proposed supervision mechanism that the weights obtained by the gradient backpropagation in the Grad-CAM are used for the guidance of the attention weights, with which the attention mechanisms focus on parts that contribute significantly to classification.</p><p>photographed in a very different visual view. In order to generate discriminative features more precisely, we better have the ability to capture the key characteristics of the red head and ignore the background and other irrelevant regions, which is an obvious way for overcoming the challenge. The existing approaches can be roughly divided into two classes: <ref type="bibr" target="#b0">(1)</ref> searching the informative regions that contribute the most to the classification task <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> and (2) paying more attention to extract high-order features from the images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. For the former one, previous approaches <ref type="bibr" target="#b4">[5]</ref> usually employed the prior location information such as part-level bounding boxes and segmented masks to generate the discriminative parts. Meanwhile, for the latter one, the powerful deep networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> were employed for feature extraction, and different loss functions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> were designed for constraining these networks to improve the discrimination of the extracted features. Recently, the attention mechanisms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, which only require image labels for training, have gradually replaced the manual annotation methods, since part annotations are time-consuming and laborious that limits the flexibility and versatility of the real-world FGVC applications. Compared with the approaches that introduce complex structures, the attention-based methods add fewer learned parameters for model training, which efficiently reduces the computation cost.</p><p>The attention mechanisms fully simulate the observation habits of human eyes, which always concentrate on the most distinctive regions for observing. For example, we can easily pay attention to the head and the wings of a bird and ignore the other common regions to identify its species. Based on this motivation, many methods have been proposed by utilizing the attention mechanisms to detect the discriminative information from the images, including channel attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>, spatial attention <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, and channel-spatial attention <ref type="bibr" target="#b15">[16]</ref>. Specifically, SENet <ref type="bibr" target="#b11">[12]</ref> introduced "squeezeand-excitation" (SE) blocks to adaptively recalibrate the feature maps in channel-wise by modeling the interactions between channels. The trilinear attention sampling network <ref type="bibr" target="#b5">[6]</ref> generated attention maps by integrating feature channels with their relationship matrix and highlighted the attended parts with high resolution. The recurrent attention convolutional neural network (RA-CNN) <ref type="bibr" target="#b13">[14]</ref> introduced attention proposal network (APN) to capture the region relevance information based on the extracted features, and then amplified the attention region crops to make the network gradually focus on the key areas. The convolutional block attention mechanism (CBAM) <ref type="bibr" target="#b15">[16]</ref> is a channel-spatial attention method that utilizes both the channel-level and region-level information. It can effectively improve the characteristic expression ability of the networks. The existing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref> usually utilize different attention mechanisms to generally adjust the distributions of the attention weights for balancing the contributions of feature maps extracted from each part. Although these methods for obtaining the weights are different, they are all constructed based on the original feature maps only, without part information supervision. Obviously, if the feature maps focus on the non-significant parts such as backgrounds and distractions, the attention mechanism is meaningless under the unsupervised conditions.</p><p>To address this issue, we propose a weakly-supervised guideline for discriminative part mining and informative feature learning. It drives the networks to focus on the parts which have specific characteristic information, such as the head and the beak of a bird. In addition, as each channel of the feature maps can be also considered as a semantic part <ref type="bibr" target="#b12">[13]</ref>, supervision on discriminative parts can be transferred to that on channels. Gradient-weighted class activation mapping (Grad-CAM) <ref type="bibr" target="#b16">[17]</ref> is usually introduced to illustrate attentions of the networks with heat maps and visualize the attentions in each part by weighted averaging channels, which can be used for guiding the networks to focus on more efficient parts and discard the redundant information for the classification. In this paper, we introduce a channel-spatial attention mechanism with Grad-CAM guided, in which the channel weighted feature maps are pooled along with the channel dimensions and multiplied by the original feature maps to obtain the channel-spatial attention maps. Meanwhile, a novel Grad-CAM guided channel-spatial attention mechanism loss (GGAM-Loss) is applied for guiding the learning process of the channel weights and forcing the attention module to focus on the parts that contribute most to the classification. As shown in <ref type="figure">Figure 1</ref>, we employ the channel weights obtained from the gradient backpropagation in the Grad-CAM to constrain the channel weights of the forward propagation.</p><p>Our contributions can be summarized as follows:</p><p>• We address the challenge of the FGVC by proposing a Grad-CAM guided channel-spatial attention module, which constrains the channel-spatial attention mechanism to focus on the parts that contribute most to the classification.</p><p>• We propose a Grad-CAM guided channel-spatial attention mechanism loss (GGAM-Loss) which employs the Grad-CAM to supervise and constrain the attention weights. Moreover, it is not limited to a specific network architecture.</p><p>• We conduct comprehensive experiments on the three commonly used FGVC datasets, i.e., CUB-200-2011 <ref type="bibr" target="#b0">[1]</ref>, FGVC-Aircraft <ref type="bibr" target="#b1">[2]</ref>fi and Stanford Cars <ref type="bibr" target="#b2">[3]</ref> datasets. The results show the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Channel-spatial Attention Mechanism</head><p>The channel-spatial attention is a module that combines both the spatial attention and the channel attention. Specifically, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the input image is processed through a series of convolution and pooling operations F cp and feature maps denoted as A = [a 1 , a 2 , ..., a C ] ∈ R C×W ×H are obtained, with height H, width W , and channel number C, respectively. Then we apply a global average pooling F cg to downsample each feature map in A and a two-layer fully connected (FC) network F cr with softmax function to calculate the weights of each channel as the channel attention weights S = [s 1 , s 2 , ..., s C ] ∈ R C , according to <ref type="bibr" target="#b11">[12]</ref>. We rescale the original feature maps A by S, which obtains the weighted</p><formula xml:id="formula_0">feature maps B = [b 1 , b 1 , ..., b c ] ∈ R C×W ×H by F cm as b c = F cm (a c , s c ) = a c · F cr (F cg (A)) c ,<label>(1)</label></formula><p>where c = 1, · · · , C. After gaining the channel attention-weighted feature maps B, spatial attention is undertaken. Through the operation F f a , which combines a channel-wise summation and a 2D softmax function, the feature maps in B are flattened along the channel dimension to obtain the spatial attention weights T ∈ R W ×H . Then the channel-spatial attention-weighted feature maps D = [d 1 , d 1 , ..., d c ] ∈ R C×W ×H are obtained by rescaling A with T as  where is Hadamard product and</p><formula xml:id="formula_1">d c = F sm (a c , T ) = a c F f a (B),<label>(2)</label></formula><formula xml:id="formula_2">C W H input Backpropagation GAP F cp A F cg F cr F cm F fa F sm F tc F cl F si F mp A GGAM-Loss</formula><formula xml:id="formula_3">T = F f a (B) = C c=1 b c W i=1 H j=1 C c=1 b c,i,j .<label>(3)</label></formula><p>Then the classification is undertaken according to D. F tc is the classifier with multiple FC layers and a softmax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Grad-CAM</head><p>The Grad-CAM uses the class-specific gradient information and it flows into the final convolutional layer of a CNN to generate a heat map, which shows the main concentrated regions of the CNN. Specifically, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, given an input image, we obtain the score y k for the predicted class k before the last softmax function. Then, y k is propagated back to the elements of A through the upper line and we gain the gradient ∂y k ∂Ac,i,j . The weights β k c of the Grad-CAM, which represent the importance of feature map c with the predicted class k, can be defined as</p><formula xml:id="formula_4">β k c = 1 W × H W i=1 H j=1 ∂y k ∂A c,i,j .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Grad-CAM guided channel-spatial attention loss</head><p>In the FGVC, attention mechanism is introduced to ensure the CNN focus on more effective parts mainly, so as to improve the classification accuracy. As mentioned above, Grad-CAM can extract the key parts of the input image. In this section, we follow the same motivation and propose the Grad-CAM guided channel-spatial attention loss to enhance discriminative part searching and feature extraction abilities of CNNs in the FGVC. In <ref type="figure" target="#fig_0">Figure 2</ref>, after the F cr operation, we can obtain the weights S of each channel in A. Through the operation F si , we apply a sigmoid function for β k c , c = 1, · · · , C, to scale their intervals and obtainβ k = [β k 1 , · · · ,β k C ] ∈ R C , whereβ k c = sigmoid(β k c ). Asβ k can reflect the contribution of each channel to the classification, we constrain the channel attention weights S with it. Here, we propose the Grad-CAM guided channel-spatial attention mechanism loss, GGAM-Loss in short, to construct the regularization term. The GGAM-Loss (L GGAM ), which performs as a symmetrical Kullback-Leibler (KL) divergence between S andβ k , can be defined as</p><formula xml:id="formula_5">L GGAM = 1 2 KL(S||β k ) + KL(β k ||S) ,<label>(5)</label></formula><p>where KL(x||y) is the KL divergence from x to y. Moreover, as we use the original cross-entropy (CE) loss L CE for training the model as well, the total loss function Loss of the whole network can be defined as</p><formula xml:id="formula_6">Loss = L CE + λL GGAM ,<label>(6)</label></formula><p>where λ is a nonnegative multiplier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL RESULTS AND DISCUSSIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We evaluate our method on three challenging FGVC datasets, including CUB-200-2011 <ref type="bibr" target="#b0">[1]</ref>, FGVC-Aircraft <ref type="bibr" target="#b1">[2]</ref>, and Stanford Cars <ref type="bibr" target="#b2">[3]</ref> datasets. The statistics of the datasets mentioned above, including class numbers and the training/testing sample numbers are shown in <ref type="table" target="#tab_0">Table 1</ref>. We followed the same train/test splits as presented in the <ref type="table" target="#tab_0">Table 1</ref>. For model training, we did not use artificially marked bounding box or part annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>In order to compare the proposed method with other attention mechanisms, we resized every image to 448 × 448, which is standard in the literatures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. The backbones we used for extracting features were VGG19 and ResNet50 which were pre-trained on the ImageNet dataset. We used stochastic gradient descent optimizer. The weight dacay value and the momentum were kept as 1 × 10 −4 and 0.9, respectively, with 100 epochs. The learning rate of the FC layers was initially set at 0.1 and we used the cosine anneal schedule update strategy <ref type="bibr" target="#b20">[21]</ref>. The learning rate of the pre-trained feature extaction layers was one-tenth of the FC layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experimental Results</head><p>According to the aforementioned implementation details, the detailed results are listed in <ref type="table" target="#tab_1">Table 2</ref>. Our method achieves significant performance improvement on all the three datasets and the evaluation results can be summarized as follows:</p><p>• On the CUB-200-2011 dataset, our method achieves the best result on both VGG19 and ResNet50, respectively, comapred with their corresponding referred <ref type="table">Table 3</ref>. Ablation study of our method on classification accuracies (%). Key modules of the proposed method, including the channel attention, the spatial attention, and the Grad-CAM are compared. " " represents the module contained, otherwise "×". The best result is in bold. methods. Our method exceeds the second best method, TASN, by 1.24% with the VGG19. In addition, compared with the leading result achieved by the ACNet, our method has improved the accuracy by 0.35% with the ResNet50.</p><p>• On the FGVC-Aircraft dataset, our method also obtains the best accuracy of 93.42% with the ResNet50, around 0.4% improvement than the DCL. With the VGG19, the result of our method also improves slightly.</p><p>• On the Stanford Cars dataset, our method outperforms the most compared methods, especially with the same VGG19 backbone. The accuracy of the ACNet with the ResNet50 turns out 0.19% better than ours. Note that the ACNet depends mainly on the addition of the network parameters and the complex training process to improve the accuracy, which is much more complex than ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channel-spatial Attention Baseline</head><p>Channel Attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Attention</head><p>With GGAM-Loss Without GGAM-Loss </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Ablation Study</head><p>Attention mechanisms and Grad-CAM are major modules of our method, and the attention mechanisms include channel and spatial attention mechanisms. We analyze the influence of each module by the experimental results. The ablation experiments are all conducted on the CUB-200-2011 dataset and we use the ResNet50 as the base model if not particularly mentioned. The experimental results are shown in <ref type="table">Table 3</ref>.</p><p>• Effectiveness of the attention mechanisms. Compared with the baseline model, the spatial attention can improve performance by 0.51% and the channel attention also has a slight promotion. In particular, the combination of channel and spatial attention obtains a 1.76% increase on accuracy. This enhancement is obvious and shows that the channel-spatial attention is useful for the FGVC.</p><p>• Effectiveness of the GGAM-Loss. It can be seen that the classification accuracy of each attention mechanism model is improved after adding the GGAM-Loss as the constraint for the attention mechanism. The above results demonstrate the effectiveness of the GGAM-Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Visualizations</head><p>In order to better explain the improvement of our method, <ref type="figure" target="#fig_1">Figure 3</ref> shows the visualizations of each model in Section 3.4, which were generated by the Grad-CAM. The baseline cannot clearly focus on the right region of the object. With the addition of the attention mechanisms, the models tend to pay  attention on the beak and the neck of the bird, which are discriminative parts. After adding the GGAM-Loss, the models can focus on more accurate discriminant characteristics and pay less attention to background information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Sensitivity Study of λ</head><p>In order to evaluate the robustness of our method, we conduct the sensitivity study of the hyperparameter λ to see whether the network performance changes a lot with a change of λ. We conduct this study on the CUB-200-2011 dataset and we use the ResNet50 as the base model. We run the proposed model set with λ varying from 1 to 9 with step size of 1. The classification accuracies are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. From <ref type="figure" target="#fig_2">Figure 4</ref>, it can be observed that the performance of our method has always been better than the channel-spatial attention (without the GGAM-Loss) and does not change much by varying the value of λ, which proves the effectiveness and robustness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper, we proposed a Grad-CAM guided channelspatial attention mechanism loss (GGAM-Loss) for the FGVC task, which can constrain the channel-spatial attention module to focus on the most discriminative parts in the images. Note that the proposed GGAM-Loss can be also applied to other network architectures. The performance of our method is evaluated in the FGVC task and superior performance is achieved on three FGVC datasets (CUB-200-2011, Stanford Cars, and FGVC-Aircraft datasets). The effectiveness of the key modules of the proposed method were also evaluated. Visualizations of the feature maps illustrate the validity of the porposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The framework of our attention module. The upper line (from left to right) and the bottom line (from right to left) present the forward and the gradient backpropagation processes, respectively. A symmetrical Kullback-Leibler (KL) divergence between the weights of each channel in forward propagation and the weights of each feature map in the Grad-CAM is utilized as the loss function in backpropagation to supervise the channel-spatial attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Visualizations of the ablation models in Section 3.4. The first column represents the original image. The following four columns show visualization results of the baseline, the channel attention, the spatial attention, and the channel-spatial attention, respectively. The top row is trained without the GGAM-Loss, while the bottom row is trained with the GGAM-Loss.The red box indicates the visualization result of our proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Sensitivity study of λ for our model on the CUB-200-2011 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The statistics of the three FGVC datasets. #Class, #Training, and #Test are class number, training sample number, and test sample number, respectively.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Class #Training #Test</cell></row><row><cell>CUB-200-2011 [1]</cell><cell>200</cell><cell>5994</cell><cell>5794</cell></row><row><cell>FGVC-Aircraft [2]</cell><cell>100</cell><cell>6667</cell><cell>3333</cell></row><row><cell>Stanford Cars [3]</cell><cell>196</cell><cell>8144</cell><cell>8041</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracies (%) on the CUB-200-2011, the FGVC-Aircraft, and the Stanford Cars datasets. The best results on each dataset are in bold, and the second best results are in underline.</figDesc><table><row><cell>Datasets</cell><cell cols="4">Base Model CUB-200-2011 FGVC-Aircraft Stanford Cars</cell></row><row><cell>RA-CNN (CVPR17 [14])</cell><cell>VGG19</cell><cell>85.30</cell><cell>88.20</cell><cell>92.50</cell></row><row><cell>MA-CNN (ICCV17 [13])</cell><cell>VGG19</cell><cell>84.92</cell><cell>90.35</cell><cell>92.80</cell></row><row><cell>SENet (CVPR18 [12])</cell><cell>VGG19</cell><cell>84.75</cell><cell>90.12</cell><cell>89.75</cell></row><row><cell>SENet (CVPR18 [12])</cell><cell>ResNet50</cell><cell>86.78</cell><cell>91.37</cell><cell>93.10</cell></row><row><cell>CBAM (ECCV18 [16])</cell><cell>VGG19</cell><cell>84.92</cell><cell>90.32</cell><cell>91.12</cell></row><row><cell>CBAM (ECCV18 [16])</cell><cell>ResNet50</cell><cell>86.99</cell><cell>91.91</cell><cell>93.35</cell></row><row><cell>DFL (CVPR18 [18])</cell><cell>ResNet50</cell><cell>87.40</cell><cell>91.73</cell><cell>93.11</cell></row><row><cell>NTS (ECCV18 [7])</cell><cell>ResNet50</cell><cell>87.52</cell><cell>91.48</cell><cell>93.90</cell></row><row><cell>TASN(CVPR2019 [6] )</cell><cell>VGG19</cell><cell>86.10</cell><cell>90.83</cell><cell>92.40</cell></row><row><cell>TASN(CVPR2019 [6])</cell><cell>ResNet50</cell><cell>87.90</cell><cell>92.56</cell><cell>93.80</cell></row><row><cell>DCL(CVPR2019 [19])</cell><cell>ResNet50</cell><cell>87.80</cell><cell>93.00</cell><cell>94.50</cell></row><row><cell>ACNet(CVPR2020 [20])</cell><cell>ResNet50</cell><cell>88.10</cell><cell>92.40</cell><cell>94.60</cell></row><row><cell>Ours</cell><cell>VGG19</cell><cell>87.34</cell><cell>91.55</cell><cell>93.32</cell></row><row><cell>Ours</cell><cell>ResNet50</cell><cell>88.45</cell><cell>93.42</cell><cell>94.41</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>abs/1306.5151</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing</title>
		<meeting>the 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Part-based R-CNNfs for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning deep bilinear transformation for fine-grained image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The devil is in the channels: Mutual-channel loss for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4683" to="4695" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning multiattention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning rich part hierarchies with progressive attention networks for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="476" to="488" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a CNN for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention convolutional binary neural tree for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Snapshot ensembles: Train 1, get M for free</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
