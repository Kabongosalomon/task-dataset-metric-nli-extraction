<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmentation Is All You Need</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Reliability and Intelligence of Electrical Equipment</orgName>
								<orgName type="institution">Hebei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">AISA Research</orgName>
								<orgName type="institution" key="instit2">Hunan Agricultural University</orgName>
								<address>
									<addrLine>China 4 SnowCloud.ai</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Reliability and Intelligence of Electrical Equipment</orgName>
								<orgName type="institution">Hebei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Segmentation Is All You Need</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Region proposal mechanisms are essential for existing deep learning approaches to object detection in images. Although they can generally achieve a good detection performance under normal circumstances, their recall in a scene with extreme cases is unacceptably low. This is mainly because bounding box annotations contain much environment noise information, and non-maximum suppression (NMS) is required to select target boxes. Therefore, in this paper, we propose the first anchorfree and NMS-free object detection model, called weakly supervised multimodal annotation segmentation (WSMA-Seg), which utilizes segmentation models to achieve an accurate and robust object detection without NMS. In WSMA-Seg, multimodal annotations are proposed to achieve an instance-aware segmentation using weakly supervised bounding boxes; we also develop a run-data-based following algorithm to trace contours of objects. In addition, we propose a multi-scale pooling segmentation (MSP-Seg) as the underlying segmentation model of WSMA-Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA-Seg. Experimental results on multiple datasets show that the proposed WSMA-Seg approach outperforms the state-of-the-art detectors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection in images is one of the most widely explored tasks in computer vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Existing deep learning approaches to solve this task (e.g., R-CNN <ref type="bibr" target="#b2">[3]</ref> and its variants <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1]</ref>) mainly rely on region proposal mechanisms (e.g., region proposal networks (RPNs)) to generate potential bounding boxes in an image and then classify these bounding boxes to achieve object detection. Although such mechanisms can generally achieve a good detection performance under normal circumstances, their recall in a scene with extreme cases (e.g., complex occlusion ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>), poor illumination ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>), and large-scale small objects ( <ref type="figure" target="#fig_0">Fig. 1(c)</ref>)) is unacceptably low.</p><p>Specifically, detecting objects under extreme cases via region proposal mechanisms encounters two challenges: First, the performance of region proposal mechanisms highly depends on the purity of bounding boxes <ref type="bibr" target="#b5">[6]</ref>; however, the annotated bounding boxes in extreme cases usually contain much more environment noise than those in normal cases. This inevitably increases the difficulty of model learning and decreases the resulting confidence scores of bounding boxes, which consequently weakens the detection performance. Second, non-maximum suppression (NMS) operations are used in region proposal mechanisms to select target boxes by setting an intersection over union (IoU) threshold to filter other bounding boxes. However, it is very hard (and sometimes even impossible) to find an appropriate threshold to adapt to the very complex situations in extreme cases. Motivated by this, in this work, we propose a weakly supervised multimodal annotation segmentation (WSMA-Seg) approach, which uses segmentation models to achieve an accurate and robust object detection without NMS. It consists of two phases, namely, a training and a testing phase. In the training phase, WSMA-Seg first converts weakly supervised bounding box annotations in detection tasks to multi-channel segmentation-like masks, called multimodal annotations; then, a segmentation model is trained using multimodal annotations as labels to learn multimodal heatmaps for the training images. In the testing phase, the resulting heatmaps of a given test image are converted into an instance-aware segmentation map based on a pixel-level logic operation; then, a contour tracing operation is conducted to generate contours for objects using the segmentation map; finally, bounding boxes of objects are created as circumscribed quadrilaterals of their corresponding contours.</p><p>WSMA-Seg has the following advantages: (i) as an NMS-free solution, WSMA-Seg avoids all hyperparameters related to anchor boxes and NMS; so, the above-mentioned threshold selection problem is also avoided; (ii) the complex occlusion problem can be alleviated by utilizing the topological structure of segmentation-like multimodal annotations; and (iii) multimodal annotations are pixel-level annotations; so, they can describe the objects more accurately and overcome the above-mentioned environment noise problem.</p><p>Furthermore, it is obvious that the performance of the proposed WSMA-Seg approach greatly depends on the segmentation performance of the underlying segmentation model. Therefore, in this work, we further propose a multi-scale pooling segmentation (MSP-Seg) model, which is used as the underlying segmentation model of WSMA-Seg to achieve a more accurate segmentation (especially for extreme cases, e.g., very small objects), and consequently enhances the detection accuracy of WSMA-Seg.</p><p>The contributions of this paper are briefly as follows:</p><p>• We propose a weakly supervised multimodal annotation segmentation (WSMA-Seg) approach to achieve an accurate and robust object detection without NMS, which is the first anchor-free and NMS-free object detection approach.</p><p>• We propose multimodal annotations to achieve an instance-aware segmentation using weakly supervised bounding boxes; we also develop a run-data-based following algorithm to trace contours of objects.</p><p>• We propose a multi-scale pooling segmentation (MSP-Seg) model to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA-Seg.</p><p>• We have conducted extensive experimental studies on the Rebar Head, WIDER Face, and MS COCO datasets; the results show that the proposed WSMA-Seg approach outperforms the state-of-the-art detectors on all testing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Weakly Supervised Multimodal Annotation Segmentation</head><p>In this section, we introduce our approach to object detection using weakly supervised multimodal annotation segmentation (WSMA-Seg). WSMA-Seg generally consists of two phases: a training phase and a testing phase. In the training phase, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, WSMA-Seg first converts the weakly supervised bounding box annotations to pixel-level segmentation-like masks with three channels, representing interior, boundary, and boundary on interior masking information, respectively; the resulting annotations are called multimodal annotations; then, multimodal annotations are used  as labels to train an underlying segmentation model to learn corresponding multimodal heatmaps for the training images. In the testing phase, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we first send the given testing image into the well-trained segmentation model to obtain multimodal heatmaps; then, the resulting three heatmaps are converted into an instance-aware segmentation map based on a pixel-level logic operation; finally, a contour tracing operation is conducted to generate contours for objects using the segmentation map, and the bounding boxes of objects are created as circumscribed quadrilaterals of their contours. The rest of this section will introduce the main ingredients of WSMA-Seg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generating Multimodal Annotations</head><p>Pixel-level segmentation annotations are much more representative than bounding box annotations, so they can resolve some extreme cases that are challenging for bounding box annotations. However, creating well-designed pixel-level segmentation masks is very time-consuming, which is about 15 times of creating bounding box annotations <ref type="bibr" target="#b6">[7]</ref>. Therefore, in this work, we propose a methodology to automatically convert bounding box annotations to segmentation-like multimodal annotations, which are pixel-level geometric segmentation-like multichannel annotations. Here, "geometric segmentationlike" means that the multimodal annotations are not strict segmentation annotations; rather, they are annotations generated from simple geometries, e.g., inscribed ellipses of bounding boxes. This is motivated by the finding in <ref type="bibr" target="#b7">[8]</ref> that pixel-level segmentation information is not fully utilized by segmentation models; we thus believe that well-designed pixel-level segmentation annotations may not be essential to achieve a reasonable performance; rather, pixel-level geometric annotations should be enough. Furthermore, to generate a bounding box for each object in the image, an instance-aware segmentation is required; to achieve this, multimodal annotations are designed to have multiple channels to introduce additional information.</p><p>Specifically, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, multimodal annotations use three channels to represent pixellevel masking information regarding the interior, the boundary, and the boundary on the interior of geometries. These three different pixel-level masks are generated as follows: Given an image with bounding box annotations, we first obtain an inscribed ellipse for each bounding box, then the interior mask (channel 0) is obtained by setting the values of pixels on the edge of or inside the ellipses to 1, and setting the values of other pixels to 0. Then, the boundary mask (channel 1) is obtained by setting the values of pixels on the edge of or within the inner width w of the ellipses to 1, and setting the rest to 0. Similarly, the boundary on the interior mask (channel 2) is generated by setting the values of pixels on the edge of or within the inner width w of the area of the elliptical overlap to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-Scale Pooling Segmentation</head><p>It is obvious that the performance of the proposed WSMA-Seg approach greatly depends on the segmentation performance of the underlying segmentation model. Therefore, in this work, we further propose a multi-scale pooling segmentation (MSP-Seg) model, which is used as the underlying segmentation model of WSMA-Seg to achieve a more accurate segmentation (especially for extreme cases, e.g., very small objects), and to consequently enhance the detection accuracy of WSMA-Seg.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, MSP-Seg is an improved segmentation model of Hourglass <ref type="bibr" target="#b8">[9]</ref>. The main improvement of MSP-Seg is to introduce a multi-scale block on the skip connections, performing multi-scale pooling operations to the output feature maps of residual blocks. Specifically, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>, multi-scale pooling utilizes four pooling kernals with sizes 1 × 1, 3 × 3, 5 × 5, and 7 × 7 to simultaneously conduct average pooling operations on the previous feature maps generated by residual blocks on skip connections. Then, four feature maps generated by different pooling channels are concatenated to form a new feature map whose number of channels is four times of the previous feature maps. Here, to ensure that the four feature maps have the same size, the stride is set to 1, and zero-padding is conducted. Finally, we apply 1 × 1 convolution to restore the number of channels, and element-wise addition to merge the feature maps. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, by using multimodal annotations as labels, MSP-Seg is trained to learn three heatmaps for each image, which are called interior heatmap, boundary heatmap, and boundary on interior heatmap, respectively.</p><p>Intuitively, multi-scale pooling is capable of enhancing the segmentation accuracy, because it combines features of different scales to obtain more representative feature maps. Please note that, as a highly accurate segmentation model, MSP-Seg can be widely applied to various segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Object Detection Using Segmentation Results and Contour Tracing</head><p>After obtaining a well-trained segmentation model, we are now able to conduct object detection. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, given a test image as the input of the segmentation model, WSMA-Seg first generates three heatmaps, i.e., interior, boundary, and boundary on interior heatmaps, which are denoted as I, B, and O, respectively. These three heatmaps are then converted to binary heatmaps, where the values of pixels in interested area are set to 1, and the rest is set to 0. This conversion is conducted following the approach in <ref type="bibr" target="#b9">[10]</ref>. Furthermore, a pixel-level operation, I ⊕ (B ∧ O), is used to merge three heatmaps into an instance-aware segmentation map.  Finally, a contour tracing operation is conducted to generate contours for objects using the instanceaware segmentation map, and the bounding boxes of objects are created as circumscribed quadrilaterals of their contours. One conventional way to trace a contour is to use scan-based-following algorithm <ref type="bibr" target="#b9">[10]</ref>. However, in the case of a large image with many objects (which is common in detection tasks), scan-based-following algorithm is very time consuming.</p><p>Therefore, motivated by the work in <ref type="bibr" target="#b10">[11]</ref>, we propose a modified run-data-based (RDB) following algorithm, which greatly reduces the time and memory costs of the contour tracing operation. Pseudocode of the RDB following algorithm is shown in Algorithm 1 and an example is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.3. Differently from the pixel-following algorithm that requires to scan the entire image to find the starting point and tracing contour pixels along the clockwise direction to generate the results recurrently, the RDB following algorithm only needs to save two lines of pixel values and to scan the whole image once, which significantly reduces the memory consumption and increases the speed.</p><p>Specifically, RDB following algorithm first initialize two variables l edge and r edge with null value, then scans the binary instance-aware segmentation map row by row from the top-left corner to the bottom-right corner to find contours (lines 1-3). If a pixel's value is 1 and its left pixel's value is 0, then this pixel is on the left side of a contour, so it is assigned to l edge ; similarly, if a pixel's value is 1 and its right pixel's value is 0, then this pixel is on the right side of a contour, so it is assigned to r edge (lines 4-9). When both l edge and r edge are found, we check if there exists a pair of l edge and r edge on above line whose x-coordinates are the same as or greater/smaller by 1 than the corresponding x-coordinates of l edge and r edge ; if so, we add l edge and r edge to the same contour set as l edge and r edge ; otherwise, we create a new contour set and add l edge and r edge to it (lines <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. if pixel(i, j) == 1 and pixel(i − 1, j) == 0 then 5:</p><formula xml:id="formula_0">l edge = P ixel(i, j) 6: end if 7:</formula><p>if pixel(i, j) == 1 and pixel(i + 1, j) == 0 then 8:</p><formula xml:id="formula_1">r edge = P ixel(i, j) 9:</formula><p>end if 10:</p><p>if r edge != null and l edge != null then <ref type="bibr">11:</ref> if there exists a pair of r edge and l edge in row j − 1 and 12:</p><p>x coordOf (r edge ) − x coordOf (r edge ) ≤ 1 and 13:</p><p>x coordOf (l edge ) − x coordOf (l edge ) ≤ 1 then 14:</p><p>Add l edge and r edge to the same contour set as r edge and l edge 15: To show the strength of our proposed WSMA-Seg approach in object detection, extensive experimental studies have been conducted on three benchmark datasets, namely, the Rebar Head 3 , WIDER Face 4 , and MS COCO datasets <ref type="bibr" target="#b4">5</ref> , each of which containing many extreme cases. The important parameters of WSMA-Seg are as follows: Stack is the number of the stacked hourglass networks (see <ref type="bibr" target="#b8">[9]</ref> for more details about hourglass), Base is a pre-defined basic number, and the number of channels is always an integer multiple of Base, and Depth is the number of down-samplings. Stem represents three consecutive 3 × 3 convolution operations with stride = 1 before the first stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rebar Head Detection</head><p>We first conduct experiments on the Rebar Head detection dataset, which consists of 250 training images (including a total of 30942 rebar heads) and 200 testing images. The orignal resolution of the whole image is 2000 × 2666. Performing object detection on this dataset is very challenging, because it only contains a few training samples and also encounters very severe occlusion situations (see <ref type="figure" target="#fig_6">Figure 7</ref>). In addition, the target rebar heads are very small: the average area of each box is 7, 000 pixels, taking up only 0.13% of the whole image. The images are also poorly annotated and rich in diverse illuminations.</p><p>Two state-of-the-art anchor-based models, Faster R-CNN <ref type="bibr" target="#b4">[5]</ref> and Cascade R-CNN <ref type="bibr" target="#b11">[12]</ref>, are selected as the baselines. <ref type="table" target="#tab_2">Table 1</ref> shows the detection performances of our proposed WSMA-Seg and baselines on this dataset. As shown in <ref type="table" target="#tab_2">Table 1</ref>, our proposed method with Stack = 2, Base = 40, Depth = 5 has achieved the best performance among all solutions in terms of F1 Score. In addition, the number of parameters needed for WSMA-Seg is much less than the baselines (only 1/7 of Cascade RCNN and 1/4 of Faster RCNN), while the number of training epochs for WSMA-Seg is also less than those of the baselines. Therefore, we can conclude that, compared to the state-of-the-art baselines, WSMA-Seg is much simpler, more effective, and more efficient.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">WIDER Face Detection</head><p>We further conduct experiments on the WIDER Face detection dataset <ref type="bibr" target="#b12">[13]</ref>, which consists of 32, 203 images and 393, 703 faces. Face detections in this dataset are extremely challenging due to a high degree of variability in scale, pose, and occlusion. WIDER Face results in a much lower detection accuracy compared to other face detection datasets. WIDER Face has defined three levels of difficulties (i.e., Easy, Medium, and Hard), based on the detection accuracies of EdgeBox <ref type="bibr" target="#b13">[14]</ref>. Furthermore, the dataset also treats occlusion as an additional attribute and is partitioned into three categories: no occlusion, partial occlusion, and heavy occlusion. Specifically, a face is categorized as partial occlusion when 1% to 30% of the total face area is occluded, and a face with the occluded area over 30% is categorized as heavy occlusion. The size of the training set is 12879, that of the validation set is 3226, and that of the testing set is 16098.</p><p>Twelve state-of-the-art approaches are selected as baselines, namely, Two-stage CNN, Cascade R-CNN, and LDCF+ <ref type="bibr" target="#b14">[15]</ref>, multitask Cascade CNN <ref type="bibr" target="#b15">[16]</ref>, ScaleFace <ref type="bibr" target="#b16">[17]</ref>, MSCNN <ref type="bibr" target="#b17">[18]</ref>, HR <ref type="bibr" target="#b18">[19]</ref>, Face R-CNN <ref type="bibr" target="#b19">[20]</ref>, Face Attention Networks <ref type="bibr" target="#b20">[21]</ref>, and PyramidBox <ref type="bibr" target="#b21">[22]</ref>. The experimental results in terms of F1 score are shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MS COCO Detection</head><p>Finally, we conduct experimental studies on the MS COCO detection dataset <ref type="bibr" target="#b6">[7]</ref>, which is one of the most popular large-scale detection datasets. Our results are obtained using the test-dev split (20k images) with a host of the detection method. We have constructed the training set with 82081 samples, the validation set with 40137 samples, and the testing set with 20288 samples. We use the metrics as used in <ref type="bibr" target="#b6">[7]</ref> to characterize the performance. Four types of metrics are defined and described as follows:</p><p>• Average Precision (AP):</p><p>-AP : AP at IoU=.50:.05:.95 (primary challenge metric) -AP .50 : AP at IoU=.50 (PASCAL VOC metric) -AP at IoU=.75 (strict metric)</p><p>• AP Across Scales:</p><p>-AP s : AP for small objects: area &lt; 32 2 -AP m : AP for medium objects: 32 2 &lt; area &lt; 96 2 -AP l : AP for large objects: area &gt; 96 2</p><p>• Average Recall (AR):</p><p>-AR 1 : AR given 1 detection per image -AR 10 : AR given 10 detections per image -AR 100 : AR given 100 detections per image</p><p>• AR Across Scales:</p><p>-AR s : AR for small objects: area &lt; 32 2 -AR m : AR for medium objects: 32 2 &lt; area &lt; 96 2 -AR l : AR for large objects: area&gt; 96 2</p><p>Seven state-of-the-art solutions are selected as baselines, and the experimental results for four types of metrics are shown in <ref type="table" target="#tab_4">Tables 2 and 3</ref>. The results show that our WSMA-Seg approach outperforms all state-of-the-art baselines in terms of most metrics, including the most challenging metrics, AP , AP s , AR 1 , and AR s . For the other metrics, the performance of our proposed approach is also close to those of the best baselines. This proves that the proposed WSMA-Seg approach generally achieves more accurate and robust object detection than the state-of-the-art approaches without NMS.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we have proposed a novel approach to object detection in images, called weakly supervised multimodal annotation segmentation (WSMA-Seg), which is anchor-free and NMS-free. We observed that NMS is one of the bottlenecks of existing deep learning approaches to object detection in images. The need to tune hyperparameters on NMS has seriously hindered the scalability of high-performance detection frameworks. Therefore, to realize WSMA-Seg, we proposed to use multimodal annotations to achieve an instance-aware segmentation based on weakly supervised bounding boxes, and developed a run-data-based following algorithm to trace contours of objects. In addition, a multi-scale pooling segmentation (MSP-Seg) model was proposed as the underlying segmentation model of WSMA-Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA-Seg. Experimental results on multiple datasets concluded that the proposed WSMA-Seg approach is superior to the state-of-the-art detectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Extreme cases of object detection in images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Training phase of WSMA-Seg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Testing phase of WSMA-Seg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Multi-scale pooling segmentation model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Multi-scale block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Contour tracing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>An example of complex occlusion in the Rebar Head dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>F1 scores on the WIDER Face dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Run-data-based following algorithm Input: A binary image with h (height) and s (width). Output: A list of contour sets.1: l edge = null, r edge = null 2: for j in [0 : h) do</figDesc><table><row><cell>3:</cell><cell>for i in [0 : s) do</cell></row><row><cell>4:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Detection performances of WSMA-Seg and baselines on the Rebar Head dataset.</figDesc><table><row><cell>Method</cell><cell cols="3">#parms Epoch F1 Score</cell></row><row><cell>Faster RCNN</cell><cell>23.2M</cell><cell>100</cell><cell>98.30%</cell></row><row><cell>Cascade RCNN [12]</cell><cell>42.1M</cell><cell>100</cell><cell>98.70%</cell></row><row><cell>WSMA-Seg(stack=1,base=72,depth=3)</cell><cell>6.1M</cell><cell>70</cell><cell>94.27%</cell></row><row><cell>WSMA-Seg(stack=2,base=40,depth=5)</cell><cell>5.8M</cell><cell>70</cell><cell>98.83%</cell></row><row><cell>WSMA-Seg(stack=4,base=28,depth=5)</cell><cell>5.7M</cell><cell>70</cell><cell>96.26%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 8 .</head><label>8</label><figDesc>The results show that our proposed WSMA-Seg outperforms the state-of-the-art baselines in all three categories, reaching 94.70, 93.41, and 87.23 in Easy, Medium, and Hard categories, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Average precisions of WSAM-Seg and baselines on MS COCO (test-dev)MethodBackbone AP AP 50 AP 75 AP s AP m AP l</figDesc><table><row><cell>DSOD300</cell><cell>DS/64-192-48-1</cell><cell>29.3</cell><cell>47.3</cell><cell>30.6</cell><cell>9.4</cell><cell>31.5</cell><cell>47</cell></row><row><cell>SSD513</cell><cell>ResNet-101</cell><cell>31.2</cell><cell>50.4</cell><cell>33.3</cell><cell>10.2</cell><cell>34.5</cell><cell>49.8</cell></row><row><cell>DSSD513</cell><cell>ResNet-101</cell><cell>33.2</cell><cell>53.3</cell><cell>35.2</cell><cell>13.0</cell><cell>35.4</cell><cell>51.1</cell></row><row><cell>DeNet</cell><cell>ResNet-101</cell><cell>33.8</cell><cell>53.4</cell><cell>36.1</cell><cell>12.3</cell><cell>36.1</cell><cell>50.8</cell></row><row><cell>CoupleNet</cell><cell>ResNet-101</cell><cell>34.4</cell><cell>54.8</cell><cell>37.2</cell><cell>13.4</cell><cell>38.1</cell><cell>50.8</cell></row><row><cell cols="3">Faster R-CNN w/ TDM Inception-ResNet-v2 36.8</cell><cell>57.7</cell><cell>39.2</cell><cell>16.2</cell><cell>39.8</cell><cell>52.1</cell></row><row><cell>CornerNet511</cell><cell>Hourglass-52</cell><cell>37.8</cell><cell>53.7</cell><cell>40.1</cell><cell>17.0</cell><cell>39.0</cell><cell>50.5</cell></row><row><cell>WSMA-Seg</cell><cell>MSP-Seg</cell><cell>38.1</cell><cell>58.2</cell><cell>40.7</cell><cell>22.5</cell><cell>41.0</cell><cell>51.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Average recalls of WSAM-Seg and baselines on MS COCO (test-dev) MethodBackbone AR 1 AR 10 AR 100 AR s AR m AR l</figDesc><table><row><cell>DSOD300</cell><cell>DS/64-192-48-1</cell><cell>27.3</cell><cell>40.7</cell><cell>43</cell><cell>16.7</cell><cell>47.1</cell><cell>65</cell></row><row><cell>SSD513</cell><cell>ResNet-101</cell><cell>28.3</cell><cell>42.1</cell><cell>44.4</cell><cell>17.6</cell><cell>49.2</cell><cell>65.8</cell></row><row><cell>DSSD513</cell><cell>ResNet-101</cell><cell>28.9</cell><cell>43.5</cell><cell>46.2</cell><cell>21.8</cell><cell>49.1</cell><cell>66.4</cell></row><row><cell>DeNet</cell><cell>ResNet-101</cell><cell>29.6</cell><cell>42.6</cell><cell>43.5</cell><cell>19.2</cell><cell>46.9</cell><cell>64.3</cell></row><row><cell>CoupleNet</cell><cell>ResNet-101</cell><cell>30.0</cell><cell>45.0</cell><cell>46.4</cell><cell>20.7</cell><cell>53.1</cell><cell>68.5</cell></row><row><cell cols="3">Faster R-CNN w/ TDM Inception-ResNet-v2 31.6</cell><cell>49.3</cell><cell>51.9</cell><cell>28.1</cell><cell>56.6</cell><cell>71.1</cell></row><row><cell>CornerNet511</cell><cell>Hourglass-52</cell><cell>33.9</cell><cell>52.3</cell><cell>57.0</cell><cell>35.0</cell><cell>59.3</cell><cell>74.7</cell></row><row><cell>WSMA-Seg</cell><cell>MSP-Seg</cell><cell>35.2</cell><cell>52.1</cell><cell>57.8</cell><cell>36.1</cell><cell>58.4</cell><cell>73.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.datafountain.cn/competitions/332/details 4 http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/ 5 http://cocodataset.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on Computer Vision</title>
		<meeting>the IEEE international conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning for visual understanding: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oerlemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="27" to="48" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topological structural analysis of digitized binary images by border following</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision, graphics, and image processing</title>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="32" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A sequential approach to the extraction of shape features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="538" to="557" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European conference on computer vision</title>
		<meeting>European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">To boost or not to boost? on the limits of boosted trees for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Pattern Recognition</title>
		<meeting>International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Face detection through scale-friendly deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02863</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European conference on computer vision</title>
		<meeting>European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Detecting faces using region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05256</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Face attention network: an effective face detector for the occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07246</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pyramidbox: A context-assisted single shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
