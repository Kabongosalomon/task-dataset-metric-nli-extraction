<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bottom-up human pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids. Equipped with multi-resolution supervision for training and multiresolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHR-Net outperforms the previous best bottom-up method by 2.5% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5% AP) without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all topdown methods on CrowdPose test (67.6% AP), suggesting its robustness in crowded scene. The code and models are available at https://github.com/HRNet/ Higher-HRNet-Human-Pose-Estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>2D human pose estimation aims at localizing human anatomical keypoints (e.g., elbow, wrist, etc.) or parts. As a fundamental technique to human behavior understanding, it has received increasing attention in recent years.</p><p>Current human pose estimation methods can be categorized into top-down methods and bottom-up methods. Topdown methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b15">16</ref>] take a dependency on person detector to detect person instances each with a bounding box and then reduce the problem to a sim-   <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b29">30]</ref>. (a) Generating higher resolution and spatially more accurate heatmaps by upsampling image. Recent work PersonLab <ref type="bibr" target="#b32">[33]</ref> relies on enlarging input image size to generate high quality feature maps. (c) Our HigherHRNet uses high resolution feature pyramid. pler task of single person pose estimation. As top-down methods can normalize all the persons to approximately the same scale by cropping and resizing the detected person bounding boxes, they are generally less sensitive to the scale variance of persons. Thus, state-of-the-art performances on various multi-person human pose estimation benchmarks are mostly achieved by top-down methods. However, as such methods rely on a separate person detector and need to estimate pose for every person individually, they are normally computationally intensive and not truly end-to-end systems. By contrast, bottom-up methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b21">22]</ref> start by localizing identity-free keypoints for all the persons in an input image through predicting heatmaps of different anatomical keypoints, followed by grouping them into person instances. This strategy effectively makes bottomup methods faster and more capable of achieving real-time pose estimation. However, because bottom-up methods need to deal with scale variation, there still exists a large gap between the performances of bottom-up and top-down methods, especially for small scale persons.</p><p>There are mainly two challenges in predicting keypoints of small persons. One is dealing with scale variation, i.e. to improve the performance of small person without sacrificing the performance of large persons. The other is generating a high-resolution heatmap with high quality for precise localizing keypoints of small persons. Previous bottom-up methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b21">22]</ref> mainly focus on grouping keypoints and simply use a single resolution of feature map that is 1/4 of the input image resolution to predict the heatmap of keypoints. These methods neglect the challenge of scale variation and rely on image pyramid during inference <ref type="figure" target="#fig_1">(Figure 1  (a)</ref>). Feature pyramids are basic components for handling scale variation, however, smaller resolution feature maps in a top-down feature pyramid usually suffer from the second challenge. PersonLab <ref type="bibr" target="#b32">[33]</ref> generates high-resolution heatmaps by increasing the input resolution ( <ref type="figure" target="#fig_1">Figure 1 (b)</ref>). Although the performance of small persons increases consistently as input resolution, the performance of large persons begin decreasing when input resolution is too large. To solve these challenges, it is crucial to generate spatially more accurate and scale-aware heatmaps for bottomup keypoint prediction in a natural and simple way without sacrificing computational cost.</p><p>In this paper, we propose a Scale-Aware High-Resolution Network (HigherHRNet) to address these challenges. HigherHRNet generates high-resolution heatmaps by a new high-resolution feature pyramid module. Unlike the traditional feature pyramid that starts from 1/32 resolution and uses bilinear upsampling with lateral connection to gradually increases feature map resolution to 1/4, highresolution feature pyramid directly starts from 1/4 resolution which is the highest resolution feature in the backbone and generates even higher-resolution feature maps with deconvolution ( <ref type="figure" target="#fig_1">Figure 1 (c)</ref>). We build the high-resolution feature pyramid on the 1/4 resolution path of HRNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref>, to make it efficient. To make HigherHRNet capable of handling scale variation, we further propose a Multi-Resolution Supervision strategy to assign training target of different resolutions to the corresponding feature pyramid level. Finally, we introduce a simple Multi-Resolution Heatmap Aggregation strategy during inference to generate scale-aware high-resolution heatmaps.</p><p>We validate our method on the challenging COCO keypoint detection dataset <ref type="bibr" target="#b26">[27]</ref> and demonstrate superior keypoint detection performance. Specifically, HigherHRNet achieves AP of 70.5% on COCO2017 test-dev without any post processing, outperforming all existing bottom-up methods by a large margin. Furthermore, we observe that most of the gain comes from medium person (there is no small person annotation for the keypoint detection task), HigherHRNet outperforms the previous best bottom-up method by 2.5% AP for medium persons without sacrafic-ing the performance of large persons (+0.3% AP). This observation verifies HigherHRNet is indeed solving the scale variation challenge. We also provide a solid baseline for bottom-up methods on the new CrowdPose <ref type="bibr" target="#b23">[24]</ref> dataset. Our HigherHRNet achieves AP of 67.6% on CrowdPose test, surpassing all existing methods. This result suggests bottom-up methods naturally have the advantages in the crowded scene.</p><p>To summarize our contributions:</p><p>• We attempt to address the scale variation challenge, which is rarely studied before in bottom-up multiperson pose estimation.</p><p>• We propose a HigherHRNet that generates highresolution feature pyramid with multi-resolution supervision in the training stage and multi-resolution heatmap aggregation in the inference stage to predict scale-aware high-resolution heatmaps that are beneficial for small persons.</p><p>• We demonstrate the effectiveness of our HigherHRNet on the challenging COCO dataset. Our model outperforms all other bottom-up methods. We especially observe a large gain for medium persons.</p><p>• We achieve a new state-of-the-art result on the Crowd-Pose dataset, suggesting bottom-up methods are more robust to the crowded scene over top-down methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Top-down methods. Top-down methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref> detect the keypoints of a single person within a person bounding box. The person bounding boxes are usually generated by an object detector <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref>. Mask R-CNN <ref type="bibr" target="#b15">[16]</ref> directly adds a keypoint detection branch on Faster R-CNN <ref type="bibr" target="#b35">[36]</ref> and reuses features after ROIPooling. G-RMI <ref type="bibr" target="#b33">[34]</ref> and the following methods further break topdown methods into two steps and use separate models for person detection and pose estimation.</p><p>Bottom-up methods. Bottom-up methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30]</ref> detect identity-free body joints for all the persons in an image and then group them into individuals. OpenPose <ref type="bibr" target="#b2">[3]</ref> uses a two-branch multi-stage netork with one branch for heatmap prediction and one branch for grouping. Open-Pose uses a grouping method named part affinity field which learns a 2D vector field linking two keypoints. Grouping is done by calculating line integral between two keypoints and group the pair with the largest integral. Newell et al. <ref type="bibr" target="#b29">[30]</ref> use stacked hourglass network <ref type="bibr" target="#b30">[31]</ref> for both heatmap prediction and grouping. Grouping is done by a method named associate embedding, which assigns each keypoint with a "tag" (a vector representation) and groups keypoints based on the l 2 distance between tag vectors. PersonLab <ref type="bibr" target="#b32">[33]</ref> uses dilated ResNet <ref type="bibr" target="#b16">[17]</ref> and groups keypoints by directly learning a 2D offset field for each pair of keypoints. PifPaf <ref type="bibr" target="#b21">[22]</ref> uses a Part Intensity Field (PIF) to localize body parts and a Part Association Field (PAF) to associate body parts with each other to form full human poses.</p><p>Feature pyramid. Pyramidal representation has been widely adopted in recent object detection and segmentation frameworks to handle scale variation. SSD <ref type="bibr" target="#b28">[29]</ref> and MS-CNN <ref type="bibr" target="#b1">[2]</ref> predict objects at multiple layers of the network without merging features. Feature pyramid networks <ref type="bibr" target="#b25">[26]</ref> extend the backbone model with a top-down pathway that gradually recovers feature resolution from 1/32 to 1/4, using bilinear upsampling and lateral connection. The motivation in common is to let features from different pyramid level to predict instances of different scales. However, this pyramidal representation is less explored in bottom-up multi-person pose estimation. In this work, we design a high-resolution feature pyramid that extend the pyramid to a different direction, starting from 1/4 resolution feature and generate pyramid of features with higher resolution.</p><p>High resolution feature maps. There are mainly 4 methods to generate high resolution feature maps. (1) Encoderdecoder <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b9">10]</ref> captures the context information in the encoder path and recover high resolution features in the decoder path. The decoder usually contains a sequence of bilinear upsample operations with skip connections from encoder features with the same resolution.</p><p>(2) Dilated convolution <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12</ref>] (a.k.a. "atrous" convolution) is used to remove several stride convolutions/max poolings to preserve feature map resolution. Dilated convolution prevents losing spatial information but introduces more computational cost. (3) Deconvolution (transposed convolution) <ref type="bibr" target="#b41">[42]</ref> is used in sequence at the end of a network to efficiently increase feature map resolution. SimpleBaseline <ref type="bibr" target="#b41">[42]</ref> demonstrates that deconvolution can generate high quality feature maps for heatmap prediction. (4) Recently, a High-Resolution Network (HRNet) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> is proposed as an efficient way to keep a high resolution pass throughout the network. HRNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> consists of multiple branches with different resolutions. Lower resolution branches capture contextual information and higher resolution branches preserve spatial information. With multiscale fusions between branches, HRNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> can generate high resolution feature maps with rich semantic. We adopt HRNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> as our base network to generate high-quality feature maps. And we add a deconvolution module to generate higher resolution feature maps to predict heatmaps. The resulting model is named "Scale-Aware High-Resolution Network" (HigherHRNet). As both HR-Net <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b39">40]</ref> and deconvolution are efficient, HigherHR-Net is an efficient model for generating higher resolution feature maps for heatmap prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Higher-Resolution Network</head><p>In this section, we introduce our proposed Scale-Aware High-Resolution Representation Learning using the High-erHRNet. <ref type="figure">Figure 2</ref> illustrates the overall architecture of our method. We will firstly give a brief overview on the proposed HigherHRNet and then describe its components in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">HigherHRNet</head><p>HRNet. HigherHRNet uses HRNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> (shown in <ref type="figure">Figure 2</ref>) as backbone. HRNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> starts with a highresolution branch in the first stage. In every following stage, a new branch is added to current branches in parallel with 1 2 of the lowest resolution in current branches. As the network has more stages, it will have more parallel branches with different resolutions and resolutions from previous stages are all preserved in later stages. An example network structure, containing 3 parallel branches, is illustrated in <ref type="figure">Figure 2</ref>.</p><p>We instantiate the backbone using a similar manner as HRNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref>. The network starts from a stem that consists of two strided 3 × 3 convolutions decreasing the resolution to 1/4. The 1st stage contains 4 residual units where each unit is formed by a bottleneck with width (number of channels) 64, followed by one 3 × 3 convolution reducing the width of feature maps to C. The 2nd, 3rd, 4th stages contain 1, 4, and 3 multi-resolution blocks, respectively. The widths of the convolutions of the four resolutions are C, 2C, 4C, and 8C, respectively. Each branch in the multiresolution group convolution has 4 residual units and each unit has two 3 × 3 convolutions in each resolution. We experiment with two networks with different capacity by setting C to 32 and 48 respectively.</p><p>HRNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> was originally designed for top-down pose estimation. In this work, we adopt HRNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> to a bottom-up method by adding a 1 × 1 convolution to predict heatmaps and tagmaps similar to <ref type="bibr" target="#b29">[30]</ref>. We only use the highest resolution ( 1 4 of the input image) feature maps for prediction. Following <ref type="bibr" target="#b29">[30]</ref>, we use a scalar tag for each keypoint.</p><p>HigherHRNet. Resolution of the heatmap is important for predicting keypoints for small persons. Most existing human pose estimation methods predict Gaussiansmoothed heatmaps by preparing the ground truth headmaps with an unnormalized Gaussian kernel applyed to each keypoint location. Adding this Gaussian kernel helps training networks as CNNs tend to output spatially smooth responses as a nature of convolution operations. However, applying a Gaussian kernel also introduces confusion in precise localization of keypoints, especially for keypoints belonging to small persons. A trivial solution to reduce this confusion is to reduce the standard deviation of the Gaus- <ref type="figure">Figure 2</ref>. An illustration of HigherHRNet. The network uses HRNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> as backbone, followed by one or more deconvolution modules to generate multi-resolution and high-resolution heatmaps. Multi-resolution supervision is used for training. More details are given in Section 3. sian kernel. However, we empirically find that it makes optimization harder and leads to even worse results.</p><p>Instead of reducing standard deviation, we solve this problem by predicting heatmaps at higher resolution with standard deviation unchanged at different resolutions. Bottom-up methods usually predict heatmaps at resolution <ref type="bibr">1 4</ref> of the input image. Yet we find this resolution is not high enough for predicting accurate heatmaps. Inspired by <ref type="bibr" target="#b41">[42]</ref>, which shows that deconvolution can be used to effectively generate high quality and high resolution feature maps, we build HigherHRNet on top of the highest resolution feature maps in HRNet as shown in <ref type="figure">Figure 2</ref> by adding a deconvolution module as discussed in Section 3.3.</p><p>The deconvolution module takes as input both features and predicted heatmaps from HRNet and generates new feature maps that are 2 times larger in resolution than the input feature maps. A feature pyramid with two resolutions is thus generated by the deconvolution module together with the feature maps from HRNet. The deconvolution module also predicts heatmaps by adding an extra 1 × 1 convolution. We follow Section 3.4 to train heatmap predictors at different resolutions and use a heatmap aggregation strategy as described in (Section 3.5) for inference.</p><p>More deconvolution modules can be added if larger resolution is desired. We find the number of deconvolution modules is dependent on the distribution of person scales of the dataset. Generally speaking, a dataset containing smaller persons requires larger resolution feature maps for predic-tion and vice versa. In experiments, we find adding a single deconvolution module achieves the best performance on the COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Grouping.</head><p>Recent works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23]</ref> have shown that grouping can be solved with high accuracy by a simple method using associative embedding <ref type="bibr" target="#b29">[30]</ref>. As an evidence, experimental results in <ref type="bibr" target="#b29">[30]</ref> show that using the ground truth detections with the predicted tags improves AP from 59.2 to 94.0 on a held-out set of 500 training images of the COCO keypoint detection dataset <ref type="bibr" target="#b26">[27]</ref>. We follow <ref type="bibr" target="#b29">[30]</ref> to use associative embedding for keypoint grouping. The grouping process clusters identity-free keypoints into individuals by grouping keypoints whose tags have small l 2 distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Deconvolution Module</head><p>We propose a simple deconvolution module for generating high quality feature maps whose resolution is two times higher than the input feature maps. Following <ref type="bibr" target="#b41">[42]</ref>, we use a 4 × 4 deconvolution (a.k.a. transposed convolution) followed by BatchNorm and ReLU to learn to upsample the input feature maps. Optionally, we could further add several Basic Residual Blocks <ref type="bibr" target="#b16">[17]</ref> after deconvolution to refine the upsampled feature maps. We add 4 Residual Blocks in HigherHRNet.</p><p>Different from <ref type="bibr" target="#b41">[42]</ref>, the input to our deconvolution module is the concatenation of the feature maps and the pre-dicted heatmaps from either HRNet or previous deconvolution modules. And the output feature maps of each deconvolution module are also used to predict heatmaps in a multi-scale fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multi-Resolution Supervision</head><p>Unlike other bottom-up methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3]</ref> that only apply supervision to the largest resolution heatmaps, we introduce a multi-resolution supervision during training to handle scale variation. We transform ground truth keypoint locations to locations on the heatmaps of all resolutions to generate ground truth heatmaps with different resolutions. Then we apply a Gaussian kernel with the same standard deviation (we use standard deviation = 2 by default) to all these ground truth heatmaps. We find it important not to scale standard deviation of the Gaussian kernel. This is because different resolution of feature pyramid is suitable to predict keypoints of different scales. On higher-resolution feature maps, a relatively small standard deviation (compared to the resolution of the feature map) is desired to more precisely localize keypoints of small persons.</p><p>At each prediction scale in HigherHRNet, we calculate the mean squared error between the predicted heatmaps of that scale and its associated ground truth heatmaps. The final loss for heatmaps is the sum of mean squared errors for all resolutions.</p><p>It is worth highlighting that we do not assign different scale of persons to different levels in the feature pyramid, due to the following reasons. First, the heuristic used for assigning training target depends on both the dataset and network architecture. It is hard to transform the heuristic for FPN <ref type="bibr" target="#b25">[26]</ref> to HigherHRNet as both the dataset (scale distribution of person v.s. all objects) and architecture (High-erHRNet only has 2 levels of pyramid while FPN has 4) change. Second, ground truth keypoint targets interact with each other since we apply the Gaussian kernel. Thus, it is very hard to decouple keypoints by simply setting ignored regions. We believe model has the ability to automatically focus on specific scales in different levels of the feature pyramid.</p><p>Tagmaps are trained differently from heatmaps in High-erHRNet. We only predict tagmaps at the lowest resolution, instead of using all resolutions. This is because learning tagmaps requires global reasoning and it is more suitable to predict tagmaps in lower resolution. Empirically, we also find higher resolutions do not learn to predict tagmaps well and even do not converge. Thus, we follow <ref type="bibr" target="#b29">[30]</ref> to train the tagmaps on feature maps at 1 4 resolution of input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Heatmap Aggregation for Inference</head><p>We propose a heatmap aggregation strategy during inference. We use bilinear interpolation to upsample all the predicted heatmaps with different resolutions to the reso-lution of the input image and average the heatmaps from all scales for final prediction. This strategy is quite different from previous methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> which only use heatmaps from a single scale or single stage for prediction.</p><p>The reason that we use heatmap aggregation is to enable scale-aware pose estimation. For example, the COCO Keypoint dataset <ref type="bibr" target="#b26">[27]</ref> contains persons of large scale variance from 32 2 pixels to more than 128 2 pixels. Top-down methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b41">42]</ref> solve this problem by normalizing person regions approximately into a single scale. However, bottom-up methods need to be aware of scales to detect keypoints from all scales. We find heatmaps from different scales in HigherHRNet captures keypoints with different scales better. For example, keypoints for small persons missed in lower-resolution heatmap can be recovered in the higher-resolution heatmap. Thus, averaging predicted heatmaps from different resolutions makes HigherHRNet a scale-aware pose estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">COCO Keypoint Detection</head><p>Dataset. The COCO dataset <ref type="bibr" target="#b26">[27]</ref> contains over 200, 000 images and 250, 000 person instances labeled with 17 keypoints. COCO is divided into train/val/test-dev sets with 57k, 5k and 20k images respectively. All the experiments in this paper are trained only on the train set. We report results on the val set for ablation studies and compare with other state-of-the-art methods on the test-dev set. Evaluation metric. The standard evaluation metric is based on Object Keypoint Similarity (OKS):</p><formula xml:id="formula_0">OKS = i exp(−d 2 i /2s 2 k 2 i )δ(vi&gt;0) i δ(vi&gt;0)</formula><p>. Here d i is the Euclidean distance between a detected keypoint and its corresponding ground truth, v i is the visibility flag of the ground truth, s is the object scale, and k i is a per-keypoint constant that controls falloff. We report standard average precision and recall scores 1 : AP 50 (AP at OKS = 0.50), AP 75 , AP (the mean of AP scores at OKS = 0.50, 0.55, . . . , 0.90, 0.95), AP M for medium objects, AP L for large objects, and AR (the mean of recalls at OKS = 0.50, 0.55, . . . , 0.90, 0.95). Training. Following <ref type="bibr" target="#b29">[30]</ref>, we use data augmentation with random rotation ([−30 • , 30 • ]), random scale ([0.75, 1.5]), random translation <ref type="bibr">([−40, 40]</ref>) to crop an input image patch of size 512 × 512 as well as random flip. As mentioned in Section 3.4, we generate two ground truth heatmaps with resolutions 128 × 128 and 256 × 256 respectively.</p><p>We use the Adam optimizer <ref type="bibr" target="#b20">[21]</ref>. The base learning rate is set to 1e−3, and dropped to 1e−4 and 1e−5 at the 200th and 260th epochs respectively. We train the model for a total of 300 epochs. To balance the heatmap loss and the grouping loss, we set the weight to 1 and 1e−3 respectively for the two losses.  <ref type="bibr" target="#b8">[9]</ref> 73.0 91.7 80.9 69.5 78.1 79.0 SimpleBaseline <ref type="bibr" target="#b41">[42]</ref> 73.7 91.9 81.1 70.3 80.0 79.0 HRNet-W48 <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> 75.5 92.5 83.3 71.9 81.5 80.5 HRNet-W48 + extra data <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref>   Testing. We first resize the short side of the input image to 512 and keep the aspect ratio. Heatmap aggregation is done by resizing all the predicted heatmaps to the size of input image and taking the average. Following <ref type="bibr" target="#b29">[30]</ref>, flip testing is used for all the experiments. All reported numbers have been obtained with single model without ensembling.</p><p>Results on COCO2017 test-dev.  <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>. <ref type="table" target="#tab_1">Table 2</ref> lists both bottom-up and top-down methods on the COCO2017 test-dev dataset. HigherHRNet further closes the performance gap between bottom-up and topdown methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Experiments</head><p>We perform a number of ablation experiments to analyze Scale-Aware High-Resolution Network (HigherHRNet) on the COCO2017 <ref type="bibr" target="#b26">[27]</ref> val dataset.</p><p>HRNet vs. HigherHRNet. We perform ablation study comparing HRNet and HigherHRNet. For HigherHRNet, deconvolution module without extra residual blocks is used, and heatmaps aggregation is used for inference. Results are shown in <ref type="table" target="#tab_2">Table 3</ref>. A simple bottom-up baseline by using HRNet with a feature stride of 4 achieves AP = 64.4. By adding one deconvolution module, our HigherHRNet with a feature stride of 2 outperforms HRNet by a large margin of +2.5 AP (achieving 66.9 AP). Furthermore, the main   improvement comes from medium persons, where AP M is improved from 57.1 for HRNet to 61.0 for HigherHRNet. These results show that HigherHRNet performs much better with small scales thanks to its higher resolution heatmaps. We also find the AP for large person pose does no drop. This is mainly because we also use smaller resolution heatmaps for prediction. It demonstrates that 1) making prediction at higher resolution is beneficial to bottom-up pose estimation and 2) scale-aware prediction is important.</p><p>If we add a sequence of two deconvolution modules after HRNet to generate feature maps that is of the same resolution as the input image, we observe that the performance decreases to 66.5 AP from 66.9 AP for adding only one deconvolution module. The improvement for medium person is marginal (+0.1 AP) but there is a large drop in the performance of large person (−0.8 AP). We hypothesize this is because the misalignment between feature map scale and object scales. Larger resolution feature maps (feature stride = 1) are good for detecting keypoints from even smaller persons but the small persons in COCO are not considered for pose estimation. Therefore, we only use one deconvolution module by default for the COCO dataset. But we would like to point out that the number of cascaded deconvolution modules should be dependent on datasets and we will validate this on more datasets in our future work.</p><p>HigherHRNet gain breakdown. To better understand the gain of the proposed components, we perform detailed ablation studies on each individual component. <ref type="figure" target="#fig_3">Figure 3</ref> il-lustrates all the architectures of our experiments. Results are shown in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Effect of deconvolution module. We perform ablation study on the effect of adding deconvolution module to generate higher resolution heatmaps. For a fair comparison, we only use the highest resolution feature maps to generate heatmaps for prediction ( <ref type="figure" target="#fig_3">Figure 3 (b)</ref>). HRNet <ref type="figure" target="#fig_3">(Figure 3 (a)</ref>) achieves a baseline of 64.4 AP. By adding one deconvolution module, the model achieves 66.0 AP which is 1.6 AP better than the baseline. This improvement is completely due to predicting on larger feature maps with higher quality. The result verifies our claim that it is important to predict on higher resolution feature maps for bottom-up pose estimation.</p><p>Effect of feature concatenation. We concatenate feature maps with predicted heatmaps from HRNet as input to the deconvolution module <ref type="figure" target="#fig_3">(Figure 3 (c)</ref>) and the performance is further improved to 66.3 AP. We also observe there is a large gain in medium persons while the performance for large persons decreases. Comparing method (a) and (c), the gain of predicting heatmaps at higher resolution mainly comes from medium persons (+3.7AP M ). Moreover, the drop in large persons (−1.6 AP) justifies our claim that different different resolutions of feature maps are sensitive to different scales of persons.</p><p>Effect of heatmap aggregation. We further use all resolutions of heatmaps following the heatmap aggregation strategy for inference ( <ref type="figure" target="#fig_3">Figure 3 (d)</ref>    <ref type="table" target="#tab_6">Table 5</ref>, all three models are tested using the training image size. We find that by increasing training image size to 640, there is a significant gain of 1.4 AP. Most of the gain comes from medium person while the performance of large person degrades slightly. When we further change the training image size to 768, the overall AP does not change anymore. We observe a marginal improvement in medium person along with large degradation in large person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Larger backbone.</head><p>In previous experiments, we use HRNet-W32 (1/4 resolution feature map has 32 channels) as backbone. We perform experiments with larger backbones HRNet-W40 and HRNet-W48. Results are shown in <ref type="table" target="#tab_7">Table 6</ref>. We find using larger backbone consistently improves performance for both medium and large person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">CrowdPose</head><p>The CrowdPose <ref type="bibr" target="#b23">[24]</ref> dataset consists of 20,000 images, containing about 80,000 persons. The training, validation and testing subset are split in proportional to 5:1:4. Crowd-Pose has more crowded scenes than the COCO keypoint dataset, posing more challenges to pose estimation methods. The evaluation metric is the same as COCO <ref type="bibr" target="#b26">[27]</ref>.</p><p>The strong assumption of top-down methods that each person detection only contains a single person in the center, is no more valid in crowded scene. As shown in <ref type="table" target="#tab_8">Table 7</ref>, top-down methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref> that perform well on COCO fail on the CrowdPose dataset.</p><p>On the other hand, bottom-up methods naturally have the advantage in crowded scene. To validate the robustness of HigherHRNet in crowded scene, as well as setting up a strong baseline for bottom-up methods. We train our best HigherHRNet-W48 model on the CrowdPose train and val set and report performance on the test set. All training parameters follow COCO exactly and we use a crop size of 640 × 640 for both training and testing.</p><p>Results are shown in <ref type="table" target="#tab_8">Table 7</ref>. Our HigherHRNet outperforms naïve top-down methods by a large margin of 6.6 AP. HigherHRNet also outperforms the previous best method <ref type="bibr" target="#b23">[24]</ref> (which performs a global refinement of topdown method <ref type="bibr" target="#b14">[15]</ref>) by a healthy margin of 1.6 AP and most of the gain comes from AP M (+1.8 AP) and AP H (+1.5 AP), which contains images with the most crowd. Even without multi-scale test, HigherHRNet outperforms SPPE <ref type="bibr" target="#b23">[24]</ref> by 0.5 in AP H .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a Scale-Aware High-Resolution Network (HigherHRNet) to solve the scale variation challenge in the bottom-up multi-person pose estimation problem, especially for precisely localizing keypoints of small persons. We find multi-scale image pyramid and larger input size partially solve the problem, but these methods suffer from high computational cost. To solve the problem, we propose an efficient high-resolution feature pyramid based on HR-Net and train it with multi-resolution supervision. During the inference, HigherHRNet with multi-resolution heatmap aggregation is capable of efficiently generating muilt-and higher-resolution heatmaps for more accurate human pose estimation. HigherHRNet outperforms all existing bottomup methods by a large margin on the challenging COCO dataset, especially for small persons.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>(a) Using image pyramid for heatmap prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>(a) Baseline method using HRNet<ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> as backbone. (b) HigherHRNet with multi-resolution supervision (MRS). (c) High-erHRNet with MRS and feature concatenation. (d) HigherHRNet with MRS and feature concatenation. (e) HigherHRNet with MRS, feature concatennation and extra residual blocks. For (d) and (e), heatmap aggregation is used. Network w/ MRS feature concat. w/ heatmap aggregation extra res. blocks AP AP M AP L (a) HRNet 64.4 57.1 75.6 (b) HigherHRNet 66.0 60.7 74.2 (c) HigherHRNet 66.3 60.8 74.0 (d) HigherHRNet 66.9 61.0 75.7 (e) HigherHRNet 67.1 61.5 76.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Our implementation, not reported in<ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> Comparisons with bottom-up methods on the COCO2017 test-dev set. All GFLOPs are calculated at single-scale. For Person-Lab<ref type="bibr" target="#b32">[33]</ref>, we only calculate its backbone's #Params and GFLOPs. Top: w/o multi-scale test. Bottom: w/ multi-scale test. It is worth noting that our results are achieved without refinement.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Input size</cell><cell>#Params</cell><cell>GFLOPs</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">w/o multi-scale test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OpenPose [3]  †</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.8</cell><cell>84.9</cell><cell>67.5</cell><cell>57.1</cell><cell>68.2</cell></row><row><cell>Hourglass [30]</cell><cell>Hourglass</cell><cell>512</cell><cell>277.8M</cell><cell>206.9</cell><cell>56.6</cell><cell>81.8</cell><cell>61.8</cell><cell>49.8</cell><cell>67.0</cell></row><row><cell>PersonLab [33]</cell><cell>ResNet-152</cell><cell>1401</cell><cell>68.7M</cell><cell>405.5</cell><cell>66.5</cell><cell>88.0</cell><cell>72.6</cell><cell>62.4</cell><cell>72.3</cell></row><row><cell>PifPaf [22]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>66.7</cell><cell>-</cell><cell>-</cell><cell>62.4</cell><cell>72.9</cell></row><row><cell>Bottom-up HRNet  ‡</cell><cell>HRNet-W32</cell><cell>512</cell><cell>28.5M</cell><cell>38.9</cell><cell>64.1</cell><cell>86.3</cell><cell>70.4</cell><cell>57.4</cell><cell>73.9</cell></row><row><cell>HigherHRNet (Ours)</cell><cell>HRNet-W32</cell><cell>512</cell><cell>28.6M</cell><cell>47.9</cell><cell>66.4</cell><cell>87.5</cell><cell>72.8</cell><cell>61.2</cell><cell>74.2</cell></row><row><cell>HigherHRNet (Ours)</cell><cell>HRNet-W48</cell><cell>640</cell><cell>63.8M</cell><cell>154.3</cell><cell>68.4</cell><cell>88.2</cell><cell>75.1</cell><cell>64.4</cell><cell>74.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">w/ multi-scale test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hourglass [30]</cell><cell>Hourglass</cell><cell>512</cell><cell>277.8M</cell><cell>206.9</cell><cell>63.0</cell><cell>85.7</cell><cell>68.9</cell><cell>58.0</cell><cell>70.4</cell></row><row><cell>Hourglass [30]  †</cell><cell>Hourglass</cell><cell>512</cell><cell>277.8M</cell><cell>206.9</cell><cell>65.5</cell><cell>86.8</cell><cell>72.3</cell><cell>60.6</cell><cell>72.6</cell></row><row><cell>PersonLab [33]</cell><cell>ResNet-152</cell><cell>1401</cell><cell>68.7M</cell><cell>405.5</cell><cell>68.7</cell><cell>89.0</cell><cell>75.4</cell><cell>64.1</cell><cell>75.5</cell></row><row><cell>HigherHRNet (Ours)</cell><cell>HRNet-W48</cell><cell>640</cell><cell>63.8M</cell><cell>154.3</cell><cell>70.5</cell><cell>89.3</cell><cell>77.2</cell><cell>66.6</cell><cell>75.8</cell></row><row><cell cols="2">Top-down methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask-RCNN [16]</cell><cell cols="3">63.1 87.3 68.7 57.8 71.4 -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>G-RMI [34]</cell><cell cols="3">64.9 85.5 71.3 62.3 70.0 69.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Integral Pose Regression [39] 67.8 88.2 74.8 63.9 74.0 -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>G-RMI + extra data [34]</cell><cell cols="3">68.5 87.1 75.5 65.8 73.3 73.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CPN [9]</cell><cell cols="3">72.1 91.4 80.0 68.7 77.2 78.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RMPE [15]</cell><cell cols="3">72.3 89.2 79.1 68.0 78.6 -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CFN [18]</cell><cell cols="3">72.6 86.1 69.7 78.3 64.1 -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CPN (ensemble)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>† Indicates using refinement.‡Method AP AP 50 AP 75 AP M AP L AR</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparisons with both top-down and bottom-up methods on COCO2017 test-dev dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="5">77.0 92.7 84.5 73.4 83.1 82.0</cell></row><row><cell></cell><cell cols="3">Bottom-up methods</cell><cell></cell><cell></cell></row><row><cell cols="2">OpenPose  *  [3]</cell><cell cols="5">61.8 84.9 67.5 57.1 68.2 66.5</cell></row><row><cell cols="2">Hourglass  * + [30]</cell><cell cols="5">65.5 86.8 72.3 60.6 72.6 70.2</cell></row><row><cell cols="2">PifPaf [22]</cell><cell>66.7</cell><cell>-</cell><cell>-</cell><cell>62.4 72.9</cell><cell>-</cell></row><row><cell cols="2">SPM [32]</cell><cell cols="4">66.9 88.5 72.9 62.6 73.1</cell><cell>-</cell></row><row><cell cols="2">PersonLab + [33]</cell><cell cols="5">68.7 89.0 75.4 64.1 75.5 75.4</cell></row><row><cell cols="2">Ours: HigherHRNet-W48 +</cell><cell cols="5">70.5 89.3 77.2 66.6 75.8 74.9</cell></row><row><cell>Method</cell><cell cols="5">Feat. stride/resolution AP AP M AP L</cell></row><row><cell>HRNet</cell><cell></cell><cell>4/128</cell><cell></cell><cell cols="2">64.4 57.1 75.6</cell></row><row><cell>HigherHRNet</cell><cell></cell><cell>2/256</cell><cell></cell><cell cols="2">66.9 61.0 75.7</cell></row><row><cell>HigherHRNet</cell><cell></cell><cell>1/512</cell><cell></cell><cell cols="2">66.5 61.1 74.9</cell></row></table><note>* means using refinement.+ means using multi-scale test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of HRNet vs. HigherHRNet on COCO2017 val dataset. Using one deconvolution module for HigherHRNet performs best on the COCO dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>summarizes the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of HigherHRNet's components on COCO2017 val dataset. MSR: multi-resolution supervision. feature concat.: feature concatenation. res. blocks: residual blocks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation study of HigherHRNet with different training image size on COCO2017 val dataset.</figDesc><table><row><cell cols="2">Backbone #Params GFLOPs AP AP M AP L</cell></row><row><cell>HRNet-W32 28.6</cell><cell>47.8 68.5 64.3 75.3</cell></row><row><cell>HRNet-W40 44.5</cell><cell>110.7 69.2 64.9 75.9</cell></row><row><cell>HRNet-W48 63.8</cell><cell>154.3 69.9 65.4 76.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Ablation study of HigherHRNet with different backbone on COCO2017 val dataset. Effect of extra residual blocks. We add 4 residual blocks in the deconvolution module and our best model achieves 67.1 AP. Adding residual blocks can further refine the feature maps and it increases AP for both medium and large persons equally. Training with larger image size. A natural question is can training with larger input size further improve performance? To answer this question, we train HigherHRNet with 640 × 640 and 768 × 768 and results are shown in</figDesc><table><row><cell>ure 3 (c) (66.3 AP) that only uses the highest resolu-</cell></row><row><cell>tion heatmaps for inference, applying heatmap aggrega-</cell></row><row><cell>tion strategy achieves 66.9 AP. Comparing method (d) and</cell></row><row><cell>(e), the gain of heatmap aggregation comes from large</cell></row><row><cell>person (+1.7 AP). And the performance of large person</cell></row><row><cell>is even marginally better than predicting at lower resolu-</cell></row><row><cell>tion (method (a)). It means that predicting heatmaps using</cell></row><row><cell>heatmap aggregation strategy is truly scale-aware.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>MethodAP AP 50 AP 75 AP E AP M AP H Top-down methods Mask-RCNN<ref type="bibr" target="#b15">[16]</ref> 57.2 83.5 60.3 69.4 57.9 45.8 AlphaPose [15] 61.0 81.3 66.0 71.2 61.4 51.1 Top-down with refinement SPPE [24] 66.0 84.2 71.5 75.5 66.3 57.4 Ours: HigherHRNet-W48 65.9 86.4 70.6 73.3 66.5 57.9 Ours: HigherHRNet-W48 + 67.6 87.4 72.6 75.8 68.1 58.9 Comparisons with both top-down and bottom-up methods on CrowdPose test dataset. Superscripts E, M, H of AP stand for easy, medium and hard. + means using multi-scale test.</figDesc><table><row><cell></cell><cell cols="2">Bottom-up methods</cell><cell></cell><cell></cell></row><row><cell>OpenPose [3]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>62.7 48.7 32.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://cocodataset.org/#keypoints-eval</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rogerio S Feris, and Nuno Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spgnet: Semantic prediction guidance for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04751</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Panoptic-deeplab. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Decoupled classification refinement: Hard false positive suppression for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04002</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revisiting rcnn: On awakening the classification power of faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A coarsefine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with local joint-to-person associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a part-based geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Liang Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Chris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The devil is in the decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05093</idno>
		<title level="m">Deeperlab: Single-shot image parser</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
