<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">(a) Depth sensor and RGB color image (RGB-D) (b) LiDAR Sensor (c) RGB 360 o camera Fig. 1. Example of sensor modalities used in LboroHAR Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>MÃ¶ncks</surname></persName>
							<email>mirco.moencks@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">The Broadcast Centre</orgName>
								<orgName type="institution" key="instit1">are with Institute for Digital Technologies</orgName>
								<orgName type="institution" key="instit2">Loughborough University London</orgName>
								<address>
									<addrLine>3 Lesney Avenue, Here East, Queen Elizabeth Olympic Park</addrLine>
									<postCode>E15 2GZ</postCode>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Roche</surname></persName>
							<email>a.j.roche@lboro.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">The Broadcast Centre</orgName>
								<orgName type="institution" key="instit1">are with Institute for Digital Technologies</orgName>
								<orgName type="institution" key="instit2">Loughborough University London</orgName>
								<address>
									<addrLine>3 Lesney Avenue, Here East, Queen Elizabeth Olympic Park</addrLine>
									<postCode>E15 2GZ</postCode>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varuna</forename><surname>De Silva</surname></persName>
							<email>v.d.de-silva@lboro.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">The Broadcast Centre</orgName>
								<orgName type="institution" key="instit1">are with Institute for Digital Technologies</orgName>
								<orgName type="institution" key="instit2">Loughborough University London</orgName>
								<address>
									<addrLine>3 Lesney Avenue, Here East, Queen Elizabeth Olympic Park</addrLine>
									<postCode>E15 2GZ</postCode>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Kondoz</surname></persName>
							<email>a.kondoz@lboro.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">The Broadcast Centre</orgName>
								<orgName type="institution" key="instit1">are with Institute for Digital Technologies</orgName>
								<orgName type="institution" key="instit2">Loughborough University London</orgName>
								<address>
									<addrLine>3 Lesney Avenue, Here East, Queen Elizabeth Olympic Park</addrLine>
									<postCode>E15 2GZ</postCode>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">(a) Depth sensor and RGB color image (RGB-D) (b) LiDAR Sensor (c) RGB 360 o camera Fig. 1. Example of sensor modalities used in LboroHAR Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Varuna De Silva is the corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Human Activity Recognition</term>
					<term>HAR</term>
					<term>Machine Learning</term>
					<term>Autonomous Driving</term>
					<term>Pose Estimation</term>
					<term>Indoor Mobility</term>
					<term>LboroHAR dataset</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human Activity Recognition (HAR) is a key building block of many emerging applications such as intelligent mobility, ambient-assisted living and human-robot interaction. With robust HAR, systems will become more human-aware, leading towards much safer and empathetic autonomous systems. While human pose detection has made significant progress with the dawn of deep convolutional neural networks (CNNs), the state-of-the-art research has almost exclusively focused on a single sensing modality, especially video. However, in safety critical applications it is imperative to utilize multiple sensor modalities for robust operation. To exploit the benefits of state-of-the-art machine learning techniques for HAR, it is extremely important to have multimodal datasets. In this paper, we present a novel, multimodal sensor dataset that encompasses nine indoor activities, performed by 16 participants, and captured by four types of sensors that are commonly used in indoor applications and autonomous vehicles. This multimodal dataset is the first of its kind to be made openly available and can be exploited for many applications that require HAR, including sports analytics, healthcare assistance and indoor intelligent mobility. We propose a novel data preprocessing algorithm to enable context dependent feature extraction from the dataset to be utilized by different machine learning algorithms. Through rigorous experimental evaluations, this paper reviews the performance of machine learning approaches to posture recognition, and analyses the robustness of the algorithms. When performing HAR with the RGB-Depth data from our new dataset, machine learning algorithms such as a deep neural network reached a mean accuracy of up to 96.8% for classification across all stationary and dynamic activities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Human Activity Recognition (HAR) is a key building block of many emerging applications such as intelligent mobility, ambient-assisted living and human-robot interaction. With robust HAR, systems will become more human-aware, leading towards much safer and empathetic autonomous systems. While human pose detection has made significant progress with the dawn of deep convolutional neural networks (CNNs), the state-of-the-art research has almost exclusively focused on a single sensing modality, especially video. However, in safety critical applications it is imperative to utilize multiple sensor modalities for robust operation. To exploit the benefits of state-of-the-art machine learning techniques for HAR, it is extremely important to have multimodal datasets. In this paper, we present a novel, multimodal sensor dataset that encompasses nine indoor activities, performed by 16 participants, and captured by four types of sensors that are commonly used in indoor applications and autonomous vehicles. This multimodal dataset is the first of its kind to be made openly available and can be exploited for many applications that require HAR, including sports analytics, healthcare assistance and indoor intelligent mobility. We propose a novel data preprocessing algorithm to enable context dependent feature extraction from the dataset to be utilized by different machine learning algorithms. Through rigorous experimental evaluations, this paper reviews the performance of machine learning approaches to posture recognition, and analyses the robustness of the algorithms. When performing HAR with the RGB-Depth data from our new dataset, machine learning algorithms such as a deep neural network reached a mean accuracy of up to 96.8% for classification across all stationary and dynamic activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recent developments within the fields of artificial intelligence (AI), machine learning (ML), advances in cyber-physical systems, and human-machine interaction allow technology to become more human-centric. This eventually enables machines to support people in their daily lives and increase the effectiveness of human work force by supporting harmful or physically demanding tasks. By allowing machines to intervene in human decision-making processes, it must be ensured that they effectively respond to human influence and interaction. Towards this end, machines need to analyze sensor data streams to detect, recognize, and precisely predict human activities. This branch of applied machine learning is known as Human Activity Recognition (HAR). HAR is an important element of many emerging applications such as personal assistant robots, retail analytics, sports analytics, smart homes and in connected autonomous vehicles (CAVs) <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b6">[7]</ref>. For example, HAR enabled systems can be applied to check the rate of degeneration of neurological disorders <ref type="bibr" target="#b7">[8]</ref>. In certain applications like in CAVs, HAR acts as a safety critical block, where, incorrect or inadequate pedestrian detection could lead to fatal accidents, thus indicating the ongoing need for further improvement to HAR <ref type="bibr" target="#b8">[9]</ref>.</p><p>The majority of the research in the HAR is originated from Adaptive Feature Processing for Robust Human Activity Recognition on a Novel Multi-Modal Dataset Mirco MÃ¶ncks, Varuna De Silva, Jamie Roche, and Ahmet Kondoz computer vision literature on analyzing videos/images, and since the recent developments in Internet of Things other sensors such as body worn accelerometers has been used for HAR. The computer vision based HAR systems can be effectively seen as two components: body pose estimation and the temporal classification of body pose changes <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>. As such, body pose estimation, which predicts the human body joints on a video frame, acts as a feature extraction step. Body pose estimation has massively benefitted from recent advances in convolutional neural networks (CNNs) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>. It is often mentioned that effective feature extraction is the major challenge to robust HAR <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Recent work, therefore, strives to increase the information density of respective feature vectors without increasing its dimensionality <ref type="bibr" target="#b13">[14]</ref>. Due to the curse of dimensionality <ref type="bibr" target="#b14">[15]</ref>, and increased computation efforts, exploiting all extracted or available features often does not result in higher classification accuracy. The method of static / inelastic feature selection is particularly well suited to classifications that differ explicitly. In the static feature selection method, the subset of features for a feature vector is predefined manually (or automatically by other ML algorithms) and not altered during the classification task. For example, the classification of the activities "walking" and "sitting" can be realized by selecting the velocity of participants' center of mass. However, the accurate classification of activities that differ less explicitly requires case related consideration of various features. For example, considering the velocity of hip movement in sitting activities is of minor interest. On the other hand, for the classification of dynamic activities such as walking, jogging and sprinting, it is of particular interest to include the relative velocity of the joints of the lower body into a feature vector. Consequently, the importance of dynamic or elastic feature selection for enhanced classification accuracy is a major interest in previous work <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>.</p><p>The recent fatal accident involving a driverless vehicle and a pedestrian accentuates the fact that a single sensor stream cannot be relied upon for HAR. Therefore, utilizing multiple sensor streams that offer sensing diversity, i.e. multimodality, is crucial for the success of HAR activities in safety critical applications such as CAVs. Therefore, multimodal data analysis is an important avenue that warrants further research towards the success of HAR. Availability of appropriate multimodal datasets is crucial for this purpose. For mobility solutions in outdoor environments, previous work mostly relied on multi-and omni-directional cameras and laser sensors <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref>. Research related to indoor HAR often exploits RGB-D sensors, stereo video camera or wearable sensors <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>. However, none of the multimodal datasets reviewed encompass sensor data that are commonly used for both indoor and outdoor HAR. Compared with outdoor environments such as driverless cars, there is a lack of rigorous research experimentally testing multimodal HAR algorithms for indoor mobility applications (IMA). This is critical because humans tend to have a different behavior and attention spans in indoor environments <ref type="bibr" target="#b25">[26]</ref>.</p><p>In order to enable robust HAR in applications that work both indoors and outdoors, this paper presents a novel algorithm for elastic (dynamic) feature processing method and new multimodal dataset for use by the research community. The contributions of this paper are as follows: ï· After a thorough investigation of the limitations of existing work, we present a novel multi-modal sensor dataset for indoor HAR of AV (Different sensor modalities used are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>) and ï· We propose a novel data pre-processing and flexible feature recognition technique to improve classifier performance of state-of-the-art machine learning techniques. This paper is organized as follows: section 2 provides a literature review on the applicability of HAR in its various contexts and scrutinizes both the underlying sensors and algorithms. Section 3 describes our methodology, followed by the presentation of our results in section 4. Whilst concluding the paper in section 5, we state that under dynamic conditions the accuracy of the classification approaches is inadequate. For future work, we suggest fusing RGB-D and LiDAR data to overcome major limitations of both sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we analyze the application range of HAR. We also discuss state-of-the-art datasets for HAR with a focus on intelligent mobility applications. We provide evidence that there is no sensor dataset that can sufficiently address the gap of HAR for Intelligent Mobility Applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Healthcare and Ambient-Assisted Living</head><p>HAR is often used in ambient-assisted living environments, hospitals or rehabilitation clinics; especially for elderly care.</p><p>Here it is of interest to identify scenarios which require urgent medical assistance, thus differentiating daily-life activities from abnormal or potentially harmful behavior <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>. Since falling implies one of the biggest risks to the health of elderly people <ref type="bibr" target="#b29">[30]</ref>, this activity is addressed by many previous works <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref>. Other research also strives to recognize vomiting, chest pain, and fainting <ref type="bibr" target="#b1">[2]</ref>. Work that is more related to prophylactic healthcare analyses people's movements and calculates calories burnt during the day <ref type="bibr" target="#b33">[34]</ref>. Moreover, HAR is commonly used as a diagnostic tool to check the rate of degeneration of neurological disorders <ref type="bibr" target="#b7">[8]</ref>. Recent work is aiming towards more proactive approaches in ambient-assistant living where a system responds to the anticipated intention of people, rather than the actual activity itself <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Smart Mobility and Autonomous Vehicles</head><p>Efforts which are more related to vehicles strive to decrease road accidents by recognizing human traffic participants, establishing a communication system between vehicles and smart devices <ref type="bibr" target="#b35">[36]</ref>, or reporting accidents in a timely manner <ref type="bibr" target="#b36">[37]</ref>. In this context, <ref type="bibr" target="#b37">[38]</ref> or <ref type="bibr" target="#b38">[39]</ref> provide a promising approach for pedestrian intention recognition: a state-of-the-art sensor system, which matches the predicted intention of a pedestrian with the current direction of a driver's attention, and initiates an emergency brake when needed. This research is related to scenarios with vehicles driven by humans in outdoor environments. Among others, a robust model for autonomous vehicles predicting pedestrian's intention in indoor environments remains open <ref type="bibr" target="#b39">[40]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Human Activity Recognition Datasets</head><p>Derived from notable surveys addressing 3D data for HAR <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b42">[43]</ref>, and recent work related to camera analysis and wearable technologies <ref type="bibr" target="#b43">[44]</ref>. <ref type="table" target="#tab_0">Table I</ref> provides an overview about the datasets that could be considered useful for addressing our research gap. It can be seen that there are three different classes of datasets in the literature: (a) stereo images captured by video cameras (RGB), (b) three-dimensional (3D) images captured by depth sensors (RGB-D), and (c) data captured by wearable sensors (e.g. acceleration of specific body joints). In terms of subjects, samples and classes, the NTU RGB-D dataset <ref type="bibr" target="#b44">[45]</ref> represents one of the most comprehensive datasets for HAR. The activities performed in <ref type="bibr" target="#b35">[36]</ref> or <ref type="bibr" target="#b45">[46]</ref> deviate significantly from our focus. The MPII Human Pose dataset can be considered a state of the art benchmark for the evaluation of articulated 2D human pose estimation <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. For navigation tasks, however, depth information as well as sequential movements and related velocities of body joints are of necessity. Consequently, this dataset is not suitable for our research objective. With the data captured, the dataset in <ref type="bibr" target="#b46">[47]</ref> points into the right direction of our research. Due to the use of outdated sensors, this dataset would not be longer adequate for our research intention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Datasets for CAV Research suitable for HAR</head><p>In the context of autonomous driving, we identified five considerable datasets using various sensors. The CamVid Database was one of the first experimentally collected video dataset with object class semantic labels for the purpose of visual object analysis <ref type="bibr" target="#b19">[20]</ref>. Captured from the perspective of a driving vehicle and semantically labelled, these images address the need for experimental data to quantitatively evaluate emerging algorithms. The work in <ref type="bibr" target="#b18">[19]</ref> presents a comprehensive dataset collected with a multimodal sensor ensemble attached to an autonomous ground vehicle testbed. We also identified work which is more related to real urban street scenarios, suited for the appearance-based recognition methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b55">[56]</ref>. The combination of the sensors is ideal for navigation, localization or mapping, but at the same time limits the implementation of HAR considerably. A high number of pedestrians and the respective 3D-depth information can be found in <ref type="bibr" target="#b21">[22]</ref>. The autonomous driving datasets available are primarily addressing the challenges of (a) urban scene under-standing, (b) state-of-the-art computer vision, (c) simultaneous localization and mapping for AV. They mostly rely on sensors such as lidar and video camera. The gathered data is optimized to detect objects and people in traffic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Motivations for the proposed work</head><p>It is often mentioned that effective feature extraction is a major challenge to HAR <ref type="bibr" target="#b11">[12]</ref>. Most recent work proposes to tackle this issue by increasing the information density of respective feature vectors without increasing its dimensionality. While most literature has focused on elastic (dynamic) feature selection methods, we propose a static feature selection method as a feature processing technique.</p><p>Our review illustrates that the research in most activity classes captured were derived from the outdoor datasets reviewed. Research related to indoor HAR often applies RGB-D sensors, stereo video camera or wearable sensors. For outdoor environments, most HAR systems rely on multi-and omni-directional cameras and laser sensors. The available datasets do not encompass sensors that are commonly used for both indoor and outdoor HAR. In other words, there is a research gap in HAR for AV considering hybrid (indooroutdoor) scenarios. This gap is critical in so far as ambientassistant living assistants need to be able to support people in every situation of everyday life. Such an assistant must be able to navigate the way in outdoor environments and both indoor environments. We could not identify any previous work that considers this aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED ALGORITHM AND THE DATASET</head><p>To overcome the limitations summarized in the previous section, we created a dataset with 16 participants, performing 9 activities. The activities were captured by a LiDAR, RGB- D, and 360â¢-camera. Furthermore, we propose a novel algorithm allowing flexible feature extraction and data utilization which improves the performance of multiple machine learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Collection</head><p>Our LboroHAR dataset contains data of typical human activities in indoor environments, captured by three sensors <ref type="table" target="#tab_0">(Table II)</ref>. The 9 activities performed by 16 participants were chosen on the basis of related work <ref type="figure" target="#fig_1">(Figure 2</ref>). To enable a comparison of the LboroHAR dataset with other datasets, most activity classes captured were derived from the datasets reviewed ( <ref type="figure" target="#fig_0">Figure 1, Figure 2</ref>). We focused on those indoor activities where humans have a limited attention span, resulting in scenarios where it is highly likely that humans do not pay attention to an indoor AV. In addition to our research, this approach facilitates the later derivation of driving policies for indoor AV in critical scenarios.</p><p>For further analysis, we solely considered the most distinguishing postures of each participant for each activity. When the coordinates of respective body joints were of interest, we selected 51 frames per person and activity (50 fames for velocity, and 49 frames for acceleration respectively). Our publicly made LbroHAR dataset, however, contains the whole sequence of activities. This allows a wider usability of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DAEFE -Data Pre-processing Algorithm for Elastic Feature Extraction</head><p>We propose an elastic feature extraction method to overcome shortcomings of previous algorithms that solely allow inelastic feature extraction for the utilization of one particular machine learning algorithm <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>. Our algorithm encompasses three main steps. They are represented in <ref type="figure" target="#fig_2">Figure 3</ref>, and are discussed in the following: 1) Parameter of Interest: The algorithm allows the selection of specific body joints of interests, and their describing parameters of interest (coordinates, velocity, or acceleration). We intentionally refrain from automatic feature extraction with CNN to enable the addressing of potential research interest that are related to specific body joints, or regions of the human body. 2) Feature extraction: The coordinates (velocities or accelerations) of the skeleton joints are used to create the respective feature vectors which represent human postures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Activity Features Computation: A matrix containing all</head><p>feature vectors that represent the activities is created and used for classification. For supervised ML algorithms, this matrix is extended by one row labelling the activity. Instead of using all frames captured, we solely consider a subset of P frames that depict significant poses per action and participant, to increase generalizability and decrease complexity:</p><formula xml:id="formula_0">â 51, 50, 49 .<label>( 1 )</label></formula><p>Then, the features of interest need to be extracted from skeleton data representing the input to our ML algorithms. This can be done by an in-built ready to use joint extraction algorithm included in the RGB-D libraries <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b58">[59]</ref>. The selection of these features is motivated by the fact that it is applied in related work, thus allowing us to evaluate the performance of the algorithms in comparison to previous work reviewed. Moreover, the joint extraction algorithm allows us a compact representation of the human body; extracted body joints can then be used as features <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. A straight-forward approach for the feature extraction can be seen in considering locations of body joints, distances, velocity or acceleration, while relying on spatial information. Previous work, however, typically relies on more complex features that are based on the estimation of a plane that intersects with some joints <ref type="bibr" target="#b56">[57]</ref>. The features can then be extracted by measuring the distances between plane and body joints. Following <ref type="bibr" target="#b57">[58]</ref>, our algorithm also utilizes spatial features computed from 3D skeleton data <ref type="figure" target="#fig_4">(Figure 4</ref>). The neglection of the respective time stamp eliminates temporal dependency. In order to evaluate how HAR can be performed by considering the speed of movement, we integrate velocity and acceleration of body joints. Each joint is represented by a 3D vector in the cartesian coordinate space of the RGB-D sensor . Specific activities were performed in certain regions of the test area. For instance, participants always stand on the same place while texting (class 2). It is, therefore, necessary to make the activity independent of the region in which it was carried out. To compensate this effect, we normalized the data by replacing the absolute coordinate system with a relative coordinate system centered on one body joint. Following <ref type="bibr" target="#b48">[49]</ref>, we consider a skeleton of n body joints, being the coordinates of the head joint, being the coordinates of the neck joint, the i-th joint feature is the distance vector between and : </p><p>These features are invariant to the position of the participant within the coverage area of the RGB-D sensor. They may be seen as a set of distance vectors which connect each joint to the joint of the torso. A posture feature vector can then be created for each frame captured:</p><formula xml:id="formula_2">, , â¦ , .<label>(3)</label></formula><p>Penultimately, we aggregate each feature vector F of activity a and participant p to a combined input matrix M which is exploited by the algorithm:</p><formula xml:id="formula_3">, .<label>(4)</label></formula><p>For the purpose of testing unsupervised learning algorithms, we lastly created a classification vector C, that encompasses the label of the activity described by the respective feature vectors:</p><p>, , â¦ , , â 1, â¦ , 9 (5) * , .</p><p>The matrix M (and * for supervised learning procedures respectively) represents the data collected in an organized, recognisable form that can now be processed by ML algorithms. This work follows the common approach and splits the dataset into three disjointed entities for the purpose of training (60%), testing (20%), and validation (20%). After training a respective model, the validation dataset is used as a proxy for generalization error underlying. S-fold cross-validation (S = 5) was applied, in order to provide a sufficient number of data points to refine the models <ref type="bibr" target="#b61">[62]</ref>.</p><p>The selection of six algorithms is based on initial pre-tests <ref type="table" target="#tab_0">(Table III)</ref>. We evaluated the performance of 22 ML algorithms for their applicability and continued our experiments only with those which performed best against similar algorithmic types: fine decision tree, bagged trees, fine k-nearest neighbor classifier, cubic support vector machine, and deep neural networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND DISCUSSIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup 1) Sensor Test Bed</head><p>To draw conclusions from our experimental research regarding generalizability and applicability for HAR of AV in indoor environments later on, we endeavored to create the    <ref type="bibr" target="#b62">[63]</ref>, which is shown in <ref type="figure" target="#fig_3">Figure 5</ref>. The LboroHAR dataset encompasses (raw and pre-processed) data from three sensors attached to this vehicle: (a) IR RGB-D Sensor (Sensor 3), (b) VLP-16 LiDAR (Sensor 4), and (c) 360Â° Richo RGB-Camera (Sensor 5). These sensors, or the sensor types, are commonly used in the automotive industry for the purpose of assisted or autonomous driving <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Environmental Setup</head><p>To evaluate the general applicability of ML algorithms for certain purposes -or to compare the effectiveness of various sensors accordingly -it is critical to guarantee an experimental setup that allows optimal parameter input for subsequent tests <ref type="bibr" target="#b63">[64]</ref>. In other words, we need to provide conditions where each sensor reaches its optimal performance level by reducing the impact of sensor-specific limitations, such as light conditions or minimum operative distance, to a negligible amount. This premise eventually led to our experimental setup depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>We have laid the foundation for fully understanding our methodology and enabling future work to reproduce our experiment. Therefore, we now present our experimental results that answer our research intention. When similar patterns occur in each algorithm, we exemplify these findings and results by referring to one specific ML approach, rather than individually mentioning all algorithms considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Applicable Feature Modalities</head><p>We tested which of the extracted feature modalities, namely relative coordinates c , velocity v and acceleration a of body joints, are applicable for the purpose of HAR. As presumed, selected algorithms perform best when using c as input parameter <ref type="table" target="#tab_0">(Table IV)</ref>. The accuracy of correctly predicted classes significantly drops when using v , or a accordingly. A reason for this can be found in our initial considerations and the algorithmic classes used. We are striving to classify human activities based on time-invariant spatial posture recognition. Although the amount of existing data of our feature vector remains constant, the information density decreases continuously. This is mainly due to the physical relationship of the measured quantities. The use of c allows pattern recognition in different postures. However, especially with static postures, v and a of individual body joints do not provide any information about the underlying activity. This may lead to a first conclusion that coordinates are the only suitable input parameters that should be used for reliable classification. On a closer look, however, there are some issues with such a statement. Velocity and acceleration tend to be insufficient indicators to classify all types of human activity. However, classification of dynamic activities (5-9) is comparatively accurate under these conditions <ref type="figure">(Figure 8</ref>). The overall objective of this research field is to derive driving policies for AV and enable safe human-robot interaction. Therefore, it is rather necessary to estimate future activities after classifying human beings and its current activity. For example, determining the future location of a pedestrian crossing a street requires consideration of its velocity. Most of the related work that addresses HAR with visual sensors relies  on a feature space of coordinates. To ensure comparability we therefore proceed with c as applicable input variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Efficient Feature Space</head><p>In the context of lower computation costs, and higher generalizability, previous work often accentuates the importance of effective feature extraction <ref type="bibr" target="#b64">[65]</ref>- <ref type="bibr" target="#b66">[67]</ref>. Within the next step of our research, we investigate how different input feature vectors influence the algorithms' accuracy. Following our method described in <ref type="figure" target="#fig_2">Figure 3</ref>, we tested how the respective accuracy of selected algorithms is affected when considering a subspace C containing i relative body joint coordinates, where i 9, 18, 28 . Moreover, we explored further possibilities for dimensionality reduction. In doing so, we compared the accuracy for a three-dimensional posture recognition (c , , c , , c , ) to a two-dimensional posture (c , , c , . After this, we repeated this procedure while integrating principal component analysis (PCA). PCA (explained variance Ï 95.00%) allows us to transform features and remove dimensions.</p><p>Without previous PCA, algorithms such as DNN and fine k-NN reach an accuracy of up to 96.50% and 94.40% respectively. This might indicate that these algorithms are suitable for HAR of AV. With further integration of PCA, however, accuracy of all algorithms considered dropped significantly. In general, a decrease of the accuracy of a model might not be a desirable outcome. Due to neglection of outliers in PCA, however, the models tend to be less overfitting to the LboroHAR dataset. Following this, these values give a more reliable estimation of the algorithmic performance when applied in realistic scenarios <ref type="table" target="#tab_5">(Table V)</ref>. On the other hand, outliers might be of non-neglectable importance for HAR in real-life applications. This is because an outlier can represent a person that has a different way of performing an activity than the majority of people considered (e.g. due to a disability, disease or drug influence). Our results show that an increase of the feature space does not lead to significant increase of accuracy. This observation is coherent with the curse of dimensionality discussed above.</p><p>The consideration of a 2-dimensional body posture representation leads to a significantly lower accuracy. Following this, the depth data of an RGB-D sensor is adding value for the classification procedure. We conclude that an efficient feature space can therefore be seen in (c | c , , c , , c , or (c | c , , c , , c , . Within this feature space and with the integration of PCA, fine k-NN (77.20%) and bagged trees (79.70%) provided the highest accuracy for classifying HAR. The confusion matrix and receiver operating curve (ROC) can be seen in <ref type="figure">Figure 7</ref> and <ref type="figure">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stationary and Dynamic Activities</head><p>Our simulations indicate that there are significant differences in the classification of stationary (class 1-4) and dynamic (class 5-9) human activities. Regardless of the model applied, the classification of stationary classes reaches a comparatively higher accuracy (k-NN classifier, <ref type="figure">Figure 7; DNN Figure 8</ref>). For example, the confusion matrix in <ref type="figure">Figure 8</ref> depicts that walking (class 5) and running (class 9) are often prone to misclassification, when feature vector is based on coordinates. In particular, for these two classes, the accuracy increases when using a velocity-based input vector <ref type="figure">(Figure 8</ref>). Due to the relative coordinate system, other distinctive characteristics (e.g. the so called "flight phase" in which none of the legs of the runner has contact with the ground) are of negligible importance.</p><p>While running (class 9) mostly got mistaken for walking (class 5), walking mostly got mistaken for walking while texting (class 6) and transition, or an object (class 7,8) ( <ref type="figure">Figure  8</ref>). In all activities, the participant's arms are located at different distances and positions in front of their center of gravity. A distinction should therefore be possible, especially since the objects differ significantly in size and appearance. One reason for this misclassification can be seen in the selected feature space: no objects are included in the feature vector. We conclude that it might not be possible to realize a robust HAR without integrating the context of an activity into the feature space.</p><p>The classification of dynamic activities would potentially benefit from the integration of velocity into the feature vector. However, given the curse of dimensionality, it would be useful to integrate only the speed of major body joints instead of simply adding together both feature spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>After presenting, describing, analyzing, and synthesizing our data, we will now discuss the generalizability and limitations of this work, and give an outlook for further improvements needed to realize the objective of AV in ambient-assisted living environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Derivations from the Results</head><p>The findings of this work indicate that it is possible to realize HAR for autonomous vehicles in indoor environments using RGB-D sensors and cognitive approaches. Initial results show promising accuracy (&gt; 95.0%) for classifying human activities, in particular for stationary activities. However, considering the PCA, we found a significant performance decrease (âª 75.0%). It is expected that the accuracy under real conditions will deteriorate even more. For example, more realistic scenarios would include more noise into the input data, thus decreasing the probability of correct classification. The potential applications that build on our models interact in close physical contact with humans. Consequently, the accuracy of the individual classification models must be considered inadequate. A reliable statement of which algorithm is most suitable for our research goal can therefore be made exclusively by further experimental investigations of our models under real conditions. Thus, the present work should not be construed as an attempt to provide a fully functional solution. This research project is rather to be understood as a proof of concept that formed the foundation for which further research teams can utilize autonomous vehicles for the purposes of ambient-assistant living (e.g. smart wheel chairs).</p><p>Instead of relying exclusively on two-dimensional camera data, it seems useful to include the depth information for classifying activities. The relative coordinates of body postures provide a robust basis for classifying (stationary) activities. The inclusion of information about the speed of execution of an activity, however, may further improve the performance of the algorithms. Furthermore, our results suggest that only the use of a multimodal system of sensors and classification algorithms, which reduce individual inadequacies, can sufficiently realize HAR. This outcome coincides with ongoing efforts in computer vision as well as in activity recognition for AV in outdoor environments <ref type="bibr" target="#b20">[21]</ref>. Although our experimental framework allows for exact reproducibility, it limits the generalizability of our research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Limitations of the current work 1) Limitations of RGB-D Sensor</head><p>In addition to the applicability of different ML algorithms, we were interested in how the RGB-D sensor is suitable for our research purpose. For ambient assistant home applications, an AV (e.g. smart wheel chair) will most likely have its operative range between a human's feet and hips. Therefore, it is of particular importance that a human's lower body can be reliably recognized to prevent accidents in indoor environments. During our experiment, however, we found that black fabrics, especially sweatpants, limit the correct recognition of human silhouettes. We therefore conclude that the RGB-D sensor alone cannot meet the requirements of a robust HAR system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Stationary Conditions</head><p>During our data collection, the sensor testbed was in a stationary state. This was due to two reasons. In order to ensure comparability of different sensors, the activities needed to be carried out exclusively in the optimal operating range of all available sensors. The RGB-D used was developed for gaming applications and it is therefore to be assumed that its performance is better if the sensor is stationary during data collection. On the other hand, we were interested in the general sensor-algorithm applicability, rather than their respective robustness against external influences and noise such as vibrations of the vehicle. Our findings are therefore limited to scenarios in which an AV classifies human activities from a stationary perspective. This includes, for example, checking the environment before driving off.</p><p>The tracking of 16 participants started from a standardized position (T-position). This setup results in two further limitations. Although we included more than 7000 postures in our models, conclusions about the accuracy of the algorithm for a larger quantity are critical due to the relatively small number of participants. For example, one could address this issue of a small dataset by creating an artificial dataset containing data that is added with gaussian noise. With this small number of participants, the individual characteristics of the execution of an activity have a higher weight, resulting in the risk for a biased model. The approach of an artificial dataset would further amplify this inherent distortion. Additionally, people do not take a standard position before performing a new activity. Consequently, an algorithm operating under real conditions must be able to track a person without a prior "T-position". Also, our approach is not intended to classify activities of persons who are in a group.</p><p>The classification was realized in retrospect with a predefined feature space. We have not investigated how the trained models perform in real-time scenarios, yet. For example, the delay of the classification could be significantly different. Moreover, our findings regarding an efficient feature space suggest that relative coordinates should be combined with other information (e.g. velocity). Because distinguishing factors may differ for individual activities, previous work often recommended a dynamic feature extraction instead of a pre-defined setting <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b67">[68]</ref>. Although the improvement of feature extraction was not a focus of this work, a dynamic feature extraction might have had an impact on an algorithm's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Spatial Time Invariant Data Representation</head><p>An activity is often defined as a type of human behavior in relation to his/her environment <ref type="bibr" target="#b3">[4]</ref>. Following this, a robust HAR system needs to include a contextual evaluation of the activity and its relation to the environment. The input data for the ML algorithms, however, is represented as a number of frames containing data of body postures. This implies that classification of a human activity is not possible, when the distinguishing factor of an activity is the interaction with the environment. For example, our approach cannot accurately distinguish class 6 (human texting while walking) from class 7 (carrying objects) because the underlying feature vector does not contain any information about the smart phone or the carried object. Therefore, the models developed in this work are only suitable for activities that can be sufficiently characterized by the posture alone (e.g. sitting and standing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generalizability of the ML algorithms</head><p>A central challenge in ML is how to ensure an algorithm will perform well not just on the training data but also on new inputs <ref type="bibr" target="#b68">[69]</ref>. However, a defining characteristic of an ML classifier is that the mathematical model which predicts the class of a new input value is developed internally <ref type="bibr" target="#b69">[70]</ref>. In general, these models are incomprehensible to a human supervisor. As a result, we can only approximately quantify how the selected algorithms will perform in reality. Nonetheless, it is possible to make statements about the basic behavior of the underlying models.</p><p>As the chosen SVM is using a cubic kernel function for classification, this approach generally implies a comparatively higher risk of overfitting. Moreover, SVM represents decisions rather than posterior probabilities <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b70">[71]</ref>. In the context of dynamic feature extraction, an SVM might also be problematic, since adjustment in the underlying dimension requires a retraining of the model. A possible alternative might be seen in the relevance vector machine (RVM). This classification (or regression) technique is based on a Bayesian sparse kernel that shares many of the characteristics of an SVM, whilst avoiding its limitations discussed above <ref type="bibr" target="#b61">[62]</ref>. RVM provides posterior probabilistic outputs and has much sparser solutions than the SVM. A major disadvantage of RVM is the relatively long training time.</p><p>Decision trees are also prone to overfitting. This especially applies to tree classifiers with a certain depth. To limit overfitting without substantially increasing errors caused by bias or high variance, it might have been beneficial to consider random forests instead of fine decision trees, and bagged trees. This method describes a set of multiple decision tress whose results are aggregated into one final result.</p><p>Overall, it can be stated that due to the characteristics of the ML algorithms, no explicit conclusion about generalizability can be drawn, unless the models were experimentally tested in other scenarios. This directly points towards suggestions for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Future Work</head><p>This work explores how ML algorithms can enable HAR for AV in indoor environments. In order to derive reliable driving policies for AV in indoor environments, however, it is necessary to conduct further work. We introduce six major aspects that would mitigate the limitations of this study and improve the algorithmic models, thus further narrowing down the existing gaps in the research field of interest: (a) algorithm-sensor performance, (b) sensor fusion, (c) enhanced feature extraction, (c) dynamic data capture, (d) real-time applicability and prediction of intentions, (e) human-object recognition.</p><p>Here we only consider input data captured by an RGB-D sensor. It would be of great interest to see how the performance of the selected algorithms changes depending on different sensor inputs. Thus, one objective is to find out which interaction of sensor and algorithm provides the best performance for HAR. Since the activities were recorded by six sensors, the LboroHAR dataset provides a basis for first strides towards an optimal match of sensors and algorithms.</p><p>After identifying the best algorithm-sensor match, a logical step would be to fuse multiple data streams for a robust multimodal HAR system. The sensor fusion implied herein could help mitigate the observed weaknesses of individual sensors.</p><p>In section 3 and 4 we also emphasized the importance of efficient feature extraction. Following the approach of <ref type="bibr" target="#b71">[72]</ref>, we decided to classify the activities using single body postures. However, it might be beneficial to the classification accuracy if HAR is performed based on a sequence of frames n &gt; 1. For example, one could think of classifying a contiguous sequence of postures to eventually predict the class of the "parent activity". Another possibility would be to integrate multiple postures (represented by body joints only) into a single Vector F feature. Here it would be of the utmost importance that the dimensionality of F does not increase significantly (curse of dimensionality). However, both approaches require a larger amount of data because the available training and test samples decrease at least by the factor . Regardless of the chosen method, we consider it important to further optimize feature extraction. In <ref type="bibr" target="#b13">[14]</ref> a recent work addresses this topic. Here, various features are aggregated into a feature vector of high dimension. However, the crucial vector optimization that counteracts the curse of dimensionality is still missing.</p><p>Input data captured by a moving sensor testbed will most certainly affect the accuracy as well, since it adds additional noise. Thus, a future step in the research agenda might be seen in a repetition of the experiment with a dynamic and realistic framework.</p><p>As discussed, a major limitation of our approach is the focus on body joints. For more complex activities, the environment and the object that is used or altered during an activity needs to be considered as well. The objective of enabling the recognition of humans with objects can also be realized with our dataset, the visual perception of objects of interest is given by the camera, while depth data is provided by point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>The availability of appropriate datasets is a crucial element of any data science activity. Human activity recognition is not any different to this. While most of the HAR activity has focused on a single sensing modality, this work discussed the utility of multimodality for robustness of HAR. Furthermore, a specific limitation of multi-modal datasets for indoor environments was identified and in response we focused on indoor HAR for applications such as autonomous vehicles, and healthcare assistant robots. We comprehensively the existing datasets and identified the experimental setup needed to capture the open dataset proposed in this paper. To make the dataset useful to as many researchers as possible, who work on different branches of machine learning algorithms (i.e. other than deep networks), we developed a novel datapreprocessing technique for dynamic feature extraction from the body pose detections. This algorithm allows for a flexible, pre-determined selection of body joints. We analyzed a selection of 6 different ML algorithms on RGB-D sensor modalities: decision trees, linear discriminant, cubic SVM, fine k-NN, bagged tree, and DNN. To justify the proposed preprocessing algorithm we assess how various input parameter influence the performance of the selected ML algorithms. Our investigations indicate that activities can be successfully recognized by the pre-processing method proceeded by state-of-the-art ML algorithms. Most accurate results, for RGB-D modality, can be reached when using the three-dimensional body joints as representation of postures with relative coordinates as input feature (positive likelihood ratio with DNN: 99%). While enhancing our feature extraction we observe the curse of dimensionality, the principle of overfitting and the bias-variance trade-off. While HAR can be realized by relying only on RGB-D data, we argue that only the use of a multimodal system of sensors and classification algorithms can sufficiently realize HAR for safety critical systems such as autonomous vehicles to amplify individual human capabilities in ambient-assistant living. It is expected that the proposed dataset and algorithms for pre-processing will advance the application of multimodal machine learning techniques for human activity recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Depth sensor and RGB color image (RGB-D) (b) LiDAR Sensor (c) RGB 360 o camera Example of sensor modalities used in LboroHAR Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>LboroHAR dataset: Activities. Panel (a): activity 1 -Sitting on an office chair (camera and IR sensor perspective with evacuated background). Panel (b): activity 2 -Standing and texting with smart phone: retro fitted 3D representation of participant. Panel (c): activity 3 -Sitting on a stool: camera and IR sensor perspective with evacuated background. Panel (d): activity 4 -laying on a couch: retro fitted 3D representation of participant. Panel (e): activity 5: Walking -camera and IR sensor perspective with evacuated background. Panel (f): activity 6 -Walking while texting: camera and IR sensor perspective with evacuated background. Panel (g): activity 7 -Carrying Boxes: camera and IR sensor perspective with evacuated background. Panel (h): activity 8 -Moving Object: camera and IR sensor perspective with evacuated background. Panel (i): activity 9 -Running: camera and IR sensor perspective with evacuated background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>DAEFE -Data Pre-processing Algorithm for Adaptive (Elastic) Feature Extraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Loughborough University London Autonomous Driving Sensor Test Bed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Subsets of joints considered in the evaluation of the selected ML algorithms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Aggregated Confusion Matrix of k-NN Classifier Confusion Matrix of Deep Neural Network Classifier</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>OF HAR DATASETS</cell></row><row><cell>Reference</cell><cell>Classes</cell><cell cols="2">Subjects</cell><cell>Sensors</cell></row><row><cell>[47]</cell><cell>20</cell><cell>10</cell><cell cols="2">RGB-D</cell></row><row><cell>[48]</cell><cell>27</cell><cell>8</cell><cell cols="2">RGB-D (and wearable sensors)</cell></row><row><cell>[49]</cell><cell>30</cell><cell>10</cell><cell cols="2">RGB-D</cell></row><row><cell>[45]</cell><cell>60</cell><cell>40</cell><cell cols="2">RGB-D</cell></row><row><cell>[50]</cell><cell>10</cell><cell>10</cell><cell cols="2">Wearable RGB-D</cell></row><row><cell>[51]</cell><cell>n. a.</cell><cell>n. a.</cell><cell cols="2">Accelerometer</cell></row><row><cell>[52]</cell><cell>10</cell><cell>11</cell><cell cols="2">Wearable (Google Glasses)</cell></row><row><cell>[53]</cell><cell>12</cell><cell>49</cell><cell cols="2">Wearable stereo video camera</cell></row><row><cell>[54]</cell><cell>65</cell><cell>n. a.</cell><cell cols="2">Stereo video camera</cell></row><row><cell>[55]</cell><cell>52</cell><cell>10</cell><cell cols="2">Stereo video camera</cell></row><row><cell>[46]</cell><cell>n.a.</cell><cell>12</cell><cell cols="2">Stereo video camera</cell></row><row><cell>[11]</cell><cell>473</cell><cell>40k</cell><cell cols="2">Stereo video camera (2D Images)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II LBOROHAR</head><label>II</label><figDesc>DATASET: OVERVIEW.</figDesc><table><row><cell>Name</cell><cell>Description</cell></row><row><cell>Title</cell><cell>LboroLdnHAR</cell></row><row><cell>Summary</cell><cell>human activities captured by three sensors</cell></row><row><cell>Date of Collection</cell><cell>17/06/2018; 18/06/2018</cell></row><row><cell>Location</cell><cell>Loughborough University London -London, United Kingdom</cell></row><row><cell>Size</cell><cell>16 participants x 9 activities x 3 Sensors (RGB-D; Lidar; 360Â° Camera)</cell></row><row><cell>Activity Classes</cell><cell>{1 -sitting on office chair; 2 -standing and texting; 3 -sitting on stool; 4 -lying on couch; 5 -walking; 6 -walking and texting; 7 -carrying objects; 8 -pulling object; 9 -running}</cell></row><row><cell></cell><cell>360Â° Camera Stream (each file contains a scenario, ca. 2</cell></row><row><cell>Data Streams</cell><cell>min); Lidar Stream (each file contains a scenario, ca. 2 min);</cell></row><row><cell></cell><cell>RGB-D Stream (captured by RGB-D sensor);</cell></row><row><cell>Extracted Parameters for further Consideration</cell><cell>coordinates (51 frames of respective activity); velocity (50 frames of respective activity); acceleration (49 frames of respective activity)</cell></row><row><cell>Extracted</cell><cell></cell></row><row><cell>Body Joints</cell><cell></cell></row><row><cell>for further</cell><cell></cell></row><row><cell>Consideration</cell><cell></cell></row></table><note>Head; Neck; Chest; MiddleSpine; LowerSpine; Hip; Center_of_mass; Center_of_mass_projection_to_ground; LHand; REye; EffectorHead; RClavicle; RShoulder; RForearm; RHand; LClavicle; LShoulder; LForehand; LHand; RThigh; RShin; RFoot; RToe; EffectorRToe; LThigh; LShin; LFoot; LToe; EffectorToe Orientation Cartesian Coordinate System</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III LBOROHAR</head><label>III</label><figDesc>DATASET: OVERVIEW.</figDesc><table><row><cell>Type</cell><cell>Name</cell><cell>Accuracy</cell></row><row><cell>Decision Tree</cell><cell>Fine Tree</cell><cell>79.20%</cell></row><row><cell>Decision Tree</cell><cell>Medium Tree</cell><cell>59.90%</cell></row><row><cell>Decision Tree</cell><cell>Coarse Tree</cell><cell>45.40%</cell></row><row><cell cols="3">Discriminant Analysis Quadratic Discriminant Failed</cell></row><row><cell cols="2">Discriminant Analysis Linear Discriminant</cell><cell>80.00%</cell></row><row><cell>Ensemble</cell><cell>Bagged Trees</cell><cell>92.50%</cell></row><row><cell>Ensemble</cell><cell>Subspace k-NN</cell><cell>89.50%</cell></row><row><cell>Ensemble</cell><cell cols="2">Subspace Discriminant 78.60%</cell></row><row><cell>Ensemble</cell><cell>Boosted Trees</cell><cell>63.70%</cell></row><row><cell>Ensemble</cell><cell>EUSBoosted Trees</cell><cell>59.90%</cell></row><row><cell>Neural Network</cell><cell>Deep Neural Net</cell><cell>95.00%</cell></row><row><cell>k-NN</cell><cell>Fine k-NN</cell><cell>94.40%</cell></row><row><cell>k-NN</cell><cell>Weighted k-NN</cell><cell>91.80%</cell></row><row><cell>k-NN</cell><cell>Cosine k-NN</cell><cell>87.40%</cell></row><row><cell>k-NN</cell><cell>Cubic k-NN</cell><cell>86.20%</cell></row><row><cell>k-NN</cell><cell>Medium k-NN</cell><cell>85.40%</cell></row><row><cell>SVM</cell><cell>Cubic SVM</cell><cell>97.90%</cell></row><row><cell>SVM</cell><cell>Quadratic SVM</cell><cell>92.80%</cell></row><row><cell>SVM</cell><cell>Fine Gaussian SVM</cell><cell>91.60%</cell></row><row><cell>SVM</cell><cell>Linear SVM</cell><cell>81.90%</cell></row><row><cell>SVM</cell><cell cols="2">Medium Gaussian SVM 79.40%</cell></row><row><cell>SVM</cell><cell cols="2">Coarse Gaussian SVM 65.10%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV TEST</head><label>IV</label><figDesc>RESULTS OF APPLICABLE FEATURES FOR HAR</figDesc><table><row><cell></cell><cell>Overall</cell><cell>Overall</cell><cell>Overall</cell></row><row><cell>Classifier</cell><cell>Accuracy ( )</cell><cell>Accuracy ( )</cell><cell>Accuracy ( )</cell></row><row><cell>Decision Tree</cell><cell>79.20%</cell><cell>34.20%</cell><cell>17.00%</cell></row><row><cell>Lin. Discriminant.</cell><cell>80.00%</cell><cell>29.40%</cell><cell>12.90%</cell></row><row><cell>Cubic SVM</cell><cell>91.60%</cell><cell>44.00%</cell><cell>24.40%</cell></row><row><cell>Fine k-NN</cell><cell>94.40%</cell><cell>45.50%</cell><cell>21.60%</cell></row><row><cell>Bagged Tree</cell><cell>92.50%</cell><cell>46.30%</cell><cell>22.50%</cell></row><row><cell>DNN</cell><cell>95.10%</cell><cell>29.00%</cell><cell>10.80%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V RELATIONSHIP</head><label>V</label><figDesc>OF FEATURES AND CLASSIFICATION ACCURACY</figDesc><table><row><cell>Classifier</cell><cell>PCA enabled (Y/N)</cell><cell>Accuracy (c | c , , c , , c , )</cell><cell>Accuracy (c | c , , c , )</cell><cell>Accuracy (c | c , , c , , c , )</cell><cell>Accuracy (c | c , , c ,</cell><cell>Accuracy (c | c , , c , , c , )</cell><cell>Accuracy (c | c , , c , )</cell></row><row><cell>Decision Tree</cell><cell>N Y</cell><cell>79.2% 68.9%</cell><cell>66.3% 48.1%</cell><cell>79.6% 69.2%</cell><cell>75.2% 48.9%</cell><cell>79.2% 69.4%</cell><cell>66.1% 48.7%</cell></row><row><cell>Lin. Discr.</cell><cell>N Y</cell><cell>80.0% 51.8%</cell><cell>71.9% 32.0%</cell><cell>80.2% 54.9%</cell><cell>76.2% 31.9%</cell><cell>80.0% 55.1%</cell><cell>71.9% 32.0%</cell></row><row><cell>Cubic SVM</cell><cell>N Y</cell><cell>91.6% 57.0%</cell><cell>83.5% 15.9%</cell><cell>97.6% 51.9%</cell><cell>91.4% 10.5%</cell><cell>97.2% 53.6%</cell><cell>78.8% 13.8%</cell></row><row><cell>Fine k-NN</cell><cell>N Y</cell><cell>94.4% 77.2%</cell><cell>79.8% 39.4%</cell><cell>93.4% 76.1%</cell><cell>87.6% 37.9%</cell><cell>92.7% 75.1%</cell><cell>74.7% 38.8%</cell></row><row><cell>Bagged Tree</cell><cell>N Y</cell><cell>92.5% 76.7%</cell><cell>75.7% 39.7%</cell><cell>91.7% 76.1%</cell><cell>87.6% 38.9%</cell><cell>91.7% 74.6%</cell><cell>73.8% 39.3%</cell></row><row><cell>DNN (w=175)</cell><cell>N</cell><cell>95.1%</cell><cell>84.4%</cell><cell>96.5%</cell><cell>90.6%</cell><cell>90.7%</cell><cell>87.1%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mirco MÃ¶ncks is currently reading for an MSc in Industrial Engineering and Business Management at Karlsruhe Institute of Technology. Mirco holds an MSc in Internet Technologies with Business Management from Loughborough University London. He received awards from Loughborough University London for academic excellence, and for the most impactful postgraduate dissertation project of the year 2018. Before that, he was working as a bachelor's thesis student and intern at the production system of AUDI AG. In this role Mirco was improving the companies' logistics processes, and solving problems occurring at the assembly lines. In 2017 Mirco graduated from Ilmenau University of Technology with a BSc in Industrial Engineering and Management with a focus on mechanical engineering. His current research interests are in human activity recognition for ambient-assistant living, and wearable technologies for development of manufacturing skills in human-centered production systems.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human Activity Recognition for Physical Rehabilitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leightley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Darby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moi Hoon</forename><surname>Mcphee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Syst. Man, Cybern</title>
		<imprint>
			<biblScope unit="page" from="261" to="266" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abnormal human activity recognition system based on R-transform and independent component features for elderly healthcare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chinese Inst. Eng. Trans. Chinese Inst. Eng. A/Chung-kuo K. Ch&apos;eng Hsuch K&apos;an</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="441" to="451" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic human activity recognition: A literature review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ziaeefard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bergevin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2329" to="2345" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human activity recognition: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">C</forename><surname>Ann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Theng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="page" from="28" to="30" />
			<date type="published" when="2014-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Suspicious human activity recognition: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="57" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Comprehensive Review on Handcrafted and Learning-Based Action Representation Approaches for Human Activity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sargano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Habib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">110</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human Activity Recognition in a Car with Embedded Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burbano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Carrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lat. Am. J. Comput. -LAJC</title>
		<imprint>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="33" to="39" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The use of laboratory gait analysis for understanding gait deterioration in people with multiple sclerosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>CofrÃ© Lizama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Galea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mult. Scler. J</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1768" to="1776" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-driving Uber kills Arizona woman in first fatal crash involving pedestrian | Technology | The Guardian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Guardian</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fine-grained activity recognition with holistic and pose based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel framework of continuous human-activity recognition using Kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Dogra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">311</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Dover Publications</publisher>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic classifier selection: Recent advances and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M O</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D C</forename><surname>Cavalcanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="195" to="216" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Features and models for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>GonzÃ¡lez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sedano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Corchado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ã</forename><surname>Herrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baruque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="52" to="60" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature reduction and selection for EMG signal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Phinyomark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phukpattaranont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Limsakul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="7420" to="7431" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ford Campus vision and lidar data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Eustice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Rob. Res</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1543" to="1552" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Rob. Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cvpr</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature extraction for robust physical activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>San-Segundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Human-centric Comput</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust human activity recognition from depth video using spatiotemporal multi-fused features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="295" to="308" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A novel sequence representation for unsupervised analysis of human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Isbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1221" to="1244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">School-based experiential outdoor education: A neglected necessity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Educ</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="71" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Increasing trend of wearables and multimodal interface for human activity monitoring: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Syal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biosensors and Bioelectronics</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="298" to="307" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combining emerging patterns with random forest for complex activity recognition in smart homes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tabatabaee</forename><surname>Malazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="315" to="330" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Review on Human Activity Recognition Using Vision-Based Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Healthcare Engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Falls and fallrelated injuries among the elderly: A survey of residential-care facilities in a Swedish municipality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Laflamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Community Health</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="140" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distinguishing fall activities from normal activities by velocity characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomech</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1497" to="1500" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An intelligent emergency response system: Preliminary development and testing of automated fall detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihailidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Telemed. Telecare</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="194" to="198" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust video surveillance for fall detection based on human shape deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rougier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>St-Arnaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Smartphone Applications for Patients&apos; Health and Fitness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am J Med</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From Activity Recognition to Intention Recognition for Assisted Living Within Smart Homes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Nugent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Human-Machine Syst</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="368" to="379" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">WiFiHonk: Smartphone-based beacon stuffed WiFi Car2X-communication system for vulnerable road user safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dhondge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vehicular Technology Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">IBump: Smartphone application to detect car accidents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aloul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zualkernan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abu-Salma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Al-Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Merri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Head detection and orientation estimation for pedestrian safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rehder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kloeden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 17th IEEE International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2292" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Driver Intention Algorithm for Pedestrian Protection and Automated Emergency Braking Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Diederichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schuttke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Spath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC</title>
		<imprint>
			<publisher>Octob</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1049" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Comparison and evaluation of pedestrian motion models for vehicle safety systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kloeden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2207" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">RGB-D datasets using microsoft kinect or similar sensors: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4313" to="4355" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Survey of Applications and Human Motion Recognition with Microsoft Kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Recognit. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page">1555008</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Space-time representation of people based on 3D skeletal data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="85" to="105" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A Survey on Activity Detection and Classification Using Wearable Sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornacchia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ozcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Velipasalar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sens. J</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="386" to="403" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Okutama-Action: An Aerial View Video Dataset for Concurrent Human Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barekatain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2153" to="2160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Broadband polarimetry with the square kilometre array: A unique astrophysical probe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Gaensler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="97" to="106" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">UTD-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -International Conference on Image Processing</title>
		<meeting>-International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
	<note>2015-Decem</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Histogram of Oriented Principal Components for Cross-View Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2430" to="2443" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Experiments on an RGB-D wearable vision system for egocentric activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moghimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Azagra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="611" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Physical activity recognition using multiple sensors embedded in a wearable device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Embed. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">BioGlass: Physiological parameter estimation using a head-mounted wearable device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 4th International Conference on Wireless Mobile Communication and Healthcare -&quot;Transforming Healthcare Through Innovations in Mobile and Wireless Technologies</title>
		<meeting>the 2014 4th International Conference on Wireless Mobile Communication and Healthcare -&quot;Transforming Healthcare Through Innovations in Mobile and Wireless Technologies</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Using wearable cameras to categorise type and context of accelerometer-identified episodes of physical activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Behav. Nutr. Phys. Act</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The MÃ¡laga urban dataset: High-rate stereo and LiDAR in a realistic urban scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Blanco-Claraco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Ã</forename><surname>Moreno-DueÃ±as</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>GonzÃ¡lez-JimÃ©nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Rob. Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="214" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A Human Activity Recognition System Using Skeleton Data from RGBD Sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cippitelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gasparrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gambi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Spinsante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Intell. Neurosci</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Evolutionary joint selection to improve human action recognition with RGB-D devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Chaaraoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Padilla-LÃ³pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Climent-PÃ©rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>FlÃ³rez-Revuelta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="786" to="794" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stud. Comput. Intell</title>
		<imprint>
			<biblScope unit="volume">411</biblScope>
			<biblScope unit="page" from="119" to="135" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Visual motion perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Am</title>
		<imprint>
			<biblScope unit="volume">232</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="76" to="88" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A computational theory for the perception of coherent visual motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Grywacz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="71" to="74" />
			<date type="published" when="1988-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>8th ed</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fusion of LiDAR and Camera Sensor Data for Environment Sensing in Driverless Vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kondoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv Comput. Vis. Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Abiotic stresses induce total phenolic, total flavonoid and antioxidant properties in Malaysian indigenous microalgae and cyanobacterium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Azim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">N B</forename><surname>Yusof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Integrating the enriched feature with machine learning algorithms for human movement and fall detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Supercomput</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="854" to="865" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Robust smooth feature extraction from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ochotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -IEEE International Conference on Shape Modeling and Applications</title>
		<meeting>-IEEE International Conference on Shape Modeling and Applications</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
	<note>SMI&apos;07</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Activity recognition by learning structural and pairwise mid-level features using random forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A human activity recognition framework using max-min features and key poses with differential evolution random forests classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">M</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Faria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peixoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="21" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Driverless: Intelligent Cars and the Road Ahead</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melba</forename><surname>Kurman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>no. 320321. 2012</idno>
		<title level="m">Machine Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Application of Big Data in Economic Policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Sahoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research Advances in the Integration of Big Data and Smart Computing, P. K. Mallick</title>
		<imprint>
			<publisher>Biographies</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
	<note>IGI Global book series Advances in Computational Intelligence and Robotics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
