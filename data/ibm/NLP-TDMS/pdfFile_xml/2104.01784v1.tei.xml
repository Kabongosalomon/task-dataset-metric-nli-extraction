<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BTS-NET: BI-DIRECTIONAL TRANSFER-AND-SELECTION NETWORK FOR RGB-D SALIENT OBJECT DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution" key="instit1">Sichuan University National Key Laboratory of Fundamental Science on Synthetic Vision</orgName>
								<orgName type="institution" key="instit2">Sichuan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution" key="instit1">Sichuan University National Key Laboratory of Fundamental Science on Synthetic Vision</orgName>
								<orgName type="institution" key="instit2">Sichuan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution" key="instit1">Sichuan University National Key Laboratory of Fundamental Science on Synthetic Vision</orgName>
								<orgName type="institution" key="instit2">Sichuan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution" key="instit1">Sichuan University National Key Laboratory of Fundamental Science on Synthetic Vision</orgName>
								<orgName type="institution" key="instit2">Sichuan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BTS-NET: BI-DIRECTIONAL TRANSFER-AND-SELECTION NETWORK FOR RGB-D SALIENT OBJECT DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-RGB-D SOD</term>
					<term>saliency detection</term>
					<term>bi- directional interaction</term>
					<term>attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth information has been proved beneficial in RGB-D salient object detection (SOD). However, depth maps obtained often suffer from low quality and inaccuracy. Most existing RGB-D SOD models have no cross-modal interactions or only have unidirectional interactions from depth to RGB in their encoder stages, which may lead to inaccurate encoder features when facing low quality depth. To address this limitation, we propose to conduct progressive bidirectional interactions as early in the encoder stage, yielding a novel bi-directional transfer-and-selection network named BTS-Net, which adopts a set of bi-directional transfer-andselection (BTS) modules to purify features during encoding. Based on the resulting robust encoder features, we also design an effective light-weight group decoder to achieve accurate final saliency prediction. Comprehensive experiments on six widely used datasets demonstrate that BTS-Net surpasses 16 latest state-of-the-art approaches in terms of four key metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Salient object detection (SOD) aims to locate image regions that attract much human visual attention. It is useful in many computer vision tasks, e.g., object segmentation <ref type="bibr" target="#b0">[1]</ref>, tracking <ref type="bibr" target="#b1">[2]</ref>, image/video compression <ref type="bibr" target="#b2">[3]</ref>. Though RGB SOD methods have made great progresses in recent years thanks to deep learning <ref type="bibr" target="#b3">[4]</ref>, they still encounter problems in challenging scenarios, e.g., similar foreground and background, cluttered/complex background, or low-contrast environment.</p><p>With the increasing access to depth sensors, RGB-D SOD recently becomes a hot research topic <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. Additional useful spatial information embedded in depth maps could somewhat help overcome the aforementioned challenges. Although a lot of advances <ref type="bibr" target="#b8">[9]</ref> have been made in this field by exploring cross-modal complementarity <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, we notice that existing models are still insufficient on extracting robust saliency features. As shown in <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>    <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b14">15]</ref> and (b) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>) as well as the proposed bi-directional strategy for the encoder (c).</p><p>with no interactions or unidirectional interactions. For instance, in <ref type="figure" target="#fig_1">Fig.1 (a)</ref>, parallel encoders <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b14">15]</ref> are deployed to extract individual features of RGB and depth, and then cross-modal fusion is handled by the following decoder. In <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>, tailor-maid sub-networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16</ref>] are adopted to inject depth cues into RGB as guidance/enhancement. The resulting features are then decoded to obtain the saliency map. We argue that both the above strategies may have ignored the quality issue of depth maps, since depth maps obtained from no matter depth sensors or existing datasets, are often noisy with low quality. It is obvious that in <ref type="figure" target="#fig_1">Fig. 1 (a)</ref> and (b), if the input depth is inaccurate, the extracted/injected depth features will be easily affected and may degrade the final saliency map from the decoder.</p><p>To address this issue, we propose to conduct progressive bi-directional interactions as early in the encoder, instead of late in the decoder stage. This idea is illustrated by <ref type="figure" target="#fig_1">Fig. 1 (c)</ref>. In this paper, we propose a novel bi-directional transfer-andselection network, named BTS-Net, which is characterized by a new bi-directional transfer-and-selection (BTS) module applied to the encoder, enabling RGB and depth to mutually correct/refine each other as early as possible. Thus, the burden of the decoder can be well relieved. Our BTS is inspired by the attention mechanism <ref type="bibr" target="#b22">[23]</ref> and cross attention <ref type="bibr" target="#b23">[24]</ref>, and it makes features from different modalities refine each other to achieve purified features with less noise. In addition, thanks to the proposed early interaction strategy, the extracted robust hierarchical features enable us to design an effective lightweight group decoder to generate the final saliency map.</p><p>The contributions of this paper are three-fold:</p><p>• We propose BTS-Net, which is the first RGB-D SOD model to introduce bi-directional interactions across RGB and depth during the encoder stage.</p><p>• To achieve bi-directional interactions, we design a transfer-and-selection (BTS) module based on spatialchannel attention. • We design an effective light-weight group decoder to achieve accurate final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The utilization of RGB-D data for SOD has been extensively explored for years. Traditional methods rely on hand-crafted features <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>, while recently, deep learning-based methods have made great progress <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. Based on the scope of this paper, we divide existing deep-based models into two types according to how they extract RGB and depth features, namely: parallel independent encoders ( <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>), and tailor-maid sub-networks from depth to RGB ( <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>). Parallel Independent Encoders. This strategy, illustrated in <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>, first extracts features from RGB and depth images parallelly, and then fuses them using decoders. Chen et al. <ref type="bibr" target="#b9">[10]</ref> proposed a cross-modal complementarity-aware fusion module. Piao et al. <ref type="bibr" target="#b10">[11]</ref> fused RGB and depth features via residual connections and refined with a depth vector and recurrent attention module. Fu et al. <ref type="bibr" target="#b5">[6]</ref> extracted RGB and depth features in a parallel manner but through a Siamese network. Features are then fused and refined in a densely connected manner. Zhang et al. <ref type="bibr" target="#b11">[12]</ref> introduced a complimentary interaction module to select useful features. In <ref type="bibr" target="#b12">[13]</ref>, Li et al. enhanced feature representations by taking depth features as priors. Chen et al. <ref type="bibr" target="#b13">[14]</ref> proposed to extract depth features with a light-weight depth branch and conduct progressive refinement. Pang et al. <ref type="bibr" target="#b7">[8]</ref> combined cross-modal features to generate dynamic filters, which were used to filter and enhance the decoder features.</p><p>Tailor-maid Sub-networks from Depth to RGB. Recently, this unidirectional interaction from depth to RGB is introduced to the encoding stage ( <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>), leveraging depth cues as guidance or enhancement. Zhu et al. <ref type="bibr" target="#b14">[15]</ref> used depth features extracted from a subsidiary network as a weight matrix to enhance RGB features. Zhao et al. <ref type="bibr" target="#b15">[16]</ref> computed a contrast-enhanced depth map and treated it as attention to enhance feature representations of the RGB stream. In <ref type="bibr" target="#b4">[5]</ref>, depth features were enhanced by an attention mechanism and then were fused with RGB features. Such fused features were later fed to an elaborately-designed cascaded decoder. Different from all above methods, our BTS-Net introduces progressive bi-directional interactions in the encoder ( <ref type="figure" target="#fig_1">Fig. 1(c)</ref>) to enforce mutual correction and refinement across RGB and depth branches, yielding robust encoder features. It follows the typical encoder-decoder architecture, where the encoder is equipped with several BTS (bi-directional transferand-selection) modules to enforce cross-modal interactions and compensation during encoder feature extraction, resulting in hierarchical modality-aware features. Meanwhile, the decoder is elaborately designed for group-wise decoding and ultimate saliency prediction. Specifically, the encoder consists of an RGB-related branch, a depth-related branch, and five BTS modules. These two master branches adopt the widely used ResNet-50 <ref type="bibr" target="#b28">[29]</ref> as backbones, leading to five feature hierarchies (note the stride of the last hierarchy is modified from 2 to 1). The input to an intermediate hierarchy in either branch is the corresponding output of the previous BTS module. Besides, in order to capture multi-scale semantic information, we add ASPP (atrous spatial pyramid pooling <ref type="bibr" target="#b29">[30]</ref>) modules at the end of each branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head><p>Let the feature outputs of the five RGB/depth ResNet hierarchies be denoted as bf i m (m ∈ {r, d}, i = 0, ..., 4), and the enhanced cross-modal features from BTS modules as well as ASPPs be denoted as f i m (m ∈ {r, d}, i = 0, ..., 5). We regard f i m (m ∈ {r, d}, i = 0, 1, 2) as low-level features and f i m (m ∈ {r, d}, i = 3, 4, 5) as high-level features, and such low-/high-level features are then fed to the subsequent lightweight group decoder. In the followings, we describe the proposed BTS modules which can be utilized in the encoder, as well as the light-weight group decoder in detail.</p><p>Bi-directional Transfer-and-Selection (BTS). The detailed structure of BTS module is shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, which is inspired by the well-known spatial-channel attention mechanisms <ref type="bibr" target="#b22">[23]</ref>. A BTS has two processing stages: bi-directional transfer, and feature selection. Note that the former is based on spatial attention, while the latter is associated with channel attention. The underlying rational of BTS is that using spatial attention first is able to tell "where" a salient object is, and channel attention then can select feature channels to tell "what" features matter. In detail, the bi-directional transfer stage performs cross-modal attention transfer, namely applying the derived spatial attention map from either modality to the other, as illustrated in <ref type="figure" target="#fig_4">Fig. 3</ref>. Since BTS is designed in a symmetric manner, for brevity, below we only elaborate the operations of transferring RGB features to the depth branch.</p><p>Given the RGB features bf r at a certain hierarchy, we first compute its corresponding spatial attention map SA r as:</p><formula xml:id="formula_0">SA r = Sigmoid(Conv sr (bf r )),<label>(1)</label></formula><p>where Sigmoid means the sigmoid activation function, and Conv sr represents a (3×3, 1) convolutional layer with singlechannel output. Next, the resulting spatial attentive cue SA r is transferred to depth, which is mainly implemented by element-wise multiplication (denoted by mathematical symbol "×"). Before the multiplication, SA r is also added with a term SA r × SA d , where SA d is the counterpart from the depth branch, to preserve certain modality individuality. Therefore, the depth features compensated by RGB information are formulated as:   where bf d are the corresponding depth features. After this stage, features of each modality are spatially compensated by the information from the other modality. Next, the obtained features cf d are selected along the channel dimension, which is implemented by a typical channel-attention operation <ref type="bibr" target="#b22">[23]</ref>:</p><formula xml:id="formula_1">cf d = (SA r + SA r × SA d ) × bf d ,<label>(2)</label></formula><formula xml:id="formula_2">CA d = Sof tmax(Conv cd (GAP (cf d ))), (3) f d = CA d × cf d ,<label>(4)</label></formula><p>where f d indicate features that BTS outputs as in <ref type="figure" target="#fig_2">Fig. 2</ref> and <ref type="figure" target="#fig_4">Fig. 3</ref>. CA d denotes the channel weight vector, GAP is the global average pooling operation, Sof tmax denotes the softmax function, and Conv cd is a 1×1 convolution, of which the input and output channel numbers are equal. Note that after the entire transfer and selection stages, our BTS maintains the features channel and spatial dimensions. This makes the proposed BTS applicable in a "plug-and-play" manner in most parallel independent encoders ( <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>). We also note that we choose not to use the widely adopted residual attention strategy <ref type="bibr" target="#b31">[32]</ref>, which adds the attended features f d (f r ) with the original features bf d (bf r ). This is because the residual connection may limit the extent to which the complementary information can be transferred. Instead, our design allows the encoder to determine such extent adaptively. Ablation experiments in Section 4.3 show that without this residual connection, more improvement can be obtained. Group Decoder. Our group decoder is characterized by feature grouping and three-way supervision. As well-known, deeper features from a convolutional neural network encode high-level knowledge that helps locate objects, whereas shallower features characterize low-level edge details. Our motivation of grouping is that the same-level features have better compatibility, which facilitate subsequent decoding. Therefore, after visualizing features which the encoder extracts, we roughly divide these 12 hierarchical features into four types, i.e., high-level RGB features (f i r , i = 3, 4, 5), lowlevel RGB features (f i r , i = 0, 1, 2), high-level depth features (f i d , i = 3, 4, 5), and low-level depth features (f i d , i = 0, 1, 2). During decoding, we first conduct feature merging within each group to save memory and computation cost. These 12 hierarchical features, denoted by f i m with different channels, are first all transformed to unified k-channel features f i mt (in practice k = 256) by a process consisting of convolution, BatchNorm, and ReLU. Such a process is denoted by "BConv" in <ref type="figure" target="#fig_2">Fig. 2</ref>. They are then grouped together into four types according to their properties, i.e., low-/high-level features and modalities, which can be defined as:</p><formula xml:id="formula_3">f h m = f 3 mt + f 4 mt + f 5 mt ,<label>(5)</label></formula><formula xml:id="formula_4">f l m = f 0 mt + U p(f 1 mt ) + U p(f 2 mt ),<label>(6)</label></formula><p>where the subscript m ∈ {r, d} indicates the RGB/depth modality, and U p is the bilinear up-sampling operation. Then we utilize the grouped features f h m , f l m , m ∈ {r, d} to predict three ultimate saliency maps.</p><p>To achieve fused saliency prediction S c , we excavate cross-modal complementarity by multiplication and addition on different levels, which guarantees explicit information fusion across RGB and depth. The fused features at different <ref type="table">Table 1</ref>. Quantitative RGB-D SOD results. ↑/↓ denotes that a larger/smaller value is better. The best results are highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PCF MMCI CPFP DMRA D3Net SSF A2dele UCNet JL-DCF cmMS CoNet PGAR Cas-Gnn DANet HDFNet BBS-Net BTS-Net</head><p>CVPR18 PR19 CVPR19 ICCV19 TNNLS20 CVPR20 CVPR20 CVPR20 CVPR20 ECCV20 ECCV20 ECCV20 ECCV20 ECCV20 ECCV20 ECCV20 Ours  levels are then concatenated and fed to a prediction head. The above operations can be summarized as:</p><formula xml:id="formula_5">f h c = BConv([f h r × f h d , f h r + f h d ]),<label>(7)</label></formula><formula xml:id="formula_6">f l c = BConv([f l r × f l d , f l r + f l d ]),<label>(8)</label></formula><formula xml:id="formula_7">S c = P ([U p(f h c ), f l c ]),<label>(9)</label></formula><p>where P is a prediction head consisting of two "BConv", a (1×1, 1) convoluion, a Sigmoid layer, and an up-sampling operation, and [·] denotes the concatenation operation. BConv is the "BConv" process mentioned before. Moreover, in order to enhance feature learning efficacy and avoid degradation in BTS, allowing both branches to fully play their roles, we impose extra supervision to both RGB and depth branches simultaneously. The two saliency maps, namely S r and S d , are generated from individual branches by using their own features:</p><formula xml:id="formula_8">S r = P ([U p(f h r ), f l r ]), S d = P ([U p(f h d ), f l d ]),<label>(10)</label></formula><p>where P , U p and [·] are the same defined as in Eq. (7)- <ref type="bibr" target="#b8">(9)</ref>. Supervision. Similar to previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref>, we use the standard cross-entropy loss to implement three-way supervision to S r , S d and S c , which is formulated as:</p><formula xml:id="formula_9">L total = m∈{r,d,c} λ m L bce (S m , G)<label>(11)</label></formula><p>where L total is the total loss, L bce is the binary cross-entropy loss, G denotes the ground truth, and λ m emphasizes each supervision. λ c = 1, λ r = λ d = 0.5 are set in our experiments. During inference, S c is used as the final prediction result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets, Metrics and Implementation Details</head><p>We test BTS-Net on six widely used RGB-D datasets, i.e., NJU2K, NLPR, STERE, RGBD135, LFSD, SIP. Following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16]</ref>, we use the same 1500 samples from NJU2K and 700 samples from NLPR for training, and the remaining samples for testing. Four metrics are adopted for evaluation, including S-measure (S α ), maximum E-measure (E max ξ ), maximum Fmeasure (F max β ), and mean absolute error (MAE, M). We implemented BTS-Net by Pytorch, and an input RGB-depth pair is resized to 352 × 352 resolution. The learning rate is set to 1e-4 for the Adam optimizer, and is later degraded by 10 at 60 epochs. Batch size is set as 10, and the model is trained in total with 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-Arts</head><p>To demonstrate the effectiveness of the proposed method, we compare it with 16 state-of-the-art (SOTA) methods, i.e.: PCF <ref type="bibr" target="#b9">[10]</ref>, MMCI <ref type="bibr" target="#b16">[17]</ref>, CPFP <ref type="bibr" target="#b15">[16]</ref>, DMRA <ref type="bibr" target="#b10">[11]</ref>, D3Net <ref type="bibr" target="#b19">[20]</ref>, SSF <ref type="bibr" target="#b11">[12]</ref>, A2dele <ref type="bibr" target="#b17">[18]</ref>, UCNet <ref type="bibr" target="#b6">[7]</ref>, JLDCF <ref type="bibr" target="#b5">[6]</ref>, cmMS <ref type="bibr" target="#b12">[13]</ref>, CoNet <ref type="bibr" target="#b18">[19]</ref>, PGAR <ref type="bibr" target="#b13">[14]</ref>, Cas-Gnn <ref type="bibr" target="#b30">[31]</ref>, DANet <ref type="bibr" target="#b20">[21]</ref>, HDFNet <ref type="bibr" target="#b7">[8]</ref>, BBSNet <ref type="bibr" target="#b4">[5]</ref>. Quantitative results are shown in <ref type="table">Table 1</ref>. It can be seen that our BTS-Net achieves superior performance over SOTAs consistently on almost all metrics.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Interaction Directions of BTS. To validate the rationality of the proposed BTS module, we set up five experiments with different settings. Notation "R←D" means introducing depth to the RGB branch, and "R→D" means the vice versa. "R↔D" means the proposed bi-directional interactions. "Res" means introducing residual connections into BTS for both branches as mentioned in Section 3. For fair comparison, these settings are conducted by only switching connections inside BTS while keeping the main components (e.g., spatial and channel attention) maintained. Ablation results are shown in <ref type="table" target="#tab_2">Table 2</ref>, where row #1 means no interaction exists between the two branches, leading to the worst results. Rows #2 and #3 are better than #1, showing that unidirectional interaction is better than none. Specially, row #3 shows much better results than #2 on STERE dataset, indicating that transferring RGB to depth could mitigate the influence from inaccurate depth 1 , which rightly supports our claim. Comparing row #4 (the default BTS) to #2 and #3, the improvement is consistent and notable. This validates the proposed bi-directional interaction strategy in BTS. Lastly, row #5 leads to no boost over #4. This may be caused by the limitation on transfer ability brought by the residual connection. <ref type="figure">Fig. 5</ref> shows a comparative example between visualized features #1 and #4, where one can see #4 results in more robust <ref type="bibr" target="#b0">1</ref> According to our observation and also <ref type="bibr" target="#b5">[6]</ref>, the depth quality of STERE is relatively poor among the six datasets. Image (a) in <ref type="figure" target="#fig_7">Fig. 4</ref>   features as well as the final saliency map.</p><p>Internal Attention Designs of BTS. To validate the current attention design in BTS, we also set up three different experiments, whose results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Notations "Only SA", "CA-SA", and "SA-CA" denote: only applying spatial attention without channel attention, changing the order of the spatial and channel attention (i.e., the latter comes first), and the default design of BTS-Net (i.e., the spatial attention comes first), respectively. Comparing the default design of BTS, namely SA-CA, to the other two variants, one can see it consistently achieves the best performance. Such comparative experiments show that the order of spatial-channel attention is crucial for introducing attention-aware interactions, whereas combining channel attention with bi-directional spatial attention transfer is effective.</p><p>Light-weight Group Decoder. To validate our lightweight group decoder, we evaluate four results. Performance during inference is shown in <ref type="table">Table 4</ref>, where "U-net" denotes the results generated by a typical U-net decoder. Basically, we concatenate RGB and depth features at the same hierarchies first and then feed the concatenated 512-channel features to a typical U-net decoder consisting of progressive upsampling, concatenation and convolution, at the end of which the same prediction head is applied to obtain the saliency map S c . Note that in this experiment, the three-way supervision was preserved. Notation "GD-D", "GD-R", and "GD-C" denote the decoders deployed to obtain results S d , S r and S c in BTS-Net as shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. Note that regarding GD-D/GD-R, their parameters come mainly from the prediction heads.</p><p>From <ref type="table">Table 4</ref>, one can see that the proposed decoder, which consists of GD-D, GD-R and GD-C, has much fewer parameters and is more light-weight. Specially, GD-C's parameters are only ∼12.7% of those of the U-net, and meanwhile, GD-C which outputs S c achieves the best performance. Also note that GD-D/GD-R are even lighter, since they only involve the prediction heads. We attribute the success of the proposed group decoder partly to the efficacy of BTS modules in the encoder (with BTS, the encoder parameters increase from 80.3M to 91.5M), as the resulted robust features from the two branches make it possible for using relatively simple decoding. Also, the superior performance of GD-C comparing to GD-D and GD-R shows that fusing encoder features of the two modalities is essential for better RGB-D SOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We introduce BTS-Net, the first RGB-D SOD model that adopts the idea of using bi-directional interactions between RGB and depth in the encoder. A light-weight group decoder is proposed to collaborate with the encoder in order to achieve high-quality saliency maps. Comprehensive comparisons to SOTA approaches as well as ablation experiments have validated the proposed bi-directional interaction strategy, internal designs of the BTS module, and also the group decoder. Since BTS can be applied in a "plug-and-play" fashion, it will be interesting to use BTS to boost existing models in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and (b), in their encoder stages, modality-aware features are usually extracted * Corresponding author (email: fkrsuper@scu.edu.cn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Feature extraction strategies of existing RGB-D SOD models ((a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>shows the block diagram of the proposed BTS-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Block diagram of the proposed BTS-Net, which follows the typical encoder-decoder architecture. The encoder is shown on the left, whereas the decoder is shown on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Detailed structure of the proposed BTS (bi-directional transfer-and-selection) module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>↑ 0 .</head><label>0</label><figDesc>872 0.852 0.877 0.886 0.950 0.896 0.872 0.895 0.903 0.897 0.892 0.907 0.916 0.880 0.910 0.920 0.924 E max ξ ↑ 0.924 0.915 0.926 0.927 0.950 0.935 0.914 0.936 0.944 0.936 0.937 0.940 0.948 0.932 0.944 0.949 0.954 M ↓ 0.059 0.079 0.053 0.051 0.041 0.043 0.052 0.043 0.043 0.044 0.047 0.042 0.036 0.048 0.039 0.035 0.036 NLPR Sα ↑ 0.874 0.856 0.888 0.899 0.912 0.914 0.890 0.920 0.925 0.915 0.908 0.930 0.920 0.915 0.923 0.930 0.934 F max β ↑ 0.841 0.815 0.867 0.879 0.897 0.896 0.875 0.903 0.916 0.896 0.887 0.916 0.906 0.901 0.917 0.918 0.923 E max ξ ↑ 0.925 0.913 0.932 0.947 0.953 0.953 0.937 0.956 0.961 0.949 0.945 0.961 0.955 0.953 0.963 0.961 0.965 M ↓ 0.044 0.059 0.036 0.031 0.025 0.026 0.031 0.025 0.022 0.027 0.031 0.024 0.025 0.029 0.023 0.023 0.023 STERE Sα ↑ 0.875 0.873 0.879 0.835 0.899 0.893 0.885 0.903 0.905 0.895 0.908 0.907 0.899 0.892 0.900 0.908 0.915 F max β ↑ 0.860 0.863 0.874 0.847 0.891 0.890 0.885 0.899 0.901 0.891 0.904 0.898 0.901 0.881 0.900 0.903 0.911 E max ξ ↑ 0.925 0.927 0.925 0.911 0.938 0.936 0.935 0.944 0.946 0.937 0.948 0.939 0.944 0.930 0.943 0.942 0.949 M ↓ 0.064 0.068 0.051 0.066 0.046 0.044 0.043 0.039 0.042 0.042 0.040 0.041 0.039 0.048 0.042 0.041 0.038 RGBD135 Sα ↑ 0.842 0.848 0.872 0.900 0.898 0.905 0.884 0.934 0.929 0.932 0.910 0.913 0.899 0.904 0.926 0.933 0.943 F max β ↑ 0.804 0.822 0.846 0.888 0.885 0.883 0.873 0.930 0.919 0.922 0.896 0.902 0.896 0.894 0.921 0.927 0.940 E max ξ ↑ 0.893 0.928 0.923 0.943 0.946 0.941 0.920 0.976 0.968 0.970 0.945 0.945 0.942 0.957 0.970 0.966 0.979 M ↓ 0.049 0.065 0.038 0.030 0.031 0.025 0.030 0.019 0.022 0.020 0.029 0.026 0.026 0.029 0.022 0.021 0.018 LFSD Sα ↑ 0.786 0.787 0.828 0.839 0.825 0.859 0.834 0.864 0.854 0.849 0.862 0.853 0.847 0.845 0.854 0.864 0.867 F max β ↑ 0.775 0.771 0.826 0.852 0.810 0.867 0.832 0.864 0.862 0.869 0.859 0.843 0.847 0.846 0.862 0.859 0.874 E max ξ ↑ 0.827 0.839 0.863 0.893 0.862 0.900 0.874 0.905 0.893 0.896 0.906 0.890 0.888 0.886 0.896 0.901 0.906 M ↓ 0.119 0.132 0.088 0.083 0.095 0.066 0.077 0.066 0.078 0.074 0.071 0.075 0.074 0.083 0.077 0.072 0.070 SIP Sα ↑ 0.842 0.833 0.850 0.806 0.860 0.874 0.829 0.875 0.879 0.867 0.858 0.876 0.842 0.878 0.886 0.879 0.896 F max β ↑ 0.838 0.818 0.851 0.821 0.861 0.880 0.834 0.879 0.885 0.871 0.867 0.876 0.848 0.884 0.894 0.883 0.901 E max ξ ↑ 0.901 0.897 0.903 0.875 0.909 0.921 0.889 0.919 0.923 0.907 0.913 0.915 0.890 0.920 0.930 0.922 0.933 M ↓ 0.071 0.086 0.064 0.085 0.063 0.053 0.070 0.051 0.051 0.061 0.063 0.055 0.068 0.054 0.048 0.055 0.044</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 Fig. 4 .</head><label>44</label><figDesc>further shows several visual comparisons of BTS-Net with the latest representative models. From top to bottom, the quality of depth maps varies from poor to good: (a) the depth almost misses the entire object; (b) depth lacks details of the bird's head and feet; (c) depth has good contrast but Visual comparisons with SOTA RGB-D SOD models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Table 4 .</head><label>4</label><figDesc>Results from the U-net and our group decoder (GD). Details are in Section 4.3: "Light-weight Group Decoder". 0.921 0.924 0.036 0.915 0.911 0.038 0.896 0.901 0.044</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results of different interaction strategies. Details are in Section 4.3: "Interaction Directions of BTS".</figDesc><table><row><cell># Direction Res</cell><cell>NJU2K Sα F max β</cell><cell>STERE M Sα F max β</cell><cell>SIP M Sα F max β</cell><cell>M</cell></row><row><cell>1 None</cell><cell cols="4">0.868 0.862 0.064 0.730 0.685 0.117 0.873 0.875 0.060</cell></row><row><cell>2 R←D</cell><cell cols="4">0.912 0.914 0.040 0.892 0.888 0.047 0.891 0.896 0.048</cell></row><row><cell>3 R→D</cell><cell cols="4">0.920 0.921 0.035 0.911 0.905 0.039 0.890 0.895 0.047</cell></row><row><cell>4 R↔D</cell><cell cols="4">0.921 0.924 0.036 0.915 0.911 0.038 0.896 0.901 0.044</cell></row><row><cell>5 R↔D</cell><cell cols="4">0.918 0.918 0.037 0.912 0.909 0.038 0.890 0.896 0.048</cell></row><row><cell cols="5">non-salient regions are adjoined; (d) depth is relatively good</cell></row><row><cell cols="5">but the RGB has low contrast. Our BTS-Net performs well</cell></row><row><cell cols="5">and robustly in all the above cases, especially in (a) and (b),</cell></row><row><cell cols="5">where the depth presents low quality and missing information.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>is from STERE.</figDesc><table><row><cell>RGB</cell><cell>fr 3</cell><cell>fd 3</cell><cell>Salmap</cell></row><row><cell>with BTS</cell><cell></cell><cell></cell><cell>GT</cell></row><row><cell>Depth</cell><cell>fr 3</cell><cell>fd 3</cell><cell>Salmap</cell></row><row><cell>w\o BTS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Fig. 5. Visualized features (f 3 r and f 3 d in BTS-Net) from set-</cell></row><row><cell cols="4">ting #4 (with BTS) and #1 (w\o BTS) in Table 2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results of different internal attention designs. Details are in Section 4.3: "Internal Attention Designs of BTS".</figDesc><table><row><cell>Settings</cell><cell>NJU2K Sα F max β</cell><cell>M</cell><cell>STERE Sα F max β</cell><cell>M</cell><cell>SIP Sα F max β</cell><cell>M</cell></row><row><cell cols="7">Only SA 0.914 0.917 0.039 0.903 0.900 0.044 0.887 0.892 0.050</cell></row><row><cell cols="7">CA-SA 0.914 0.915 0.039 0.901 0.896 0.044 0.892 0.899 0.047</cell></row><row><cell cols="7">SA-CA 0.921 0.924 0.036 0.915 0.911 0.038 0.896 0.901 0.044</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Saliency-aware video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="20" to="33" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-rigid object tracking via deep multi-scale spatial-temporal discriminative saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">107130</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bbs-net: Rgb-d salient object detection with a bifurcated backbone strategy network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="275" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jl-dcf: Joint learning and densely-cooperative fusion framework for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3052" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="8582" to="8519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical dynamic filtering network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="235" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rgb-d salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVM</title>
		<imprint>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="3051" to="3060" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7254" to="7263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Select, supplement and focus for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3469" to="3478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rgb-d salient object detection with cross-modality modulation and selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="225" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressively guided alternate refinement network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="520" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pdnet: Prior-model guided depth-enhanced network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="199" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y.</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-modal fusion network with multiscale multi-path and cross-modal interactions for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A2dele: Adaptive and attentive depth distiller for efficient rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9060" to="9069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate rgb-d salient object detection via collaborative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="52" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A single stream network for robust and real-time rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="520" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning selective self-mutual attention for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13753" to="13762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIMCS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An in depth view of saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting global priors for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cascade graph neural networks for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="346" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6450" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
