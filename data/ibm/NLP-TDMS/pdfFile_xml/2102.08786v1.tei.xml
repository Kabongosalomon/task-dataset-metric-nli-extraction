<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Learning with 1D Convolutions on Random Walks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Toenshoff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrikus</forename><surname>Wolf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Learning with 1D Convolutions on Random Walks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose CRaWl (CNNs for Random Walks), a novel neural network architecture for graph learning. It is based on processing sequences of small subgraphs induced by random walks with standard 1D CNNs. Thus, CRaWl is fundamentally different from typical message passing graph neural network architectures. It is inspired by techniques counting small subgraphs, such as the graphlet kernel and motif counting, and combines them with random walk based techniques in a highly efficient and scalable neural architecture. We demonstrate empirically that CRaWl matches or outperforms state-of-theart GNN architectures across a multitude of benchmark datasets for graph learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph data is ubiquitous across multiple domains, reaching from cheminformatics and social network analysis to knowledge graphs. Being able to effectively learn on such graph data is thus extremely important. We propose a novel neural network architecture called CRaWl (CNNs for Random Walks) that is based on random walks and standard 1D CNNs. Essentially, CRaWl samples a set of random walks and extracts features that fully describe the subgraphs visible within a sliding window over these walks. The walks with the subgraph features are then processed with standard 1D convolutions. We experimentally verify that this approach consistently achieves state-of-the-art performance on various modern graph learning benchmarks and in many cases outperforms the state of the art.</p><p>The CRaWl architecture was originally motivated from the empirical observation that in many application scenarios, random walk based methods, for example, node2vec <ref type="bibr" target="#b14">Grover and Leskovec [2016]</ref> in combination with various classifiers, perform surprisingly well in comparison with graph neural networks (GNNs). A second empirical observation is that GNNs are not very good at detecting small subgraphs, for example, cycles of length 6. Yet, the distribution of such subgraphs in a graph carries relevant information about the structure of a graph, as witnessed by the extensive research on motif detection and counting [e.g. <ref type="bibr" target="#b2">Alon, 2007]</ref>.</p><p>We believe that the key to the strength of CRaWl is a favorable combination of engineering and expressiveness aspects. Even large numbers of random walks can be sampled very efficiently, and once the random walks are available, we can rely on existing highly optimized code for 1D CNNs, which allows us to fully exploit the strengths of modern hardware. In terms of expressiveness, CRaWl detects both the global connectivity structure in a graph by sampling longer random walks as well as the full local structure within its window size. The gain in expressiveness compared to GNNs is mainly due to the detailed view on the local structure in the sliding window, which standard message passing GNNs [e.g. <ref type="bibr" target="#b13">Gilmer et al., 2017]</ref> do not have. We show that the expressiveness of CRaWl is incomparable to that of GNNs (Theorem 1). In particular, CRaWl detects features that are not even accessible by higher-order GNNs. Sampling small subgraphs in a sliding window on random walks is scalable and has the advantage that even in sparse graphs it usually yields meaningful subgraph patterns.</p><p>CRaWl empirically outperforms advanced message passing GNN architectures on major benchmark datasets. On the molecular regression dataset ZINC <ref type="bibr" target="#b10">Dwivedi et al. [2020]</ref>, CRaWl improves the best results currently listed on the leaderboard by roughly 30%. (And ∼40% when combined with virtual nodes.) CRaWl also places first on the leaderboard for MOLPCBA, a large molecular property prediction dataset from the OGB Project <ref type="bibr" target="#b16">Hu et al. [2020]</ref>.</p><p>A basic requirement for graph learning methods is their isomorphism invariance, which guarantees that the result of a computation only depends on the structure and not on the specific representation of the input graph. A CRaWl model represents a random variable defined on graphs. This random variable is invariant [in the sense of <ref type="bibr" target="#b27">Maron et al., 2019b]</ref>, which means that it does not depend on a particular node numbering, but only on the isomorphism type of the input graph. Note that it does not contradict the invariance that on every single random walk that we sample we see the vertices in a specific order and can process the vertices in this order by 1D CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Over the last few years, message passing GNNs (MPGNNs) have been the dominant type of architecture in all kinds of graph related learning tasks <ref type="bibr" target="#b21">[Wu et al., 2020]</ref>. Thus, MPGNNs constitute the main baselines in our experiments. Many variants of this architecture exist, such as <ref type="bibr">GCN Kipf and Welling [2017]</ref>, <ref type="bibr">GIN Xu et al. [2019]</ref>, <ref type="bibr">GAT Veličković et al. [2018]</ref>, GraphSage <ref type="bibr" target="#b15">Hamilton et al. [2017]</ref>, and <ref type="bibr">GatedGCN Bresson and Laurent [2017]</ref>. A novel variant of MPGNNs is PNA  which combines multiple types of local aggregation to improve performance. Another recent advance is DeeperGCN (DGCN)  which is designed for significantly deeper GNNs.</p><p>Multiple extensions to the standard message passing framework have been proposed that strengthen the theoretical expressiveness which otherwise is bounded by the 1dimensional Weisfeiler-Leman algorithm. With <ref type="bibr">3WLGNN Maron et al. [2019a]</ref> suggested a higher-order GNN, which is equivalent to the 3-dimensional Weisfeiler-Leman kernel and thus more expressive than standard MPGNNs. In <ref type="bibr">HIMP Fey et al. [2020]</ref>, the backbone of a molecule graph is extracted and then two GNNs are run in parallel on the backbone and the full molecule graph. This allows HIMP to detect structural features that are otherwise neglected. Explicit counts of fixed substructures such as cycles or small cliques have been added to the node and edge features by <ref type="bibr" target="#b4">Bouritsas et al. [2020]</ref>  <ref type="bibr">(GSN)</ref>. Similarly, <ref type="bibr" target="#b35">Sankar et al. [2017]</ref>, <ref type="bibr" target="#b23">Lee et al. [2019]</ref>, and <ref type="bibr" target="#b33">Peng et al. [2020]</ref> added the frequencies of motifs, i.e., common connected induced subgraphs, to improve the predictions of GNNs. <ref type="bibr" target="#b36">Sankar et al. [2020]</ref> introduce motif-based regularization, a framework that improves multiple MPGNNs. A novel approach with strong empirical performance is <ref type="bibr">GINE+ Brossard et al. [2020]</ref>. It is based on GIN and aggregates information from higher-order neighborhoods, allowing it to detect small substructures such as cycles.  proposed DGN, which incorporates directional awareness into message passing.</p><p>A different way to learn on graph data is to use similarity measures on graphs with graph kernels <ref type="bibr" target="#b22">Kriege et al. [2020]</ref>. Graph kernels often count induced subgraphs such as graphlets, label sequences, or subtrees, which relates them conceptually to our approach. The graphlet kernel <ref type="bibr" target="#b38">Shervashidze et al. [2009]</ref> counts the occurrences of all 5-node (or more general k-node) subgraphs. The Weisfeiler-Leman kernel <ref type="bibr" target="#b39">Shervashidze et al. [2011]</ref> is based on iterated degree sequences and effectively counts occurrences of local subtrees. The Weisfeiler-Leman algorithm is the traditional yardstick for the expressiveness of GNN architectures.</p><p>A few previous approaches utilize either random walks or conventional CNNs in the context of end-to-end graph learning, two concepts our method is also based on. <ref type="bibr" target="#b32">Nikolentzos and Vazirgiannis [2020]</ref> propose a differentiable version of the random walk kernel and integrate it into a GNN architecture. In <ref type="bibr" target="#b12">Geerts [2020]</ref> the -walk MPGNN adds random walks directly as features to the nodes and connects the architecture theoretically to the 2-dimensional Weisfeiler-Leman algorithm. Patchy- <ref type="bibr">SAN Niepert et al. [2016]</ref> normalizes graphs in such a way that they can be interpreted by <ref type="bibr">CNN layers. Zhang et al. [2018]</ref> proposed a pooling strategy based on sorting intermediate node embeddings and presented DGCNN which applies a 1D CNN to the sorted embeddings of a graph. Recently, Yuan and Ji [2021] used a 1D CNN layer and attention for neighborhood aggregation to compute node embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>CRaWl processes random walks with convolutional neural networks. We initially sample a large enough set of (relatively long) random walks. Each CRaWl layer uses these walks to update a latent node embedding as follows. For each of the walks, the layer constructs features that contain the sequences of node and edge labels that occurred. Additionally, for every position in each walk, the features encode to which of its s predecessors the current node is identical or adjacent. The window size s is a hyperparameter of the model. These walk features are then processed by a 1D CNN. The output of the CNN is an embedding for every position in each random walk. These embeddings are pooled into the nodes, that is, for each node in the graph, we average over the embeddings of all f (v 1 ) 0000000 0000 000</p><formula xml:id="formula_0">f (v 2 ) g(v 1 , v 2 ) 0000 000 f (v 3 ) g(v 2 , v 3 ) 0000 100 f (v 4 ) g(v 3 , v 4 ) 0000 010 f (v 5 ) g(v 4 , v 5 ) 0000 001 f (v 6 ) g(v 5 , v 6 ) 0000 110 f (v 4 ) g(v 6 , v 4 ) 0010 101 . . . . . . . . . . . .                              </formula><p>n o d e l a b e l e d g e l a b e l i d e n t i t y c o n n e c t i v i t y <ref type="figure">Figure 1</ref>.: Example of the information flow in a CRaWl layer for a graph with 8 nodes.</p><formula xml:id="formula_1">1 2 3 4 5 6 c 1 c 2 c 3 c 4 c 5 . . .                     X(W, f, g, 4) random walk W = {v 1 , v 2 , v 3 , v 4 , v 5 , v 6 , v 4 , . . . } 1D CNN Pool Update</formula><p>We sample a walk W and compute the feature matrix X based on node embeddings f, edge embeddings g, and a window size of s = 4. To this matrix we apply a 1D CNN with receptive field r = 5 and pool the output into the nodes to update their embeddings.</p><p>the positions at which it occurs in the random walks. Finally, the CRaWl layer uses a simple MLP to produce the new node embedding from this information. In the full network, each layer uses the embedding produced by the preceding layer as node labels. After the last layer, the node embeddings can be pooled to perform graph level tasks. Effectively, through the CNN, we extract structural information from many small subgraphs of the size of the CNN's receptive field. Those subgraphs are always connected since they are induced from the nodes of a random walk.</p><p>The process of sampling random walks in a graph is not deterministic and therefore the final output of CRaWl is a random variable. However, the output of a trained CRaWl model has low variance such that the inherent randomization does not limit our method's usefulness in real world prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Random Walks</head><formula xml:id="formula_2">A walk of length ∈ N in a graph G = (V, E) is a sequence of nodes (v 0 , . . . , v ) ∈ V +1 with v i−1 v i ∈ E for all i ∈ [ ].</formula><p>That is, the walk has steps and therefore contains + 1 nodes. A random walk in a graph is obtained by starting at some initial node v 0 ∈ V and then iteratively sampling the next node v i+1 randomly from the neighbors N G (v i ) of the current node v i . We consider two different random walk strategies: uniform and non-backtracking. The uniform walks are obtained by sampling the next node uniformly from all neighbors:</p><formula xml:id="formula_3">v i+1 ∼ U N G (v i ) .</formula><p>On sparse graphs with nodes of small degree (such as molecules) this walk strategy has a tendency to backtrack often. This slows the traversal of the graph and interferes with the discovery of long-range patterns. The non-backtracking walk strategy addresses this issue by excluding the previous node from the sampling (unless the degree is one):</p><formula xml:id="formula_4">v i+1 ∼ D NB (v i ) D NB (v i ) = U N G (v i ) , if i = 0 ∨ deg(v i ) = 1 U N G (v i )\{v i−1 } , else.</formula><p>The choice of the walk strategy is a hyperparameter of CRaWl. In our experiments the non-backtracking strategy usually performs better as shown in Section 3.5.</p><p>Our method CRaWl initially samples m random walks Ω = {W 1 , . . . , W m } of length from the input graph G. The values for m and are not fixed hyperparameters of the model but instead can be chosen at runtime.</p><p>By default, we start one walk at every node, i.e., m = |V |. We noted that reducing the number of walks at train time can help against overfitting and of course is a way to reduce the memory footprint which is important for large graphs. If we choose to use less random walks, we sample m = p * · |V | starting nodes uniformly at random from the nodes of the graph with chosen probability p * . We typically choose ≥ 50, practically ensuring that each node appears multiple times in the walks. For evaluation, we choose a larger of up to 100 which improves the predictions.</p><p>While, in theory, every layer of CRaWl may use different random walks, we sample the random walks once in the beginning of a run and then make use of the same walks in every layer. This allows us to increase the number of random walks that each layer may work with as the total number of walks is bounded by the GPU memory. This empirically improves stability in training and also the overall performance.</p><p>We call contiguous segments W [i : j] := (w i , . . . , w j ) of a walk W = (w 0 , . . . , w ) walklets. The center of a walklet (w i , . . . , w j ) of even length j − i is the node w (i+j)/2 . For each walklet w = (w i , . . . , w j ), by G[w] we denote the subgraph induced by G on the set {w i , . . . , w j }. Note that G[w] contains all edges w k w k+1 for i ≤ k &lt; j, but may contain additional edges. Also note that the w k are not necessarily distinct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Walk Features</head><p>Based on the walks and a local window size s, we define feature vectors which can then be processed by 1D CNNs. Those feature vectors consist of four parts: one for node features, one for edge features along the walk, and the last two for local structural information. <ref type="figure">Figure 1</ref> depicts an example of a walk feature matrix and its use in a CRaWl layer.</p><p>Given a walk W ∈ V of length − 1 in a graph G = (V, E), a d-dimensional node embedding f : V → R d , a d -dimensional edge embedding g : E → R d , and a local window size s &gt; 0 we define the walk feature matrix X(W, f, g, s) ∈ R ×d X with feature</p><formula xml:id="formula_5">dimension d X = d + d + s + (s − 1) as X(W, f, g, s) = (f W g W I s W A s W ).</formula><p>For ease of notation, the first dimensions of the matrices f W , g W , I s W , A S W are indexed from 0 to −1. Here, f W ∈ R ×d is the node feature sequence defined as the concatenation of the node features along the walk</p><formula xml:id="formula_6">(f W ) i, = f (v i ).</formula><p>Analogously, g W ∈ R ×d is the edge feature sequence g W ∈ R ×d defined as the concatenation of edge features</p><formula xml:id="formula_7">(g W ) i, = 0, if i = 0 g(v i−1 v i ), else.</formula><p>We define the local identity relation I s W ∈ {0, 1} ×s and the local adjacency relation A s W ∈ {0, 1} ×(s−1) as:</p><formula xml:id="formula_8">(I s W ) i,j = 1, if i−j ≥ 0 ∧ v i = v i−j 0, else (A s W ) i,j = 1, if i−j ≥ 1 ∧ v i v i−j−1 ∈ E 0, else.</formula><p>Intuitively, I s W and A s W are binary matrices that contain one row for every node v i in the walk W . The bitstring for v i in I s W encodes which of the s predecessors of v i in W are identical to v i , that is, where the random walk looped or backtracked. Similarly, A s W stores to which of its predecessors v i has an edge in G. The direct predecessor v i−1 must share an edge with v i and is thus omitted in A s W . Note that we do not leverage edge labels of edges not on the walk, only the existence of such edges within the local window is encoded in A s W . For any walklet w = W [i : i + s], the restriction of the walk feature matrix to rows i, . . . , i + s contains a full description about the induced subgraphs G[w]. Hence when we apply a CNN with receptive field of size 2r + 1 ≤ s to the walk feature matrix, the CNN filter has full access to the subgraph induced by the walklet within its scope.</p><p>For the sampled set of walks Ω = {W 1 , . . . , W m } we define the walk feature tensor X(Ω, f, g, s) ∈ R m× ×d X as the stack of the individual feature matrices of each walk:</p><p>X(Ω, f, g, s) i = X(W i , f, g, s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">CRaWl Layer</head><p>A CRaWl network iteratively updates latent embeddings for each node. For a graph G = (V, E), let F V : V → R d V and F E : E → R d E be initial node and edge feature maps, respectively. The function h t : V → R dt stores the output of the t-th layer and the initial node features are stored in h 0 = F V . In principle, the size of the output node embedding d t is an independent hyperparameter for each layer. In practice, we use the same size d for the output node embeddings of all layers for simplicity.</p><p>The t-th layer of a CRaWl network constructs the walk feature tensor X t = X(Ω, h t−1 , F E , s) using h t−1 as its input node embedding and the graph's edge features F E . This walk feature tensor is then processed by a 1D CNN which we call CNN t . The first dimension of X t of size m is viewed as the batch dimension. The convolutional filters move along the second dimension (and therefore along each walk) while the third dimension contains the feature channels. The CNN stacks 2 trainable 1D Convolutions with Batch Normalization (BN) and ReLU activation and thus performs the steps</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRaWl Layer</head><formula xml:id="formula_9">X t = X(Ω, h t−1 , F E , s) h t−1 ∈ R |V |×d Ω ∈ V m× 1D CNN C t node pool MLP U t h t ∈ R |V |×d</formula><formula xml:id="formula_10">Conv1D → BN → ReLU → Conv1D → BN → ReLU.</formula><p>Each convolution has a receptive field of size r + 1 for r = s/2 and d ∈ N output channels. The combined receptive field of the two stacked convolutions is therefore 2r + 1. Hence, the output of the CNN module is a tensor C t ∈ R m×( −2r)×d . Note that the second dimension is − 2r instead of as no padding is used in the convolutions. Through its receptive field, the CNN operates on walklets of size 2r + 1 and produces embeddings for those. We pool those embeddings into the nodes of the graph by collecting for each node v ∈ V all embeddings of walklets centered at v. Formally, for a set of walks Ω = {W 1 , . . . , W m } with W i = {v i,1 , . . . , v i, } the positions where v occurs as center is given as</p><formula xml:id="formula_11">center(Ω, r, v) = {(i, j) | v i,j = v, i ∈ [m], r &lt; j &lt; −r}.</formula><p>With this set we can define the pooling operation as</p><formula xml:id="formula_12">p t (v) = mean (i,j)∈center(Ω,r,v) C t i,j−r where C t i,j−r ∈ R d is the embedding computed by the CNN for the walklet w = W i [j −r : j +r] centered at v i,j .</formula><p>An illustration of how the output of the CNN is pooled into the nodes of the graph can be found in <ref type="figure">Figure 1</ref>. The output of the pooling step is again a vector in R d for each v. This vector is then processed by a trainable MLP U t with a single hidden layer of dimension 2d to compute the next intermediate node embedding h t (v). Formally, the update procedure of a CRaWl layer is defined by</p><formula xml:id="formula_13">h t (v) = U t p t (v) .</formula><p>The upper part of <ref type="figure" target="#fig_1">Figure 2</ref> gives an overview over the elements of each CRaWl layer. As the input and output of a CRaWl layer consist of an embedding for each node, CRaWl can be combined with other graph learning techniques such as graph convolutions or virtual nodes <ref type="bibr" target="#b13">Gilmer et al. [2017]</ref>, <ref type="bibr" target="#b25">Li et al. [2017]</ref>, <ref type="bibr" target="#b17">Ishiguro et al. [2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Architecture</head><p>The architecture we use in the experiments, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (bottom), works as follows. The first step for running CRaWl is to compute a set of random walks Ω as described in Section 2.1. We then apply multiple CRaWl layers with residual connections. In each CRaWl layer, we typically choose s = 8 which results in two convolutions with a receptive field of r + 1 = 5. Thus, the CNN embeds walklets of size 9. After the final CRaWl layer, we apply batch normalization and a ReLU activation to the latent node embeddings before we perform a global pooling step. As pooling we use either sum-pooling or mean-pooling. Finally, a simple feedforward neural network is used to produce a graph-level output. In our experiments, we use either an MLP with one hidden layer of dimension d or a single linear layer. The output of this network can then be used in classification or regression tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Complexity Analysis</head><p>The runtime of each CRaWl layer is linear in the number of walk steps m · for the CNN and nodes |V | for the final MLP. In order to capture the graph structure by the random walks, we choose m · roughly linear in the number of edges which results in a total runtime of O(|E| + |V |). The initial generation of the random walks is in O(m · ). Both, the walk generation and the CNNs are highly parallelizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Implementation Details</head><p>We implemented CRaWl in PyTorch, a public repository is available at GitHub 1 . CRaWl requires the fast computation of random walks and their feature matrices. Therefore, we opted to implement an efficient random walk generator in PyTorch. Given a batch of graphs, all walks and their feature matrices are computed in parallel on the GPU. This also helps to limit data exchanges between CPU and GPU, since the walk feature tensors do not have to be moved from CPU to GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>Recently, two initiatives were launched by <ref type="bibr" target="#b10">Dwivedi et al. [2020]</ref> (Benchmarking GNNs) and <ref type="bibr" target="#b16">Hu et al. [2020]</ref> (Open Graph Benchmark, OGB) to improve the experimental standards used in graph learning research. Both projects aim to solve common problems of previous experimental settings. Those problems included varying training and evaluation protocols as well as the use of small datasets without standardized splits into training, validation, and test sets. This made the results hard to compare. Both projects introduced novel benchmark datasets with fixed splits and specified training and evaluation procedures. Here, we will use datasets from both projects to evaluate the empirical capabilities of CRaWl. In the appendix, we provide additional results for some of the formerly more common social datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We use two molecular property prediction datasets from the OGB Project: MOLHIV contains 40K molecules. It represents a binary classification task that aims to classify whether or not a substance impacts the replication of HIV. MOLPCBA is a larger dataset consisting of over 400K molecules. It is a multitask dataset with 128 binary targets. Each target states whether or not a molecule is active towards a particular bioassay (a method that quantifies the effect of a substance on a particular kind of living cells or tissues). Both datasets are adapted from the MoleculeNet <ref type="bibr" target="#b41">Wu et al. [2018]</ref> and represent molecules as graphs of atoms. They both contain multidimensional node and edge features which encode information such as atomic number and chirality. The two datasets provide a train/val/test split that separates structurally different types of molecules for a more realistic experimental setting.</p><p>From the other initiative, started by <ref type="bibr" target="#b10">Dwivedi et al. [2020]</ref>, we use 4 datasets. The first dataset ZINC is a molecular regression dataset. It is a subset of 12K molecules from the larger ZINC database. The aim is to predict the constrained solubility, an important chemical property of molecules. The node label is the atomic number and the edge labels specify the bond type. The datasets CIFAR10 and MNIST are graph datasets derived from the corresponding image classification tasks and contain 60K and 70K graphs, respectively. The original images are modeled as networks of super-pixels. Both datasets are 10-class classification problems. The last dataset CSL is a synthetic dataset containing 150 Cyclic Skip Link graphs <ref type="bibr" target="#b30">[Murphy et al., 2019]</ref>. Those are 4-regular graphs obtained by adding cords of a fixed length to a cycle. The formal definition and an example are provided in the appendix. The aim is to classify the graphs by their isomorphism class. Since all graphs are 4-regular and no node or edge features are provided, this task is unsolvable for most message passing architectures such as standard GNNs. <ref type="table">Table 1</ref>.: Performance achieved on ZINC, MNIST, CIFAR10, and CSL. For ZINC, we measure the Mean Absolute Error (MAE). For MNIST, CIFAR10, and CSL we measure the accuracy. "+VN" indicates the usage of a virtual node. "+Lap" indicates the usage of positional encodings based on Laplacian eigenvectors <ref type="bibr" target="#b10">Dwivedi et al. [2020]</ref> as additional node features. " * " indicates that no parameter budget was reported for the method. " †" indicates that a method uses a the smaller budget than our CRaWl network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Setting</head><p>We adopt the training procedure specified by <ref type="bibr" target="#b10">Dwivedi et al. [2020]</ref>. In particular, the learning rate is initialized as 10 −3 and decays with a factor of 0.5 if the performance on the validation set stagnates for 10 epochs. The training stops once the learning rate falls below 10 −6 . <ref type="bibr" target="#b10">Dwivedi et al. [2020]</ref> also specify that networks need to stay within parameter budgets of either 100K or 500K parameters. This ensures a fairer comparison between different methods. For ZINC, we use the larger budget of 500K parameters. For the other three datasets we build CRaWl models with the smaller budget of 100K. The OGB Project does not specify a standardized training procedure or parameter budgets. We use the same training procedure as for the previous datasets. For MOLPCBA, we reduce the patience of the learning rate decay to 5 to decrease training time, which we generally limit to 12 hours. We use the standard evaluators provided by OGB to measure the performance of the trained networks.</p><p>During training, we set the walk length to = 50, except for MOLPCBA, where we use = 40 to reduce the memory footprint. For evaluation, we select the walk length ∈ {50, 75, 100} that yields the highest performance on the validation data. This value is then used during testing. All hyperparameters and the exact number of trainable parameters are listed in the appendix. There, we also specify the sets of hyperparameters we searched for each dataset.</p><p>For each dataset, we train 5 models with different random seeds. In all tables, we report the mean performance and standard deviation across those 5 models. The output of each model depends on the sampled random walks. Thus, we evaluate each model on 10 different seeds used for the generation of random walks and take the average of those runs as the model's performance. Only on MOLPCBA we restrict the evaluation to 5 seeds for efficiency. In the appendix we provide extended results that additionally specify the internal model deviation, that is, the impact of the random walks on the performance. Since this internal model deviation is substantially lower than the differences between the models, they are comparatively insignificant when comparing CRaWl to other models.</p><p>In several experiments we extend the network with a virtual node (VN). Intuitively, the VN is a special node that is connected to every node in the graph and uses its own update and message generation functions. Adding such a virtual node is a common method to strengthen message passing GNNs on chemical prediction tasks. We implemented the VN as an intermediate layer that updates the latent embeddings of all nodes between any two CRaWl layers. A formal definition is given in the appendix. Note that it is not necessary to actually insert the VN into the graph and thus, it does not occur in the sampled random walks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Baselines</head><p>We compare the results obtained with CRaWl to a wide range of graph learning methods.</p><p>We report values that are currently listed on the leaderboard for the benchmark datasets as well as additional results from the literature that are not officially listed yet. Our main baselines are numerous message passing GNN architectures that have been proposed in recent years (see Section 1.1), as well as a few others, such as an MPNN implemented by . Where available, we report the results for models trained with FLAG <ref type="bibr" target="#b21">Kong et al. [2020]</ref> which was proposed to improve the training of GNNs with adversarial data augmentation. We also report results for non-neural graph embeddings combined with random forests that serve as a general baseline for GNNs. In particular, we consider WEGL <ref type="bibr" target="#b20">Kolouri et al. [2020]</ref> and Morgan Fingerprints <ref type="bibr" target="#b34">Rogers and Hahn [2010]</ref>.</p><p>For the CSL dataset we provide the accuracies achieved with positional encodings used as additional node features for the standard MPGNN baselines. These features are based on Laplacian eigenvectors, as suggested by <ref type="bibr" target="#b10">Dwivedi et al. [2020]</ref>, and provide structural information about the graph. Without these extra features, MPGNNs cannot distinguish the 4-regular CSL graphs and achieve at most 10% accuracy. We do not use these additional features for CRaWl networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results</head><p>Table 1 provides our results on ZINC, MNIST, CIFAR10, and CSL. On the ZINC dataset, CRaWl achieves an MAE of 0.101. This is approximately a 30% improvement over the current first place (PNA) of the leaderboard. When extending CRaWl with virtual node layers, the result improves even further to 0.088 MAE. On the MNIST dataset, CRaWl also achieves the highest accuracy but with only a small margin to PNA which is the next best approach. On CIFAR10, CRaWl achieves the fourth highest accuracy among the eleven compared approaches. On CSL, CRaWl achieves an accuracy of 100% which indicates that the task is comparatively easy for it. None of the 5 trained CRaWl models misclassified a single graph in the test folds. Standard MPGNNs can only achieve more then 10% accuracy when the additional positional encodings (+LAP) are provided. Even with these features, only GCN achieves 100% accuracy. 3WLGNN is theoretically capable of solving the task without additional node features. However, it does not achieve 100%, unlike CRaWl. <ref type="table" target="#tab_1">Table 2</ref> provides the performance achieved on the OGB datasets MOLHIV and MOLPCBA. Again, the provided baseline results are taken from the leaderboard of the OGB project. On MOLHIV, we observed significant issues with overfitting. CRaWl outperforms popular GNN architectures such as GIN and GCN, but only ranks 11-th on the current leaderboard. We point out that the current state-of-the-art for MOLHIV is Morgan Fingerprints, a handcrafted molecular fingerprint combined with a decision tree classifier. Therefore, deep learning methods seem to generally struggle on this comparatively small dataset.</p><p>On MOLPCBA, CRaWl yields state-of-the-art results. Without using a virtual node, our method places second on the leaderboard and is only outperformed by GINE+(+VN). When combining CRaWl with a virtual node, the performance increases further and our approach places first.</p><p>Overall, CRaWl performs very well on a variety of datasets across several domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Ablation Study</head><p>In an ablation study on the ZINC and CSL datasets, we evaluated the importance of the identity and adjacency encodings in the walk features and the effects of the two walk strategies. We observed that each individual encoding significantly improves the results and the combination of both yields the best performance. When looking at the walk strategies, non-backtracking walks perform as good or better than uniform walks for all tested node feature variants in the ablation study. On ZINC with the full walk features, both strategies yield virtually identical results. The detailed results are provided in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Expressiveness</head><p>In this section, we report on theoretical results comparing the expressiveness of CRaWl with that of other methods. The additional strength of CRaWl is mainly derived from the fact that it detects small subgraphs (of size determined by the window size hyperparameter s) and can sample such subgraphs from a non-uniform, but well-defined, distribution determined by the random walks. In this sense, it is similar to network analysis techniques based on motif detection <ref type="bibr" target="#b2">Alon [2007]</ref> and graph kernels based on counting subgraphs, such as the graphlet kernel <ref type="bibr" target="#b38">Shervashidze et al. [2009]</ref>.</p><p>The following results are concerned with the basic question which graphs can be distinguished by the various methods, assuming that optimal parameters for the models are available. They do not discuss how such parameters can be learned. This limits the scope of these results, but they still give useful intuition about the different approaches.</p><p>It is known that the expressiveness of GNNs corresponds exactly to that of the 1dimensional Weisfeiler-Leman algorithm (1-WL) <ref type="bibr" target="#b28">Morris et al. [2019]</ref>, Xu et al. <ref type="bibr">[2019]</ref>, in the sense that two graphs are distinguished by 1-WL if and only if they can be distinguished by a GNN. It is also known that higher-dimensional versions of WL characterize the expressiveness of higher-order GNNs <ref type="bibr" target="#b28">Morris et al. [2019]</ref>.</p><p>Theorem 1.</p><p>1. For every k ≥ 1 there are graphs that are distinguishable by CRaWl, but not by k-WL (and hence not by k-dimensional GNNs).</p><p>2. There are graphs that are distinguishable by 1-WL (and hence by GNNs), but not by CRaWl.</p><p>We state a precise quantitative version of the theorem and give a proof in Appendix D. Let us just note that for assertion (1) we need a window size s and walk length quadratic in k, and for <ref type="formula">(2)</ref> we can allow CRaWl to use a window size and path length linear in the size of the graphs. It can also be shown that CRaWl with a window size polynomial in the size of the graphlets is strictly more expressive than graphlet kernels. We omit the precise result, which can be proved similarly to Theorem 1(1), due to space limitations.</p><p>Let us finally remark that the expressiveness of GNNs can be considerably strengthened by adding a random node initialization <ref type="bibr" target="#b0">Abboud et al. [2020]</ref>, <ref type="bibr" target="#b37">Sato et al. [2020]</ref>. The same can be done for CRaWl, but so far the need for such a strengthening (at the cost of a higher variance) did not arise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced a novel neural network architecture CRaWl for graph learning that is based on random walks and 1D CNNs. Thus, CRaWl is fundamentally different from standard graph neural networks. We demonstrated that this approach works very well across a variety of graph level tasks and is able to outperform state-of-the-art GNNs. By construction, CRaWl can detect arbitrary substructures up to the size of its local window. In particular, on the regular graphs of CSL where pure MPGNNs fail because of the lack of expressiveness, CRaWl is able to extract useful features and solve this task.</p><p>Future work includes extending the experimental framework to node-level tasks and to motif counting. In both cases, one needs to scale CRaWl to work on individual large graphs instead of many medium sized ones. On those large graphs the current implementation struggles to stay within the available RAM of most GPUs. We expect the quality of CRaWl's predictions in those settings to be at least comparable to that of message passing based approaches.</p><p>CRaWl can be viewed as an attempt to process random walks and the structures they induce with end-to-end neural networks. The strong empirical performance demonstrates the potential of this general approach. However, many variations remain to be explored, including different walk strategies, variations in the walk features, and alternative pooling functions for pooling walklet embeddings into nodes or edges. In view of the incomparability of the expressiveness of GNNs and CRaWl, hybrid approaches that interleave CRaWl layers and GNN layers seem attractive as well. Beyond plain 1D-CNNs, other deep learning architectures for sequential data, such as transformer networks, could be used to process random walks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extended Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Additional Experiments</head><p>In this section we evaluate CRaWl on commonly used benchmark datasets from the domain of social networks. We use a subset from the TUDataset <ref type="bibr" target="#b22">[Morris et al., 2020]</ref>, a list of typically small graph datasets from different domains e.g. chemistry, bioinformatics, and social networks. We focus on three datasets originally proposed by Yanardag and Vishwanathan <ref type="bibr">[2015]</ref>: COLLAB, a scientific collaboration dataset, IMDB-MULTI, a multiclass dataset of movie collaboration of actors/actresses, and REDDIT-BIN, a balanced binary classification dataset of Reddit users which discussed together in a thread. These datasets do not have any node or edge features and the tasks have to be solved purely with the structure of the graphs. We stick to the experimental protocol suggested by Xu et al. <ref type="bibr">[2019]</ref>. Specifically, we perform a 10-fold cross validation. Each dataset is split into 10 stratified folds. We perform 10 training runs where each split is used as test data once, while the remaining 9 are used for training. We then select the epoch with the highest mean test accuracy across all 10 runs. We report this mean test accuracy as the final result. This is not the most realistic setup for simulating real world tasks, since there is no clean split between validation and test data. But in fact, it is the most commonly used experimental setup for these datasets and is mainly justified by the comparatively small number of graphs. Therefore, we adopt the same procedure for the sake of comparability to the previous literature. For COLLAB and IMDB-MULTI we use the same 10-fold split used by Zhang et al. <ref type="bibr">[2018]</ref>. For REDDIT-BIN we computed our own stratified splits. We also computed separate stratified 10-fold splits for hyperparameter tuning.</p><p>We adapt the training procedure of CRaWl towards this setup. Here, the learning rate decays with a factor of 0.5 in fixed intervals. These intervals are chosen to be 20 epochs on COLLAB and REDDIT-BINARY and as 50 epochs on IMDB-MULTI. We train for 200 epochs on COLLAB and REDDIT-BINARY and for 500 epochs on IMDB-MULTI. This ensures a consistent learning rate profile across all 10 runs for each dataset. <ref type="table" target="#tab_4">Table 3</ref> reports the achieved accuracy of CRaWl and several key baselines. For the baselines, we provide the results as reported in the literature. For comparability, we only report values for baselines with the same experimental protocol. On IMDB-MULTI, the smallest of the three datasets, CRaWl yields a slightly lower accuracy then most baselines. On COLLAB, our method performs similarly to standard MPGNN architectures, such as GIN. CRaWl outperforms all baselines that report values for REDDIT-BIN. Note that GSN, the method with the best results on COLLAB and IMDB-MULTI, does not scale as well as CRaWl and is infeasible for REDDIT-BIN.   <ref type="bibr" target="#b39">[Shervashidze et al., 2011]</ref> 78.9 ± 1.9 50.9 ± 3.8 81.0 ± 3.1 WEGL <ref type="bibr" target="#b20">[Kolouri et al., 2020]</ref> 79.8 ± 1.5 52.0 ± 4.1 92.0 ± 0.8 GNTK <ref type="bibr" target="#b9">[Du et al., 2019]</ref>  Recall that the output of CRaWl is a random variable. The predictions for a given input graph may vary when different random walks are sampled. To quantify this additional source of randomness, we measure two deviations for each experiment: The cross model deviation (CMD) and the internal model deviation (IMD). For clarity, let us define these terms formally. For each experiment, we perform q ∈ N training runs with different random seeds. Let m i be the model obtained in the i-th training run with i ∈ [q]. When evaluating (both on test and validation data), we evaluate each model r ∈ N times, with different random walks in each evaluation run. Let p i,j ∈ R measure the performance achieved by the model m i in its j-th evaluation run. Note that the unit of p i,j varies between experiments (Accuracy, MAE, . . . ). We formally define the internal model deviation as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Detailed Results for all Experiments</head><formula xml:id="formula_14">IMD = 1 q · 1≤i≤q STD ({p i,j | 1 ≤ j ≤ r}) ,</formula><p>where STD(·) is the standard deviation of a given distribution. Intuitively, the IMD measures how much the performance of a trained model varies when applying it multiple times to the same input. It quantifies how the model performance depends on the random walks that are sampled during evaluation. We formally define the cross model deviation as</p><formula xml:id="formula_15">CMD = STD      1 r · 1≤j≤r p i,j | 1 ≤ i ≤ q      .</formula><p>The CMD measures the deviation of the average model performance between different training runs. It therefore quantifies how the model performance depends on the random initialization of the network parameters before training.</p><p>In the main section, we only reported the CMD for simplicity. Note that the CMD is significantly larger then the IMD across all experiments. Therefore, trained CRaWl models can reliably produce high quality predictions, despite their dependence on randomly sampled walks.  <ref type="table" target="#tab_7">Table 5</ref> provides the hyperparameters used in each experiment. These consist of the following:</p><p>• The number of layers L. We tried out L ∈ {2, 3, 4} on all datasets, except for MOLPCBA, where we searched through L ∈ {4, 5, 6}.</p><p>• The latent state size d. On ZINC, CIFAR10, MNIST and CSL we chose sizes that would roughly use the chosen parameter budgets. On MOLHIV, COLLAB, IMDB-MULTI and REDDIT-BIN we searched through d ∈ {32, 64} and for MOLPCBA we chose the largest feasible size for our hardware (d = 250).</p><p>• The global pooling function (either mean or sum)</p><p>• The architecture of the final output network (either mlp or linear )</p><p>• The number of random walk steps during training ( train ) and evaluation ( eval ). By default we set train = 50. Only on MOLPCBA we set train = 40 to stay within GPU memory limits. We searched through eval ∈ {50, 75, 100} on the validation data of all datasets.</p><p>• The probability of starting a walk from each node during training p * . We choose p * = 1 by default. On MOLPCBA we set p * = 0.25 to stay within GPU memory limits. On MOLHIV we searched through p * ∈ {0.1, 0.25, 0.5, 1} to reduce overfitting.</p><p>• The walk strategy (either uniform (un) or non-backtracking (nb))  <ref type="table" target="#tab_1">ZINC  CIFAR10  MNIST  CSL  MOLHIV  MOLPCBA  COLLAB  IMDB-M  REDDIT-B   L  3  3  3  2  3  5  3  3  3  d  100  50  50  64  64  250  64  64  64  pool  sum  mean  mean  mean  mean  mean  mean  mean  mean  out  mlp  mlp  mlp  mlp  linear  linear  mlp  mlp</ref>  • The number of evaluation runs with different random walks for validation (r val ) and testing (r test ).</p><p>• The dropout rate. We searched through {0.0, 0.5}. A dropout layer is placed behind the hidden layer of the update MLP U t in each CRaWl layer.</p><p>• The initial learning rate lr was chosen as 0.001 in all experiments.</p><p>• The patience for learning rate decay (with a factor of 0.5) is 10 by default. It is set to 5 on MOLPCBA to decrease runtime. For CSL it is set to 20 due to the small dataset size.</p><p>• For the batch size we searched through {50, 75, 100} for most datasets. We chose a batch size of 120 for CSL, which combines all of the training data into one batch. <ref type="table" target="#tab_9">Table 6</ref> provides the number of trainable parameters in each model. Additionally, we report the runtime observed during training. All experiments were run on a machine with 32GB RAM, an Intel Xeon 8160 CPU and an Nvidia Tesla V100 GPU with 16GB GPU memory. Note that we enforced a time limit of 12 hours when training on MOLPCBA. However, several models finished training in under 12 hours, hence, the mean runtime on MOLPCBA is less than this limit. <ref type="bibr" target="#b13">Gilmer et al. [2017]</ref>, <ref type="bibr" target="#b25">Li et al. [2017]</ref>, and <ref type="bibr" target="#b17">Ishiguro et al. [2019]</ref> suggested the use of a virtual node to enhance GNNs for chemical datasets. Intuitively, a special node is inserted into the graph that is connected to all other nodes. This node aggregates the states of all other nodes and uses this information to update its own state. The virtual node has its own distinct update function, that is not shared by other nodes. The updated state is then sent back to all nodes in the graph. Effectively, a virtual node allows global information flow after each layer. Formally, a virtual node updates a latent state h t vn ∈ R d , where h t vn is computed after the t-th layer and h 0 vn is initialized as a zero vector. The update procedure is defined by:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Model Size and Runtime</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Virtual Node</head><formula xml:id="formula_16">h t vn = U t vn h t−1 vn + v∈V h t (v) h t (v) = h t (v) + h t vn .</formula><p>Here, U t vn is a trainable MLP and h t is the latent node embedding computed by the t-th CRaWl layer.h t is an updated node embedding that is used as the input for the next CRaWl layer instead of h t . In our experiments, we choose U t vn to contain a single hidden layer of dimension d. When using a virtual node, we perform this update step after every CRaWl layer, except for the last one.</p><p>Note that we view the virtual node as an intermediate update step that is placed between our CRaWl layers to allow for global communication between nodes. No additional node is actually added to the graph and, most importantly, the "virtual node" does not occur in the random walks sampled by CRaWl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Cross Validation on CSL</head><p>Let us briefly discuss the experimental protocol used for the CSL dataset. Unlike the other benchmark datasets provided by <ref type="bibr" target="#b10">Dwivedi et al. [2020]</ref>, CSL is evaluated with 5-fold  <ref type="bibr" target="#b30">Murphy et al., 2019]</ref> with 11 nodes and a skip distance of 2 and 3 respectively. cross-validation. We use the 5-fold split <ref type="bibr" target="#b10">Dwivedi et al. [2020]</ref> provide in their repository. In each training run, three folds are used for training and one is used for validation. After training, the remaining fold is used for testing.</p><p>Finally, <ref type="figure" target="#fig_2">Figure 3</ref> provides an example of two skip-link graphs. The task of CSL is to classify such graphs by their isomorphism class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>We perform an ablation study to understand how the key aspects of CRaWl influence the empirical performance. We aim to answer two main questions:</p><p>• How useful are the identity and adjacency features we construct for the walks?</p><p>• How do different strategies for sampling random walks impact the performance?</p><p>Here, we use the ZINC and CSL datasets to answer these questions empirically. We trained multiple versions of CRaWl with varying amounts of structural features used in the walk feature matrices. The simplest version only uses the sequences of node and edge features without any structural information. We then trained versions that add either the adjacency encoding or the identity encoding. Finally, we measure the performance of the standard CRaWl architecture, where both encodings are incorporated into the walk feature matrices. For each version, we measure the performance with both walk strategies.</p><p>On both datasets, the experimental setup and hyperparameters are identical to those used in the previous experiments on both datasets. In particular, we train five models with different seeds and provide the average performance as well as the standard deviation across models. Note that we repeat the experiment independently for each walk strategy. Switching strategies between training and evaluation does not yield good results.  <ref type="table" target="#tab_10">Table 7</ref> reports the performance of each studied version of CRaWl. On ZINC, the networks without any structural encoding yield the worst predictions. Adding either the adjacency or the identity encoding improves the results substantially. The best results are obtained when both encodings are utilized. Curiously, the uniform walks actually yield marginally better results on the test split when all structural features are considered. On the validation split, the non-backtracking strategy performs slightly better, which is why it was used in our main experiments.</p><formula xml:id="formula_17">F V + F E NB 0.17655 ± 0.00303 0.12000 ± 0.08055 F V + F E UN 0.25827 ± 0.02115 0.12000 ± 0.08055 F V + F E + A NB 0.10636 ± 0.00326 1.00000 ± 0.00000 F V + F E + A UN 0.11053 ± 0.00417 0.98000 ± 0.02357 F V + F E + I NB 0.11094 ± 0.00398 0.96267 ± 0.02175 F V + F E + I UN 0.13297 ± 0.00769 0.75200 ± 0.03284 F V + F E + I + A NB 0.10083 ± 0.00299 1.00000 ± 0.00000 F V + F E + I + A UN 0.09932 ± 0.00402 0.96533 ± 0.02050</formula><p>On CSL, the adjacency feature is sufficient to reach 100%, but adding the identity feature does not decrease this performance. The non-backtracking walks perform consistently better than the uniform walks, which did not achieve 100% on this dataset. Finally, we point out that the network with no structural features should not be able to achieve more than 10% on CSL if the labels in the folds were balanced. However, we noticed that the splits provided by <ref type="bibr" target="#b10">Dwivedi et al. [2020]</ref> are not stratified and a network can achieve a mean accuracy of roughly 12% through random guessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Theory</head><p>We need some additional notation. Throughout the paper, graphs are undirected and simple (that is, without self-loops and parallel edges). <ref type="bibr" target="#b42">2</ref> In this appendix, all graphs will be unlabeled, and we assume that they have no isolated nodes, which enables us to start a random walk from every node. We denote the edge set of a graph G by V (G) and the node set by E(G). The order |G| of G is the number of nodes, that is, |G| := |V (G)|. For a set X ⊆ V (G), the induced subgraph G[X] is the graph with node set X and edge set {vw ∈ E(G) | v, w ∈ X}. A walk of length in G is a sequence W = (w 0 , . . . , w ) ∈ V (G) +1 such that w i−1 w i ∈ E(G) for 1 ≤ i ≤ . The walk is non-backtracking if for 1 &lt; i &lt; we have w i+1 = w i−1 unless the degree of vertex w i is 1.</p><p>Before we prove the theorem, let us precisely specify what it means that CRaWl distinguishes two graphs. Recall that CRaWl has three (walk related) hyperparameters:</p><p>• the window size s;</p><p>• the walk length ;</p><p>• the samples size m.</p><p>Recall furthermore that with every walk W = (w 0 , . . . , w ) we associate a walk feature matrix X ∈ R ( +1)×(d+d +s+(s−1)) . For 0 ≤ i ≤ , the first d entries of the i-th row of X describe the label of the node w i , the next d entries the label of the edge w i−1 w i (0 for i = 0), the following s entries are indicators for the equalities between w i and the nodes w i−j for j = 1, . . . , s (1 if w i = w i−j , 0 if i − j &lt; 0 or w i = w i−j ), and the remaining s − 1 entries are indicators for the adjacencies between w i and the nodes w i−j for j = 2, . . . , s (1 if w i , w i−j are adjacent in G, 0 if i − j &lt; 0 or w i , w i−j are non-adjacent; note that w i , w i−1 are always be adjacent because W is a walk in G). In this section, we always consider unlabeled graphs and therefore obtain walk feature matrices X ∈ {0, 1} ( +1)×(2s−1) . We denote the entries of the matrix X by X i,j and the rows by X i,− . So X i,− = (X i,1 , . . . , X i,2s−1 ) ∈ {0, 1} 2s−1 . We denote the walk feature matrix of a walk W by X(W ). It is immediate from the definitions that for walks W = (w 1 , . . . , w ), W = (w 1 , . . . , w ) in graphs G, G with feature matrices X := X(W ), X := X(W ), we have:</p><p>1. if X i−j,− = X i−j,− for j = 0, . . . , s−1 then the mapping w i−j → w i−j for j = 0, . . . , s is an isomorphism from the induced subgraph G[{w i−j | j = 0, . . . , s}] to the induced subgraph G [{w i−j | j = 0, . . . , s}];</p><p>2. if the mapping w i−j → w i−j for j = 0, . . . , 2s − 1 is an isomorphism from the induced subgraph G[{w i−j | j = 0, . . . , 2s − 1}] to the induced subgraph G [{w i−j | j = 0, . . . , 2s − 1}], then X i−j,− = X i−j,− for j = 0, . . . , s − 1.</p><p>The reason that we need to include the vertices w i−2s+1 , . . . , w i−s and w i−2s+1 , . . . , w i−s into the subgraphs in <ref type="formula">(2)</ref> is that row X i−s+1,− of the feature matrix records edges and equalities between w i−s+1 and w i−2s+1 , . . . , w i−s . For every graph G we denote the distribution of random walks on G starting from a node chosen uniformly at random by W(G) and W nb (G) for the non-backtracking walks. We let X (G) and X nb (G)) be the push-forward distributions on {0, 1} ( +1)×(2s−1) , that is, for every X ∈ {0, 1} ( +1)×(2s−1) we let</p><formula xml:id="formula_18">Pr X (G) (X) = Pr W(G) {W | X(W ) = X}</formula><p>A CRaWl run on G takes m samples from X (G). So to distinguish two graphs G, G , CRaWl must detect that the distributions X (G), X (G ) are distinct using m samples.</p><p>As a warm-up, let us prove the following simple result.</p><p>Theorem 2. Let G be a cycle of length n and G the disjoint union of two cycles of length n/2. Then G and G cannot be distinguished by CRaWl with window size s &lt; n/2 (for any choice of parameters and m).</p><p>Proof. With a window size smaller than the length of the shortest cycle, the graph CRaWl sees in its window is always a path. Thus for every walk W in either G or G the feature matrix X(W ) only depends on the backtracking pattern of W . This means that X (G) = X (G ).</p><p>It is worth noting that the graphs G, G of Theorem 2 can be distinguished by 2-WL (the 2-dimensional Weisfeiler-Leman algorithm), but not by 1-WL.</p><p>Proving that two graphs G, G have identical feature-matrix distributions X (G) = X (G ) is the ultimate way of proving that they are not distinguishable by CRaWl. Yet for more interesting graphs, we rarely have identical feature-matrix distributions. However, if the distributions are sufficiently close we will still not be able to distinguish them. To quantify closeness, we use the total variation distance of the distributions. Recall that the total variation distance between two probability distributions D, D on the same finite sample space Ω is</p><formula xml:id="formula_19">dist T V (D, D ) := max S⊆Ω | Pr D (S) − Pr D (S)|.</formula><p>It is known that the total variation distance is half the 1 -distance between the distributions, that is,</p><formula xml:id="formula_20">dist T V (D, D ) = 1 2 D − D 1 = 1 2 ω∈Ω | Pr D ({ω}) − Pr D ({ω})|.</formula><p>Let ε &gt; 0. We say that two graphs G, G are ε-indistinguishable by CRaWl with window size s, walk length , and sample size m if dist T V (X (G), X (G )) &lt; ε m .</p><p>(1)</p><p>The rationale behind this definition is that if dist T V (X (G), X (G )) &lt; ε m then for every property of feature matrices that CRaWl may want to use to distinguish the graphs, the expected numbers of samples with this property that CRaWl sees in both graphs are close together (assuming ε is small).</p><p>Often, we want to make asymptotic statements, where we have two families of graphs (G n ) n≥1 and (G n ) n≥1 , typically of order |G n | = |G n | = Θ(n), and classes S, L, M of functions, such as the class O(log n) of logarithmic or the class n O(1) of polynomial functions. We say that (G n ) n≥1 and (G n ) n≥1 are indistinguishable by CRaWl with window size S, walk length L, and sample size M if for all ε &gt; 0 and all s ∈ S, ∈ L, m ∈ M there is an n such that G n , G n are ε-indistinguishable by CRaWl with window size s(n), walk length (n), and sample size m(n).</p><p>We could make similar definitions for distinguishability, but we omit them here and deal with distinguishability in an ad-hoc fashion (in the following subsection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Proof of Theorem 1(1)</head><p>Here is a precise quantitative version of the theorem.</p><p>Theorem 3. For all k ≥ 1 there are families of graphs (G n ) n≥1 , (G n ) n≥1 of order |G n | = |G n | = n + O(k) that are distinguishable by CRaWl with window size s = O(k 2 ), walk length s = O(k 2 ), and samples size m = O(n), but not by k-WL (and hence not by k-dimensional GNNs).</p><p>Fortunately, to prove this theorem we do not need to know any details about the Weisfeiler-Leman algorithm (the interested reader is deferred to <ref type="bibr" target="#b18">Kiefer [2020]</ref>). We can use the following well-known inexpressibility result as a black box.</p><p>Theorem 4 <ref type="bibr" target="#b7">(Cai et al. [1992]</ref>). For all k ≥ 1 there are graphs H k , H k such that |H k | = |H k | = O(k), the graphs H k and H k are 3-regular, and k-WL cannot distinguish H k and H k .</p><p>It is a well-known fact that the cover time of a connected graph of order n with m edges, that is, the expected time it takes a random walk starting from a random node to visit all nodes of the graph, is bounded from above by 4nm <ref type="bibr" target="#b1">Aleliunas et al. [1979]</ref>. By Markov's inequality, a path of length 8nm visits all nodes with probability at least 1/2. Sampling several such paths, we can bring the success probability arbitrarily close to 1.</p><p>Proof of Theorem 1(1). Let k ≥ 1 and let H k , H k be the graphs obtained from the Theorem 4. Let n k := |H k |. For every n ≥ 1, we let G n be the disjoint union of H k with a path of length n, and let G n be defined in the same way from H k . Then |G n | = |G n | = n k + n + 1.</p><p>Let m k = 3 2 n k be the number of edges of the 3-regular graphs H k and H k , and let s := 8n k m k = O(k 2 ). This will be our window size, and in fact also our walk length: := s. Let ε &gt; 0. We choose a sufficiently large m = m(n) ∈ O(n) to make sure that a sample of m nodes from V (G n ) or V (G n ) contains sufficiently many nodes in the subset V (H k ) ⊆ V (G k ) resp. V (H k ) ⊆ V (G k ). Then, if we sample m paths of length from W(G n ), with probability at least 1 − ε, one of these paths covers V (H k ). This means that m random walks of length will detect the subgraphs H k , and as the window size s is equal to , these subgraphs will appear in the feature matrix. Since the subgraph H k does not appear as a subgraph of G n , this means that with probability at least 1 − ε, CRaWl can distinguish the two graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Proof of Theorem 1(2)</head><p>To prove the second part of the theorem, it will be necessary to briefly review the 1dimensional Weisfeiler-Leman algorithm (1-WL), which is also known as color refinement and as naive node classification. The algorithm iteratively computes a partition of the nodes of its input graph. It is convenient to think of the classes of the partition as colors of the nodes. Initially, all nodes have the same color. Then in each iteration step, for all colors c in the current coloring and all nodes v, w of color c, the nodes v and w get G 3 G 3 <ref type="figure">Figure 4</ref>.: The graphs G 3 and G 3 in the proof of Theorem 5 with their stable coloring computed by 1-WL different colors in the new coloring if there is some color d such that v and w have different numbers of neighbors of color d. This refinement process is repeated until the coloring is stable, that is, any two nodes v, w of the same color c have the same number of neighbors of any color d. We say that 1-WL distinguishes two graphs G, G if, after running the algorithm on the disjoint union G G of the two graphs, in the stable coloring of G G there is a color c such that G and G have a different number of nodes of color c.</p><p>For the results so far, it has not mattered if we allowed backtracking or not. Here, it makes a big difference. For the non-backtracking version, we obtain a stronger result with an easier proof. The following theorem is a precise quantitative statement of Theorem 1(2).</p><p>Theorem 5. There are families (G n ) n≥1 , (G n ) n≥1 of graphs of order |G n | = |G n | = 3n−1 with the following properties. size n O(1) . The reason is that by going back and forth between a node and all its neighbors within its window, CRaWl can distinguish the two degree-3 nodes x, y from the remaining degree-2 nodes. Thus, the feature matrix reflects traversal times between degree-3 nodes, and the distribution of traversal times is different in G n and G n . With sufficiently many samples, CRaWl can detect this.</p><p>So, let us turn to the proof that with walks of linear length this is not possible, that is, assertion <ref type="bibr" target="#b43">(3)</ref>. The reason for this is simple: random walks of length O(n) are very unlikely to traverse a path of length at least n − 1 from x to y. It is well known that the expected traversal time is Θ(n 2 ) (this follows from the analysis of the gambler's ruin problem). However, this does not suffice for us, we need to bound the probability that a path of length O(n) is a traversal. Using a standard, Chernoff type tail bound, it is straightforward to prove that for every constant c ≥ 0 there is a constant d ≥ 1 such that the probability that a random walk of length cn in either G n or G n visits both x and y is at most exp(−n/d). As only walks visiting both x and y can differentiate between the two graphs, this gives us an upper bound of exp(−n/d) for the total variation distance between X (G n ) and X (G n ).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>: Top: Update procedure of latent node embeddings h t in a CRaWl layer. Ω is a set of random walks. Bottom: Architecture of a 3-layer CRaWl network as used in the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>: Two cyclic skip-link graphs [see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>: Results achieved on MOLHIV and MOLPCBA. "+VN" indicates the usage of a virtual node. "+F" indicates the usage of FLAG<ref type="bibr" target="#b21">Kong et al. [2020]</ref>.</figDesc><table><row><cell>Method</cell><cell>MOLHIV</cell><cell>MOLPCBA</cell></row><row><cell></cell><cell>Test ROC-AUC</cell><cell>Test AP</cell></row><row><cell>MFP WEGL GCN GCN+VN GCN+VN+F GIN GIN+VN GIN+VN+F DGCN DGCN+VN DGCN+F DGCN+VN+F GSN HIMP DGN PNA GINE+(+VN)</cell><cell cols="2">0.8060 ± 0.0010 0.2054 ± 0.0004 -0.7757 ± 0.0111 0.7606 ± 0.0097 0.2020 ± 0.0024 0.7599 ± 0.0119 0.2424 ± 0.0034 -0.2483 ± 0.0037 0.7558 ± 0.0140 0.2266 ± 0.0028 0.7707 ± 0.0149 0.2703 ± 0.0023 0.7748 ± 0.0096 0.2834 ± 0.0038 0.7858 ± 0.0117 0.2745 ± 0.0025 -0.2781 ± 0.0038 -0.7942 ± 0.0120 -0.2842 ± 0.0043 -0.7799 ± 0.0100 -0.7880 ± 0.0082 -0.7970 ± 0.0097 -0.7905 ± 0.0132 0.7880 ± 0.0080 0.2917 ± 0.0015</cell></row><row><cell>CRaWl CRaWl + VN</cell><cell cols="2">0.7696 ± 0.0167 0.2866 ± 0.0016 0.7737 ± 0.0099 0.2923 ± 0.0024</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Z.Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 2020. K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks?</figDesc><table><row><cell>In Proceedings of the Seventh International Conference on Learning Representations</cell></row><row><cell>(ICLR), 2019.</cell></row><row><cell>P. Yanardag and S. Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM</cell></row><row><cell>SIGKDD international conference on knowledge discovery and data mining, pages</cell></row><row><cell>1365-1374, 2015.</cell></row><row><cell>H. Yuan and S. Ji. Node2seq: Towards trainable convolutions in graph neural networks.</cell></row><row><cell>arXiv preprint arXiv:2101.01849, 2021.</cell></row><row><cell>M. Zhang, Z. Cui, M. Neumann, and Y. Chen. An end-to-end deep learning architecture</cell></row><row><cell>for graph classification. In AAAI, pages 4438-4445, 2018.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>provides the full results from our experimental evaluation. It reports the performance on the train, validation and test data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>: Accuracy on Social Datasets.</figDesc><table><row><cell>Method</cell><cell>COLLAB</cell><cell>IMDB-MULTI</cell><cell>REDDIT-BIN</cell></row><row><cell></cell><cell>Test Acc</cell><cell>Test Acc</cell><cell>Test Acc</cell></row><row><cell>WL-Kernel</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>: Extended results for CRaWl on all datasets. Note that different metrics are used to measure the performance on the datasets. "+VN" indicates models with an additional virtual node. For each experiment we provide the cross model deviation (CMD) and the internal model deviation (IMD).</figDesc><table><row><cell>Dataset / Model</cell><cell>Metric</cell><cell></cell><cell>Test</cell><cell></cell><cell></cell><cell>Validation</cell><cell></cell><cell></cell><cell>Train</cell></row><row><cell></cell><cell></cell><cell>Score</cell><cell>CMD</cell><cell>IMD</cell><cell>Score</cell><cell>CMD</cell><cell>IMD</cell><cell>Score</cell><cell>CMD</cell></row><row><cell>ZINC</cell><cell>MAE</cell><cell>0.10083</cell><cell>±0.00299</cell><cell>±0.00157</cell><cell>0.12619</cell><cell>±0.00166</cell><cell>±0.00113</cell><cell>0.04818</cell><cell>±0.00439</cell></row><row><cell>ZINC+VN</cell><cell>MAE</cell><cell>0.08762</cell><cell>±0.00326</cell><cell>±0.00125</cell><cell>0.11362</cell><cell>±0.00433</cell><cell>±0.00107</cell><cell>0.05743</cell><cell>±0.00539</cell></row><row><cell>CIFAR10</cell><cell>Acc.</cell><cell>0.68106</cell><cell>±0.00340</cell><cell>±0.00147</cell><cell>0.68961</cell><cell>±0.00523</cell><cell>±0.00230</cell><cell>0.79197</cell><cell>±0.00913</cell></row><row><cell>MNIST</cell><cell>Acc.</cell><cell>0.97978</cell><cell>±0.00079</cell><cell>±0.00070</cell><cell>0.98054</cell><cell>±0.00086</cell><cell>±0.00068</cell><cell>0.98680</cell><cell>±0.00198</cell></row><row><cell>CSL</cell><cell>Acc.</cell><cell>1.00000</cell><cell>±0.00000</cell><cell>±0.00000</cell><cell>1.00000</cell><cell>±0.00000</cell><cell>±0.00000</cell><cell>1.00000</cell><cell>±0.00000</cell></row><row><cell>MOLHIV</cell><cell>ROC-AUC</cell><cell>0.76956</cell><cell>±0.01669</cell><cell>±0.00175</cell><cell>0.81737</cell><cell>±0.01592</cell><cell>±0.00196</cell><cell>0.86264</cell><cell>±0.04626</cell></row><row><cell>MOLHIV+VN</cell><cell>ROC-AUC</cell><cell>0.77376</cell><cell>±0.00990</cell><cell>±0.00195</cell><cell>0.82189</cell><cell>±0.00434</cell><cell>±0.00134</cell><cell>0.86840</cell><cell>±0.03913</cell></row><row><cell>MOLPCBA</cell><cell>AP</cell><cell>0.28662</cell><cell>±0.00163</cell><cell>±0.00062</cell><cell>0.29310</cell><cell>±0.00157</cell><cell>±0.00056</cell><cell>0.50987</cell><cell>±0.01113</cell></row><row><cell>MOLPCBA+VN</cell><cell>AP</cell><cell>0.29230</cell><cell>±0.00235</cell><cell>±0.00043</cell><cell>0.29647</cell><cell>±0.00413</cell><cell>±0.00056</cell><cell>0.51144</cell><cell>±0.00999</cell></row><row><cell>COLLAB</cell><cell>Acc.</cell><cell>0.80580</cell><cell>±0.01481</cell><cell>±0.00670</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.82289</cell><cell>±0.00267</cell></row><row><cell>IMDB-MULTI</cell><cell>Acc.</cell><cell>0.48907</cell><cell>±0.02715</cell><cell>±0.01290</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.50030</cell><cell>±0.00730</cell></row><row><cell>REDDIT-BIN</cell><cell>Acc.</cell><cell>0.93150</cell><cell>±0.01210</cell><cell>±0.00300</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.93894</cell><cell>±0.00590</cell></row><row><cell cols="4">B. Model and Setup Details</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">B.1. Hyperparameters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>: Configurations used for each dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>: Number of parameters and runtime for each model in our experiments. The reported times are averaged over all training runs and include the time used to perform a validation run after each training epoch.</figDesc><table><row><cell>Model</cell><cell>#Param.</cell><cell>Time/Epoch (MM:SS.00)</cell><cell>Train Time (HH:MM:SS)</cell></row><row><cell>ZINC</cell><cell>423,901</cell><cell>00:20.66</cell><cell>01:21:44</cell></row><row><cell>ZINC + VN</cell><cell>464,301</cell><cell>00:26.01</cell><cell>01:36:46</cell></row><row><cell>CIFAR10</cell><cell>109,760</cell><cell>01:58.69</cell><cell>04:56:26</cell></row><row><cell>MNIST</cell><cell>109,160</cell><cell>01:46.17</cell><cell>04:45:37</cell></row><row><cell>CSL</cell><cell>110,794</cell><cell>00:00.26</cell><cell>00:00:57</cell></row><row><cell>MOLHIV</cell><cell>206,465</cell><cell>00:29.44</cell><cell>00:48:17</cell></row><row><cell>MOLHIV + VN</cell><cell>223,105</cell><cell>00:32.16</cell><cell>00:52:57</cell></row><row><cell>MOLPCBA</cell><cell>4,372,128</cell><cell>06:42.28</cell><cell>11:41:21</cell></row><row><cell cols="2">MOLPCBA + VN 4,874,128</cell><cell>07:13.71</cell><cell>11:07:15</cell></row><row><cell>COLLAB</cell><cell>173,315</cell><cell>00:14.35</cell><cell>00:47:50</cell></row><row><cell>IMDB-MULTI</cell><cell>173,315</cell><cell>00:01.63</cell><cell>00:13:35</cell></row><row><cell>REDDIT-BIN</cell><cell>173,185</cell><cell>00:36.48</cell><cell>02:01:35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>: Results of our ablation study. Node features F V , edge features F E , adjacency encoding A, and identity encoding I. Walk strategies no-backtrack (NB) and uniform (UN).</figDesc><table><row><cell>Features</cell><cell>Walk Strat.</cell><cell>ZINC (MAE)</cell><cell>CSL (Acc)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/toenshoff/CRaWl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">It is possible to simulate directed edges and parallel edges through edge labels and loops through node labels, but so far, we have only worked with undirected simple, though possibly labeled graphs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the German Research Foundation (DFG) under grant GR 1492/16-1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">İ</forename><forename type="middle">İ</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01179</idno>
		<title level="m">The surprising power of graph neural networks with random node initialization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random walks, universal traversal sequences, and the complexity of maze problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aleliunas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rackoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS79</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="page" from="218" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network motifs: theory and experimental approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrg2102</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="450" to="461" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Létourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02863</idno>
		<title level="m">Directional graph networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<title level="m">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<title level="m">Residual gated graph convnets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dehaene</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.15069</idno>
		<title level="m">Graph convolutions that can finally model local structure</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fürer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05718</idno>
		<title level="m">Principal neighbourhood aggregation for graph nets</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/663fd3c5144fd10bd5ca6611a9a5b92d-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5723" to="5733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hierarchical inter-message passing for learning on molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12179</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Walk message passing neural networks and second-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09499</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth International Conference on Machine Learning (ICML)</title>
		<meeting>the Thirty-Fourth International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939754</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>B. Krishnapuram, M. Shah, A. J. Smola, C. C. Aggarwal, D. Shen, and R. Rastogi</editor>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graph warp module: an auxiliary module for boosting the power of graph neural networks in molecular graph analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01020</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Weisfeiler-Leman algorithm: An exploration of its power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGLOG News</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5" to="27" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Wasserstein embedding for graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naderializadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Flag: Adversarial data augmentation for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey on graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Network Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with motif-based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepergcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<title level="m">All you need to train deeper GCNs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning graph-level representation for drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03741</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tu-Dataset</surname></persName>
		</author>
		<ptr target="URLwww.graphlearning.io" />
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Random walk graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis ; H. Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Motif-matching based subgraphlevel attentional convolutional network for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5387" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
		<idno type="DOI">10.1021/ci100050t</idno>
		<idno type="PMID">20426451</idno>
		<ptr target="https://doi.org/10.1021/ci100050t" />
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. C.-C</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05697</idno>
		<title level="m">Motif-based convolutional neural network on graphs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Beyond localized graph neural networks: An attributed motif regularization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sundaram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05197</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<idno>abs/2002.03155</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MoleculeNet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>For all n ≥ 1, 1-WL distinguishes G n and G n</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">G n ) n≥1 , (G n ) n≥1 are indistinguishable by the non-backtracking version of CRaWl with window size s(n) = o(n) (regardless of the walk length and sample size)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">G n ) n≥1 (G n ) n≥1 are indistinguishable by CRaWl with walk length (n) = O(n), and samples size m(n) = n O(1) (regardless of the window size)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The graphs G n and G n both consist of three internally disjoint paths with the same endnodes x and y. In G n the lengths of all three paths is n</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Proof</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In G n , the length of the paths is n − 1, n, n + 1 (see Figure 4</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Then for all i and j with i − s ≤ j ≤ i we have w i = w j , and unless j = i − 1, there is no edge between w i and w j . Thus X(W ) = X(W ) for all walks W of the same length , and since it does not matter which of the two graphs G n , G n the walks are from</title>
		<imprint/>
	</monogr>
	<note>Then the length of the shortest cycle in G n , G n is s + 2. Now consider a non-backtracking walk W = (w 1 , . . . , w ) in either G n or G n (of arbitrary length ). It follows that X nb (G n ) = X nb (G n</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">we remark that the backtracking version of CRaWl can distinguish (G n ) n≥1 and (G n ) n≥1 with a constant window size 6</title>
	</analytic>
	<monogr>
		<title level="m">Before we prove</title>
		<imprint/>
	</monogr>
	<note>and samples</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
