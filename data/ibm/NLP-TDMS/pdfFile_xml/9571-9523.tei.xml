<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">National Lab for Information Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">National Lab for Information Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">National Lab for Information Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Jiangsu Collaborative Innovation Center for Language Competence</orgName>
								<address>
									<settlement>Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Samsung R&amp;D Institute of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Samsung R&amp;D Institute of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments , we evaluate our models on three tasks including link prediction, triple classification and rela-tional fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH. The source code of this paper can be obtained from https: //github.com/mrlyk423/relation extraction.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Knowledge graphs encode structured information of entities and their rich relations. Although a typical knowledge graph may contain millions of entities and billions of relational facts, it is usually far from complete. Knowledge graph completion aims at predicting relations between entities under supervision of the existing knowledge graph. Knowledge graph completion can find new relational facts, which is an important supplement to relation extraction from plain texts.</p><p>Knowledge graph completion is similar to link prediction in social network analysis, but more challenging for the following reasons: (1) nodes in knowledge graphs are entities with different types and attributes; and (2) edges in knowledge graphs are relations of different types. For knowledge graph completion, we not only determine whether there is a relation between two entities or not, but also predict the specific type of the relation.</p><p>For this reason, traditional approach of link prediction is not capable for knowledge graph completion. Recently, a promising approach for the task is embedding a knowledge graph into a continuous vector space while preserving certain information of the graph. Following this approach, many methods have been explored, which will be introduced in detail in Section "Related Work".</p><p>Among these methods, <ref type="bibr">TransE (Bordes et al. 2013</ref>) and TransH ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>) are simple and effective, achieving the state-of-the-art prediction performance. TransE, inspired by <ref type="bibr" target="#b10">(Mikolov et al. 2013b</ref>), learns vector embeddings for both entities and relationships. These vector embeddings are set in R k and we denote with the same letters in boldface. The basic idea behind TransE is that, the relationship between two entities corresponds to a translation between the embeddings of entities, that is, h + r ≈ t when (h, r, t) holds. Since TransE has issues when modeling 1-to-N, Nto-1 and N-to-N relations, TransH is proposed to enable an entity having different representations when involved in various relations.</p><p>Both TransE and TransH assume embeddings of entities and relations being in the same space R k . However, an entity may have multiple aspects, and various relations focus on different aspects of entities. Hence, it is intuitive that some entities are similar and thus close to each other in the entity space, but are comparably different in some specific aspects and thus far away from each other in the corresponding relation spaces. To address this issue, we propose a new method, which models entities and relations in distinct spaces, i.e., entity space and multiple relation spaces (i.e., relation-specific entity spaces), and performs translation in the corresponding relation space, hence named as TransR.</p><p>The basic idea of TransR is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. For each triple (h, r, t), entities in the entity space are first projected into r-relation space as h r and t r with operation M r , and then h r + r ≈ t r . The relation-specific projection can make the head/tail entities that actually hold the relation (denoted as colored circles) close with each other, and also get far away from those that do not hold the relation (denoted as colored triangles).</p><p>Moreover, under a specific relation, head-tail entity pairs usually exhibit diverse patterns. It is insufficient to build only a single relation vector to perform all translations from head to tail entities. For example, the head-tail entities of the relation "location location contains" have many patterns such as country-city, country-university, continent-country and so on. Following the idea of piecewise linear regression (Ritzema and others 1994), we extend TransR by clustering diverse head-tail entity pairs into groups and learning distinct relation vectors for each group, named as cluster-based TransR (CTransR).</p><p>We evaluate our models with the tasks of link prediction, triple classification and relation fact extraction on benchmark datasets of WordNet and Freebase. Experiment results show significant and consistent improvements compared to state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TransE and TransH</head><p>As mentioned in Section "Introduction", <ref type="bibr">TransE (Bordes et al. 2013</ref>) wants h+r ≈ t when (h, r, t) holds. This indicates that (t) should be the nearest neighbor of (h + r). Hence, TransE assumes the score function</p><formula xml:id="formula_0">f r (h, t) = 񮽙h + r − t񮽙 2 2 (1)</formula><p>is low if (h, r, t) holds, and high otherwise.</p><p>TransE applies well to 1-to-1 relations but has issues for N-to-1, 1-to-N and N-to-N relations. Take a 1-to-N relation r for example. ∀i ∈ {0, . . . , m}, (h i , r, t) ∈ S. This indicates that h 0 = . . . = h m , which does not comport with the facts.</p><p>To address the issue of TransE when modeling N-to-1, 1-to-N and N-to-N relations, TransH ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>) is proposed to enable an entity to have distinct distributed representations when involved in different relations. For a relation r, TransH models the relation as a vector r on a hyperplane with w r as the normal vector. For a triple (h, r, t), the entity embeddings h and t are first projected to the hyperplane of w r , denoted as h ⊥ and t ⊥ . Then the score function is defined as</p><formula xml:id="formula_1">f r (h, t) = 񮽙h ⊥ + r − t ⊥ 񮽙 2 2 .<label>(2)</label></formula><p>If we restrict 񮽙w r 񮽙 2 = 1, we will have h ⊥ = h − w 񮽙 r hw r and t ⊥ = t−w 񮽙 r tw r . By projecting entity embeddings into relation hyperplanes, it allows entities playing different roles in different relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other Models</head><p>Besides TransE and TransH, there are also many other methods following the approaches of knowledge graph embedding. Here we introduce several typical models, which will also be compared as baselines with our models in experiments.</p><p>Unstructured Model (UM). UM model ( <ref type="bibr" target="#b3">Bordes et al. 2012;</ref>) was proposed as a naive version of TransE by assigning all r = 0, leading to score function f r (h, t) = 񮽙h − t񮽙 2 2 . This model cannot consider differences of relations.</p><p>Structured Embedding (SE). SE model ( <ref type="bibr" target="#b2">Bordes et al. 2011</ref>) designs two relation-specific matrices for head and tail entities, i.e., M r,1 and M r,2 , and defines the score function as an L 1 distance between two projected vectors, i.e., f r (h, t) = 񮽙M r,1 h − M r,2 t񮽙 1 . Since the model has two separate matrices for optimization, it cannot capture precise relations between entities and relations.</p><p>Single Layer Model (SLM). SLM model was proposed as a naive baseline of NTN ( <ref type="bibr" target="#b18">Socher et al. 2013</ref>). The score function of SLM model is defined as</p><formula xml:id="formula_2">f r (h, t) = u 񮽙 r g(M r,1 h + M r,2 t),<label>(3)</label></formula><p>where M r,1 and M r,2 are weight matrices, and g() is the tanh operation. SLM is a special case of NTN when the tensor in NTN is set to 0. Semantic Matching Energy (SME). SME model <ref type="bibr">(Bor- des et al. 2012;</ref>) aims to capture correlations between entities and relations via multiple matrix products and Hadamard product. SME model simply represents each relation using a single vector, which interacts with entity vectors via linear matrix products, with all relations share the same parameters. SME considers two definitions of semantic matching energy functions for optimization, including the linear form</p><formula xml:id="formula_3">f r (h, t) = (M 1 h + M 2 r + b 1 ) 񮽙 (M 3 t + M 4 r + b 2 ), (4)</formula><p>and the bilinear form</p><formula xml:id="formula_4">f r (h, t) = 񮽙 (M 1 h)⊗(M 2 r)+b 1 񮽙 񮽙 񮽙 (M 3 t)⊗(M 4 r)+b 2 񮽙 , (5)</formula><p>where M 1 , M 2 , M 3 and M 4 are weight matrices, ⊗ is the Hadamard product, b 1 and b 2 are bias vectors. In ( <ref type="bibr" target="#b5">Bordes et al. 2014</ref>), the bilinear form of SME is re-defined with 3-way tensors instead of matrices.</p><p>Latent Factor Model (LFM). LFM model ( <ref type="bibr">Jenatton et al. 2012;</ref><ref type="bibr" target="#b20">Sutskever, Tenenbaum, and Salakhutdinov 2009)</ref> considers second-order correlations between entity embeddings using a quadratic form, and defines a bilinear score function f r (h, t) = h 񮽙 M r t.</p><p>Neural Tensor Network (NTN). NTN model ( <ref type="bibr" target="#b18">Socher et al. 2013</ref>) defines an expressive score function for graph embedding as follows,</p><formula xml:id="formula_5">f r (h, t) = u 񮽙 r g(h 񮽙 M r t + M r,1 h + M r,2 t + b r ),<label>(6)</label></formula><p>where u r is a relation-specific linear layer, g() is the tanh operation, M r ∈ R d×d×k is a 3-way tensor, and M r,1 , M r,2 ∈ R k×d are weight matrices. Meanwhile, the corresponding high complexity of NTN may prevent it from efficiently applying on large-scale knowledge graphs.</p><p>In experiments we will also compare with RESCAL, a collective matrix factorization model presented in <ref type="bibr" target="#b14">(Nickel, Tresp, and Kriegel 2011;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Method</head><p>To address the representation issue of TransE and TransH, we propose TransR, which represent entities and relations in distinct semantic space bridged by relation-specific matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TransR</head><p>Both TransE and TransH assume embeddings of entities and relations within the same space R k . But relations and entities are completely different objects, it may be not capable to represent them in a common semantic space. Although TransH extends modeling flexibility by employing relation hyperplanes, it does not perfectly break the restrict of this assumption. To address this issue, we propose a new method, which models entities and relations in distinct spaces, i.e., entity space and relation spaces, and performs translation in relation space, hence named as TransR.</p><p>In TransR, for each triple (h, r, t), entities embeddings are set as h, t ∈ R k and relation embedding is set as r ∈ R d . Note that, the dimensions of entity embeddings and relation embeddings are not necessarily identical, i.e., k 񮽙 = d.</p><p>For each relation r, we set a projection matrix M r ∈ R k×d , which may projects entities from entity space to relation space. With the mapping matrix, we define the projected vectors of entities as h r = hM r , t r = tM r .</p><p>The score function is correspondingly defined as</p><formula xml:id="formula_7">f r (h, t) = 񮽙h r + r − t r 񮽙 2 2 .<label>(8)</label></formula><p>In practice, we enforce constraints on the norms of the embeddings h, r, t and the mapping matrices, i.e. ∀h, r, t, we have 񮽙h񮽙 2 ≤ 1,</p><formula xml:id="formula_8">񮽙r񮽙 2 ≤ 1, 񮽙t񮽙 2 ≤ 1, 񮽙hM r 񮽙 2 ≤ 1, 񮽙tM r 񮽙 2 ≤ 1.</formula><p>Cluster-based TransR (CTransR)</p><p>The above mentioned models, including TransE, TransH and TransR, learn a unique vector for each relation, which may be under-representative to fit all entity pairs under this relation, because these relations are usually rather diverse. In order to better model these relations, we incorporate the idea of piecewise linear regression ( <ref type="bibr">Ritzema and others 1994)</ref> to extend TransR. The basic idea is that, we first segment input instances into several groups. Formally, for a specific relation r, all entity pairs (h, t) in the training data are clustered into multiple groups, and entity pairs in each group are expected to exhibit similar r relation. All entity pairs (h, t) are represented with their vector offsets (h − t) for clustering, where h and t are obtained with TransE. Afterwards, we learn a separate relation vector r c for each cluster and matrix M r for each relation, respectively. We define the projected vectors of entities as h r,c = hM r and t r,c = tM r , and the score function is defined as</p><formula xml:id="formula_9">f r (h, t) = 񮽙h r,c + r c − t r,c 񮽙 2 2 + α񮽙r c − r񮽙 2 2 ,<label>(9)</label></formula><p>where 񮽙r c − r񮽙 2 2 aims to ensure cluster-specific relation vector r c not too far away from the original relation vector r, and α controls the effect of this constraint. Besides, same to TransR, CTransR also enforce constraints on norm of embeddings h, r, t and mapping matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Method and Implementation Details</head><p>We define the following margin-based score function as objective for training</p><formula xml:id="formula_10">L = 񮽙 (h,r,t)∈S 񮽙 (h 񮽙 ,r,t 񮽙 )∈S 񮽙 max 񮽙 0, f r (h, t) + γ − f r (h 񮽙 , t 񮽙 ) 񮽙 ,<label>(10)</label></formula><p>where max(x, y) aims to get the maximum between x and y, γ is the margin, S is the set of correct triples and S 񮽙 is the set of incorrect triples.</p><p>Existing knowledge graphs only contain correct triples. It is routine to corrupt correct triples (h, r, t) ∈ S by replacing entities, and construct incorrect triples (h 񮽙 , r, t 񮽙 ) ∈ S 񮽙 . When corrupting the triple, we follow ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>) and assign different probabilities for head/tail entity replacement. For those 1-to-N, N-to-1 and N-to-N relations, by giving more chance to replace the "one" side, the chance of generating false-negative instances will be reduced. In experiments, we will denote the traditional sampling method as "unif" and the new method in ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>) as "bern".</p><p>The learning process of TransR and CTransR is carried out using stochastic gradient descent (SGD). To avoid overfitting, we initialize entity and relation embeddings with results of TransE, and initialize relation matrices as identity matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Analysis Data Sets and Experiment Setting</head><p>In this paper, we evaluate our methods with two typical knowledge graphs, built with WordNet (Miller 1995) and Freebase ( <ref type="bibr" target="#b1">Bollacker et al. 2008)</ref>. WordNet provides semantic knowledge of words. In WordNet, each entity is a synset consisting of several words, corresponding to a distinct word sense. Relationships are defined between synsets indicating their lexical relations, such as hypernym, hyponym, meronym and holonym. In this paper, we employ two data sets from WordNet, i.e., WN18 used in ( <ref type="bibr" target="#b5">Bordes et al. 2014</ref>) and WN11 used in ( <ref type="bibr" target="#b18">Socher et al. 2013</ref>). WN18 contains 18 relation types and WN11 contains 11. Freebase provides general facts of the world. For example, the triple (Steve Jobs, founded, Apple Inc.) builds a relation of founded between the name entity Steve Jobs and the organization entity Apple Inc. In this paper, we employ two data sets from Freebase, i.e., FB15K used in ( <ref type="bibr" target="#b5">Bordes et al. 2014</ref>) and FB13 used in ( <ref type="bibr" target="#b18">Socher et al. 2013</ref>). We list statistics of these data sets in <ref type="table" target="#tab_0">Table 1</ref>.  <ref type="table" target="#tab_0">Dataset  #Rel  #Ent  #Train #Valid  # Test  WN18  18 40,943 141,442  5,000  5,000  FB15K 1,345 14,951 483,142 50,000 59,071  WN11  11 38,696 112,581  2,609 10,544  FB13  13 75,043 316,232  5,908 23,733  FB40K 1,336</ref> 39528 370,648 67,946 96,678</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link Prediction</head><p>Link prediction aims to predict the missing h or t for a relation fact triple (h, r, t), used in ( <ref type="bibr" target="#b2">Bordes et al. 2011;</ref>. In this task, for each position of missing entity, the system is asked to rank a set of candidate entities from the knowledge graph, instead of only giving one best result. As set up in ( <ref type="bibr" target="#b2">Bordes et al. 2011;</ref>, we conduct experiments using the data sets WN18 and FB15K. In testing phase, for each test triple (h, r, t), we replace the head/tail entity by all entities in the knowledge graph, and rank these entities in descending order of similarity scores calculated by score function f r . Following ( <ref type="bibr" target="#b4">Bordes et al. 2013</ref>), we use two measures as our evaluation metric: (1) mean rank of correct entities; and (2) proportion of correct entities in top-10 ranked entities (Hits@10). A good link predictor should achieve lower mean rank or higher Hits@10. In fact, a corrupted triple may also exist in knowledge graphs, which should be also considered as correct. However, the above evaluation may under-estimate those systems that rank these corrupted but correct triples high. Hence, before ranking we may filter out these corrupted triples which have appeared in knowledge graph. We name the first evaluation setting as "Raw" and the latter one as "Filter".</p><p>Since we use the same data sets, we compare our models with baselines reported in ( <ref type="bibr" target="#b4">Bordes et al. 2013;</ref><ref type="bibr" target="#b21">Wang et al. 2014</ref>). For experiments of TransR and CTransR, we select learning rate λ for SGD among {0.1, 0.01, 0.001}, the margin γ among {1, 2, 4} , the dimensions of entity embedding k and relation embedding d among {20, 50, 100} , the batch size B among {20, 120, 480, 1440, 4800}, and α for CTransR among {0.1, 0.01, 0.001}. The best configuration is determined according to the mean rank in validation set. The optimal configurations are λ = 0.001, γ = 4, k = 50, d = 50, B = 1440, α = 0.001 and taking L 1 as dissimilarity on WN18; λ = 0.001, γ = 1, k = 50, d = 50, B = 4800, α = 0.01 and taking L 1 as dissimilarity on FB15K. For both datasets, we traverse all the training triplets for 500 rounds.</p><p>Evaluation results on both WN18 and FB15K are shown in <ref type="table" target="#tab_2">Table 2</ref>. From the table we observe that: (1) TransR and CTransR outperform other baseline methods including TransE and TransH significantly and consistently. It indicates that TransR finds a better trade-off between model complexity and expressivity. (2) CTransR performs better than TransR, which indicates that we should build finegrained models to handle complicated internal correlations under each relation type. CTransR is a preliminary exploration; it will be our future work to build more sophisticated models for this purpose. (3) The "bern" sampling trick works well for both TransH and TransR, especially on FB15K which have much more relation types.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, we show separate evaluation results by mapping properties of relations 1 on FB15K. We can see TransR achieves great improvement consistently on all mapping categories of relations, especially when (1) predicting "1-to-1" relations, which indicates that TransR provides more precise representation for both entities and relation and their complex correlations, as illustrated in <ref type="figure" target="#fig_0">Fig. 1; and (2)</ref> predicting the 1 side for "1-to-N" and "N-to-1" relations, which shows the ability of TransR to discriminate relevant from irrelevant entities via relation-specific projection.  <ref type="table" target="#tab_1">Table 4</ref> gives some cluster examples for the relation "location location contains" in FB15K training triples. We can find obvious patterns that: Cluster#1 is about continent containing country, Cluster#2 is about country containing city, Cluster#3 is about state containing county, and Cluster#4 is about country containing university. It is obvious that by clustering we can learn more precise and fine-grained relation embeddings, which can further help improve the performance of knowledge graph completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Triple Classification</head><p>Triple classification aims to judge whether a given triple (h, r, t) is correct or not. This is a binary classification task, which has been explored in <ref type="bibr" target="#b18">(Socher et al. 2013;</ref><ref type="bibr" target="#b21">Wang et al. 2014</ref>) for evaluation. In this task, we use three data sets, WN11, FB13 and FB15K, following ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>), where the first two datasets are used in <ref type="bibr" target="#b18">(Socher et al. 2013</ref>).</p><p>We need negative triples for evaluation of binary classification. The data sets WN11 and FB13 released by <ref type="bibr">NTN (Socher et al. 2013</ref>) already have negative triples, which are obtained by corrupting correct triples. As FB15K with negative triples has not been released by previous work, we construct negative triples following the same setting in <ref type="bibr" target="#b18">(Socher et al. 2013</ref>). For triple classification, we set a relationspecific threshold δ r . For a triple (h, r, t), if the dissimilarity score obtained by f r is below δ r , the triple will be classified  1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-N Unstructured ( <ref type="bibr" target="#b3">Bordes et al. 2012</ref> as positive, otherwise negative. δ r is optimized by maximizing classification accuracies on the validation set. For WN11 and FB13, we compare our models with baseline methods reported in ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>) who used the same data sets. As mentioned in ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>), for a fair comparison, all reported results are without combination with word embedding.</p><note type="other">SME (linear) (Bordes et al. 2012) 545 533 65.1 74.1 274 154 30.7 40.8 SME (bilinear) (Bordes et al. 2012) 526 509 54.7 61.3 284 158 31.3 41.3 LFM (Jenatton et al. 2012) 469 456 71.4 81.6 283 164 26.0 33.1 TransE (Bordes et al. 2013) 263 251 75.4 89.2 243 125 34.9 47.1 TransH (unif) (Wang et al. 2014) 318 303 75.4 86.7 211 84 42.5 58.5 TransH (bern) (Wang et al. 2014) 401 388 73.0 82.3 212 87 45.7 64.4 TransR (unif) 232 219 78.3 91.7 226 78 43.8 65.5 TransR (bern) 238 225 79.8 92.0 198 77 48.2 68.7 CTransR (unif) 243 230 78.9 92.3 233 82 44 66.3 CTransR (bern) 231 218 79.4 92.3 199 75 48.4 70.2</note><p>Since FB15K is generated by ourselves according to the strategy in ( <ref type="bibr" target="#b18">Socher et al. 2013</ref>), the evaluation results are not able to compare directly with those reported in ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>). Hence, we implement TransE and TransH, and use the NTN code released by <ref type="bibr" target="#b18">(Socher et al. 2013</ref>), and evaluate on our FB15K data set for comparison.</p><p>For experiments of TransR we select learning rate λ for SGD among {0.1, 0.01, 0.001, 0.0001}, the margin γ among {1, 2, 4} , the dimensions of entity embedding k , relation embedding d among {20, 50, 100} and the batch size B among {20, 120, 480, 960, 4800} . The best configuration is determined according to accuracy in validation set.The optimal configurations are: λ = 0.001, γ = 4, k, d = 20, B = 120 and taking L 1 as dissimilarity on WN11; λ = 0.0001, γ = 2, k, d = 100 and B = 480 and taking L 1 as dissimilarity on FB13. For both datasets, we traverse all the training triples for 1000 rounds.</p><p>Evaluation results of triple classification is shown in <ref type="table" target="#tab_5">Table   5</ref>. From <ref type="table" target="#tab_5">Table 5</ref>, we observe that: (1) On WN11, TransR significantly outperforms baseline methods including TransE and TransH. (2) Neither TransE, TransH nor TransR can outperform the most expressive model NTN on FB13. In contrast, on the larger data set FB15K, TransE, TransH and TransR perform much better than NTN. The results may correlate with the characteristics of data sets: There are 1, 345 relation types in FB15K and only 13 relations types in FB13. Meanwhile, the number of entities and relational facts in the two data sets are close. As discussed in ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>), the knowledge graph in FB13 is much denser than FB15K and even WN11. It seems that the most expressive model NTN can learn complicated correlations using tensor transformation from the dense graph of FB13. In contrast, simpler models are able to better handle the sparse graph of FB15K with good generalization. (3) Moreover, the "bern" sampling technique improves the performance of TransE, TransH and TransR on all three data sets.</p><p>As shown in ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>), the training time of TransE and TransH are about 5 and 30 minutes, respectively. The computation complexity of TransR is higher than both TransE and TransH, which takes about 3 hours for training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Extraction from Text</head><p>Relation extraction aims to extract relational fact from largescale plain text, which is an important information source to enrich knowledge graphs. Most exiting methods ( <ref type="bibr" target="#b13">Mintz et al. 2009;</ref><ref type="bibr" target="#b16">Riedel, Yao, and McCallum 2010;</ref><ref type="bibr" target="#b6">Hoffmann et al. 2011;</ref><ref type="bibr" target="#b19">Surdeanu et al. 2012</ref>) take knowledge graphs as distant supervision to automatically annotate sentences in large-scale text corpora as training instances, and then extract textual features to build relation classifiers. These methods only use plain texts to reason new relational fact; meanwhile knowledge graph embeddings perform link prediction only based on existing knowledge graphs. It is straightforward to take advantage of both plain texts and knowledge graphs to infer new relational facts. In <ref type="bibr">(We- ston et al. 2013</ref>), TransE and text-based extraction model were combined to rank candidate facts and achieved promising improvement. Similar improvements are also found over TransH ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>). In this section, we will investigate the performance of TransR when combining with textbased relation extraction model.</p><p>We adopt NYT+FB, also used in ( <ref type="bibr" target="#b22">Weston et al. 2013</ref>), to build text-based relation extraction model. In this data set, entities in New York Times Corpus are annotated with Stanford NER and linked to Freebase.</p><p>In our experiments, we implement the same text-based extraction model proposed in ) which is named as Sm2r. For the knowledge graph part, ( ) used a subset restricted to the top 4 million entities with 23 thousand relation types. As TransH has not released the dataset and TransR will take too long to learn from 4 million entities, we generate a smaller data set FB40K ourselves, which contains all entities in NYT and 1, 336 relation types. For test fairness, from FB40K we remove all triples whose entity pairs have appeared in the testing set of NYT. As compared to previous results in ( <ref type="bibr" target="#b22">Weston et al. 2013;</ref><ref type="bibr" target="#b21">Wang et al. 2014</ref>), we find that learning with FB40K does not significantly reduce the effectiveness of TransE and TransH. Hence we can safely use FB40K to demonstrate the effectiveness of TransR.</p><p>Following the same method in ( <ref type="bibr" target="#b22">Weston et al. 2013</ref>), we combine the scores from text-based relation extraction model with the scores from knowledge graph embeddings to rank test triples, and get precision-recall curves for TransE, TransH and TransR. Since the freebase part of our data set is built by ourselves, different from the ones in ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>), the evaluation results cannot be compared directly with those reported in ( <ref type="bibr" target="#b21">Wang et al. 2014</ref>). Hence, we implement TransE, TransH and TransR by ourselves. We set the embedding dimensions k, d = 50, the learning rate λ = 0.001, the margin γ = 1.0, B = 960, and dissimilarity metric as L 1 . Evaluation curves are shown in <ref type="figure" target="#fig_1">Figure  2</ref>. Recently, the idea of embeddings has also been widely used for representing words and texts ( <ref type="bibr" target="#b0">Bengio et al. 2003;</ref><ref type="bibr" target="#b9">Mikolov et al. 2013a;</ref><ref type="bibr" target="#b10">2013b;</ref><ref type="bibr" target="#b11">Mikolov, Yih, and Zweig 2013)</ref>, which may be used for text-based relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper, we propose TransR, a new knowledge graph embedding model. TransR embeds entities and relations in distinct entity space and relation space, and learns embeddings via translation between projected entities. In addition, we also propose CTransR, which aims to model internal complicated correlations within each relation type based on the idea of piecewise linear regression. In experiments, we evaluate our models on three tasks including link prediction, triple classification and fact extraction from text. Experiment results show that TransR achieves consistent and significant improvements compared to TransE and TransH.</p><p>We will explore the following further work:</p><p>• Existing models including TransR consider each relational fact separately. In fact, relations correlate with each other with rich patterns. For example, if we know (goldfish, kind of, fish) and (fish, kind of, animal), we can infer (goldfish, kind of, animal) since the relation type kind of is transitive. We may take advantages of these relation patterns for knowledge graph embeddings.</p><p>• In relational fact extraction from text, we simply perform linear weighted average to combine the scores from text-side extraction model and the knowledge graph embedding model. In future, we may explore a unified embedding model of both text side and knowledge graph.</p><p>• CTransR is an initial exploration for modeling internal correlations within each relation type. In future, we will investigate more sophisticated models for this purpose.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Simple illustration of TransR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision-recall curves of TransE, TransH and TransR for relation extraction from text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Statistics of data sets.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 : 񮽙Head, Tail񮽙 examples of some clusters for the rela- tion "location location contains".</head><label>4</label><figDesc></figDesc><table>񮽙Head, Tail񮽙 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Evaluation results on link prediction.</head><label>2</label><figDesc></figDesc><table>Data Sets 
WN18 
FB15K 

Metric 
Mean Rank 
Hits@10 (%) 
Mean Rank 
Hits@10 (%) 
Raw Filter Raw Filter 
Raw Filter Raw Filter 
Unstructured (Bordes et al. 2012) 
315 
304 35.3 
38.2 1,074 
979 
4.5 
6.3 
RESCAL (Nickel, Tresp, and Kriegel 2011) 1,180 1,163 37.2 
52.8 
828 
683 28.4 
44.1 
SE (Bordes et al. 2011) 
1,011 
985 68.5 
80.5 
273 
162 28.8 
39.8 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Evaluation results on FB15K by mapping properties of relations. (%) 

Tasks 
Predicting Head(Hits@10) 
Predicting Tail(Hits@10) 
Relation Category 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 5 : Evaluation results of triple classification. (%)</head><label>5</label><figDesc></figDesc><table>Data Sets 
WN11 FB13 FB15K 
SE 
53.0 
75.2 
-
SME (bilinear) 
70.0 
63.7 
-
SLM 
69.9 
85.3 
-
LFM 
73.8 
84.3 
-
NTN 
70.4 
87.1 
68.5 
TransE (unif) 
75.9 
70.9 
79.6 
TransE (bern) 
75.9 
81.5 
79.2 
TransH (unif) 
77.7 
76.5 
79.0 
TransH (bern) 
78.8 
83.3 
80.2 
TransR (unif) 
85.5 
74.7 
81.7 
TransR (bern) 
85.9 
82.5 
83.9 
CTransR (bern) 
85.7 
-
84.5 

</table></figure>

			<note place="foot" n="1"> Mapping properties of relations follows the same rules in (Bordes et al. 2013).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the 973 Program (No. 2014CB340501), the National Natural Science Foundation of China (NSFC No. 61133012 and 61202140) and Tsinghua-Samsung Joint Lab. We thank all anonymous reviewers for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint learning of words and meaning representations for open-text semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="259" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Obozinski</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">;</forename><surname>-T</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A threeway model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Factorizing yago: scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Drainage principles and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ritzema</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modelling relational data using bayesian clustered tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1821" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
