<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attending to Discriminative Certainty for Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><forename type="middle">Kumar</forename><surname>Kurmi</surname></persName>
							<email>vinodkk@iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanu</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
							<email>vinaypn@iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attending to Discriminative Certainty for Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we aim to solve for unsupervised domain adaptation of classifiers where we have access to label information for the source domain while these are not available for a target domain. While various methods have been proposed for solving these including adversarial discriminator based methods, most approaches have focused on the entire image based domain adaptation. In an image, there would be regions that can be adapted better, for instance, the foreground object may be similar in nature. To obtain such regions, we propose methods that consider the probabilistic certainty estimate of various regions and specify focus on these during classification for adaptation. We observe that just by incorporating the probabilistic certainty of the discriminator while training the classifier, we are able to obtain state of the art results on various datasets as compared against all the recent methods. We provide a thorough empirical analysis of the method by providing ablation analysis, statistical significance test, and visualization of the attention maps and t-SNE embeddings. These evaluations convincingly demonstrate the effectiveness of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the advent of deep learning, there has been substantial progress for solving image classification tasks with state of the art methods obtaining lesser than 3% error (on top five results) on the imagenet dataset. However, it was observed that these results do not transfer to other datasets <ref type="bibr" target="#b25">[26]</ref> due to the effect of dataset bias <ref type="bibr" target="#b44">[45]</ref>. The classifiers trained on one dataset (termed source dataset) show a significant drop in accuracy when tested on another dataset (termed target dataset). To address this issue, some methods have been proposed for adapting domains <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b50">51]</ref>. One of the more successful approaches towards addressing this domain shift has been based on the adversarial adaptation of features us- * Equal contributions from both authors. ing an adversarial discriminator <ref type="bibr" target="#b12">[13]</ref> that aims to distinguish between samples drawn from the source and target datasets. Due to the adversarial training, the feature representations are brought close such that the discriminator is not able to distinguish between samples from source and target dataset. However, all approaches that are based on this idea consider the whole image as being adapted. This usually is not the case as there are predominant regions in an image that may be better adapted and useful for improving classification on target dataset. We address this issue and propose a simple approach for solving this problem.</p><p>To specify regions that can be adapted we propose the use of certainty of a probabilistic discriminator. During training, we identify regions where the discriminator is certain, i.e., the probabilistic uncertainty for these regions is low. These regions can be adapted because there exists a clear distinction between the source and target regions. <ref type="figure" target="#fig_0">Figure 1</ref> shows that using measures such as data uncertainty (known as aleatoric uncertainty) <ref type="bibr" target="#b19">[20]</ref> and predictive uncertainty <ref type="bibr" target="#b24">[25]</ref>, we can obtain regions that can be adapted better. We also observe from the <ref type="figure" target="#fig_0">Figure 1</ref> and 5 that for most of the duration during training, discriminator is certain on the foreground regions, as the foreground is hard to adapt. Hence, when the classifier is trained with the emphasis being placed on these regions, then we observe that the classifier focuses on these regions during prediction and therefore generalizes better on target dataset. Quantitatively we observe results that are up to 7.4% better than the current state of the art methods on Office-Home dataset <ref type="bibr" target="#b49">[50]</ref>.</p><p>To summarize, through this paper we make the following contributions:</p><p>• We propose a method to identify adaptable regions using the certainty estimate of the discriminator, and this is evaluated using various certainty estimates. • We use these certainty estimate weights for improving the classifier performance on target dataset by focusing the training of the classifier on the adaptable regions of the source dataset. • We provide a thorough evaluation of the method by considering detailed comparison on standard benchmark datasets against the state of the art methods and also provide an empirical analysis using statistical significance analysis, visualization and ablation analysis of the proposed method. • An additional observation is that by using Bayesian classifiers we also improve the robustness of the classifier in addition to obtaining certainties of classification accuracy. This aids in better understanding of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature Survey</head><p>Some studies have examined different adaptation methods. One study by <ref type="bibr" target="#b47">[48]</ref> examined domain adaptation by minimizing the maximum mean discrepancy distance. The maximum mean discrepancy based approaches were further extended to multi-kernel MMD in <ref type="bibr" target="#b25">[26]</ref>. In adversarial learning framework <ref type="bibr" target="#b12">[13]</ref> has proposed a method to minimize the source target discrepancy using a gradient reversal layer at discriminator. Recently many adversarial methods have been applied in the domain adaptation task to bring the source and target distribution closer. Adversarial discriminative domain adaptation <ref type="bibr" target="#b46">[47]</ref> considers an inverted label GAN loss. Wasserstein distance based discriminator was used in <ref type="bibr" target="#b39">[40]</ref> to bring the two distributions closer. Domain confusion network <ref type="bibr" target="#b45">[46]</ref> was also used to solve the adaptation problem in two domains by minimizing the discriminate distance between two domains. The discriminative feedback of the discriminator also applied in the paraphrase generation problem <ref type="bibr" target="#b33">[34]</ref>. Another adversarial discriminator based model is <ref type="bibr" target="#b34">[35]</ref>, where multiple discriminators (MADA) have been used to solve the mode collapse problem in the domain adaptation. Some works closely related to MADA have been proposed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>. The labeled discriminator <ref type="bibr" target="#b23">[24]</ref> used to tackle the mode collapse problem in domain adaptation. The adversarial domain adaptation also explored in scene graph <ref type="bibr" target="#b22">[23]</ref>. Other source and target discrepancy minimization based methods such as <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b54">55]</ref> also address the domain adaptation problem. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32]</ref> have proposed an exemplar based discrepancy minimization method. Recently <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref> have applied generative adversarial network <ref type="bibr" target="#b14">[15]</ref> for the domain adaptation problems. Image generation methods are used to adapt the source and target domain <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b30">31]</ref>. Other work in <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b42">[43]</ref> have used Cycle consistency <ref type="bibr" target="#b55">[56]</ref> loss and deep coral loss <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41]</ref> for ensuring the closeness of source and target domain respectively. Deep Bayesian models have been used to play an important role in the estimation of the deep model uncertainty. The Bayesian formulation in domain adaptation natural language processing has been proposed in <ref type="bibr" target="#b9">[10]</ref>. In <ref type="bibr" target="#b11">[12]</ref>, it has been justified that dropout can also work as an approximation of the deep Bayesian network. Works on the uncertainty estimation have been reported in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>. In <ref type="bibr" target="#b29">[30]</ref> predictive uncertainty has been calculated over the prior networks. Uncertainty in the ensemble model along with adversarial training has been discussed in <ref type="bibr" target="#b24">[25]</ref>. Another work on the Bayesian uncertainty estimation has been reported in the <ref type="bibr" target="#b43">[44]</ref>.</p><p>Attention-based networks have been widely applied in many computer vision applications such as image captioning <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b53">54]</ref>, visual question answer <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19]</ref> and speech recognition <ref type="bibr" target="#b5">[6]</ref>. The advantage of the attention model is that it helps to learn some set of weights over a set of representation input which has relatively more importance than others. Recently <ref type="bibr" target="#b48">[49]</ref> showed that the attention mechanism can also be achieved by dispensing with recurrence and convolutions. A recent work <ref type="bibr" target="#b17">[18]</ref> addresses the domain adaptation problem by obtaining the synthetic source and target images from CycleGAN <ref type="bibr" target="#b55">[56]</ref>, and then aligned the attention map of all the pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In the unsupervised domain adaptation problem, the source dataset D s = (x s i , y s i ) consists of data sample (x s i ) with corresponding label (y s i ) where D s ∈ P s and the target dataset D t (x t i ) consists of unlabeled data samples (x t i ) where D t P t . P s and P t are the source and target distributions. We further assume that both the domains are complex and unknown. For solving this task, we are following the adversarial domain adaptation framework, where a discriminator is trained to learn domain invariant features domain invariant while a classifier is trained to learn class discriminative features. In this paper, we are proposing a discriminator certainty based domain adaption model represented in the <ref type="figure" target="#fig_1">Figure 2</ref>, which consists of three major modules: Feature extractor, Bayesian Classifier, and Bayesian Discriminator. The feature extractor is pretrained on the Imagenet dataset, while both the classifier and discriminator are Bayesian neural networks (BNN). We have followed the approach defined in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> for transforming deep neural networks into BNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bayesian Classifier</head><p>Bayesian framework is one of the efficient ways to predict uncertainty. Gal et.al <ref type="bibr" target="#b10">[11]</ref> has shown that by applying dropout after every fully connected (fc) layer, we can perform probabilistic inference for deep neural networks. Hence we have followed a similar approach for defining the classifier. For estimating uncertainty, similar to <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b8">9]</ref>, we trained the classifier to output class probabilities along with aleatoric uncertainty (data uncertainty). The predictive uncertainty includes both model uncertainty and data uncertainty, where model uncertainty results from uncertainty in model parameters. Estimation of aleatoric uncertainty for the classifier makes the features more robust for prediction, and estimation of predictive uncertainty provides a tool for visualizing model's predictions.</p><p>For the input sample</p><formula xml:id="formula_0">x i , the feature extractor G f outputs features f i , represented by f i = G f (x i ).</formula><p>The predicted class logits y c i and aleatoric uncertainty v c i are obtained as:</p><formula xml:id="formula_1">y c i = G cy (G c (f i )), v c i = G cv (G c (f i )) (1)</formula><p>where G cy and G cv are the logits and aleatoric uncertainty prediction modules of the classifier G c respectively. The classification loss for predicted logits is defined as:</p><formula xml:id="formula_2">L cy = 1 n s xi∈Ds L(y c i , y i )<label>(2)</label></formula><p>where L is the cross entropy loss function and y i is the true class label for the input x i . The total number of data samples in the source domain is denoted as n s . The classifier aleatoric loss L cv for predicted uncertainty v c i is defined as:</p><formula xml:id="formula_3">y c i,t = y c i + σ c i * t , t ∼ N (0, I) L cv = − 1 n s xi∈Ds log 1 T t L(ŷ c i,t , y i )<label>(3)</label></formula><p>where σ c i is the standard deviation, v c i = (σ c i ) 2 . The classifier is trained by jointly minimizing both the classification loss L cy and aleatoric loss L cv .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bayesian Discriminator</head><p>In the proposed method, the discriminator is also modeled in the Bayesian framework similar to the Bayesian classifier. The uncertainty in the discriminator network implies the region where it is uncertain about its prediction about the domain. The uncertainty estimation of the discriminator can guide the feature extractor more efficiently for domain adaptation. All real-world images contain some type of aleatoric uncertainty or noise. These regions which contain aleatoric uncertainty, are not adaptable. By aligning these uncertain regions, we are corrupting the feature representation, thus confusing the classifier during predictions for the target domain. So, by estimating aleatoric uncertainty, the discriminator is avoiding the learning of feature representations for these regions, which also reduces negative transfer <ref type="bibr" target="#b34">[35]</ref>. The negative transfer introduces false alignment of the mode of two distributions across domains, which needs to be prevented during adaptation. Similarly, the predictive uncertainty tells us about the model's incapability to classify the domains, as the discriminator is not sure about the domain. Predictive uncertainty occurs in the region where either it is already adapted, or there is noise which corresponds to aleatoric uncertainty. We obtain the discriminator predicated logits and variance using the following equations</p><formula xml:id="formula_4">y d i = G dy (G d (f i )), v d i = G dv (G d (f i )) (4)</formula><p>where G dy and G dv predict domain class logits y d i and domain aleatoric uncertainty v d i respectively using features from G d . The domain classification loss L dy is defined as:</p><formula xml:id="formula_5">L dy = 1 n s + n t xi∈Ds∪Dt L(y d i , d i )<label>(5)</label></formula><p>where L is the cross entropy loss function, d i is the true domain of the image, and n s and n t are the number of source and target samples. The domain label d i is defined to be 0 if x i ∈ D s and 1 if x i ∈ D t . The discriminator aleatoric loss L dv is defined as:</p><formula xml:id="formula_6">y d i,t = y d i + σ d i * t , t ∼ N (0, I) L dv = − 1 n s + n t xi∈Ds∪Dt log 1 T t L(ŷ d i,t , d i )<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">v d i = (σ d i ) 2 .</formula><p>Discriminator is trained by jointly minimizing both the domain classification loss L dy and discriminator aleatoric loss L dv .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Certainty Based Attention</head><p>Uncertainty estimation of the discriminator can help in identifying those regions which can be adapted, cannot be adapted, or already adapted. The regions which are already aligned will confuse the discriminator for predicting the domain. Hence discriminator will be highly uncertain on these regions. The discriminator will also be highly uncertain on the regions containing aleatoric uncertainty, and these regions can't be adapted. Uncertainty estimation can also help to identify regions where discriminator is certain or the regions which can be further aligned.</p><p>In most of the datasets, the discriminator can easily discriminate between the source and target images by only attending on the background during the initial phase of the training. Hence, the discriminator will be more certain in these regions, which results in easier adaptation of background regions after some adversarial training. But the foreground regions are difficult to adapt, as foreground varies a lot across all the classes and images. Therefore for most of the span during training, the discriminator will be certain on the transferable regions of the foreground. Thus, if the classifier attends to certain regions of the discriminator, it will focus more on the transferable regions of the foreground during training.</p><p>The Certainty Attention based Domain Adaption (CADA) model is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In the proposed work, we propose the two variants of CADA: aleatoric certainty based attention (CADA-A) and predictive certainty based attention (CADA-P).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Aleatoric Certainty based Attention</head><p>The aleatoric uncertainty (v d i ) of the domain discriminator occurs due to corruption or noise in the regions. These regions are not suited for the adaptation as well as for object classification and should be less focused as compared to the certain regions for the classification task. For identifying the aleatoric uncertain regions, we compute the gradient of aleatoric uncertainty with respect to the features f i . These gradients ( ∂v d i ∂fi ) will flow back through the gradient rever-sal layer, and will correspond to the gradients of aleatoric certainty, i.e., −</p><formula xml:id="formula_8">∂v d i ∂fi . p i = f i * − ∂v d i ∂f i<label>(7)</label></formula><p>Therefore the positive sign of the product of features and gradients of the aleatoric certainty denotes the positive influence of aleatoric certainty on these features. i.e. the discriminator is certain on these regions.</p><formula xml:id="formula_9">a i = ReLU(p i ) + c * ReLU(−p i )<label>(8)</label></formula><p>For obtaining the regions where the discriminator is certain, the product p i is passed through a ReLU function. But for ignoring the negative values that correspond to uncertain regions, −p i is again passed through a ReLU function. This is then multiplied with a large number c, such that after applying softmax over a i all negative values of p i become zero and all the positive values are normalized.</p><formula xml:id="formula_10">w i = (1 − v d i ) * Softmax(a i )<label>(9)</label></formula><p>To focus on more attentive (certain) regions, we follow the residual setting <ref type="bibr" target="#b27">[28]</ref> for obtaining the effective weighted features h i . Images with high aleatoric uncertainty (lower certainty) should be less attentive, and it is obtained by multiplying the normalized softmax attention value to its certainty value (1 − v d i ) using the Eq. 9. The weighted feature h i is generated as follows:</p><formula xml:id="formula_11">h i = f i * (1 + w i )<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Predictive Certainty based Attention</head><p>The predictive uncertainty measures the model's capability for prediction. It occurs in the regions which are either already domain invariant or which contain noise. The regions corresponding to discriminator's predictive certainty are domain transferable regions and should be attended during classification. We follow the approach proposed in <ref type="bibr" target="#b19">[20]</ref> for computing the predictive uncertainty of discriminator. It is obtained by the entropy of the average class probabilities of the Monte Carlo (MC) samples. We sample weights from G d , and perform MC simulations over domain probabilities p(y d i,t ) and aleatoric uncertainty v d i,t for estimating predictive uncertainty Var d (f i ).</p><formula xml:id="formula_12">g i,t = G t d (f i ), G t d ∼ G d<label>(11)</label></formula><p>Using the sampled model, we calculate domain probabilities and aleatoric uncertainty.</p><formula xml:id="formula_13">p(y d i,t ) = Softmax(G dy (g i,t )), v d i,t = G dv (g i,t ) (12) H(y d i,t ) = − 2 c=1 p(y d i,t = c) * log(p(y d i,t = c))<label>(13)</label></formula><p>Var</p><formula xml:id="formula_14">d (f i ) ≈ 1 T T t=1 v d i,t + H(y d i,t )<label>(14)</label></formula><p>where H(y d i,t ) is the Entropy of the p(y d i,t ). For identifying the predictive uncertain regions, we compute gradients of the predictive uncertainty Var d (f i ) of the discriminator with respect to the features f i i.e. ∂Var d (fi) ∂fi , and negative of these gradients which returns through the gradient reversal layer will correspond to the gradients of the predictive certainty, i.e., − ∂Var d (fi)</p><formula xml:id="formula_15">∂fi . p i = f i * − ∂Var d (f i ) ∂f i<label>(15)</label></formula><formula xml:id="formula_16">a i = ReLU(p i ) + c * ReLU(−p i ) (16) w i = (1 − Var d i (f i )) * Softmax(a i )<label>(17)</label></formula><p>Similar to aleatoric certainty based attention, for obtaining the predictive certain regions we apply ReLU function to p i and ignoring its negative values (corresponds to uncertain regions), a ReLU function is applied to negative of p i , and multiply it with a large number c using Eq. 16. After applying the Softmax, the features that are activated by the predictive uncertainty will have zero weight and for features that are highly activated by the predictive certainty will get more weight. The residual weighted features are obtained by following equations</p><formula xml:id="formula_17">h i = f i * (1 + w i )<label>(18)</label></formula><p>Therefore the images with high predictive uncertainty have a lower value of w i , have less attention, while images have high predictive certainty have a high value of w i , produce high attentive features. This will ensure that already adapted regions or non-adaptive region (both cases have high uncertainty) have a lower attention value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Algorithm</head><p>We employ Certainty Attention based Domain Adaption (CADA) models for solving the task of unsupervised domain adaptation. Both the CADA-P and CADA-A models jointly learn domain invariant and class invariant features, by focusing the classifier's attention on the discriminator's certain regions. So, the class probabilities y c i and aleatoric uncertainty v c i for the classifier will be estimated using weighted feature h i .</p><formula xml:id="formula_18">y c i = G cy (G c (h i )), v c i = G cv (G c (h i ))<label>(19)</label></formula><p>The final objective function J for optimizing both the models is defined as:</p><formula xml:id="formula_19">J = L cy + L cv − λ * (L dy + L dv )<label>(20)</label></formula><p>where λ is a trade-off parameter between classifier and discriminator. The optimization problem is to find the parametersθ f ,θ c ,θ cy ,θ cv ,θ d ,θ dy ,θ dv that jointly satisfy:</p><formula xml:id="formula_20">(θ f ,θ c ,θ cy ,θ cv ) = arg min θ f ,θc, θcy,θcv J(θ f , θ c , θ cy , θ cv , θ d , θ dy , θ dv ) (θ d ,θ dy ,θ dv ) = arg max θ d ,θ dy ,θ dv J(θ f , θ c , θ cy , θ cv , θ d , θ dy , θ dv )</formula><p>The implementation details are provided in supplementary material, and other details are provided in the project page 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Office-31 Dataset The Office-31 <ref type="bibr" target="#b36">[37]</ref> consists of three domains: Amazon, Webcam and DSLR with 31 classes in each domain. There are 2817 images in Amazon (A) domain, 795 images in Webcam (W) and 498 images are in DSLR (D) domain makes total 4,110 images. To enable unbiased evaluation, we evaluate all methods on 6 transfer tasks A→W, D→A, W→A, A →D, D→ W and W → D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Office-Home Dataset</head><p>We also evaluated our model on the Office-Home dataset <ref type="bibr" target="#b49">[50]</ref> for unsupervised domain adaptation. This dataset consists of four domains: Art (Ar), Clip-art (Cl), Product (Pr) and Real-World (Rw). Each domain has common 65 categories and total 15,500 images. We evaluated our model by considering all the 12 adaptation tasks. The performance is reported in the <ref type="table" target="#tab_2">Table 3</ref>.</p><p>ImageCLEF Dataset ImageCLEF-2014 dataset consists of three datasets: Caltech-256 (C), ILSVRC 2012 (I), and Pascal VOC 2012 (P). There are 12 common classes and total 600 images in each domain. We evaluate our model on all the 6 transfer tasks and results are reported in <ref type="table" target="#tab_3">Table 4</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Following the common setting in unsupervised domain adaption, we used the pre-trained Alexnet <ref type="bibr" target="#b21">[22]</ref> and pretrained ResNet <ref type="bibr" target="#b15">[16]</ref> architecture as our base model. For Office-31 dataset, results are reported in the <ref type="table" target="#tab_0">Table 1 and   Table 2</ref>. From the table, it is clear that the proposed CADA outperforms the other methods on most transfer tasks, where CADA-P is the top-performing variant for both Alexnet and Resnet model. On average, we obtain improvements of 3.3% and 1.8% over the state of the art methods A; red: W), (e) shows ProxyA-distance for A → W task for method Reset <ref type="bibr" target="#b15">[16]</ref>, GRL <ref type="bibr" target="#b12">[13]</ref> and the proposed model CADA-P  <ref type="figure">Figure 4</ref>: Analysis of statistically significant difference for A→W and A →D in ResNet <ref type="bibr" target="#b15">[16]</ref>, GRL <ref type="bibr" target="#b12">[13]</ref>, CADA-A, and CADA-P methods, with a significance level of 0.05.</p><p>as it can be seen that the difference between other methods are usually less than 1% and therefore this amount of improvement is fairly significant. In some cases such as, in Amazon-Webcam (A-W) we obtain almost 4% improvement over the state of the art method. Note that for DSLR-Amazon (D-A) and Webcam-Amazon (W-A), we do not obtain state of the art. A very recent work <ref type="bibr" target="#b17">[18]</ref> obtain state of the art results for these two cases. The difference between the domains is significant in these cases, and our method was not trained to optimally for these cases. The proposed method has obtained better results in all other cases, and even in these two cases, our results are competitive. For the Office-Home dataset, the results are reported in <ref type="table" target="#tab_2">Table 3</ref>. For this more challenging dataset, we have achieved state-of-art performance. It is noteworthy that the proposed model provides the classification accuracy that is substantially better on this Office-Home dataset which is harder dataset for domain adaptation problem obtaining on average an improvement of 7.4% and 7.3% over the state of the art methods using CADA-P and CADA-A respectively.</p><p>The results on the ImageCLEF are reported in <ref type="table" target="#tab_3">Table 4</ref>. Both CADA-P and CADA-A outperform the other state of the art models for all the transfer tasks except I→C, with 0.8% and 0.7% improvement on average over the state of the art methods respectively. The room for improvement is smaller than the Office-Home Dataset, as ImageCLEF only have 12 classes and datasets in each domain, and the class category is equal, making it much easier for domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study</head><p>We investigate the Bayesian model with and without attention for both Alexnet and ResNet model on the office-31 dataset. From <ref type="table" target="#tab_0">Table 1</ref> and 2, it is clear that the Bayesian model without attention (CADA-W) performs significantly better than the most of the other previous models, as predicting uncertainty for discriminator reduces negative transfer, by neglecting the regions which contain data uncertainty. <ref type="table" target="#tab_0">Table 1</ref> and 2 demonstrates that CADA-P (predictive certainty) performs better than CADA-A (aleatoric certainty), as predictive uncertainty includes both model and aleatoric uncertainty, providing a better estimate of certain regions for the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Empirical Analysis</head><p>We further provide empirical analysis in terms of qualitative analysis of attention maps, feature visualization, discrepancy distance and statistical significance for additional insights about the performance of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Qualitative Analysis of Attention Maps</head><p>To provide the effectiveness of proposed certainty based adaptation, we provide the certainty map of the discriminator at different training stages (chosen randomly) in the <ref type="figure" target="#fig_3">Figure 5</ref>. In the figure, we see that at the initial phase of the training (after 4 epochs), the discriminator discriminates the source and target domains just by some random location. As the training progress, the discriminator learns the domain by attending the background of the images (In A→W, domains are mostly dissimilar in the background). After some more training, the background is adapted (after 125 epochs), and now the discriminator attends to foreground part of the image to differentiate the domains (after 535 epochs). However, the foreground varies a lot across all the images. Hence discriminator is highly certain in the class object regions. Now with further training of the model, these class object regions are also adapted (after 1300 epochs). The remaining regions of the image cannot be further adapted because there is data uncertainty. At the end of the training, the discriminator will be highly uncertain regarding the domain, and the attention weight on the regions which are not adaptable will have low weights as we are using the certainty of the discriminator as the measure for the weights. Note that at the time of inference, we do not use the attention weights obtained by certainty for aiding classifier. These are used only at the time of training. The results show that these attention weights based training aids the classifier to better generalize to the target domain. We have provided more visualization examples in the supplementary material for further justification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Feature Visualization</head><p>Adaptability of the target to source features can be visualized using the t-SNE embeddings of images feature. We follow similar setting in <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35]</ref> and plot t-SNE embeddings of the dataset in the <ref type="figure" target="#fig_2">Figure 3</ref>. We can observe that the proposed model correctly aligns the source and target domain images with exactly 31 clusters which are equal to the number of class labels with clear boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Discrepancy Distance</head><p>A-distance is a measure of cross domain discrepancy <ref type="bibr" target="#b0">[1]</ref>, which, together with the source risk, will bound the target risk. The proxy A-distance is defined as d A = 2(1 − 2 ), where is the generalization error of a classifier(e.g. kernel SVM) trained on the binary task of discriminating source and target. <ref type="figure" target="#fig_2">Figure 3</ref> (e) shows d A on tasks A →W with features of ResNet <ref type="bibr" target="#b15">[16]</ref>, GRL <ref type="bibr" target="#b12">[13]</ref>, and our model. We observe that d A using our model features is much smaller than d A using ResNet and GRL features, which suggests that our features can reduce the cross-domain gap more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Statistical Significance Test</head><p>We analyzed statistical significance <ref type="bibr" target="#b7">[8]</ref> for variants of proposed method against GRL <ref type="bibr" target="#b12">[13]</ref>. The Critical Difference (CD) for Nemenyi test depends upon the given confidence level (which is 0.05 in our case) for average ranks and number of test datasets. If the difference in the rank of the two methods lies within CD, then they are not significantly different. <ref type="figure">Figure 4</ref> visualizes the post hoc analysis using the CD diagram for A→W dataset. From the figures, it is clear that our models are significantly different from GRL <ref type="bibr" target="#b12">[13]</ref>. We can see as the training progress, the discriminator's certainty activation map changes from the background to the foreground, and then to the regions which can not be adapted further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose the use of certainty estimates of the discriminator to aid the generalization of the classifier by increasing the attention of the classifier on these regions. As it can be observed through our results, the attention maps obtained through certainty agree well with the classifier certainty for true labels and this aids in the generalization of the classifier for the target domain as well. The proposed method is thoroughly evaluated by comparison with state of the art methods and shows improved performance over all the other methods. Further, the analysis is provided in terms of statistical significance tests, discrepancy distance, and visualizations for better insight about the proposed method. The proposed method shows a new direction of using probabilistic measures for domain adaptation, and in the future, we aim to further explore this approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of the uncertainty and certainty maps of the discriminator during the midst of training is provided for the input image (a). The aleatoric and predictive uncertain regions of the discriminator are shown in image (b) and (d). While aleatoric and predictive certain regions of the discriminator are shown in (c) and (e). From the figure, it is clear that the certain regions of the discriminator during training mostly corresponds to (f) classifier's activation map based on true label at the end of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of Certainty based Attention for Domain Adaptation (CADA), consists of a shared feature extractor, Bayesian classifier and Bayesian discriminator where both the classifier and discriminator predict the variance value along with the prediction score. Discriminator's predictive or aleatoric uncertainty is used to highlight the regions where the discriminator is certain about its predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The t-SNE visualization of representations learned by (a) ResNet, (b) DANN, (c) CADA-A, and (d) CADA-P (blue:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Attention visualization of the last convolutions layer of the proposed model CADA-P. The first and the third row shows the image from source domain (A) whereas the second and the fourth row shows the image from target domain (W). In each row, the leftmost image (a) represents the original image and the rightmost image (f) represents the classifier's activation maps for ground truth class label at the end of the training. From left to right, the attention map of discriminator's predictive certainty is illustrated at different training stages: (b) 4 epochs, (c) 125 epochs, (d) 535 epochs, and (e) 1300 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy (%) on Office-31 dataset for unsupervised domain adaptation (AlexNet[22]) ± 0.4 95.0± 0.2 99.5± 0.1 64.2 ± 0.3 45.5± 0.5 48.3± 0.5 68.8 MMD[48] 61.0 ± 0.5 95.0± 0.3 98.5± 0.3 64.9 ± 0.4 47.2± 0.5 49.4± 0.4</figDesc><table><row><cell>Method</cell><cell>A → W</cell><cell>D→ W</cell><cell>W → D</cell><cell>A → D</cell><cell>D → A</cell><cell>W → A</cell><cell>Average</cell></row><row><cell cols="8">Alexnet[22] 60.6 69.3</cell></row><row><cell>RTN[28]</cell><cell cols="6">73.3 ± 0.3 96.8 ± 0.2 99.6 ± 0.1 71.0± 0.2 50.5± 0.3 51.0± 0.1</cell><cell>74.1</cell></row><row><cell>DAN[26]</cell><cell cols="6">68.5 ± 0.4 96.0 ± 0.3 99.0 ± 0.2 66.8± 0.2 50.0± 0.4 49.8± 0.3</cell><cell>71.7</cell></row><row><cell>GRL [13]</cell><cell cols="6">73.0 ± 0.5 96.4 ± 0.3 99.2 ± 0.3 72.3 ± 0.3 52.4± 0.4 50.4± 0.5</cell><cell>74.1</cell></row><row><cell>JAN [29]</cell><cell cols="6">75.2 ± 0.4 96.6 ± 0.2 99.6 ± 0.1 72.8 ± 0.3 57.5± 0.2 56.3± 0.2</cell><cell>76.3</cell></row><row><cell cols="7">CDAN[27] 77.9 ± 0.3 96.9 ± 0.2 100.0 ± 0 74.6 ± 0.2 55.1± 0.3 57.5± 0.4</cell><cell>77.0</cell></row><row><cell cols="7">MADA[35] 78.5 ± 0.2 99.8 ± 0.1 100.0 ± 0 74.1 ± 0.1 56.0± 0.2 54.5± 0.3</cell><cell>77.1</cell></row><row><cell>CADA-W</cell><cell cols="6">82.3± 0.3 99.2± 0.1 99.6 ± 0.1 75.9± 0.2 57.7± 0.1 53.3± 0.2</cell><cell>78.0</cell></row><row><cell>CADA-A</cell><cell cols="6">84.1 ± 0.2 99.2 ± 0.2 99.8± 0.2 77.3 ± 0.1 61.3± 0.2 54.1± 0.3</cell><cell>79.3</cell></row><row><cell>CADA-P</cell><cell cols="6">83.4 ± 0.2 99.8 ± 0.1 100.0 ± 0 80.1 ± 0.1 59.8± 0.2 59.5± 0.3</cell><cell>80.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>A → W</cell><cell>D→ W</cell><cell>W → D</cell><cell>A → D</cell><cell>D → A</cell><cell>W → A</cell><cell>Average</cell></row><row><cell cols="2">ResNet-50[16] 68.4±0.2</cell><cell>96.7±0.1</cell><cell>99.3±0.1</cell><cell cols="3">68.9±0.2 62.5±0.3 60.7± 0.3</cell><cell>76.1</cell></row><row><cell>DAN[26]</cell><cell>80.5±0.4</cell><cell>97.1±0.2</cell><cell>99.6±0.1</cell><cell cols="3">78.6±0.2 63.6±0.3 62.8±0.2</cell><cell>80.4</cell></row><row><cell>RTN[28]</cell><cell>84.5±0.2</cell><cell>96.8±0.1</cell><cell>99.4±0.1</cell><cell cols="3">77.5±0.3 66.2±0.2 64.8±0.3</cell><cell>81.6</cell></row><row><cell>DANN[13]</cell><cell>82.0±0.4</cell><cell>96.9±0.2</cell><cell>99.1±0.1</cell><cell cols="3">79.7±0.4 68.2±0.4 67.4±0.5</cell><cell>82.2</cell></row><row><cell>ADDA [47]</cell><cell>86.2±0.5</cell><cell>96.2±0.3</cell><cell>98.4±0.3</cell><cell cols="3">77.8±0.3 69.5±0.4 68.9±0.5</cell><cell>82.9</cell></row><row><cell>JAN[29]</cell><cell>85.4±0.3</cell><cell>97.4± 0.2</cell><cell cols="4">99.8±0.2 84.7± 0.3 68.6±0.3 70.0±0.4</cell><cell>84.3</cell></row><row><cell>MADA[35]</cell><cell cols="6">90.0± 0.1 97.4 ±0.1 99.6± 0.1 87.8±0.2 70.3±0.3 66.4±0.3</cell><cell>85.2</cell></row><row><cell>SimNet[36]</cell><cell>88.6 ±0.5</cell><cell>98.2±0.2</cell><cell>99.7±0.2</cell><cell cols="3">85.2±0.3 73.4±0.8 71.6±0.6</cell><cell>86.2</cell></row><row><cell>GTA[39]</cell><cell>89.5±0.5</cell><cell>97.9±0.3</cell><cell>99.8±0.4</cell><cell cols="3">87.7±0.5 72.8±0.3 71.4±0.4</cell><cell>86.5</cell></row><row><cell>DAAA [18]</cell><cell>86.8±0.2</cell><cell cols="5">99.3±0.1 100.0±0.0 88.8±0.4 74.3±0.2 73.9 ±0.2</cell><cell>87.2</cell></row><row><cell>CDAN[27]</cell><cell>93.1±0.1</cell><cell cols="5">98.6±0.1 100.0±0.0 93.4±0.2 71.0±0.3 70.3 ±0.3</cell><cell>87.7</cell></row><row><cell>CADA-W</cell><cell cols="6">93.9± 0.1 99.1 ± 0.2 99.6± 0.2 93.2± 0.3 68.9± 0.1 68.3± 0.2</cell><cell>87.2</cell></row><row><cell>CADA-A</cell><cell cols="2">96.8 ± 0.2 99.0±0.1</cell><cell cols="4">99.8 ±0.1 93.4±0.1 71.7±0.2 70.5±0.3</cell><cell>88.5</cell></row><row><cell>CADA-P</cell><cell cols="2">97.0 ± 0.2 99.3 ±0.1</cell><cell>100.0±0</cell><cell cols="3">95.6±0.1 71.5±0.2 73.1±0.3</cell><cell>89.5</cell></row></table><note>Classification accuracy (%) on Office-31 dataset for unsupervised domain adaptation (ResNet-50 [16])</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy (%) on Office-Home dataset for unsupervised domain adaptation (ResNet-50<ref type="bibr" target="#b15">[16]</ref>)</figDesc><table><row><cell>Method</cell><cell cols="9">Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg</cell></row><row><cell cols="2">ResNet-50[16] 34.9 50.0</cell><cell>58.0</cell><cell>37.4 41.9</cell><cell>46.2</cell><cell>38.5 31.2</cell><cell>60.4</cell><cell>53.9</cell><cell>41.2</cell><cell>59.9 46.1</cell></row><row><cell>DAN[26]</cell><cell>43.6 57.0</cell><cell>67.9</cell><cell>45.8 56.5</cell><cell>60.4</cell><cell>44.0 43.6</cell><cell>67.7</cell><cell>63.1</cell><cell>51.5</cell><cell>74.3 56.3</cell></row><row><cell>DANN[13]</cell><cell>45.6 59.3</cell><cell>70.1</cell><cell>47.0 58.5</cell><cell>60.9</cell><cell>46.1 43.7</cell><cell>68.5</cell><cell>63.2</cell><cell>51.8</cell><cell>76.8 57.6</cell></row><row><cell>JAN[29]</cell><cell>45.9 61.2</cell><cell>68.9</cell><cell>50.4 59.7</cell><cell>61.0</cell><cell>45.8 43.4</cell><cell>70.3</cell><cell>63.9</cell><cell>52.4</cell><cell>76.8 58.3</cell></row><row><cell>CDAN[27]</cell><cell>50.6 65.9</cell><cell>73.4</cell><cell>55.7 62.7</cell><cell>64.2</cell><cell>51.8 49.1</cell><cell>74.5</cell><cell>68.2</cell><cell>56.9</cell><cell>80.7 62.8</cell></row><row><cell>CADA-A</cell><cell>56.9 75.4</cell><cell>80.2</cell><cell>61.7 74.6</cell><cell>74.9</cell><cell>62.9 54.4</cell><cell>80.9</cell><cell>74.3</cell><cell>61.1</cell><cell>84.4 70.1</cell></row><row><cell>CADA-P</cell><cell>56.9 76.4</cell><cell>80.7</cell><cell>61.3 75.2</cell><cell>75.2</cell><cell>63.2 54.5</cell><cell>80.7</cell><cell>73.9</cell><cell>61.5</cell><cell>84.1 70.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">Classification accuracy (%) on ImageCLEF dataset</cell></row><row><cell cols="2">for unsupervised domain adaptation (ResNet-50 [16])</cell></row><row><cell cols="2">Method I → P P→ I I → C C → I C → P P → C Avg</cell></row><row><cell cols="2">ResNet [16] 74.8 83.9 91.5 78.0 65.5 91.2 80.7</cell></row><row><cell cols="2">DAN[26] 75.0 86.2 93.3 84.1 69.8 91.3 83.3</cell></row><row><cell cols="2">RTN[28] 75.6 86.8 95.3 86.9 72.7 92.2 84.9</cell></row><row><cell cols="2">GRL [13] 75.0 86.0 96.2 87.0 74.3 91.5 85.0</cell></row><row><cell cols="2">JAN[29] 76.8 88.0 94.7 89.5 74.2 91.7 85.8</cell></row><row><cell cols="2">MADA [35] 75.0 87.9 96.0 88.8 75.2 92.2 85.8</cell></row><row><cell cols="2">CDAN[27] 77.2 88.3 98.3 90.7 76.7 94.0 87.5</cell></row><row><cell cols="2">CADA-A 78.0 91.5 96.3 91.0 77.1 95.3 88.2</cell></row><row><cell cols="2">CADA-P 78.0 90.5 96.7 92.0 77.2 95.5 88.3</cell></row><row><cell>(a) A → W</cell><cell>(b) A → D</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://delta-lab-iitk.github.io/CADA/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment:</head><p>We acknowledge travel support from Microsoft Research India and Google Research India. We also acknowledge resource support from Delta Lab, IIT Kanpur. Vinod Kurmi acknowledges support from TCS Research Scholarship Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A theory of learning from different domains. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised pixellevel domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Partial transfer learning with selective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Partial adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijia</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain adaptation for visual applications: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05374</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bayesian neural network blogpost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Dorman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical bayesian domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Uncertainty in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>University of Cambridge</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1994" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Show, ask, attend, and answer: A strong baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Elqursh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03162</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adversarial adaptation of scene graph models for understanding civic issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Atreja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjali</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10124</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Looking back at labels: A class based domain adaptation technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Kumar Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7047" to="7058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Differential attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badri</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multimodal differential network for visual question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Badri Narayana Patro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><forename type="middle">Kumar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4002" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning semantic sentence embeddings using sequential pair-wise discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><forename type="middle">Kumar</forename><surname>Badri Narayana Patro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2715" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Element</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Wasserstein distance guided representation learning for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Correlation alignment for unsupervised domain adaptation. Domain Adaptation in Computer Vision Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">153</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bayesian uncertainty estimation for batch normalized deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Teye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4914" to="4923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Aligning infinite-dimensional covariance matrices in reproducing kernel hilbert spaces for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mianzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arye</forename><surname>Nehorai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3437" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
