<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Identity-Aware Transfer of Facial Attributes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Wangmeng</forename><forename type="middle">Zuo</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">David</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jane</forename><surname>You</surname></persName>
						</author>
						<title level="a" type="main">Deep Identity-Aware Transfer of Facial Attributes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Facial attribute transfer</term>
					<term>generative adversarial nets</term>
					<term>convolutional networks</term>
					<term>perceptual loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a Deep convolutional network model for Identity-Aware Transfer (DIAT) of facial attributes. Given the source input image and the reference attribute, DIAT aims to generate a facial image that owns the reference attribute as well as keeps the same or similar identity to the input image. In general, our model consists of a mask network and an attribute transform network which work in synergy to generate photorealistic facial image with the reference attribute. Considering that the reference attribute may be only related to some parts of the image, the mask network is introduced to avoid the incorrect editing on attribute irrelevant region. Then the estimated mask is adopted to combine the input and transformed image for producing the transfer result. For joint training of transform network and mask network, we incorporate the adversarial attribute loss, identity aware adaptive perceptual loss and VGG-FACE based identity loss. Furthermore, a denoising network is presented to serve for perceptual regularization to suppress the artifacts in transfer result, while an attribute ratio regularization is introduced to constrain the size of attribute relevant region. Our DIAT can provide a unified solution for several representative facial attribute transfer tasks, e.g., expression transfer, accessory removal, age progression and gender transfer, and can be extended for other face enhancement tasks such as face hallucination. The experimental results validate the effectiveness of the proposed method. Even for the identity-related attribute (e.g., gender), our DIAT can obtain visually impressive results by changing the attribute while retaining most identity-aware features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Face attributes, e.g., gender and expression, can not only provide a natural description of facial images <ref type="bibr" target="#b0">[1]</ref>, but also offer a unified viewpoint for understanding many facial animation and manipulation tasks. For example, the goal of facial avatar <ref type="bibr" target="#b1">[2]</ref> and reenactment <ref type="bibr" target="#b2">[3]</ref> is to transfer the facial expression attributes of a source actor to a target actor. In most applications such as expression transfer, accessory removal and age progression, the animation only modifies the related attribute without changing the identity. But for some other Manuscript received XXX; revised XXX tasks, the change of some attributes, e.g., gender and ethnicity, will inevitably alter the identity of the source image. In recent years, a variety of methods have been developed for specific facial attribute transfer tasks, and have achieved impressive results. For expression transfer, approaches have been suggested to create 3D or image-based avatars from handheld video <ref type="bibr" target="#b1">[2]</ref>, while face trackers and expression modeling have been investigated for offline and online facial reenactment <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. For age progression, explicit and implicit synthesis methods have been proposed for different image models <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Hair style generation and replacement have also been studied in literatures <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>Convolutional neural network (CNN)-based models have also been investigated for human face generation with attributes. Kulkarni et al. <ref type="bibr" target="#b8">[9]</ref> propose deep convolution inverse graphic network (DG-IGN). This method requires a large number of faces of a single person for training, and can only generate faces with different pose and light. Gauthier <ref type="bibr" target="#b9">[10]</ref> developes a conditional generative adversarial network (cGAN) to generate facial image from a noise distribution and conditional attributes. Yan et al. <ref type="bibr" target="#b10">[11]</ref> suggest an attribute-conditioned deep variational auto-encoder which extracts the latent variables from a reference image and combines them with attributes to produce the generated image with a generative model. Oord et al. <ref type="bibr" target="#b11">[12]</ref> propose a conditional image generation model based on PixelCNN decoder for image generation conditioned on an arbitrary feature vector. However, the identity of the generated face is not emphasized in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, making them not directly applicable to attribute transfer.</p><p>Motivated by the strong capability of CNN in modeling complex transformation <ref type="bibr" target="#b12">[13]</ref> and capturing perceptual similarity <ref type="bibr" target="#b13">[14]</ref>, several approaches have also been suggested for facial attribute transfer. Li et al. <ref type="bibr" target="#b14">[15]</ref> suggest a CNN-based attribute transfer model from the optimization perspective, but both run time and transfer quality is far from satisfying. Considering that it is impracticable to collect labeled data for supervised learning, the generative adversarial net (GAN) framework <ref type="bibr" target="#b15">[16]</ref> usually is adopted for handling this task <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. However, visible artifacts and over-smoothing usually are inevitable in the transfer result for these methods.</p><p>In this paper, we present a novel Deep CNN model for Identity-Aware Transfer (DIAT) of facial attributes which can provide a unified solution to several facial animation and manipulation tasks, e.g., expression transfer, accessory removal, age progression, and gender transfer. For each reference attribute label, we train a CNN model for the transfer of the input image to the desired attribute. Note that the reference attribute may be only related to some parts of the image. To avoid the the incorrect editing on attribute irrelevant region, our model consists of a mask network and an attribute transform network. The attribute transform network is presented to edit the input image for generating the desired attribute. While the mask network is adopted to estimate a mask of the attribute relevant region for guiding the combination of the input image and transformed image. Then attribute transform network and mask network work collaboratively to generate final photorealistic transfer result.</p><p>For attribute transfer, the ground truth transfer results generally are very difficult or even impossible to obtain. Therefore, we follow the GAN framework to train the model. As for training data, we only consider the binary attribute labels presented in the large-scale CelebFaces Attributes (CelebA) dataset <ref type="bibr" target="#b20">[21]</ref>. To capture the convolutional feature distribution of each attribute, we construct an attribute guided set using all the images with the desired attribute in CelebA. Then, the input set is defined as a set of input images without the reference attribute. Due to the infeasibility of ground truth transfer results, two alternative losses, i.e., adversarial attribute loss and identity-aware perceptual loss, are incorporated for unsupervised training of our DIAT model. Furthermore, two regularizers, i.e., perceptual regularization and attribute ratio regularization, are also introduced in the learning of DIAT.</p><p>In terms of attribute transfer, the generated image should have the desired attribute label. Following the GAN framework, we define the adversarial attribute loss on the attribute discriminator to require the generated image to have the desired attribute. As for identity-aware transfer, our DIAT requires that the generated image should keep the same or similar identity to the input image. To this end, the identity-aware perceptual loss is introduced on the convolutional feature map of a CNN model to model the content similarity between the reference face and the generated face. Instead of adopting any pre-trained CNNs, we suggest to define the perceptual loss on the attribute discriminator, which can be adaptively trained along with the learning procedure, and is named as adaptive perceptual loss. Compared with conventional perceptual loss, ours is more effective in computation, tailored to our specific attribute transfer task, and can serve as a kind of hiddenlayer supervision <ref type="bibr" target="#b21">[22]</ref> or regularization to ease the training of the DIAT model. To further encourage the identity keeping property, we add an identity loss by minimizing the distance between feature representations of the generated face and the reference. Pre-trained VGG-Face is used to extract the identity related features for face verification.</p><p>The model objective of DIAT also consider two regularizers, i.e., perceptual regularization and attribute ratio regularization. To suppress the artifacts, we propose a denoising network to serve for a perceptual regularization on the generated image. To guide the learning of mask network, an attribute ratio regularization is introduced to constrain the size of attribute relevant region. Finally, our DIAT model can be learned from training data by incorporating adversarial attribute loss, adaptive perceptual loss with perceptual regularization and attribute ratio regularization.</p><p>Extensive experiments are conducted on CelebA and real images from the website iStock 1 . As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, our DIAT performs favorably in attribute transfer with minor or no modification on the identity of the input faces. Even for some identity-related attributes (e.g., gender), our DIAT can obtain visually impressive transfer result while retaining most identity-relevant features. Computational efficiency is also a prominent merit of our method. In the testing stage, our DIAT can process more than one hundred of images within one second. Furthermore, our model can be extended to face hallucination, and is effective in generating photo-realistic high resolution images. A preliminary report of this work is given in 2016 <ref type="bibr" target="#b22">[23]</ref>. To sum up, our contribution is three-fold:</p><p>1) A novel DIAT model is developed for facial attribute transfer. For better preserving of attribute irrelevant feature, our model comprises a mask network and an attribute transform network, which collaborate to generate the transfer result and can be jointly learned from training data. 2) Adversarial attribute loss, adaptive perceptual loss, identity loss, perceptual regularization, and attribute ratio regularization are incorporated for training our DIAT model. The adversarial attribute loss is adopted to make the transfer result exhibit the desired attribute, and the adaptive perceptual loss is defined on the discriminator for identity-aware transfer while improving training efficiency. Moreover, perceptual regularization and attribute ratio regularization are further introduced for suppressing the artifacts and constraining the mask network. 3) Experimental results validate the effectiveness and efficiency of our method for identity-aware attribute transfer. Our DIAT can be used for the transfer of either local (e.g., mouth), global (e.g., age progression) or identityrelated (e.g., gender) attributes, and can be extended to face hallucination. The remainder of the paper is organized as follows. Section II gives a brief survey on relevant work. Section III describes the model and learning of our DIAT method. Section IV reports the experimental results on facial attribute transfer and face hallucination. Finally, Section V ends this work with several concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Deep convolutional neural networks (CNNs) not only have achieved unprecedented success in versatile high level vision problems <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, but also exhibited their remarkable power in understanding, generating, and recovering images <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. In this section, we focus on the task of facial attribute transfer, and briefly survey the CNN models for image generation and face generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CNN for image generation</head><p>Generative image modeling is a critical issue for image generation and many low level vision problems. Conventional sparse <ref type="bibr" target="#b33">[34]</ref>, low rank <ref type="bibr" target="#b34">[35]</ref>, FRAME <ref type="bibr" target="#b35">[36]</ref> and non-local similarity <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> based models usually are limited in capturing highly complex and long-range dependence between pixels. For better image modeling, a number of CNN-based methods have been proposed, including convolutional auto-encoder <ref type="bibr" target="#b8">[9]</ref>, PixelCNN and PixelRNN <ref type="bibr" target="#b38">[39]</ref>, and they have been applied to image completion and generation.</p><p>Several CNN architectures have been developed for image generation. Fully convolutional networks can be trained in the supervised learning manner to generate an image from an input image <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. The generative CNN model <ref type="bibr" target="#b41">[42]</ref> stacks four convolution layers upon five fully connected layers to generate images from object description. Kulkarni et al. suggest the Deep Convolution Inverse Graphics Network (DC-IGN), which follows the variational autoencoder architecture <ref type="bibr" target="#b42">[43]</ref> to transform the input image into different pose and lighting condition. However, both generative CNN <ref type="bibr" target="#b41">[42]</ref> and DC-IGN <ref type="bibr" target="#b42">[43]</ref> require many labeled images in training.</p><p>To visualize and understand CNN features, several methods have been proposed to reconstruct images by inverting deep representation <ref type="bibr" target="#b29">[30]</ref> or maximizing class score <ref type="bibr" target="#b43">[44]</ref>. Subsequently, Gatys et al. <ref type="bibr" target="#b13">[14]</ref> suggest to combine content and style losses defined on deep representation on the off-the-shelf CNNs for artistic style transfer. To improve the efficiency, alternative approaches have been proposed by substituting the iterative optimization procedure with pre-trained feed-forward CNN <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b44">[45]</ref>. And perceptual loss has also been adopted for style transfer and other generation tasks <ref type="bibr" target="#b12">[13]</ref>. Motivated by these works, both identity-aware adaptive perceptual loss and perceptual regularization are exploited in our DIAT model to meet the requirement of facial attribute transfer.</p><p>Another representative approach is generative adversarial network (GAN), where a discriminator and a generator are alternatingly trained as an adversarial game <ref type="bibr" target="#b15">[16]</ref>. The generator aims to generate images to match the data distribution, while the discriminator attempts to distinguish between the generated images and the training data. Laplacian Pyramid of GANs is further suggested to generate high quality image in a coarseto-fine manner <ref type="bibr" target="#b45">[46]</ref>. Radford et al. <ref type="bibr" target="#b46">[47]</ref> extend GAN with the fully deep convolutional networks (i.e., DCGAN) for image generation. To learn disentangled representations, informationtheoretic extension of GAN is proposed by maximizing the mutual information between a subset of noise variables and the generated results <ref type="bibr" target="#b47">[48]</ref>. In <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, WGAN and WGAN-GP minimize the Wasserstein-1 distance between the generated distribution and the real distribution to improve the stability of learning generator. In this work, we adopt the WGAN framework to learn our DIAT model, and further suggest adaptive perceptual loss for identity-aware transfer and perceptual regularization to suppress visual artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CNN for face generation</head><p>Facial attribute transfer has received considerable recent attention. Larsen et al. <ref type="bibr" target="#b17">[18]</ref> present to combine variational autoencode with GAN (VAE/GAN) for image generation. By modeling the attribute vector as the difference between the mean latent representations of the images with and without the reference attribute, VAE/GAN can provide a flexible solution to arbitrary facial attribute transfer, but is limited in transfer performance. Li et al. <ref type="bibr" target="#b14">[15]</ref> suggest an attribute driven and identity-preserving face generation model by solving an optimization problems with perceptual loss, which is computationally expensive and cannot obtain high quality results. Perarnau et al. <ref type="bibr" target="#b16">[17]</ref> adopt an encoder-decoder architecture, where attribute transfer can be conducted by editing the latent representation. Shen et al. <ref type="bibr" target="#b19">[20]</ref> learn the residual image in the GAN framework, and adopt dual learning to learn two reverse attribute transfer models simultaneously. Zhou et al. <ref type="bibr" target="#b18">[19]</ref> propose a model to learn object transfiguration from two sets of unpaired images that have the opposite attribute. However, most existing methods cannot achieve high quality transfer results, and visible artifacts and over-smoothing usually are inevitable. In comparison, our DIAT model can achieve much better transfer results than the competing methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b50">[51]</ref>.</p><p>Besides, CNNs have also been developed for other face generation tasks. For painting style transfer of head portrait, Selim et al. <ref type="bibr" target="#b51">[52]</ref> modify the perceptual loss to balance the contribution of the input photograph and the aligned exemplar painting. Gucluturk et al. train a feed-forward CNN with perceptual loss for sketch inversion. Yeh et al. <ref type="bibr" target="#b50">[51]</ref> apply DCGAN to semantic face inpainting in an optimization manner. </p><formula xml:id="formula_0">(x) = M (x) • T (x) + (1 − M (x)) • x.</formula><p>In order to learn F (x) from training data, we incorporate adversarial attribute loss att, identity loss id , adaptive perceptual loss id per with perceptual regularization smooth . Besides, an attribute ratio regularization is also adopted to constrain the estimated mask M (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP CNNS FOR IDENTITY-AWARE ATTRIBUTE TRANSFER</head><p>In this section, we present our DIAT model for identityaware transfer of facial attribute. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, our model involves a mask network and an attribute transform network which collaborate to produce the transfer result. To train our model, we incorporate the adversarial attribute loss, adaptive perceptual loss, perceptual regularization and attribute ratio regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network architecture</head><p>Most facial attributes, e.g., expression and accessory, are local-based and only related to part of facial image. Even for global attributes such as age and gender, some parts, e.g., the background, should also keep the same with the source image. For the sake of preserving attribute irrelevant feature, it is natural to only perform attribute transfer in image region related to specific attribute. However, it is not a trivial issue to find the attribute relevant region. One possible solution is to manually specify the relevant region for each attribute given a new transfer task, but it undoubtedly restricts the universality and adaptivity of the solution.</p><p>In this work, we aim to provide a unified solution to attribute transfer, which indicates that we only require to prepare training data and retrain the model when a new transfer task comes. To this end, our whole attribute transfer network is comprised of two sub-networks, i.e., mask network and attribute transform network. Both mask network and attribute transform network take the source image x as input. The mask network is utilized to predict a mask M (x) to indicate the attribute relevant region, while the attribute transform network is used to produce the transformed image T (x). Given M (x) and T (x), the final transfer result can be obtained by,</p><formula xml:id="formula_1">F (x) = M (x) • T (x) + (1 − M (x)) • x,<label>(1)</label></formula><p>where • denotes the element-wise product operator. We also note that both attribute transform network and mask network can be learned from training data in an end-to-end manner.</p><p>In the following, we describe the architecture of attribute transform network and mask network, respectively. Attribute transform network. We adopt the Unet <ref type="bibr" target="#b52">[53]</ref> for attribute transform due to its good tradeoff between efficiency and reconstruction ability. In general, the Unet architecture involves an encoder subnetwork and a decoder subnetwork, then skip connection and pooling operation are further introduced to exploit multi-scale information. As for attribute transform, we design a 10-layer Unet, which includes 5 convolution layers for encoding and another 5 convolution layers for decoding. In the encoder, we use convolution with stride 2 for downsampling. In the decoder, a depth to width (DTOW) layer <ref type="bibr" target="#b53">[54]</ref> is deployed for upsampling, and the element-wise summation operation is adopted to fuse the feature maps from the encoder and decoder subnetworks. The detailed parameters of the attribute transform network are summarized in <ref type="table" target="#tab_1">Table I</ref>.</p><p>Mask network. As for mask network, we first adopt a 5-layer fully convolutional network to generate a 32 × 32 binary mask for indicating the attribute relevant region. A batch normalization layer is added after each convolution layer. Then, 4× upsampling is deployed by simply replicating each  In our mask network, ReLU is adopted for nonlinearity for the first 4 convolution layers. As for the fifth convolution layer, we adopt the sigmoid nonlinearity, and the binarization operation is then used to obtain the binary mask,</p><formula xml:id="formula_2">B(e ijk ) = 1, if e ijk &gt; 0.5 0, if e ijk ≤ 0.5<label>(2)</label></formula><p>where e ijk denotes an element of the feature map. However, the gradient of the binarizer B(e ijk ) is zero almost everywhere except that it is infinite when e ijk = 0.5, making any layer before the binarizer never be updated during training. As a remedy, we follow the straight-through estimator on gradient <ref type="bibr" target="#b54">[55]</ref>, and introduce a piecewise linear proxy functioñ B(e ijk ) to approximate B(e ijk ),</p><formula xml:id="formula_3">B(e ijk ) =      1, if e ijk &gt; 1 e ijk , if 1 ≤ e ijk ≤ 0 0, if e ijk &lt; 0 .<label>(3)</label></formula><p>During training, B(e ijk ) is still used in forward-propagation calculation, whileB(e ijk ) is used in back-propagation, with its gradient computed by,</p><formula xml:id="formula_4">B (e ijk ) = 1, if 1 ≤ e ijk ≤ 0 0, otherwise .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model objective</head><p>By enforcing proper constraints on transfer result F (x), both the attribute transform network and mask network can be learned from training data. However, the ground truth of attribute transfer usually is unavailable and not unique. For example, it is generally impossible to obtain the ground truth of gender transfer in reality. Instead, the training data used in this work includes a guided set X G of images with the desired reference attribute and a source set X S of input images not with the reference attribute. And we do not require the images from guided set to have the same identity with theose from source set.</p><p>For the sake of identity-aware attribute transfer, we define two alternative losses: (i) adversarial attribute loss to make the transfer result exhibit the desired attribute, and (ii) adaptive perceptual loss and identity loss to make the generated image keep the same or similar identity to input image. To suppress the visual artifacts of the transfer result, we further include (iii) a perceptual regularization defined on a denoising network. Finally, (iv) an attribute ratio regularization is deployed on i,j,k B(e ijk ) to guide the learning of mask network. In the following, we provide more details on these losses and regularization terms.</p><p>Adversarial attribute loss. Adversarial strategy is a common stratey that is widely used in security problems <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>. For computer vision, an adversarial learning framework called generative adversarial networks(GANs) also shows powerful ability on generating images that fulfil the distribution of a set of images without any ground truth targets. Considering the infeasibility of obtaining the ground truth transfer result, we define the adversarial attribute loss based on the guided set X G and the source set X S . If the guided set X G is of large scale, it can provide a natural representation of the attribute distribution. Therefore, the goal of adversarial attribute loss is to make that the distribution of generated images matches the real attribute distribution. To this end, we adopt the generative adversarial network framework, where the generator is the attribute transfer network F (x), and the discriminator D is used to define the adversarial attribute loss. The details of the discriminator are provided in <ref type="table" target="#tab_1">Table III</ref>, which contains 6 convolution layers followed by another two fully-connected layers.</p><p>Denote by x an input image from X S , and a an image from X G . Let p source (x) be the distribution of the input images, p att (a) be the distribution of the images with the reference attribute. The discriminator is defined as D(a) to output the probability that the image a comes from the set X G . To train the generator and the discriminator, we take use of the following adversarial attribute loss, In order to improve the training stability, the improved Wasserstein GAN is adopted by defining the loss as,</p><formula xml:id="formula_5">min F max D E a∼patt(a) log D(a) + E x∼psource(x) log[1 − D(F (x))].<label>(5)</label></formula><formula xml:id="formula_6">min F max D E a∼patt(a) [D(a)] − E x∼psource(x) [D(F (x))]. (6)</formula><p>For simplicity, we respectively define the adversarial attribute losses for the generator and discriminator as follows,</p><formula xml:id="formula_7">min F att,F = {−E x∼psource(x) [D(F (x))]},<label>(7)</label></formula><formula xml:id="formula_8">min D att,D = {E x∼psource(x) [D(F (x))] − E a∼patt(a) [D(a)]}. (8)</formula><p>Adaptive perceptual loss. The adaptive perceptual loss is introduced to guarantee that the transfer result keeps the same or similar identity with the input image. Due to identity is a high level semantic concept, it is not proper to define identityaware loss by forcing two images to be exactly the same in pixel domain. Instead, we define the squared-error loss on the feature representations of the discriminator, resulting in our adaptive perceptual loss.</p><p>Denote by D the discriminator, and D l (x) the feature map of the l-th convolution layer. C l , H l and W l represent the channel number, height, and width of the feature map, respectively. We then define the perceptual loss between x andx = F (x) on the l-th convolution layer as,</p><formula xml:id="formula_9">D,l adaptive (x, x) = 1 2C l H l W l D l (x) − D l (x) 2 F .<label>(9)</label></formula><p>And the identity-aware adaptive perceptual loss is further defined as,</p><formula xml:id="formula_10">id per (x) = 4 l=3 w l D,l adaptive (T (x), x).<label>(10)</label></formula><p>We note that the discriminator D is learned from training data. Thus, the network parameters of id per (x) will be changed along with the updating of discriminator, and thus we name id per (x) as adaptive perceptual loss. In contrast, conventional perceptual loss <ref type="bibr" target="#b12">[13]</ref> is defined on the off-the-shelf CNNs (e.g., VGG-Face <ref type="bibr" target="#b24">[25]</ref>). Compared with conventional perceptual loss <ref type="bibr" target="#b12">[13]</ref>, our adaptive perceptual loss generally is more effective in improving the training efficiency and attribute transfer performance:</p><p>1) The training efficiency of adaptive perceptual loss can be further explained from two aspects. (i) For conventional perceptual loss, the forward and backward calculations are required for both the off-the-shelf CNN and the discriminator during training. Due to that the adaptive perceptual loss is defined on the discriminator, it is sufficient to only conduct forward and backward calculation on the discriminator, making our DIAT more efficient in training. (ii) For conventional GAN, the generator usually is difficult to be trained. As for our DIAT, it can be trained by both the adversarial attribute loss and adaptive perceptual loss, greatly accelerating the training speed. Actually, the adaptive perceptual loss is defined on the third and fourth convolution layers of the discriminator, which can serve as some kind of hiddenlayer supervision and benefit the convergence of network training <ref type="bibr" target="#b21">[22]</ref>. 2) For conventional perceptual loss, the off-the-shelf CNNs generally are pre-trained using other training data and are not tailored to attribute transfer. One plausible choice is the VGG-Face <ref type="bibr" target="#b24">[25]</ref>, which, however, is trained for face recognition and may not be suitable for identityaware attribute transfer. In comparison, our adaptive perceptual loss is defined on the discriminator which is trained for modeling p att (a). Such loss can thus provide natural balance between identity similarity and attribute transfer and benefit transfer performance. For example, in terms of gender transfer, the introduction of adaptive perceptual loss will allow the adaptive adjustment on the length of hair. Identity Loss. The proposed adaptive perceptual loss does help keep the content similarity between the generated face and the reference. However, it cannot guarantee the identity by itself. To further enhance the identity keeping property, we add constrains on the feature representation extracted for face recognition <ref type="bibr" target="#b57">[58]</ref> or verification <ref type="bibr" target="#b58">[59]</ref>. In face verification task, two faces are from the same person when the distance between two features are smaller than ceratain threshold. Here, we adopt VGG-Face and model the distance between the features of the generated face and the reference as the identity loss,</p><formula xml:id="formula_11">id (x) = V GG(x) − V GG(T (x)) 2 2 .<label>(11)</label></formula><p>Perceptual regularization. Despite the use of adversarial attribute loss and adaptive perceptual loss, visual artifacts are still inevitable in the transfer result. Image regularization is thus required to encourage the spatial smoothness while preserving small scale details of the generated face F (x). One choice is the Total Variation (TV) regularizer which has been adopted in CNN feature visualization <ref type="bibr" target="#b29">[30]</ref> and artistic style transfer <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. However, the TV regularizer is limited in recovering small-scale texture details and suppressing complex artifacts. Moreover, it is a generic model that does not consider the characteristics of facial images.</p><p>In this work, we take the facial characteristics into account and train a denoising network for perceptual regularization. To train the denoising network, we generate the noisy image by adding Gaussian noise with the standard deviation of 15 to the clean facial image from CelebA. Inspired by residual learning <ref type="bibr" target="#b30">[31]</ref>, we train the denoising network through learning the residual between the noise image and the clean image. Taking the noise image y as input, the denoising network utilizes a fully convolutional network of 6 layers to predict the residual DN (y). The denoising result can then be obtained by y + DN (y). The architecture of DN (y) is listed in <ref type="table" target="#tab_1">Table IV.</ref>  </p><formula xml:id="formula_12">Denote by T = {(y i , x i )} n i=1</formula><p>a training set, where y i denotes the i-th noisy image and x i the corresponding clean image. The objective for learning DN (y) is given as,</p><formula xml:id="formula_13">min DN (y) + y − x 2 F .<label>(12)</label></formula><p>Given the denoising network and the transfer result F (x), we define the perceptual regularization as,</p><formula xml:id="formula_14">smooth (F (x)) = max{0, DN (F (x)) 2 F − t},<label>(13)</label></formula><p>where · F denotes the Frobenius norm. Note that DN (F (x)) predicts the residual between the latent clean image and F (x).</p><p>Minimizing DN (F (x)) 2 F makes F (x) be close to the clean image, and can be used to suppress the noise and artifacts in F (x). Furthermore, the threshold t is introduced for better preserving of small scale details, and we empirically set t be a value in the range of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref>. Note that the regularizer in Eqn. <ref type="bibr" target="#b12">(13)</ref> is defined on the denoising network DN (F (x)), and thus is named as perceptual regularization.</p><p>Attribute ratio regularization. The size of attribute relevant region varies for different attributes. For example, the region related to mouth open/close mainly includes the mouth and should be small. For glasses removal, the attribute relevant region includes the two eyes and is relatively large. As for gender transfer, all the face region and the hair should be attribute relevant. Therefore, we introduce an attribute ratio regularization term to constrain the size of attribute relevant region. Specifically, such regularization is defined on the binary mask in Eqn. <ref type="bibr" target="#b1">(2)</ref>. Denote by N the image size, and p the expected ratio of the region for a specific attribute. The attribute ratio regularization is then defined as,</p><formula xml:id="formula_15">mask (M (x)) = ( M (x) − pN ) 2<label>(14)</label></formula><p>In our experiments, we set smaller p value for local attribute and larger p value for global attribute. </p><p>where F (x) = att,F + λ( id (x) + id per (x)) + γ mask (M (x)). λ, γ, and µ are the tradeoff parameters for the adaptive perceptual loss, attribute ratio regularization, and perceptual regularization, respectively. However, it is difficult to set the tradeoff parameter µ. Instead, we empirically find that the transfer model can be stably learned by alternatingly minimizing F (x) and smooth (F (x)) during training. Finally, the discriminator D is learned by minimizing the following objective,</p><formula xml:id="formula_17">min D att,D .<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning algorithm</head><p>Generally, both the generator and the discriminator are difficult to converge in GAN. Therefore, we adopt a twostage strategy for learning the transfer model F (x) and the discriminator D: (i) we first combine the source set X S and the guided set X G to pre-train for initialization, and (ii) alternate between updating F and D. The procedure for training the transfer model F (x) is summarized in Algorithm 1.</p><p>Initialization. For the initialization of the F , we only consider the attribute transform network T , and leave the mask network M be learned in the second stage. Note that the transform network T has the architecture of auto-encoder. Thus, it can be pre-trained by minimizing the following reconstruction objective on X S and X G ,</p><formula xml:id="formula_18">rec = x∈X S ∪X G ||x − T (x)|| 2 F .<label>(17)</label></formula><p>As for the initialization of the discriminator, we use the images in X S as negative samples and the images in X G as positive samples. Then the discriminator can be pre-trained by minimizing the following objective,</p><formula xml:id="formula_19">dis = xi∈X S ∪X G ||y i − D(x i )|| 2 ,<label>(18)</label></formula><p>where y i is 1 for positive image x i and −1 for negative image. By this way, the initialization can provide a good start point and benefit the convergence and stability of DIAT training. Network training. After the initialization of T and D, network training is further performed by updating the whole F (including both T and M ) and D alternatingly. Moreover, F is updated by first tstep iterations for minimizing F and then nstep iterations for minimizing smooth . We apply the RMSProp solver <ref type="bibr" target="#b59">[60]</ref> to train the transfer network F and the discriminator D with a learning rate of 5 × 10 −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Extension to face hallucination</head><p>Besides facial attribute transfer, our DIAT can also be extended to other face editing tasks. Here we use the 8× face hallucination as an example. Face hallucination is undoubtedly a global transfer task, and thus we remove the mask network M as well as the attribute ratio regularization mask , making F (x) = T (x). Moreover, the input image in face hallucination is of low resolution (LR) while the output image is of high resolution (HR). To be consistent with attribute transfer, we super-resolve LR image to the size of HR image with the bicubic interpolator, which is taken as input to F (x). Select a mini-bath X from X S to generate the transfer results, which is further combined with another minibatch A from X G to form the set X for training the discriminator. Here we set |X | = 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>for i = 1 to dstep do <ref type="bibr">6:</ref> Use the RMSProp solver to update the discriminator D with Eqn. (16) using the mini-batch X . for i = 1 to tstep do for i = 1 to nstep do <ref type="bibr">13:</ref> Use the RMSProp solver to update F by minimizing smooth in Eqn. <ref type="bibr" target="#b14">(15)</ref>. <ref type="bibr">14:</ref> end for 15: end while <ref type="bibr">16:</ref> Return the transfer network F Furthermore, the ground truth HR images can be available to guide the network training for face hallucination. Denote by x the super-resolved image by bicubic interpolator, and y the ground truth HR image. Then, the pixel-wise reconstruction loss is defined as,</p><formula xml:id="formula_20">rec (x) = F (x) − y 2 F .<label>(19)</label></formula><p>We further modify the definition of F by removing the attribute ratio regularization and adding reconstruction loss,</p><formula xml:id="formula_21">F (x) = att,F + λ id (x) + β rec (x).<label>(20)</label></formula><p>where β is the tradeoff parameter for pixel-wise reconstruction loss, and we set β = 0.01 in our experiment. Given the training data, the models can then be learned by updating F and D alternatingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we first describe the experimental settings, including the training and testing data, competing methods, model and learning parameters. Experiments are then performed for local and global attribute transfer. Quantitative metrics and the results on real images are also reported. Moreover, we analyze the effect of adaptive perceptual loss and perceptual regularization. Finally, the results are reported to further assess the performance of our DIAT for face hallucination. The source code will be given after the publication of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental settings</head><p>Our DIAT models are trained using a subset of the aligned CelebA dataset <ref type="bibr" target="#b20">[21]</ref> by removing the images with poor quality. The size of the aligned images is 178 × 218. Due to the limitation of the GPU memory, we sample the central part of each image and resize it to 128 × 128. For each attribute transfer task, we use all the images with the reference attribute from training set to form the guided set X G , and randomly select 10,000 training images not with the reference attribute as the source set X S . After training, 2,000 images apart from the images for training are adopted to assess the attribute transfer performance. And we also test the models on other real images from the website iStock.</p><p>Only a few methods have been proposed for facial attribute transfer. In our experiments, we compare our DIAT with the convolutional attribute-driven and identity-preserving model (CNIA) <ref type="bibr" target="#b14">[15]</ref>, IcGAN <ref type="bibr" target="#b16">[17]</ref> and VAE/GAN <ref type="bibr" target="#b17">[18]</ref> due to that their codes are available. As for IcGAN and VAE/GAN, the original image size is not the same with our DIAT, so we resize the result to the same size with DIAT for comparison. For the task of glasses removal, we can first manually detect the region of glasses, and then use some face inpainting methods (e.g., semantic inpainting <ref type="bibr" target="#b50">[51]</ref>) to recover the missing pixels. Thus we also compare our DIAT with semantic inpainting <ref type="bibr" target="#b50">[51]</ref> for glasses removal.</p><p>All the experiments are conducted on a computer with the GTX TitanX GPU of 12GB memory. We set the parameters λ = 1 and γ = 0.01 for DIAT. For the threshold t in the perceptual regularization smooth , we set it to be a value in the range of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref>. As for p in the attribute ration regularization, we set it to be (i) p = 0.16 for small local attributes (e.g., mouth), (ii) p = 0.32 for large local attributes (e.g., eyes), and (iii) p = 0.62 for global attributes (e.g., gender and age).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Local attribute transfer</head><p>We assess the local attribute transfer models on three tasks, i.e., mouth open, mouth close, and eyeglasses removal. <ref type="figure">Fig. 3</ref> illustrates the transfer results by our DIAT. It can be seen that our DIAT performs favorably for transferring the input images to the desired attribute with satisfying visual quality. Benefited from the mask network, the results by DIAT can preserve more identity-aware and attribute irrelevant details. Moreover, when the training data are sufficient, it is feasible to separately train two DIAT models for reverse tasks, e.g., one for mouth open and another for mouth close.</p><p>We further compare our DIAT with three competing methods, i.e., CNIA <ref type="bibr" target="#b14">[15]</ref>, IcGAN <ref type="bibr" target="#b16">[17]</ref> and VAE/GAN <ref type="bibr" target="#b17">[18]</ref>. As shown in <ref type="figure" target="#fig_8">Fig. 6</ref>, the results by our DIAT are visually more pleasing than those by CNIA <ref type="bibr" target="#b14">[15]</ref> for all the three local attribute transfer tasks. In terms of run time, CNIA takes about 30 seconds (s) to deal with an image, while our DIAT only needs 0.0045 s. <ref type="figure" target="#fig_7">Fig. 5</ref> further provides the results by IcGAN, VAE/GAN, and our DIAT. In comparison with the competing methods, our DIAT can well address the attribute transfer tasks while recovering more visual details in both attribute relevant and attribute irrelevant regions. Finally, for glasses removal, we compare our DIAT with semantic inpainting <ref type="bibr" target="#b50">[51]</ref>, and the results in <ref type="figure">Fig. 7</ref> clearly demonstrate the superiority of DIAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Global attribute transfer</head><p>We consider two global attribute transfer tasks, i.e., gender transfer and age transfer. For gender transfer, we only evaluate the model for male-to-female. For age transfer, we only test the model for older-to-younger. <ref type="figure" target="#fig_6">Fig. 4</ref> shows the transfer results, and our DIAT is also effective for global attribute transfer. Even gender transfer certainly causes the change of the identity, as shown in <ref type="figure" target="#fig_6">Fig. 4(a)</ref>, our DIAT can still retain most identity-aware features, making the transfer result similar to the input image in appearance. Figs. 6 and 5 show the results by our DIAT, CNAI <ref type="bibr" target="#b14">[15]</ref>, IcGAN and VAE/GAN. Compared with the competing methods, the results by our DIAT well exhibit the desired attribute, and are of high visual quality with photo-realistic details. Finally, we also note that for gender transfer our DIAT is able of adjusting the hair length due to the introduction of adaptive perceptual loss.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quantitative evaluation</head><p>Given each attribute transfer task, we randomly select 2, 000 images without the reference attribute from the testing partition of CelebA to form our testing set. Then, three groups of experiments are conducted to evaluate the transfer performance quantitatively:   </p><formula xml:id="formula_22">• Attribute</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results on other real facial images</head><p>To assess the generalization ability, we use the DIAT models learned on CelebA to other real facial images from the website iStock. Each test image is first aligned with the 5 facial landmarks, and then input to the DIAT models. Taking mouth open and gender transfer as examples, <ref type="figure" target="#fig_4">Fig. 8</ref> gives the transfer results on 15 images for each task, clearly demonstrating the generalization ability of our models to other real facial images. 2 https://github.com/cmusatyalab/openface</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Evaluation on adaptive perceptual loss and perceptual regularization</head><p>We also implement a variant of DIAT (i.e., DIAT-1) by replacing the adaptive perceptual loss with the conventional perceptual loss defined on VGG-Face <ref type="bibr" target="#b24">[25]</ref>. Taking gender transfer as an example, <ref type="figure" target="#fig_10">Fig. 9</ref> compares DIAT with DIAT-1. It can be observed that DIAT converges very fast and can generate satisfying results after 4 epochs of training. In comparison, DIAT-1 requires much more epochs in training, and the gender just begins to be modified after 18 epochs. Moreover, the adoption of adaptive perceptual loss also benefits the transfer performance, and adaptive adjustment on the hair length can be observed on the transfer results by DIAT. Furthermore, <ref type="figure" target="#fig_0">Fig. 10</ref> shows the transfer results by DIAT with the perceptual regularization and the TV regularization. It can be clearly seen that the perceptual regularization is more effective on suppressing noise and artifacts while preserving sharp edges and fine details. G. Results of the learnt mask <ref type="figure" target="#fig_0">Fig. 11</ref> gives the masks generated by the mask network for different task. For the local attribute transformation tasks such as glasses removal and closing mouth, the generated masks accurately cover the local facial part which is related to the attribute. For global transformation like gender transformation, the mask covers most of the face and keep the background out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Experiments on face hallucination</head><p>Finally, we evaluate the performance of DIAT for 8× face hallucination. <ref type="table" target="#tab_1">Table VII</ref> lists the average PSNR and SSIM <ref type="bibr" target="#b60">[61]</ref> values on the 2,000 testing images by DIAT, bicubic interpolator, and Unet, while <ref type="figure" target="#fig_0">Fig. 12</ref> shows the super-resolved images. Even our DIAT achieves lower PSNR/SSIM than the baseline Unet, it is much better in terms of visual quality, and can generate hallucinated image with rich textures and sharp edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>A deep identity-aware transfer (i.e., DIAT) model is presented for facial attribute transfer. Considering that some attributes may be only related with parts of facial image, the whole transfer model consists of two subnetworks, i.e., mask network and attribute transform network, which work collaboratively to produce the transfer result. In order to train the model, we further incorporate adversarial attribute loss, adaptive perceptual loss with perceptual regularization and attribute ratio regularization. Experiments show that our model can obtain satisfying results for both local and global attribute   transfer. Even for some identity-related attributes (e.g., gender transfer), our DIAT can obtain visually impressive results with minor modification on identity-related features. Our DIAT can also be extended to face hallucination and performs favorably in recovering facial details. In future work, we will further improve the visual quality and diversity of the transfer results, and extend our model to arbitrary attribute transfer. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the results by our DIAT on several facial attribute transfer tasks, including glasses removal, mouth open/close, gender transfer and age transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Schematic illustration of our DIAT model. Here we use glasses removal as an example. The whole attribute transfer network F (x) includes two sub-networks, i.e., a mask network to find the attribute relevant region M (x) and an attribute transform network to produce the transformed image T (x). Then M (x) and T (x) collaborate to generate the transfer result F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Objective function. We define the objective function for learning the transfer model F and the discriminator D by combining the adversarial attribute loss, adaptive perceptual loss, perceptual regularization, and attribute ratio regularization. The transfer model F (x) = M (x)•T (x)+(1−M (x))•x is learned by minimizing the following objective, min M,T F (x) + µ smooth (F (x)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .Algorithm 1</head><label>31</label><figDesc>The results of local attribute transfer. For each task, the left and right columns are the input facial images and the transfer results, respectively. Learning the attribute transfer network Input: Source set X S , guided set X G , dstep = 12, tstep = 12, nstep = 6. mini-batch size is 50. Output: The attribute transfer network F 1: Pre-train the transform model T by minimizing the objective in Eqn. (17). 2: Pre-train the discriminator D by minimizing the objective in Eqn. (18). 3: while not converged do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>8 :</head><label>8</label><figDesc>Clip the parameters of the discriminator. D ← clip(D, −c, c) 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>10 :</head><label>10</label><figDesc>Use the RMSProp solver to update F by minimizing F in Eqn.<ref type="bibr" target="#b14">(15)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>The results of global attribute transfer. For each task, the left and right columns are the input facial images and the transfer results, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of transfer results by our DIAT, IcGAN<ref type="bibr" target="#b16">[17]</ref> and VAE/GAN<ref type="bibr" target="#b17">[18]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of transfer results by our DIAT and CNIA<ref type="bibr" target="#b14">[15]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Local attribute transfer (mouth open) and global attribute transfer (gender transfer) on images from the website iStock. For each task, the left and right columns are the input facial images and the transfer results, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Comparison between the adaptive perceptual loss and the VGG-Face based perceptual loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Comparison between the perceptual regularization and the TV regulization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Results of the learnt mask of different transformation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Results of face hallucination by different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This project is partially supported by the HK RGC/GRF grant (under no. PolyU 5313/12E and PolyU 152212/14E) and the National Natural Science Foundation of China (NSFC) under Grant No. 61671182. Mu Li and Jane You are with the Department of Computing, Hong Kong Polytechnic University, Hong Kong, e-mail: (csmuli@comp.polyu.edu.hk,csyjia@comp.polyu.edu.hk). Wangmeng Zuo is with the School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China, e-mail: (cswmzuo@gmail.com). David Zhang is with the School of Science and Engineering, The Chinese University of Hong Kong (Shenzhen), Shenzhen, China, e-mail: (cs-dzhang@comp.polyu.edu.hk).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ARCHITECTURE</head><label>I</label><figDesc>OF THE ATTRIBUTE TRANSFORM NETWORK. × 128 × 128 conv1, 4 × 4 × 64, pad 1, stride 2 64 × 64 × 64 conv2, 4 × 4 × 128, pad 1, stride 2 128 × 32 × 32 conv3, 4 × 4 × 256, pad 1, stride 2 256 × 16 × 16 conv4, 4 × 4 × 512, pad 1, stride 2 512 × 8 × 8 conv5, 4 × 4 × 512, pad 1, stride 2 512 × 4 × 4 DTOW, stride 2 128 × 8 × 8 conv6, 3 × 3 × 512, pad 1, stride 1 512 × 8 × 8 Element-wise add, conv6 and conv4 512 × 8 × 8 DTOW, stride 2 128 × 16 × 16 conv7, 3 × 3 × 256, pad 1, stride 1 256 × 16 × 16 Element-wise add, conv7 and conv3 256 × 16 × 16 DTOW, stride 2 64 × 32 × 32 conv8, 3 × 3 × 128, pad 1, stride 1 128 × 32 × 32 Element-wise add, conv8 and conv2 128 × 32 × 32 DTOW, stride 2 32 × 64 × 64 conv9, 3 × 3 × 64, pad 1, stride 1 64 × 64 × 64 Element-wise add, conv9 and conv1 64 × 64 × 64 DTOW, stride 2 16 × 128 × 128 conv10, 5 × 5 × 3, pad 2, stride 1 3 × 128 × 128element in the 32 × 32 binary mask 4 × 4 times. In order to make the generated image smooth, we further utilize a 5 × 5 Gaussian filter with the standard deviation of 1.6 to produce the final mask M (x). To sum up, the details of the mask network are given inTable II.</figDesc><table><row><cell>Layer</cell><cell>Activation size</cell></row><row><cell>Input</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ARCHITECTURE</head><label>II</label><figDesc>OF THE MASK NETWORK. × 4 × 32, pad 1, stride 2 32 × 64 × 64 conv, 4 × 4 × 64, pad 1, stride 2 64 × 32 × 32 conv, 3 × 3 × 64, pad 1, stride 1 64 × 32 × 32 conv, 3 × 3 × 64, pad 1, stride 1 64 × 32 × 32 conv, 3 × 3 × 1, pad 1, stride 1 1 × 32 × 32 binarization 4×upsampling 1 × 128 × 128 conv, 5 × 5 × 1, pad 2, stride 1 1 × 128 × 128</figDesc><table><row><cell>Layer</cell><cell>Activation size</cell></row><row><cell>Input</cell><cell>3 × 128 × 128</cell></row><row><cell>conv, 4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III NETWORK</head><label>III</label><figDesc>ARCHITECTURE OF THE ATTRIBUTE DISCRIMINATOR. × 128 × 128 conv, 8 × 8 × 32, pad 3, stride 2 32 × 64 × 64 conv, 3 × 3 × 32, pad 1, stride 1 32 × 64 × 64 conv, 4 × 4 × 64, pad 1, stride 2 64 × 32 × 32 conv, 3 × 3 × 64, pad 1, stride 1 64 × 32 × 32 conv, 4 × 4 × 128, pad 1, stride 2 128 × 16 × 16 conv, 4 × 4 × 128, pad 1, stride 2 128 × 8 × 8 Fully connected layer with 1000 hidden units 1000 Fully connected layer with 1 hidden units 1</figDesc><table><row><cell>Layer</cell><cell>Activation size</cell></row><row><cell>Input</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV NETWORK</head><label>IV</label><figDesc>ARCHITECTURE OF THE DENOISING NETWORK. × 3 × 64, pad 1, stride 1 64 × 128 × 128 conv, 3 × 3 × 64, pad 1, stride 1 64 × 128 × 128 conv, 3 × 3 × 64, pad 1, stride 1 64 × 128 × 128 conv, 3 × 3 × 64, pad 1, stride 1 64 × 128 × 128 conv, 3 × 3 × 64, pad 1, stride 1 64 × 128 × 128 conv, 3 × 3 × 3, pad 1, stride 1 3 × 128 × 128</figDesc><table><row><cell>Layer</cell><cell>Activation size</cell></row><row><cell>Input</cell><cell>3 × 128 × 128</cell></row><row><cell>conv, 3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>classification. For attribute transfer, it is natural to require the transfer result to exhibit the desired attribute. Thus, we first train a CNN-based attribute classifier (including two convolution layers, three residual blocks and two fully-connected layers) using the training set of CelebA. Given an attribute transfer task, we test the classification accuracy of the desired attribute for the transfer results of 2,000 testing images. Tabel V lists the classification accuracy for five attribute transfer tasks, i.e., mouth open, mouth close, glasses removal, gender transfer, and age transfer. It can be observed that our DIAT achieves satisfying accuracy (i.e., &gt; 0.70) for all the tasks, indicating that the results by our DIAT generally are with the desired attribute. As for local attribute transfer, we also require the transfer result to preserve the identity of input image. Here we use the open source faceFig. 7. Comparison of the results by semantic inpainting<ref type="bibr" target="#b50">[51]</ref> for glasses removal. The green rectangle shows the region of the input subimage for semantic inpainting recognition platform Openface 2 for matching the input image with the transfer result. By setting the threshold be 0.99,Table VIlists the identity verification accuracy for mouth open, mouth close, and glasses removal. The results demonstrate that our DIAT can well preserve the identity-aware feature for local attribute transfer. Image quality is another crucial metric to assess attribute transfer. However, the ground truth of transfer result is unavailable, making it difficult to perform quantitative evaluation. Here we use a pair of reverse attribute transfer tasks (i.e., mouth close and mouth open) as an example, and adopt an indirect scheme to compute average PSNR on the 2000 testing images. Specifically, we first perform mouth open to the images with mouth close, and then perform reverse mouth close to the transfer results. Finally, the input images are taken as ground truth and the images after two steps of transfer can be viewed as the generated images. By this way, we obtain the average PSNR of 33.27dB, indicating the effectiveness of our DIAT for local attribute transfer.</figDesc><table /><note>• Identity verification.• Image quality.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V ATTRIBUTE</head><label>V</label><figDesc>CLASSIFICATION ACCURACY FOR TRANSFER RESULTS.</figDesc><table><row><cell>mouth open</cell><cell>mouth close</cell><cell>glasses removal</cell><cell>gender</cell><cell>age</cell></row><row><cell>0.821</cell><cell>0.806</cell><cell>0.763</cell><cell>0.684</cell><cell>0.702</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI FACE</head><label>VI</label><figDesc>VERIFICATION ACCURACY FOR THE TRANSFER RESULTS.</figDesc><table><row><cell>mouth open</cell><cell>mouth close</cell><cell>glasses removal</cell></row><row><cell>0.912</cell><cell>0.903</cell><cell>0.872</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>OF PSNR (IN DB) FOR FACE HALLUCINATION.</figDesc><table><row><cell></cell><cell>Bicubic</cell><cell>Unet [?]</cell><cell>DIAT</cell></row><row><cell>PSNR</cell><cell>29.68</cell><cell>30.12</cell><cell>28.85</cell></row><row><cell>SSIM</cell><cell>0.606</cell><cell>0.672</cell><cell>0.643</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.istockphoto.com/hk</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="365" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic 3d avatar creation from hand-held video input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Ichim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<idno>45:1-45:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Being John Malkovich</title>
		<imprint>
			<biblScope unit="page" from="341" to="353" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Age synthesis and estimation via faces: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1955" to="1976" />
			<date type="published" when="2010-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Illumination-aware age progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="3334" to="3341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Single-view hair modeling using a hairstyle database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno>125:1-125:9</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transfiguring portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="94" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2530" to="2538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets for convolutional face generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convolutional Neural Networks for Visual Recognition, Winter semester</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">231</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="776" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Conditional image generation with pixelcnn decoders</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08155</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Convolutional network for attributedriven and identity-preserving human face generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06434</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Invertible Conditional GANs for image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Álvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Adversarial Training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Genegan: Learning object transfiguration and attribute subspace from unpaired data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04932</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning residual images for face attribute manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1225" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep identity-aware transfer of facial attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05586</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual tracking with convolutional random vector functional link network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3243" to="3253" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning a discriminative model for the perception of realism in composite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3943" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5188" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for inverse problems in imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Froustey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4509" to="4522" />
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked convolutional denoising auto-encoders for feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1017" to="1027" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization and its applications to low level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Filters, random fields and maximum entropy (frame): Towards a unified theory for texture modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="126" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1538" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03417</idno>
		<title level="m">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03657</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with perceptual and contextual losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Painting style transfer for head portraits using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Doyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">129</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Security games with unknown adversarial strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baykal-Gursoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2291" to="2299" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adversarial feature selection against evasion attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="766" to="777" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition by margin-based cross-modality metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1814" to="1826" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pairwise identity verification via linear concentrative metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Idrissi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="324" to="335" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Neural networks for machine learning. Coursera lecture 6e</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
