<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mascharka</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory † 2 Planck Aerosystems ‡</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory † 2 Planck Aerosystems ‡</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Soklaski</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory † 2 Planck Aerosystems ‡</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory † 2 Planck Aerosystems ‡</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual question answering requires high-order reasoning about an image, which is a fundamental capability needed by machine systems to follow complex directives. Recently, modular networks have been shown to be an effective framework for performing visual reasoning tasks. While modular networks were initially designed with a degree of model transparency, their performance on complex visual reasoning benchmarks was lacking. Current stateof-the-art approaches do not provide an effective mechanism for understanding the reasoning process. In this paper, we close the performance gap between interpretable models and state-of-the-art visual reasoning methods. We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The fidelity and interpretability of the primitives' outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. Critically, we show that these primitives are highly performant, achieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show that our model is able to effectively learn generalized representations when provided a small amount of data containing novel object attributes. Using the CoGenT generalization task, we show more than a 20 percentage point improvement over the current state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ample, in order to answer the question "What color is the cube to the right of the large metal sphere?", a model must identify which sphere is the large metal one, understand what it means for an object to be to the right of another, and apply this concept spatially to the attended sphere. Within this new region of interest, the model must find the cube and determine its color. This behavior should be compositional to allow for arbitrarily long reasoning chains. While a wide variety of models have recently been proposed for the VQA task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>, neural module networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref> are among the most intuitive. Introduced by Andreas et al. <ref type="bibr" target="#b1">[2]</ref>, neural module networks compose a question-specific neural network, drawing from a set of modules that each perform an individual operation. This design closely models the compositional nature of visual reasoning tasks. In the original work, modules were designed with an attention mechanism, which allowed for insight into the model's operation. However, the approach did not perform well on complex visual reasoning tasks such as CLEVR <ref type="bibr" target="#b12">[13]</ref>. Modifications by Johnson et al. <ref type="bibr" target="#b13">[14]</ref> address the performance issue at the cost of losing model transparency. This is problematic, because the ability to inspect each step of the reasoning process is crucial arXiv:1803.05268v2 [cs.CV] 2 Jul 2018 for real-world applications, in order to ensure proper model behavior, build user trust, and diagnose errors in reasoning.</p><p>Our work closes the gap between performant and interpretable models by designing a module network explicitly built around a visual attention mechanism. We refer to this approach as Transparency by Design (TbD), illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. As Lipton <ref type="bibr" target="#b15">[16]</ref> notes, transparency and interpretability are often spoken of but rarely defined. Here, transparency refers to the ability to examine the intermediate outputs of each module and understand their behavior at a high level. That is, the module outputs are interpretable if they visually highlight the correct regions of the input image. This ensures the reasoning process can be interpreted. We concretely define this notion in Section 4.1, and provide a quantitative analysis. In this paper, we:</p><p>1. Propose a set of composable visual reasoning primitives that incorporate an attention mechanism, which allows for model transparency.</p><p>2. Demonstrate state-of-the-art performance on the CLEVR <ref type="bibr" target="#b12">[13]</ref> dataset.</p><p>3. Show that compositional visual attention provides powerful insight into model behavior. <ref type="bibr" target="#b3">4</ref>. Propose a method to quantitatively evaluate the interpretability of visual attention mechanisms. <ref type="bibr" target="#b4">5</ref>. Improve upon the current state-of-the-art performance on the CoGenT generalization task <ref type="bibr" target="#b12">[13]</ref> by 20 percentage points.</p><p>The structure of this paper is as follows. In Section 2, we discuss related work in visual question answering and visual reasoning, which motivates the incorporation of an explicit attention mechanism in our model. Section 3 presents the Transparency by Design networks. In Section 4, we present our VQA experiments and results. A discussion of our contributions is presented in Section 5. The code for replicating our experiments is available at https://github.com/ davidmascharka/tbd-nets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual question answering (VQA) requires reasoning over both visual and textual information. A naturallanguage component must be used to understand the question that is asked, and a visual component must reason over the provided image in order to answer that question. The two main methods to address this problem are (1) to parse the question into a series of logical operations, then perform each operation over the image features or (2) to embed both the image and question into a feature space, and then reason over the features jointly.</p><p>Neural Module Networks (NMNs) follow the first approach. NMNs were introduced by Andreas et al. <ref type="bibr" target="#b1">[2]</ref>, and later extended by Andreas et al. <ref type="bibr" target="#b2">[3]</ref>, Johnson et al. <ref type="bibr" target="#b13">[14]</ref>, and Hu et al. <ref type="bibr" target="#b9">[10]</ref>. A natural-language component parses the given question and determines the series of logical steps that should be carried out to answer the question. A module is a small neural network used to perform a given logical step. By composing the appropriate modules, the logical program produced by the natural language component is carried out and an answer is produced. For example, to answer "What color is the large metal cube?", the output of a module that locates large objects can be composed with a module that finds things made of metal, then with a module that localizes cubes. A module that determines the color of objects can then be given the cube module's output to produce an answer.</p><p>The original work by Andreas et al. <ref type="bibr" target="#b1">[2]</ref> provided an attention mechanism, which allowed for a degree of model transparency. However, their model struggled with long chains of reasoning and global context. The later work of Andreas et al. <ref type="bibr" target="#b2">[3]</ref> focused on improving the flexibility of the natural-language component and on learning to compose modules rather than dictate how they should be composed. The modifications by Hu et al. <ref type="bibr" target="#b9">[10]</ref> built off this work, focusing on incorporating question features into the network modules and improving the natural-language parser that determines how modules should be composed. While achieving higher accuracy than its predecessors, this model also struggles with long chains of reasoning and does not perform as well as other methods on visual reasoning benchmarks such as CLEVR <ref type="bibr" target="#b12">[13]</ref>.</p><p>Johnson et al. <ref type="bibr" target="#b13">[14]</ref> built on the NMN approach by modifying the natural language component of their network to allow for more flexibility and developing a set of modules whose generic design is shared across several operations. These modifications led to an impressive increase in performance on the CLEVR dataset. However, their modules are not easily interpretable, because they process highdimensional features throughout their entire network. The gradient-based mechanism through which they, along with several others <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>, visualize attention can be limiting.</p><p>Gradient-based methods can provide reasonable visualizations at the penultimate layer <ref type="bibr" target="#b13">[14]</ref> of a neural module network. However, as depicted in <ref type="figure" target="#fig_2">Figure 2</ref>, the regions of attention produced for an intermediate module are unreliable, and because gradient-based methods flow backward through a network, these visualizations inappropriately depend on downstream modules in the network.</p><p>Attention. Some approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> propose an attention mechanism whereby each word corresponds to some feature of an image. One major difficulty with this type of approach is that some words have no clear semantic content in image-space. For example, the word 'sitting' does not have a clear region of focus in the question "What object is the man sitting on?", and seems to rely on further analysis of the semantics of the question. The key components of this question are 'man' and 'object being [sat] on.' This is a problem for the natural language processing pipeline rather than the visual component of a system.</p><p>Several authors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25]</ref> have proposed attention mechanisms that a network can use, optionally. In the context of providing transparent models, this can be problematic as a network can learn not to use an attended region at all. By explicitly forcing the attention mechanism to be used, we ensure our network uses attended regions in an intuitive way.</p><p>Many authors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33</ref>] use a spatial softmax to compute attention weights. This enforces a global normalization across an image, which results in scene-dependent attention magnitudes. For example, in an image with a single car, a model asked to attend to the cars would ideally put zero attention on every region that does not contain a car and full (unity) attention on the region containing the car. In an image with two cars, a spatial softmax will force each car region to have an attention magnitude of one-half. This issue is noted by Zhang et al. <ref type="bibr" target="#b30">[31]</ref> in the context of counting, but we note a more general problem. To addres this, we utilize an elementwise sigmoid to ensure that the activation at each pixel lies between zero and one, and do not introduce any form of global normalization. Further details on our network architecture and motivation are supplied in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Transparency by Design</head><p>Breaking a complex chain of reasoning into a series of smaller subproblems, each of which can be solved independently and composed, is a powerful and intuitive means for reasoning. This type of modular structure also permits inspection of the the network output at each step in the reasoning process, contingent on module designs that produce interpretable outputs. Motivated by this, we introduce a neural module network that explicitly models an attention mechanism in image space, which we call a Transparency by Design network (TbD-net), following the fact that transparency is a motivating factor in our design decisions. Meant to achieve performance at the level of the model from Johnson et al. <ref type="bibr" target="#b13">[14]</ref> while providing transparency similar to Andreas et al. <ref type="bibr" target="#b1">[2]</ref> and Hu et al. <ref type="bibr" target="#b9">[10]</ref>, our model incorporates design decisions from all three architectures. The program generator from Johnson et al. <ref type="bibr" target="#b13">[14]</ref> allows for impressive flexibility and performs exceptionally well, so we reuse this component in our network. We thus use their set of primitive operations, listed in <ref type="table" target="#tab_0">Table 1</ref>, but redesign each module according to its intended function. The resulting modules are similar in spirit to the approaches taken by Andreas et al. <ref type="bibr" target="#b1">[2]</ref> and Hu et al. <ref type="bibr" target="#b9">[10]</ref>.</p><p>To motivate this design decision, consider that some modules need only focus on local features in an image, as in the case of an Attention module which focuses on distinct objects or properties. Other modules need global context in order to carry out their operation, as in the case of Relate modules, which must be capable of shifting attention across an entire image. We combine our prior knowledge about each module's task with empirical experimentation, resulting in a set of novel module architectures optimized for each operation.</p><p>In the visual question answering task, most steps in the reasoning chain require localizing objects that have some distinguishing visible property (e.g. color, material, etc.). We ensure that each TbD module performing this type of filtering outputs a one-dimensional attention mask, which explicitly demarcates the relevant spatial regions. Thus, rather than refine high-dimensional feature maps throughout the network, a TbD-net passes only attention masks between its modules. By intentionally forcing this behavior, we produce a strikingly interpretable and intuitive model. This marks a step away from complex neural networks as black boxes. <ref type="figure" target="#fig_3">Figure 3</ref> shows an example of how a TbDnet's attention shifts appropriately throughout its reasoning chain as it solves a complex VQA problem, and that this process is easily interpretable via direct visualization of the attention masks it produces. Note that our modules' use of attention is not a learnable option, as it is in the work of Hu et al. <ref type="bibr" target="#b9">[10]</ref>. Rather, our modules must utilize the attention that is passed into them, and thus must produce precise attention maps. All the attention masks we display were generated using a perceptually-uniform color map <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture Details</head><p>We now describe the architecture of each of our modules. <ref type="table" target="#tab_0">Table 1</ref> provides an overview of each module type. Several modules share input and output types (e.g. Attention and Relate) but differ in implementation, which is suited to their particular task. Additional details on the implementation and operation of each module can be found in the supplementary material.</p><p>We use image features extracted from ResNet-101 <ref type="bibr" target="#b7">[8]</ref>  Attention modules attend to the regions of the image that contain an object with a specified property. For example, this type of module would be used to locate the red objects in a scene. The Attention module takes as input image features from the stem and a previous attention to refine (or an all-one tensor if it is the first Attention in the network) and outputs a heatmap of dimension 1 × H × W corresponding to the objects of interest, which we refer to as an attention mask. This is done by multiplying the input image features by the input attention mask elementwise, then processing those attended features with a series of convolutions.</p><p>Logical And and Or modules combine two attention masks in a set intersection and union, respectively. These operations need not be learned, since they are already welldefined and can be implemented by hand. The And module takes the elementwise minimum of two attention masks, spatially, while the Or module takes the elementwise maximum.</p><p>A Relate module attends to a region that has some spatial relation to another region. For example, in the question "What color is the cube to the right of the small sphere?", the network should determine the position of the small sphere using a series of Attention modules, then use a Relate module to attend to the region that is spatially to the right. This module needs global context in its operation, so that regions to the far right can be influenced by an object on the far left, for instance. Zhu et al. <ref type="bibr" target="#b31">[32]</ref> aptly note that common VQA architectures have receptive fields that are too small for this global information propagation. They propose a structured solution using a condi-tional random field. Our solution is to use a series of dilated convolutions <ref type="bibr" target="#b28">[29]</ref> in order to expand the receptive field to the entire image, providing the global context needed by Relate. These modules take as input image features from the stem and the attention mask from the previous module and output an attention mask.</p><p>A Same module attends to a region, extracts a relevant property from that region, and attends to every other region in the image that shares that property. As an example, when answering the question "Is anything the same color as the small cube?", the network should localize the small cube via Attention modules, then use a Same module to determine its color and output an attention mask localizing all other objects sharing that color. As with the Relate module, a Same module must take into account context from distant spatial regions. However, the Same operation differs in its execution, since it must perform a more complex function. We perform a cross-correlation between the object of interest and every other object in the scene to determine which objects share the same property as the object of interest, then send this output through a convolutional layer to produce an attention mask. Hence, the Same modules take as input stem features and an attention mask and produce an attention mask.</p><p>Query modules extract a feature from an attended region of an image. For example, these modules would determine the color of an object of interest. Each Query takes as input stem features and an attention mask and produces a feature map encoding the relevant property. The image features are multiplied by the input attention elementwise, then processed by a series of convolutions. A Query module is also used for determining whether a given description matches any object (existence questions) and for counting.</p><p>A Compare module compares the properties output by two Query modules and produces a feature map which encodes whether the properties are the same. This module would be used to answer the question "Are the cube and the metal sphere the same size?", for example. Two feature maps from Query modules are provided as input, concatenated, then processed by a series of convolutions. A feature map is output, encoding the relevant information.</p><p>The final piece of our module network is a classifier that takes as input the feature map from either a Query or Compare module and produces a distribution over answers. We again follow the work of Johnson et al. <ref type="bibr" target="#b13">[14]</ref>, using a series of convolutions, followed by max-pooling and fully-connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our model using the CLEVR dataset <ref type="bibr" target="#b12">[13]</ref> and CLEVR-CoGenT. CLEVR is a VQA dataset consisting of a training set of 70k images and 700k questions, as well as test and validation sets of 15k images and 150k questions about objects in a rendered three-dimensional scene designed to test compositional reasoning. For more details about the task, we refer the reader to Johnson et al. <ref type="bibr" target="#b12">[13]</ref>.</p><p>In our model, all convolutional filters are initialized as described by He et al. <ref type="bibr" target="#b8">[9]</ref>. Note that our architectural changes do not affect the natural language processing component from Johnson et al. <ref type="bibr" target="#b13">[14]</ref>, which determines the composition of a modular network. For simplicity, we use ground truth programs to train our network, because we do not modify the program generator. We find that training on ground truth programs does not affect the accuracy of the model from Johnson et al. <ref type="bibr" target="#b13">[14]</ref>, compared to training with generated programs. Our training procedure thus takes triplets (x, z, a) of image, program, and answer for training. We use the Adam optimization method <ref type="bibr" target="#b14">[15]</ref> with learning rate set to 10 −4 and our module network is trained end-to-end with early stopping. The training procedures for CLEVR and for CoGenT are the same. Ground truth programs are not provided with the CLEVR and CoGenT test sets, so we use the program generator from Johnson et al. <ref type="bibr" target="#b13">[14]</ref> to compute programs from questions. The testing procedure thus takes image and question pairs (x, q), produces a program π(q) to arrange modules, then produces an answerâ = M π(q) (x) where M π(q) is the arrangement of modules produced by the program generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CLEVR</head><p>Our initial model achieves 98.7% test accuracy on the CLEVR dataset, far outperforming other neural module network-based approaches. As we will describe, we utilize the attention masks produced by our model to refine this initial model, resulting in state-of-the-art performance of 99.1% accuracy. Given the large number of highperforming models on CLEVR <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19]</ref>, we train our model 5 times for a statistical measure of performance, achieving mean validation accuracy of 99.1% with standard deviation 0.07. Further, we note that none of these other models are amenable to having their reasoning processes inspected in an intuitive way. As we will show, our model provides straightforward, interpretable outputs at every stage of the visual reasoning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Attention as a Diagnostic Tool</head><p>Regularization. Examining the attention masks produced by our initial model, we noticed noise in the background. While not detrimental to our model's performance, these spurious regions of attention may be confusing to users attempting to garner an understanding of the model outputs. Our intuition behind this behavior is that the model is not penalized for producing small amounts of attention on background regions because the later Query modules are able to effectively ignore them, since they contain no objects. As a result, no error signal propagates back through the model to push these to zero. To address this, we apply weighted L 1 regularization to the intermediate attention mask outputs, providing an explicit signal to minimize unnecessary activations. Experimentally, we find a factor of 2.5 × 10 −7 to be effective in reducing spurious attention while maintaining strong attentions in true regions of interest. A comparison of an Attention module output with and without this regularization can be seen in <ref type="figure" target="#fig_4">Figure 4</ref>. Without regularization, the module produces a small amount of attention on background regions, high attention on objects of interest, and zero attention on all other objects. When we add this regularization term, the spurious background activations fade, leaving a much more precise attention mask. Spatial resolution. Examining the attention masks from our initial model indicated the resolution of the input feature maps is crucial for performance. Originally, our model took 14 × 14 feature maps as input to each module. However, this was insufficient for resolving closely-spaced objects. Increasing the resolution of the input feature maps to 28×28 alleviates these issues, which we do by extracting features from an earlier layer of ResNet. <ref type="figure">Figure 5</ref> shows our model operating over 14×14 and 28×28 feature maps when asked to attend to an object in a narrow spatial region. Operating on 14 × 14 feature maps, our model coarsely defines the region and incorrectly guesses that the object is a sphere. Using 28 × 28 feature maps refines the region and allows the model to correctly identify the cylinder.</p><p>Performance improvements. By adding regularization and increasing spatial resolution, two strategies developed by examining our model attentions, we improve the accuracy of our model on CLEVR from 98.7% to 99.1%, a new state of the art. A full table of results including compar-isons with several other approaches can be seen in <ref type="table">Table 2</ref>. While conceptually similar to prior work in modular networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref>, our model differs drastically in module design. In addition to achieving superior performance, our paradigm allows for the verification of module behavior and informs network design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Transparency</head><p>We examine the attention masks produced by the intermediate modules of our TbD model. We show that our model explicitly composes visual attention masks to arrive at an answer, leading to an unprecedented level of transparency in the neural network. <ref type="figure" target="#fig_3">Figure 3</ref> shows the composition of visual attentions through an entire question. In this section, we provide a quantitative analysis of transparency. We further examine the outputs of several modules, displayed without any smoothing, showing that each step of any composition is straightforwardly interpretable.</p><p>Quantitative Analysis of Attention. Here we propose a quantitative analysis of the interpretability of visual attention, which will allow for direct comparison with subsequent work in this area. In this context, a module's attention is interpretable if it visually highlights the correct objects in a scene, without ambiguity. Specifically, we measure how often the center-of-mass of an attended region overlaps with the appropriate regions of the ground truth segmentation. We perform this analysis for each of our Attention modules within a given chain of reasoning. The CLEVR dataset generator <ref type="bibr" target="#b12">[13]</ref> was used to produce 1k images and 10k questions for evaluation. This analysis produces precision and recall metrics to summarize the interpretable quality of a model's attention.</p><p>Our original TbD-net model has a recall of 0.86 and a precision of 0.41. This low precision is largely due to attention placed on the background (analyzing the performance on foreground objects alone yields a precision of 0.95). The modifications to our model detailed in Section 4.1.1 dramatically improve the interpretability metrics as evaluated on the full image (foreground and background). Adding regularization improves the recall and precision values to 0.92 <ref type="figure">Figure 5</ref>. An input image (left) and the attention mask produced by the model when asked to attend to the region behind the blue rubber object and in front of the large cyan rubber cylinder with 14 × 14 (middle) and 28 × 28 (right) input features. <ref type="table">Table 2</ref>. Performance comparison of state-of-the-art models on the CLEVR dataset. Our model performs well while maintaining model transparency. We achieve state of the art performance on Query questions, while remaining competitive in all other categories. Our TbD model is trained without regularizing the output attention masks, while '+ reg' indicates the use of the regularization scheme described in the text. The '+ hres' indicator shows a model was trained using higher-resolution 28 × 28 feature maps rather than 14 × 14 feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Overall  and 0.90, respectively, and increasing the spatial resolution further improves the values to 0.99 and 0.98, respectively. Qualitative Analysis of Attention. <ref type="figure" target="#fig_5">Figure 6</ref> shows the output of an Attention module that focuses on metal objects. The output is sensible and matches our intuition of where attention should be placed in the image. Maximal attention is given to the metal objects and minimal attention to the rubber objects and to the background region.</p><p>The Attention modules are the simplest of the visual primitives. However, more complex operations, such as Same and Relate, still produce intuitive attention masks. We show in <ref type="figure" target="#fig_6">Figures 7 and 8</ref> that these modules are just as transparent and easy to understand. The Relate module is given an attention mask highlighting the purple cylinder and shifts attention to the right. We see it highlights the entire region of the image, which it is able to do due to its expanded receptive field. The Same module is given an attention mask focusing on the blue sphere, then is asked to shift its focus to the objects of the same color. It gives maximal attention to the other three blue objects, minimal attention to all other objects, and a small amount of attention to the background.</p><p>Our logical And and Or modules perform set intersection and union operations, respectively. Their behaviors are as intuitive as those set operations. Since Query and Compare modules output feature maps rather than attention masks, their outputs cannot be easily visualized. This is not problematic, as their corresponding operations do not have clear visual anchors. That is, it is not sensible to highlight image regions based on concepts such as 'what shape' or 'are the same number,' which have no referent without additional textual context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CLEVR-CoGenT</head><p>The CLEVR-CoGenT dataset provides an excellent test for generalization. It is identical in form to the CLEVR dataset with the exception that it has two different conditions. In Condition A all cubes are colored one of gray, blue, brown, or yellow, and all cylinders are one of red, green, purple, or cyan; in Condition B the color palettes are swapped. This provides a check that the model does   not tie together the notions of shape and color. In this section, we report only our best-performing model. Like previous work <ref type="bibr" target="#b13">[14]</ref>, our performance is worse on Condition B than Condition A after training only using Condition A data. As <ref type="table" target="#tab_2">Table 3</ref> shows, our model achieves 98.8% accuracy on Condition A, but only 75.4% on Condition B. Following Johnson et al. <ref type="bibr" target="#b13">[14]</ref>, we then fine-tune our model using 3k images and 30k questions from the Condition B data. Whereas other models see a significant drop in performance on the Condition A data after fine-tuning, our model maintains high performance. As <ref type="table" target="#tab_2">Table 3</ref> shows, our model can effectively learn from a small amount of Condition B data. We achieve 96.9% accuracy on Condition A and 96.3% accuracy on Condition B after fine-tuning, far surpassing the highest-reported 76.1% Condition A and 92.7% Condition B accuracy.</p><p>We perform an analysis of conditional probabilities based on shape/color dependencies to determine the cause of our model's poor performance on Condition B before fine-tuning. Using the dataset from Section 4.1.2, we demonstrate that the model's ability to identify shape, in particular, depends heavily on the co-occurrence of shape and color. <ref type="table">Table 4</ref> shows that the model is only able to effectively identify shapes in colors it has seen before finetuning, while it can identify color regardless of shape. Finetuning on Condition B rectifies the entanglement. <ref type="figure">Figure 9</ref> shows this entanglement visually. When asked to attend to cubes before fine-tuning, our model correctly focuses on the gray and blue cubes in that image, but ig- <ref type="table">Table 4</ref>. Our model's ability to determine the shape of an object depends heavily on the co-occurrence of shape and color before fine-tuning, while its ability to determine the color of an object does not. P ( ) indicates the probability of correctly identifying an object. A and B correspond to the CoGenT color/shape splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predict Shape</head><p>Predict Color P ( |A) P ( |B) P ( |A) P ( |B) nores the cyan cube and incorrectly places some attention on the brown cylinder. What is particularly noteworthy is the fact that our model's representations of color are complete (with respect to CLEVR). When the network is asked to attend to the brown objects, it correctly identifies both the sphere and cylinder, even though it has only seen cubes and spheres in brown. Thus, our model has entangled its representation of shape with color, but has not entangled its representation of color with shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We have presented Transparency by Design networks, which compose visual primitives that leverage an explicit attention mechanism to perform reasoning operations. Unlike their predecessors, the resulting neural module networks are both highly performant and readily interpretable. This is a key advantage to utilizing TbD models-the ability to directly evaluate the model's learning process via the produced attention masks is a powerful diagnostic tool. One can leverage this capability to inspect the semantics of a visual operation, such as 'same color,' and redesign modules to address apparent aberrations in reasoning. Using these attentions as a means to improve performance, we achieve state-of-the-art accuracy on the challenging CLEVR dataset and the CoGenT generalization task. Such insight into a neural network's operation may also help build user trust in visual reasoning systems.</p><p>following tables, δ(·) will indicate a rectified linear function and σ(·) will indicate a sigmoid activation. The input size R × C indicates R rows and C columns in the input. In our original model, R = C = 14, while our high-resolution model uses R = C = 28.</p><p>The architecture of the Attention modules can be seen in <ref type="table">Table 5</ref>. These modules take stem features and an attention mask as input and produce an attention mask as output. We first perform an elementwise multiplication of the input features and attention mask, broadcasting the attention mask along the channel dimension of the input features. We refer to this process as 'attending to' the features. We then process the attended features with two 3x3 convolutions, each with 128 filters and a ReLU, then use a single 1x1 convolution followed by a sigmoid to project down to an attention mask. This architecture is motivated by the design of the unary module from Johnson et al. <ref type="bibr" target="#b13">[14]</ref>.</p><p>The And and Or modules, seen in <ref type="table" target="#tab_4">Table 6 and Table 7</ref>, respectively, perform set intersection and union operations. These modules return the elementwise minimum and maximum, respectively, of two input attention masks. This is motivated by the logical operations that Hu et al. <ref type="bibr" target="#b9">[10]</ref> implement, which seems a natural expression of these operations. Such simple and straightforward operations need not be learned, since they can be effectively and efficiently implemented by hand.</p><p>The Relate module, shown in <ref type="table">Table 8</ref>, needs global context to shift attention across an entire image. Motivated by this, we use a series of dilated 3x3 convolutions, with dilation factors 1, 2, 4, 8, and 1, to expand the receptive field to the entire image. The choice of dilation factors is informed by the work of Yu and Koltun <ref type="bibr" target="#b28">[29]</ref>. These modules receive stem features and an attention mask and produce an attention mask. Each convolution in the series has 128 fil- <ref type="table">Table 5</ref>. Architecture of an Attention module, which takes as input Features and an Attention and produces an Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Layer</head><p>Output Size   <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref> 1 × R × C <ref type="table">Table 8</ref>. Architecture of a Relate module. These modules receive as input Features and an Attention and produce an Attention.</p><formula xml:id="formula_0">(1) Features 128 × R × C (2) Previous module output 1 × R × C (3) Elementwise multiply (1) and (2) 128 × R × C (4) δ(Conv(3 × 3, 128 → 128)) 128 × R × C (5) δ(Conv(3 × 3, 128 → 128)) 128 × R × C (6) σ(Conv(1 × 1, 128 → 1)) 1 × R × C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Layer</head><p>Output Size <ref type="formula">(1)</ref> and <ref type="bibr" target="#b1">(2)</ref> 128</p><formula xml:id="formula_1">(1) Features 128 × R × C (2) Previous module output 1 × R × C (3) Elementwise multiply</formula><formula xml:id="formula_2">× R × C (4) δ(Conv(3 × 3, 128 → 128, dilate 1)) 128 × R × C (5) δ(Conv(3 × 3, 128 → 128, dilate 2)) 128 × R × C (6) δ(Conv(3 × 3, 128 → 128, dilate 4)) 128 × R × C (7) δ(Conv(3 × 3, 128 → 128, dilate 8)) 128 × R × C (8) δ(Conv(3 × 3, 128 → 128, dilate 1)) 128 × R × C (9) σ(Conv(1 × 1, 128 → 1)) 1 × R × C</formula><p>ters and is followed by a ReLU. A final convolution then reduces the feature map to a single-channel attention mask, and a sigmoid nonlinearity is applied. The Same module is the most complex of our modules, and the most complex operation we perform. To illustrate this, consider the Same[shape] module. It must determine the shape of the attended object, compare that shape with the shape of every other object in the scene (which requires global information propagation), and attend to all the objects that share that shape. Initially, we used a design similar to the Relate module to perform this operation, but found it did not perform well. After further reflection, we posited this was because the Relate module does not have a mechanism for remembering which object we are interested in performing the Same with respect to. <ref type="table">Table 9</ref> provides an overview of the Same module. Here we explicate the notation. Provided with stem features and an attention mask as input, we take the arg max of the feature map, spatially. This gives us the (x, y) position of the object of interest (i.e. the object to perform the Same with respect to). We then extract the feature vector at this spatial location in the input feature map, which gives us the vector encoding the property of interest (among other properties). Next, we perform an elementwise multiplication with the feature vector at every spatial dimension. This essentially performs a cross-correlation of the feature vector of interest with the feature vector of every other position in the image. Our intuition is that the vector dimensions that encode the property of interest will be 'hot' at every point sharing that property with the object of interest. At this point, a convolution could be learned that attends to the relevant regions.</p><p>However, the Same operation, by its definition in CLEVR, must not attend to the original object. That is, an object is by definition not the same property as itself. Therefore, the Same module must learn not to attend to the original object. We thus concatenate the original input attention mask with the cross-correlated feature map, allowing the convolutional filter to know which object was the original, and thus ignore it. <ref type="table">Table 9</ref>. Architecture of a Same module. These modules receive as input Features and an Attention and produce an Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Layer</head><p>Output Size</p><p>(1) Features 128 × R × C (2) Previous module output 1 × R × C (3) arg max x,y (2) 1 × 1 × 1 (4) (1) <ref type="bibr" target="#b2">(3)</ref> 128 × 1 × 1 (5) Elementwise multiply (1) and (4) 128 × R × C (6) Concatenate <ref type="formula">(5)</ref> and <ref type="formula">(2)</ref> 129 × R × C (7) σ(Conv(1 × 1, 129 → 1)) 1 × R × C</p><p>The Query module architecture can be seen in <ref type="table" target="#tab_0">Table 10</ref>. Its design is similar to that of the Attention modules and is likewise inspired by the unary module design of Johnson et al. <ref type="bibr" target="#b13">[14]</ref> and adapted for receiving an attention mask and stem features as input. These modules produce a feature map as output, and thus do not have a convolutional filter that performs a down-projection.</p><p>The Compare module, shown in <ref type="table" target="#tab_0">Table 11</ref>, is inspired by the binary module of Johnson et al. <ref type="bibr" target="#b13">[14]</ref>. These modules take two feature maps as input and produce a feature map as output. Their purpose is to determine whether the two input feature maps encode the same property. <ref type="table" target="#tab_0">Table 10</ref>. Architecture of a Query module. These modules receive as input Features and an Attention and produce an Encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Layer</head><p>Output Size   <ref type="formula">(1)</ref> and <ref type="bibr" target="#b1">(2)</ref> 128 × R × C (4) δ(Conv(1 × 1, 128 → 128)) 128 × R × C (5) δ(Conv(3 × 3, 128 → 128)) 128 × R × C (6) δ(Conv(3 × 3, 128 → 128)) 128 × R × C</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>A</head><label></label><figDesc>visual question answering (VQA) model must be capable of complex spatial reasoning over an image. For ex- * Indicates equal contribution. † This material is based upon work supported by the Assistant Secretary of Defense for Research and Engineering under Air Force Contract No. FA8721-05-C-0002 and/or FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Assistant Secretary of Defense for Research and Engineering. ‡ This work conducted while Philip was at MIT Lincoln Laboratory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>A diagram of a visual question answering task, in which our proposed Transparency by Design network (TbD-net) composes a series of attention masks that allow it to correctly count two large metal cylinders in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Gradient-based visualizations of an intermediate output (attention on the brown cylinder) of a neural module network produce unreliable attention masks. Furthermore, changing a downstream module from query color (middle) to query size (right) alters the visualization of the attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Read from top to bottom, a Transparency by Design network (TbD-net) composes visual attention masks to answer a question about objects in a scene. The tree diagram (left) indicates the modules used by the TbD-net, and their corresponding attention masks are depicted on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>An input image (left) and the attention mask produced by the Attention[large] module overlaid atop the input image. Without penalizing attention mask outputs (middle) the attention mask is noisy and produces responses on background regions. Penalizing the attention outputs (right) provides a signal to reduce extraneous attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>An input image (left) and the attention mask produced by the Attention[metal] module (right). When the attention mask is overlaid atop the input image (middle), it is apparent that the attention is appropriately focused on the metal objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>An input image (left) and the attention mask produced by the Relate[right] module (right) when it receives an attention on the purple cylinder. When the attention mask is overlaid atop the input image (middle), it is apparent that the attention is focused on the region to the right of the purple cylinder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>An input image (left) and the attention mask produced by the Same[color] module (right) when it receives an attention on the blue sphere. The attention mask overlaid atop the input image (middle) demonstrates that the module successfully performs the complex operation of (1) determining the color of the sphere (2) determining the color of all the other objects in the scene and (3) attending to the objects with the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( 1 )</head><label>1</label><figDesc>Features 128 × R × C (2) Previous module output 1 × R × C (3) Elementwise multiply (1) and (2) 128 × R × C (4) δ(Conv(3 × 3, 128 → 128)) 128 × R × C (5) δ(Conv(3 × 3, 128 → 128)) 128 × R × C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>A summary of the modules used in our Transparency by Design network. 'Attention' and 'Encoding' refer to single-and highdimensional outputs, respectively, from a preceding module. 'Stem' refers to image features produced by a trained neural network. The variables x and y refer to distinct objects in the scene, while [property] refers to one of color, shape, size, or material.</figDesc><table><row><cell>Module Type</cell><cell>Operation</cell><cell>Language Analogue</cell></row></table><note>Attention Attention × Stem → Attention Which things are [property]? Query Attention × Stem → Encoding What [property] is x? Relate Attention × Stem → Attention Left of, right of, in front, behind Same Attention × Stem → Attention Which things are the same [property] as x? Comparison Encoding × Encoding → Encoding Are x and y the same [property]? And Attention × Attention → Attention Left of x and right of y Or Attention × Attention → Attention Left of x or right of y and feed these through a simple convolutional block called the 'stem,' following the work of Johnson et al. [14]. Simi- lar to Hu et al. [10], and a point of departure from Johnson et al. [14] and Andreas et al. [2], we provide stem features to most of our modules. This ensures image features are readily accessible to each module and no information is lost in long compositions. The stem translates high-dimensional feature input from ResNet into lower-dimensional features suitable for our task.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison against the current state-of-theart model on the CoGenT dataset having trained only on Condition A data (middle column) and after fine-tuning on a small amount of data with novel attributes (right column).</figDesc><table><row><cell></cell><cell cols="2">Train A</cell><cell cols="2">Fine-tune B</cell></row><row><cell></cell><cell>A</cell><cell>B</cell><cell>A</cell><cell>B</cell></row><row><cell>PG + EE [14]</cell><cell cols="4">96.6 73.7 76.1 92.7</cell></row><row><cell cols="5">TbD + reg (Ours) 98.8 75.4 96.9 96.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Architecture of an And module. This module receives as input two Attentions and produces an Attention.</figDesc><table><row><cell>Index Layer</cell><cell>Output Size</cell></row><row><cell>(1) Previous module output</cell><cell>1 × R × C</cell></row><row><cell>(2) Previous module output</cell><cell>1 × R × C</cell></row><row><cell cols="2">(3) Elementwise minimum (1) and (2) 1 × R × C</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Architecture of an Or module. This module receives as input two Attentions and produces an Attention.</figDesc><table><row><cell>Index Layer</cell><cell>Output Size</cell></row><row><cell>(1) Previous module output</cell><cell>1 × R × C</cell></row><row><cell>(2) Previous module output</cell><cell>1 × R × C</cell></row><row><cell>(3) Elementwise maximum</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 11 .</head><label>11</label><figDesc>Architecture of a Compare module. These modules receive as input two Features and produce an Encoding.</figDesc><table><row><cell>Index Layer</cell><cell>Output Size</cell></row><row><cell>(1) Previous module output</cell><cell>128 × R × C</cell></row><row><cell>(2) Previous module output</cell><cell>128 × R × C</cell></row><row><cell>(3) Concatenate</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Module Details</head><p>Here we describe each module in detail and provide motivation for specific architectural choices. In all descriptions, 'image features' refer to features that have been extracted using a pretrained model <ref type="bibr" target="#b7">[8]</ref> and passed through our stem network, which is shared across modules. In all the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bottom-Up and Top-Down Attention for Image Captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep compositional question answering with neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<idno>abs/1511.02799</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<idno>abs/1601.01705</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ABC-CNN: an attention based convolutional neural network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno>abs/1511.05960</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D M</forename><surname>Drew Arad Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>accepted as poster</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
	<note>ArXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Segmentation-aware convolutional networks using local attention masks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno>abs/1708.04607</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1704.05526</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Matplotlib: A 2d graphics environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing In Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="90" to="95" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Video summarization with attention-based encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1708.09545</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/1612.06890</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<title level="m">The Mythos of Model Interpretability. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention in convnets and its application in saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1708.06433</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hierarchical Question-Image Co-Attention for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning Visual Reasoning Without Strong Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Person Re-identification Using Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taalimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G T</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Where To Look: Focus Regions for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">DDRprog: A CLEVR differentiable dynamic reasoning programmer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Residual Attention Network for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ask</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1502.03044</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Stacked Attention Networks for Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to count objects in natural images for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prgel-Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Structured Attentions for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Visual7W: Grounded Question Answering in Images. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
