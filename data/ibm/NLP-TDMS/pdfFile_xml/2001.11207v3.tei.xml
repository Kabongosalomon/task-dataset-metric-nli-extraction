<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Instance Segmentation by Deep Community Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaedong</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyun</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeany</forename><surname>Son</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETRI</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
							<email>bhhan@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Instance Segmentation by Deep Community Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a weakly supervised instance segmentation algorithm based on deep community learning with multiple tasks. This task is formulated as a combination of weakly supervised object detection and semantic segmentation, where individual objects of the same class are identified and segmented separately. We address this problem by designing a unified deep neural network architecture, which has a positive feedback loop of object detection with bounding box regression, instance mask generation, instance segmentation, and feature extraction. Each component of the network makes active interactions with others to improve accuracy, and the end-to-end trainability of our model makes our results more robust and reproducible. The proposed algorithm achieves state-of-theart performance in the weakly supervised setting without any additional training such as Fast R-CNN and Mask R-CNN on the standard benchmark dataset. The implementation of our algorithm is available on the project webpage: https://cv.snu.ac.kr/research/WSIS_CL. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection and semantic segmentation algorithms have achieved great success in recent years thanks to the advent of large-scale datasets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref> as well as the development of deep learning technologies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. However, most of existing image datasets have relatively simple forms of annotations such as image-level class labels, while many practical tasks require more sophisticated information such as bounding boxes and areas corresponding to object instances. Unfortunately, the acquisition of the complex labels needs significant human efforts, and it is challenging to construct a large-scale dataset containing such comprehensive annotations.</p><p>Instead of standard supervised learning formulations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, we tackle a more challenging task, weakly supervised instance segmentation, which relies only on image- <ref type="figure">Figure 1</ref>. The proposed community learning framework for weakly supervised instance segmentation. Our model is composed of object detection module, instance mask generation module, instance segmentation module and feature extractor, which constructs a positive feedback loop within a community. It first identifies positive detection bounding boxes from the detection module and generates pseudo-ground-truths of instance segmentation using class activation maps. The model is trained with multi-task loss of the three components using the pseudo-ground-truths. The final segmentation masks are obtained from the ensemble of outputs from instance mask generation and segmentation modules. level class labels for instance-wise segmentation. This task shares critical limitations with many weakly supervised object recognition problems; trained models typically focus too much on discriminative parts of objects in the scene, and, consequently, fail to identify whole object regions and extract accurate object boundaries in a scene. Moreover, there are additional challenges in handling two problems jointly, weakly supervised object detection and semantic segmentation; incomplete ground-truths incur noisy estimation of labels in both tasks, which makes it difficult to take advantage of the joint learning formulation. For example, although object detection methods typically employ object proposals to provide rough information of object location and size, a naïve application of instance segmentation module to weakly supervised object detection results may not be successful in practice due to noise in object proposals.</p><p>Our approach aims to realize the goal using a deep neural network with multiple interacting task-specific components that construct a positive feedback loop. The whole model is trained in an end-to-end manner and boosts performance of individual modules, leading to outstanding segmentation accuracy. We call such a learning concept community learning, and <ref type="figure">Figure 1</ref> illustrates its application to weakly supervised instance segmentation. The community learning is different from multi-task learning that attempts to achieve multiple objectives in parallel without tight interaction between participating modules. The contributions of our work are summarized below:</p><p>• We introduce a deep community learning framework for weakly supervised instance segmentation, which is based on an end-to-end trainable deep neural network with active interactions between multiple tasks: object detection, instance mask generation, and object segmentation.</p><p>• We incorporate two empirically useful techniques for object localization, class-agnostic bounding box regression and segmentation proposal generation, which are performed without full supervision.</p><p>• The proposed algorithm achieves substantially higher performance than the existing weakly supervised approaches on the standard benchmark dataset without post-processing.</p><p>The rest of the paper is organized as follows. We briefly review related works in Section 2 and describe our algorithm with community learning in Section 3. Section 4 analyzes the experimental results on a benchmark dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>This section reviews existing weakly supervised algorithms for object detection, semantic segmentation, and instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Weakly Supervised Object Detection</head><p>Weakly Supervised Object Detection (WSOD) aims to localize objects in a scene only with image-level class labels. Most of existing methods formulate WSOD as Multiple Instance Learning (MIL) problems <ref type="bibr" target="#b11">[12]</ref> and attempt to learn detection models via extracting pseudo-groundtruth labels <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47]</ref>. WSDDN <ref type="bibr" target="#b3">[4]</ref> combines classification and localization tasks to identify object classes and their locations in an input image. However, this technique is designed to find only a single object class and instance conceptually and often fails to solve the problems involving multiple labels and objects. Various approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> tackle this issue by incorporating additional components, but they are still prone to focus on the discriminative parts of objects instead of whole object regions. Recently, there are several research integrating semantic segmentation to improve detection performance <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref>. WCCN <ref type="bibr" target="#b10">[11]</ref> and TS2C <ref type="bibr" target="#b43">[44]</ref> filter out object proposals using semantic segmentation results, but still have trouble in identifying spatially overlapped objects in the same class. Meanwhile, SDCN <ref type="bibr" target="#b29">[30]</ref> utilizes semantic segmentation result to refine pseudo-ground-truths. WS-JDS <ref type="bibr" target="#b36">[37]</ref> leverages weakly supervised semantic segmentation module that estimates importance for object proposals. Although the core idea is valuable and the segmentation module improves detection performance, the instance segmentation performance improvement is marginal compared to simple box masking of its baselines <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Weakly Supervised Semantic Segmentation</head><p>Weakly Supervised Semantic Segmentation (WSSS) is a task to estimate pixel-level semantic labels in an image based on image-level class labels only. Class Activation Map (CAM) <ref type="bibr" target="#b47">[48]</ref> is widely used for WSSS because it generates class-specific likelihood maps using the supervision for image classification. SPN <ref type="bibr" target="#b25">[26]</ref>, one of the early works that exploit CAM for WSSS, combines CAM with superpixel segmentation result to extract accurate class boundaries in an image. AffinityNet <ref type="bibr" target="#b1">[2]</ref> propagates the estimated class labels using semantic affinities between adjacent pixels. Ge et al. <ref type="bibr" target="#b14">[15]</ref> employ a pretrained object detector to obtain segmentation labels. Recent approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref> often train their models end-to-end. DSRG <ref type="bibr" target="#b22">[23]</ref> and MCOF <ref type="bibr" target="#b42">[43]</ref> propose iterative refinement procedures starting from CAM. FickleNet <ref type="bibr" target="#b27">[28]</ref> performs stochastic feature selection in its convolutional layers and captures the regularized shapes of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Instance Segmentation</head><p>Instance segmentation can be regarded as a combination of object localization and semantic segmentation, which needs to identify individual object instances. There exist several fully supervised approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Haydr et al. <ref type="bibr" target="#b18">[19]</ref> utilize Region Proposal Network (RPN) <ref type="bibr" target="#b34">[35]</ref> to detect individual instances and leverage Object Mask Network (OMN) for segmentation. Mask R-CNN <ref type="bibr" target="#b19">[20]</ref>, Masklab <ref type="bibr" target="#b5">[6]</ref> and MNC <ref type="bibr" target="#b8">[9]</ref> have similar procedures to predict their pixellevel segmentation labels.</p><p>There have been recent works for Weakly Supervised Instance Segmentation (WSIS) based on image-level class labels only <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. Peak Response Map (PRM) <ref type="bibr" target="#b48">[49]</ref> takes the peaks of an activation map as pivots for individual instances and estimates the segmentation mask of each object using the pivots. Instance Activation Map (IAM) <ref type="bibr" target="#b49">[50]</ref> selects pseudo-ground-truths out of precomputed segment proposals based on PRM to learn segmentation networks. Label-PEnet <ref type="bibr" target="#b13">[14]</ref> combines various components with different functionalities to obtain the final segmentation masks. However, it involves many duplicate operations across the components and requires very complex training pipeline. There are a few attempts to gener-ate pseudo-ground-truth segmentation maps based on weak supervision and forward them to the well-established network <ref type="bibr" target="#b19">[20]</ref> for instance segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>. To improve accuracy, the algorithms often employ post-processing such as MCG proposals <ref type="bibr" target="#b2">[3]</ref> or denseCRF <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head><p>This section describes our community learning framework based on an end-to-end trainable deep neural network for weakly supervised instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview and Motivation</head><p>One of the most critical limitations in a naïve combination of detection and segmentation networks for weakly supervised instance segmentation is that the learned models often attend to small discriminative regions of objects and fail to recover missing parts of target objects. This is partly because segmentation networks rely on noisy detection results without proper interactions and the benefit of the iterative label refinement procedure is often saturated in the early stage due to the strong correlation between outputs from two modules.</p><p>To alleviate this drawback, we propose a deep neural network architecture that constructs a circular chain along with the components and generates desirable instance detection and segmentation results. The chain facilitates the interactions along individual modules to extract useful information. Specifically, the object detector generates proposallevel pseudo-ground-truth labels. They are used to create pseudo-ground-truth masks for instance segmentation module, which estimates the final segmentation labels of individual proposals using the masks. These three network components make up a community and collaborate to update the weights of the backbone network for feature extraction, which leads to regularized representations robust to overfitting to poor local optima. <ref type="figure">Figure 2</ref> presents the network architecture of our weakly supervised object detection and segmentation algorithm. As mentioned earlier, the proposed network consists of four parts: feature extractor, object detector with bounding box regressor, instance mask generator and instance segmentation module. Our feature extraction network is made of shared fully convolutional layers, where the feature of each proposal is obtained from the Spatial Pyramid Pooling (SPP) <ref type="bibr" target="#b20">[21]</ref> layers on the shared feature map and fed to the other modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Object Detection Module</head><p>For object detection, a 7 × 7 feature map is extracted from the SPP layer for each object proposal and forwarded to the last residual block (res5). Then, we pass these features to both the detector and the regressor. Since this process is compatible with any end-to-end trainable object detection network based on weak supervision, we adopt one of the most popular weakly supervised object detection networks, referred to as OICR <ref type="bibr" target="#b39">[40]</ref>, which has three refinement layers after the base detector. For each image-level class label, we extract foreground proposals based on their estimated scores corresponding to the label and apply a nonmaximum suppression (NMS) to reduce redundancy. Background proposals are randomly sampled from the proposals that are overlap with foreground proposals below a threshold. Among the foreground proposals, the one with the highest score for each class is selected as a pseudo-groundtruth bounding box.</p><p>Bounding box regression is typically conducted under full supervision to refine the proposals corresponding to objects. However, learning a regressor in our problem is particularly challenging since it is prone to be biased by discriminative parts of objects; such a characteristic is difficult to control in a weakly supervised setting and is aggravated in class-specific learning. Hence, unlike <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35]</ref>, we propose a class-agnostic bounding box regressor based on pseudo-ground-truths to avoid overly discriminative representation learning and provide better regularization effect. Note that a class-agnostic regressor has not been explored actively yet since fully supervised models can exploit accurate bounding box annotations and learning a regressor with weak labels only is not common. If a proposal has a higher IoU with its nearest pseudo-ground-truth proposal than a threshold, the proposal and the pseudo-ground-truth proposal are paired to learn the regressor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Instance Mask Generation (IMG) Module</head><p>This module constructs pseudo-ground-truth masks for instance segmentation using the proposal-level class labels given by our object detector. It takes the feature of each proposal from the SPP layers attached to multiple convolutional layers as shown in <ref type="figure">Figure 2</ref>. Since the IMG module utilizes hierarchical representations from different levels in a backbone network, it can deal with multi-scale objects effectively.</p><p>We construct pseudo-ground-truth masks for individual proposals by integrating the following additional features into CAM <ref type="bibr" target="#b47">[48]</ref>. First, we compute a background class activation map by augmenting a channel corresponding to the background class. This map is useful to distinguish objects from the background. Second, instead of the Global Average Pooling (GAP) adopted in the standard CAM, we employ the weighted GAP to give more weights to the center pixels within proposals. It computes a weighted average of the input feature maps, where the weights are given by an</p><formula xml:id="formula_0">CAM Network Pseudo GT SPP SPP SPP [512@28×28]×4 21@28×28 Instance Mask Generation Instance Segmentation 2048@7×7 UP Res1 64@ - . × / . Res2 256@ - 0 × / 0 Res3 512@ - 1 × 2 1 Res4 1024@ - 34 × 2 34</formula><p>Res5 2048@ 4×4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo Object Class Labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Detection</head><p>Detector Regressor <ref type="figure">Figure 2</ref>. The proposed network architecture for weakly supervised instance segmentation. Our end-to-end trainable network consists of four parts: (a) feature extraction network computes the shared feature maps and provides proposal-level features with the other networks, (b) object detection network identifies the location of objects and gives a pseudo-label of object class to each proposal, (c) instance mask generation network constructs the class activation map for given proposals using predicted pseudo-labels from the detector, (d) instance segmentation network predicts segmentation masks and is learned with the outputs of the above networks as pseudo-ground-truths.</p><p>isotropic Gaussian kernel. Third, we convert input features f of the CAM module to log scale values, i.e., log(1 + f ), which penalizes excessively high peaks in the CAM and leads to spatially regularized feature maps appropriate for robust segmentation. The output of the IMG module, denoted by M, is an average of three CAMs to which min-max normalizations <ref type="bibr" target="#b32">[33]</ref> are applied. For each selected proposal, the pseudo-groundtruth mask M ∈ R (C+1)×T <ref type="bibr" target="#b1">2</ref> for instance segmentation is given by the following equation using the three CAMs, M k (k = 1, 2, 3),</p><formula xml:id="formula_1">M = δ 1 3 3 k=1 M k &gt; ξ ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">M k ∈ R (C+1)×T 2 is the k th CAM whose size is T × T for all classes including background, δ[·]</formula><p>is an elementwise indicator function, and ξ is a predefined threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Instance Segmentation Module</head><p>For instance segmentation, the output of the res5 block is upsampled to T × T activation maps and provided to four convolutional layers along with ReLU layers and the final segmentation output layer as illustrated in <ref type="figure">Figure 2</ref>. This module learns a pixel-wise binary classification label for each proposal based on the pseudo-ground-truth mask M c , provided by the IMG module. The predicted mask of each proposal is a class-specific binary mask, where the class label c is determined by the detector. Note that our model is compatible with any semantic segmentation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Losses</head><p>The overall loss function of our deep community learning framework is given by the sum of losses from the three modules as</p><formula xml:id="formula_3">L = L det + L img + L seg ,<label>(2)</label></formula><p>where L det , L img , and L seg denote detection loss, instance mask generation loss, and instance segmentation loss, respectively. The three terms interact with each other to train the backbone network including the feature extractor in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Object Detection Loss</head><p>The object detection module is trained with the sum of classification loss L cls , refinement loss L refine , and bounding box regression loss L reg . The features extracted from the individual object proposals are given to the detection module based on OICR <ref type="bibr" target="#b39">[40]</ref>. Image classification loss L cls is calculated by computing the cross-entropy between image-level ground-truth class label y = (y 1 , . . . , y C ) T and its corresponding prediction φ = (φ 1 , . . . , φ C ) T , which is given by</p><formula xml:id="formula_4">L cls = − C c=1 y c log φ c + (1 − y c ) log(1 − φ c ),<label>(3)</label></formula><p>where C is the number of classes in a dataset. As in the original OICR, the pseudo-ground-truth of each object proposal in the refinement layers is obtained from the outputs of their preceding layers, where the supervision of the first refinement layer is provided by WSDDN <ref type="bibr" target="#b3">[4]</ref>. The loss of the k th refinement layer is computed by a weighted sum of losses over all proposals as</p><formula xml:id="formula_5">L k refine = − 1 |R| |R| r=1 C+1 c=1 w k r y k cr log x k cr ,<label>(4)</label></formula><p>where x k cr denotes a score of the r th proposal with respect to class c in the k th refinement layer, w k r is a proposal weight obtained from the prediction score in the preceding refinement layer, and |R| is the number of proposals. In the refinement loss function, there are C + 1 classes because we also consider background class.</p><p>Regression loss L reg employs smooth 1 -norm between a proposal and its matching pseudo-ground-truth, following the bounding box regression literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref>. The regression loss is defined as follows:</p><formula xml:id="formula_6">L reg = 1 |R| |R| r=1 |G| j=1 q rj k∈{x,y,w,h} smooth 1 (t rjk − v rk ),<label>(5)</label></formula><p>where G is a set of pseudo-ground-truths, q rj is an indicator variable denoting whether the r th proposal is matched with the j th pseudo-ground-truth, v rk is a predicted bounding box regression offset of the k th coordinate for r th proposal and t rjk is the desirable offset parameter of the k th coordinate between the r th proposal and the j th pseudo-groundtruth as in R-CNN <ref type="bibr" target="#b16">[17]</ref>.</p><p>The detection loss L det is the sum of image classification loss, bounding box regression loss, and K refinement losses, which is given by</p><formula xml:id="formula_7">L det = L cls + L reg + K k=1 L k refine ,<label>(6)</label></formula><p>where K = 3 in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Instance Mask Generation Loss</head><p>For training CAMs in the IMG module, we adopt average classification scores from three refinement branches of our detection network. The loss function of the k th CAM network, denoted by L k cam , is given by a binary cross entropy loss as</p><formula xml:id="formula_8">L k cam = − 1 |R| |R| r=1 C+1 c=1 y rc log p k rc + (1 − y rc ) log(1 − p k rc ),<label>(7)</label></formula><p>where y rc is an one-hot encoded pseudo-label from detection branch of the r th proposal for class c, and p k rc is a softmax score of the same proposal for the same class obtained by the weighted GAP from the last convolutional layer. The instance mask generation loss is the sum of all the CAM losses as shown in the following equation:</p><formula xml:id="formula_9">L img = 3 k=1 L k cam .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Instance Segmentation Loss</head><p>The loss in the segmentation network is obtained by comparing the network outputs with the pseudo-ground-truth M using a pixel-wise binary cross entropy loss for each class, which is given by</p><formula xml:id="formula_10">L seg = − 1 T 2 |R| r C+1 c (i,j)∈T ×T m ij rc log s ij rc<label>(9)</label></formula><formula xml:id="formula_11">+ (1 − m ij rc ) log 1 − s ij rc ,</formula><p>where m ij rc means a binary element at (i, j) of M for the r th proposal, and s ij rc is the output value of the segmentation network, S ∈ R |R|×(C+1)×T <ref type="bibr" target="#b1">2</ref> , at location (i, j) of the r th proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference</head><p>Our model sequentially predicts object detection and instance segmentation for each proposal in a given image. For object detection, we use the average scores of three refinement branches in the object detection module. Each regressed proposal is labeled as the class that corresponds to the maximum score. We apply a non-maximum suppression with IoU threshold 0.3 to the proposals. The survived proposals are regarded as detected objects and used to estimate pseudo-labels for instance segmentation.</p><p>For instance segmentation, we select the foreground activation map of the identified class c, M c , from the IMG module and the corresponding segmentation score map, S c , from instance segmentation module for each detected object. The final instance segmentation label is given by the ensemble of two results,</p><formula xml:id="formula_12">O c = δ M c + S c 2 &gt; ξ ,<label>(10)</label></formula><p>where O c is a binary segmentation mask for detected class c, δ[·] is an element-wise indicator function, and ξ is a threshold identical used in Eq. (1). For post-processing, we utilize Multiscale Combinatorial Grouping (MCG) proposals <ref type="bibr" target="#b2">[3]</ref> as used in PRM <ref type="bibr" target="#b48">[49]</ref>. Each instance segmentation mask is substituted as a max overlap MCG proposal. Since the MCG proposal is a group of superpixels, it contains boundary information. Hence, if a segmentation output covers overall shape well, MCG proposal is greatly helpful to catch details of an object. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section describes our setting for training and evaluation and presents the experimental results of our algorithm in comparison to the existing methods. We also analyze various aspects of the proposed network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training</head><p>We use Selective Search <ref type="bibr" target="#b40">[41]</ref> for generating bounding box proposals. All fully connected layers in the detection and the IMG modules are initialized randomly using a Gaussian distribution (0, 0.01 2 ). The learning rate is 0.001 at the beginning and reduced to 0.0001 after 90K iterations. The hyper-parameter in the weight decay term is 0.0005, the batch size is 2, and the total training iteration is 120K. We use 5 image scales of {480, 576, 688, 864, 1000}, which are based on the shorter size of an image, for data augmentation and ensemble in training and testing. The NMS threshold for selecting foreground proposals is 0.3 and ξ in Eq (1) is set to 0.4 following MNC <ref type="bibr" target="#b8">[9]</ref>. For regression, a proposal is associated with a pseudo-ground-truth if the IoU is larger than 0.6. The output size T of the IMG and instance segmentation modules is 28. Our model is implemented on PyTorch and the experiments are conducted on a single NVIDIA Titan XP GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Evaluation Metrics</head><p>We use PASCAL VOC 2012 segmentation dataset <ref type="bibr" target="#b12">[13]</ref> to evaluate our algorithm. The dataset is composed of 1,464, 1,449, and 1,456 images for training, validation, and testing, respectively, for 20 object classes. We use the standard augmented training set (trainaug) with 10,582 images to learn our network, following the prior segmentation research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. In our weakly supervised learning scenario, we only use image-level class labels to train the model. Detection and instance segmentation accuracies are measured on PASCAL VOC 2012 segmentation validation (val) set.</p><p>We employ the standard mean average precision (mAP) to evaluate object detection performance, where a bounding box is regarded as a correct detection if it overlaps with a ground-truth more than a threshold, i.e. IoU &gt; 0.5. Cor-Loc <ref type="bibr" target="#b9">[10]</ref> is also used to evaluate the localization accuracy on the trainaug dataset. For instance segmentation task, we evaluate performance of an algorithm using mAPs at IoU thresholds 0.25, 0.5 and 0.75. We also use Average Best Overlap (ABO) to present overall instance segmentation performance of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Other Algorithms</head><p>We compare our algorithm with existing weakly supervised instance segmentation approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows that our algorithm generally outperforms the prior arts even without post-processing. Note that our postprocessing using MCG proposals <ref type="bibr" target="#b2">[3]</ref> improves mAP at high thresholds and ABO significantly, and leads to outstanding performance in terms of both mAP and ABO after all. We believe that such large gaps come from the effective regularization given by our community learning. The accuracy of our model is not as good as the method given by Mask R-CNN re-training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>, but direct comparison is not fair due to the retraining issue. <ref type="table" target="#tab_1">Table 2</ref> illustrates that our model outperforms the methods without re-training on train split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We discuss the contribution of each component in the network and the effectiveness of our training strategy. We also compare two different regression strategies-classagnostic vs. class-specific-using detection scores. Note that we present the results without post-processing for the ablation study to verify the contribution of each component clearly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Network Components</head><p>We analyze the effectiveness of individual modules for instance segmentation and object detection. For comparisons, we measure mAP 0.5 for instance segmentation and mAP for object detection on PASCAL VOC 2012 segmentation val set while computing CorLoc on the trainaug set. Note that the instance segmentation accuracy of the detection-only model is given by using detected bounding boxes as segmentation masks. All models are trained on PASCAL VOC 2012 segmentation trainaug set. <ref type="table" target="#tab_2">Table 3</ref> presents that the IMG and Instance Segmentation (IS) modules are particularly helpful to improve accuracy for both tasks. By adding the two components, our model achieves accuracy gain in detection by 3.9% and 3.2% points in terms of mAP and CorLoc, respectively, compared to the baseline detector. Additionally, bounding box regression (REG) enhances performance by generating better pseudo-ground-truths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">IMG module</head><p>We further investigate the components in the IMG module and summarize the results in <ref type="table">Table 4</ref>. All results are from the experiments without bounding box regression to demonstrate the impact of individual components clearly. All the three tested components make substantial contribution for performance improvement. The background class activation map models background likelihood within a bounding box explicitly and facilitates the comparison with fore- ground counterparts. The feature smoothing regularizes excessively discriminative activations in the inputs to CAM module while the weighted GAP pays more attention to the proper region for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Comparison to a Simple Algorithm Combination</head><p>To demonstrate the benefit of our unified framework, we compare the proposed algorithm with a straightforward combination of weakly supervised object detection and semantic segmentation methods. <ref type="table">Table 5</ref> presents the result from a combination of weakly supervised object detection algorithm, OICR <ref type="bibr" target="#b39">[40]</ref>, and a weakly supervised semantic segmentation algorithm, AffinityNet <ref type="bibr" target="#b1">[2]</ref>. Note that both OICR and AffinityNet are competitive approaches in their target tasks. We train the two models independently and combine their results by providing a segmentation label map using AffinityNet for each detection result obtained from OICR. The proposed algorithm based on a unified end-toend training outperforms the simple combination of two separate modules even without post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Comparison to Class-Specific Box Regressor</head><p>We compare the results from the class-agnostic and classspecific bounding box regressors in terms of mAP and Cor-Loc. <ref type="table">Table 6</ref> presents that bounding box regressors turn out to be learned effectively despite incomplete supervision. It further shows that the class-agnostic bounding box regressor clearly outperforms the class-specific version. We believe that this is partly because sharing a regressor over all classes reduces the bias observed in individual classes and regularizes overly discriminative representations.   on PASCAL VOC 2012 segmentation val set. Refer to our supplementary material for more details. Our model successfully segments whole regions of objects and discriminates each object in a same class within an input image via predicted object proposals. <ref type="figure" target="#fig_2">Figure 4</ref> compares detection results from our full model and a detector-only model, OICR with the ResNet50 backbone network, on the same dataset. Our model is more robust to localize a whole object since the features are better regularized by joint learning of Object Detection, IMG, and Instance Segmentation modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a unified end-to-end deep neural network for weakly supervised instance segmentation via community learning. Our framework trains three subnetworks jointly with a shared feature extractor, which performs object detection with bounding box regression, instance mask generation, and instance segmentation. These components interact with each other closely and form a positive feed-back loop with cross-regularization for improving quality of individual tasks. Our class-agnostic bounding box regressor successfully regularizes object detectors even with weak supervisions only while the post-processing based on MCG mask proposals improves accuracy substantially.</p><p>The proposed algorithm outperforms the previous stateof-the-art weakly supervised instance segmentation methods and the weakly supervised object detection baseline on PASCAL VOC 2012 with a simple segmentation module. Since our framework does not rely on particular network architectures for object detection and instance segmentation modules, using better detector or segmentation network would improve the performance of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of Our Framework</head><p>This section discusses more details regarding our feature extractor, object detection and instance mask generation modules, which are described in our main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Feature Extractor</head><p>We use ResNet50 <ref type="bibr" target="#b21">[22]</ref> as a backbone network, which is pretrained on ImageNet. For object detection, one SPP layer is attached after res4, followed by res5. The output of the last residual block is shared with IMG and segmentation modules through upsampling. The IMG module employs multiple level of 28 × 28 features from outputs of SPP layers attached to res3 and res4, and upsampled res5 output. These features are given to the weighted GAP and the classification layers following one convolution layer for each level of the CAM subnetwork. For instance segmentation, the upsampled output of res5 is used. On our implementation, batch normalization is replaced to group normalization <ref type="bibr" target="#b44">[45]</ref> due to the small batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Object Detection Module</head><p>Object detection module is composed of detector and regressor parts. Note that any weakly supervised object detection algorithm can be used as the detector in the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Detector</head><p>We adopt OICR <ref type="bibr" target="#b39">[40]</ref> for the detector. OICR is one of the most commonly used algorithm for weakly supervised object detection relying on multiple instance learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47]</ref>. The model has two parts, multiple instance detection network (MIDN) and refinement layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 MIDN</head><p>MIDN is based on the Weakly Supervised Deep Detection Network (WSDDN) <ref type="bibr" target="#b3">[4]</ref>, which has two parallel fully connected layers for classification and detection, respectively and they are followed by two separate softmax layers. For classification, the softmax layer is given by</p><formula xml:id="formula_13">[σ cls (x c )] ij = e x c ij C k=1 e x c kj ,<label>(11)</label></formula><p>where x c ij denotes the classification score for the i th class of the j th proposal and C denotes the number of classes. On the other hand, the softmax layer for detection branch is given by</p><formula xml:id="formula_14">[σ det (x d )] ij = e x d ij |R| k=1 e x d ik ,<label>(12)</label></formula><p>where x d ij denotes the detection score for the i th class of the j th proposal and |R| is the number of proposals.</p><p>The final score, z ∈ R C×|R| is defined as</p><formula xml:id="formula_15">z = σ cls (x c ) σ det (x d ),<label>(13)</label></formula><p>where is the Hadamard product. The image-level classification score φ is given by the sum of z over all proposals. By using the image-level score, the loss from MIDN L cls is defined as an image-level cross-entropy, which is described in Eq. 3 in our main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Refinement Layer</head><p>Once MIDN predicts a class of each proposal, a refinement layer revises the labels by leveraging object classification scores from the previous stage. The refinement layer finds the proposal with the highest rank in each class, which is considered as a seed. Each proposal is given a label from the highest overlapping seed if its IoU (Intersection over Union) with the seed is higher than a threshold, 0.5; otherwise, it is labeled as a background class. The weight of the proposal w r is given by the class score of the seed. Hence, the loss of the k th refinement layer, L k refine is defined as a weighted cross-entropy loss as described in Eq. 4 in our main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.4 Regressor</head><p>For bounding box regression, we attach two fully connected layers after res5 which has 2048 channels. The final output of our regressor has a dimension of 4 for class-agnostic manner instead of 4C where C is the number of classes for traditional class-specific manner. It means that classagnostic regressor is shared with all classes.</p><p>During training, a proposal and its nearest pseudoground-truth proposal pair (p, g) is converted to a regression offset t = [t x , t y , t w , t h ] as follows: CAM <ref type="bibr" target="#b47">[48]</ref> highlights areas of discriminative parts of objects over each class and is often used for the pseudo-groundtruth for weakly supervised semantic segmentation. CAM is built on a classification task leveraging Global Average Pooling (GAP) <ref type="bibr" target="#b30">[31]</ref>. It is applied to the last convolutional layer followed by a fully connected layer and a softmax layer to predict image-level class labels. For each class c, CAM, M c (x, y) is defined as follows:</p><formula xml:id="formula_16">M c (x, y) = w T c · F(x, y),<label>(15)</label></formula><p>where F(x, y) is a feature vector from the last convolutional layer with respect to spatial grid (x, y), and w c is a weight vector of fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Time Cost of Post Processing</head><p>Note that our model without post-processing has competitive results compared to existing methods, and our postprocessing is finding best matching MCG proposal for each predicted mask. The computational cost for post-processing is not signficant compared to our main algorithm based on a deep neural network. Specifically, the inference through our network takes 4 seconds per image (5 multi-scales with flip) on a single TITAN Xp GPU but the post-processing takes 0 ∼ 4 seconds on a CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Multiple CAMs</head><p>We present the instance segmentation performance at mAP 0.5 with respect to the number of CAMs in <ref type="table" target="#tab_4">Table 7</ref>. <ref type="figure">Figure 6</ref>. Comparison between the outputs of the conventional CAM network (middle) and one with feature smoothing (right) for two images. The results come from our Detector + IMG module which does not have REG and IS modules and the postprocesing to directly show the effectiveness of multiple CAMs. The multi-scale representations are helpful to capture whole objects rather than discriminative parts only. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Qualitative Results</head><p>D.1. Instance Segmentation <ref type="figure" target="#fig_3">Figure 5</ref> shows additional instance segmentation results. Images in the first two rows are success cases and those in the last row are failure cases. In the failure cases, the model is confused with dog and cat and cannot detect human hands and leg, dark sheep. differentiate adjacent three sheep, and remove false positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Feature Smoothing</head><p>To penalize CAM focusing excessively on discriminative parts on target objects, we smooth the input features to CAM networks using a non-linear activation function. As illustrated in <ref type="figure">Figure 6</ref>, the function helps produce more spatially regularized activation maps which are more appropriate to enclose entire target objects by segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Bounding Box Regression</head><p>We qualitatively compare our model with class-agnostic regressor and with class-specific regressor on <ref type="figure" target="#fig_4">Figure 7</ref>. Our model with class-agnostic regressor achieves better performance than with class-specific regressor. The difference between two regressors is remarkable on "cat" and "dog" classes. With class-agnostic regressor, our model detects their entire bodies while the model with class-specific counterpart still spotlights their discriminative parts, faces.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>shows instance segmentation results from our model after post-processing and identified bounding boxes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Instance segmentation results on PASCAL VOC 2012 segmentation val set. Green rectangle is a detected object bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results of detection on PASCAL VOC 2012 segmentation val set. Green rectangle is generated by our model and yellow one indicates the output of detector-only model (OICR [40] based on ResNet50).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results of instance segmentation on PASCAL VOC 2012 segmentation val set. Results in the first two rows are the success cases and those in the last row are failure cases A.3.1 Class Activation Map (CAM)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results regarding class-specific and class-agnostic regressors on PASCAL VOC 2012 segmentation val set. Red rectangle is a ground-truth, blue rectangle represents the output of our model with class-agnostic regressor and orange one is our model with class-specific regressor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 8 presents the effectiveness of our class-agnostic regressor compared to our model without regressor on PASCAL VOC segmentation val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results of detection on PASCAL VOC 2012 segmentation val set. Red rectangle indicates ground-truth, green rectangle is generated by our model without regressor, and blue one represents the output of our model with class-agnostic regressor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Instance segmentation results on the PASCAL VOC 2012 segmentation val set with two different types of supervisions (I: imagelevel class label, C: object count). The numbers in red and blue denote the best and the second best scores without Mask R-CNN re-training, respectively.</figDesc><table><row><cell>Method</cell><cell>Supervision</cell><cell>Post-procesing</cell><cell>mAP 0.25</cell><cell>mAP 0.5</cell><cell>mAP 0.75</cell><cell>ABO</cell></row><row><cell>WISE [27] w/ Mask R-CNN</cell><cell>I</cell><cell></cell><cell>49.2</cell><cell>41.7</cell><cell>23.7</cell><cell>55.2</cell></row><row><cell>IRN [1] w/ Mask R-CNN</cell><cell>I</cell><cell></cell><cell>-</cell><cell>46.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Cholakkal et al. [8]</cell><cell>I + C</cell><cell></cell><cell>48.5</cell><cell>30.2</cell><cell>14.4</cell><cell>44.3</cell></row><row><cell>PRM [49]</cell><cell>I</cell><cell></cell><cell>44.3</cell><cell>26.8</cell><cell>9.0</cell><cell>37.6</cell></row><row><cell>IAM [50]</cell><cell>I</cell><cell></cell><cell>45.9</cell><cell>28.3</cell><cell>11.9</cell><cell>41.9</cell></row><row><cell>Label-PEnet [14]</cell><cell>I</cell><cell></cell><cell>49.2</cell><cell>30.2</cell><cell>12.9</cell><cell>41.4</cell></row><row><cell>Ours</cell><cell>I I</cell><cell></cell><cell>57.0 56.6</cell><cell>35.9 38.1</cell><cell>5.8 12.3</cell><cell>43.8 48.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">Instance segmentation results on the PASCAL VOC 2012</cell></row><row><cell cols="4">segmentation train set. [1, 27] report results without Mask R-CNN</cell></row><row><cell cols="2">obtained from their original papers.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">WISE [27] IRN [1] Ours</cell></row><row><cell>mAP 0.5</cell><cell>25.8</cell><cell>37.7</cell><cell>39.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Contribution of individual components integrated into our algorithm study. The evaluation is performed on PASCAL VOC 2012 segmentation val set for mAP and trainaug set for CorLoc (* indicates that detection bounding boxes are used as segmentation results as well).</figDesc><table><row><cell></cell><cell></cell><cell>Instance</cell><cell cols="2">Object</cell></row><row><cell>Architecture</cell><cell></cell><cell>Segmentation</cell><cell cols="2">Detection</cell></row><row><cell></cell><cell></cell><cell>mAP 0.5</cell><cell cols="2">mAP CorLoc</cell></row><row><cell>Detector</cell><cell></cell><cell>18.8  *</cell><cell>45.3</cell><cell>63.6</cell></row><row><cell>Detector + IMG</cell><cell></cell><cell>32.8</cell><cell>48.6</cell><cell>66.3</cell></row><row><cell cols="2">Detector + IMG + IS</cell><cell>33.7</cell><cell>49.2</cell><cell>66.8</cell></row><row><cell cols="2">Detector + REG + IMG + IS</cell><cell>35.9</cell><cell>53.2</cell><cell>70.8</cell></row><row><cell cols="5">Table 4. Accuracy of the variants of IMG module with background</cell></row><row><cell cols="5">class (BG), weighted GAP (wGAP), and feature smoothing (FS),</cell></row><row><cell cols="3">based on the ResNet50 backbone without REG</cell><cell></cell><cell></cell></row><row><cell cols="5">BG BG + wGAP BG + FS wGAP + FS All</cell></row><row><cell>mAP 0.5 28.8</cell><cell>30.0</cell><cell>31.8</cell><cell>27.4</cell><cell>33.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Comparison our model with a combination of OICR and AffinityNet on PASCAL VOC 2012 segmentation val set Comparison of class-agnostic regressor and class-specific regressor into our algorithm in terms of detection performance. The evaluation is performed on PASCAL VOC 2012 segmentation val set for mAP and trainaug set for CorLoc.</figDesc><table><row><cell>Model</cell><cell>OICR + AffinityNet</cell><cell cols="2">OICR (ResNet50) + AffinityNet</cell><cell>Ours</cell></row><row><cell>mAP 0.5</cell><cell>27.3</cell><cell>33.3</cell><cell></cell><cell>35.9</cell></row><row><cell></cell><cell>Model</cell><cell>mAP</cell><cell cols="2">CorLoc</cell></row><row><cell></cell><cell>Ours w/o REG</cell><cell>49.2</cell><cell>66.8</cell></row><row><cell cols="2">Ours (class-specific)</cell><cell>50.4</cell><cell>68.4</cell></row><row><cell cols="2">Ours (class-agnostic)</cell><cell>53.2</cell><cell>70.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Accuracy of the various number of CAMs in IMG module based on ResNet50 without REG and IS modules on PASCAL VOC 2012 segmentation val set.</figDesc><table><row><cell>The number of CAMs</cell><cell>1</cell><cell>2</cell><cell>3 (ours)</cell><cell>4</cell></row><row><cell>mAP 0.5</cell><cell cols="2">29.7 30.6</cell><cell>32.8</cell><cell>31.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t x = (g x − p x )/p w , t y = (g y − p y )/p h , t w = log(g w /p w ), t h = log(g h /p h ),(14)where g = [g x , g y , g w , g h ] is a target pseudo-ground-truth proposal for a proposal, p = [p x , p y , p w , p h ].</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly Supervised Learning of Instance Segmentation With Inter-Pixel Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning Pixel-Level Semantic Affinity With Image-Level Supervision for Weakly Supervised Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale Combinatorial Grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly Supervised Deep Detection Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prantik</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MaskLab: Instance Segmentation by Refining Object Detection With Semantic and Direction Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object Counting and Instance Segmentation With Image-Level Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Fahad Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instance-aware Semantic Segmentation via Multi-task Network Cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly Supervised Localization and Learning with Generic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly Supervised Cascaded Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Label-PEnet: Sequential Label Propagation and Enhancement Networks for Weakly Supervised Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Boundary-Aware Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeeshan</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly-Supervised Semantic Segmentation Network with Deep Seeded Region Growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ContextLocNet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly Supervised Semantic Segmentation Using Superpixel Pooling Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Where are the Masks: Instance Segmentation with Imagelevel Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Issam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">W</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FickleNet: Weakly and Semi-supervised Semantic Image Segmentation using Stochastic Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frame-to-Frame Aggregation of Active Regions in Web Videos for Weakly Supervised Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Weakly Supervised Object Detection with Segmentation Collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
	</analytic>
	<monogr>
		<title level="j">Network In Network. arXiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore Kumar</forename><surname>Sahu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.06462</idno>
		<title level="m">Normalization: A Preprocessing Stage</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1804.02767</idno>
		<title level="m">YOLOv3: An Incremental Improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cyclic Guidance for Weakly Supervised Joint Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Forget &amp; Diversify: Regularized Refinement for Weakly Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeany</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">PCL: Proposal Cluster Learning for Weakly Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple Instance Detection Network with Online Instance Classifier Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Selective Search for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Min-Entropy Latent Model for Weakly Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">TS2C: Tight Box Mining with Surrounding Segmentation Context for Weakly Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Group Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint Learning of Saliency Detection and Weakly Supervised Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weakly Supervised Instance Segmentation using Class Peak Response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning Instance Activation Maps for Weakly Supervised Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Instance Mask Generation (IMG) Module We use CAM [48] for instance mask generation module. It can be substituted by other object localization algorithms based on image-level labels such as Grad-CAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>36] and Grad-CAM++ [5</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
