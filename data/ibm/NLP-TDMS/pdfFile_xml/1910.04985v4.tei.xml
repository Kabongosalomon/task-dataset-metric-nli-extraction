<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VarGFaceNet: An Efficient Variable Group Convolutional Neural Network for Lightweight Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjia</forename><surname>Yan</surname></persName>
							<email>mengjia.yan@horizon.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horizon</forename><surname>Robotics</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengao</forename><surname>Zhao</surname></persName>
							<email>mengao.zhao@horizon.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horizon</forename><surname>Robotics</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zining</forename><surname>Xu</surname></persName>
							<email>zining.xu@horizon.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horizon</forename><surname>Robotics</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
							<email>qian01.zhang@horizon.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Wang</surname></persName>
							<email>guoli.wang@horizon.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horizon</forename><surname>Robotics</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
							<email>zhizhong.su@horizon.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horizon</forename><surname>Robotics</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VarGFaceNet: An Efficient Variable Group Convolutional Neural Network for Lightweight Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To improve the discriminative and generalization ability of lightweight network for face recognition, we propose an efficient variable group convolutional network called VarGFaceNet. Variable group convolution is introduced by VarGNet to solve the conflict between small computational cost and the unbalance of computational intensity inside a block. We employ variable group convolution to design our network which can support large scale face identification while reduce computational cost and parameters. Specifically, we use a head setting to reserve essential information at the start of the network and propose a particular embedding setting to reduce parameters of fully-connected layer for embedding. To enhance interpretation ability, we employ an equivalence of angular distillation loss to guide our lightweight network and we apply recursive knowledge distillation to relieve the discrepancy between the teacher model and the student model. The champion of deepglintlight track of LFR (2019) challenge demonstrates the effectiveness of our model and approach. Implementation of VarGFaceNet will be released at https://github.com/zma-c-137/VarGFaceNet soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the surge of computational resources, face recognition using deep representation has been widely applied to many fields such as surveillance, marketing and biometrics <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>. However, it is still a challenging task to implement face recognition on limited computational cost system such as mobile and embedded systems because of the large scale identities needed to be classified.</p><p>Many work propose lightweight networks for common computer vision tasks such as SqueezeNet <ref type="bibr" target="#b14">[15]</ref>, MobileNet <ref type="bibr" target="#b11">[12]</ref>, MobileNetV2 <ref type="bibr" target="#b20">[21]</ref>, ShuffleNet <ref type="bibr" target="#b26">[27]</ref>. SqueezeNet <ref type="bibr" target="#b14">[15]</ref> extensively uses 1 × 1 convolution, achieving 50× fewer parameters than AlexNet <ref type="bibr" target="#b15">[16]</ref> while maintains AlexNet-level accuracy on ImageNet. MobileNet <ref type="bibr" target="#b11">[12]</ref> utilizes depthwise separable convolution to achieve a trade off between latency and accuracy. Based on this work, MobileNetV2 <ref type="bibr" target="#b20">[21]</ref> proposes an inverted bottleneck structure to enhance discriminative ability of network. ShuffleNet <ref type="bibr" target="#b26">[27]</ref> and Shuf-fleNetV2 <ref type="bibr" target="#b17">[18]</ref> uses pointwise group convolution and channel shuffle operations to further reduce computation cost. Even though they cost small computation during inference and achieve good performance on various applications, optimization problems on embedded system still remain on embedded hardware and corresponding compilers <ref type="bibr" target="#b25">[26]</ref>. To handle this conflict, VarGNet <ref type="bibr" target="#b25">[26]</ref> proposes a variable group convolution which can efficiently solve the unbalance of computational intensity inside a block. Meanwhile, we explore that variable group convolution has larger capacity than depthwise convolution with the same kernel size, which helps network to extract more essential information. However, VarGNet is designed for general tasks such as image classificaiton and object detection. It decreases spatial area to the half in the head setting to save memory and computational cost, while this setting is not suitable for face recognition task since detailed information of face is necessary. And there is only an average pooling layer between last conv and fully connected layer of the embedding, which may not extract enough discriminative information.</p><p>Based on VarGNet, we propose an efficient variable group convolutional network for lightweight face recognition, shorted as VarGFaceNet. In order to enhance the discriminative ability of VarGNet for large scale face recognition task, we first add SE block <ref type="bibr" target="#b12">[13]</ref> and PReLU <ref type="bibr" target="#b7">[8]</ref> on blocks of VarGNet. Then we remove the downsample process at the start of network to preserve more information. convolution to shrink the feature tensor to 1×1×512 before fc layer. The performance of VarGFaceNet demonstrates that this embedding setting can preserve discriminative ability while reduce parameters of the network.</p><p>To enhance the interpretation ability of lightweight network, we apply knowledge distillation during the training. There are several approaches aim at making the deep network smaller and cost-efficient, such as model pruning, model quantization and knowledge distillation. Among them, knowledge distillation is being actively investigated due to its architectural flexibility. Hinton <ref type="bibr" target="#b10">[11]</ref> introduces the concept of knowledge distillation and proposes to use the softmax output of teacher network to achieve knowledge distillation. To take better advantage of information from teacher network, FitNets <ref type="bibr" target="#b19">[20]</ref> adopts the idea of feature distillation and encourages student network to mimic the hidden feature values of teacher network. After FitNets, there are variant methods attempt to exploit the knowledge of teacher network, such as transferring the feature activation map <ref type="bibr" target="#b9">[10]</ref>, activation-based and gradient-based Attention Maps <ref type="bibr" target="#b24">[25]</ref>. Recently ShrinkTeaNet <ref type="bibr" target="#b5">[6]</ref> introduces an angular distillation loss to focus on angular information of teacher model. Inspired by angular distillation loss we employ an equivalent loss with better implementation efficiency as the guide of VarGFaceNet. Moreover, to relieve the complexity of optimization caused by the discrepancy between teacher model and student model, we introduce recursive knowledge distillation which treats the model of student trained in one generation as pretrained model for the next generation. We evaluate our model and approach on LFR challenge <ref type="bibr" target="#b3">[4]</ref>. LFR challenge is a lightweight face recognition challenge which requires networks whose FLOPs is under 1G and memory footprint is under 20M. VarGFaceNet achieves the state-of-the-art performance on this challenge which is shown in Section 3. Our contributions are summarized as follows:</p><p>• To improve the discriminative ability of VarGNet <ref type="bibr" target="#b25">[26]</ref> in large-scale face recognition we employ a different head setting and propose a new embedding block. In embedding block, we first expand channels to 1024 by 1×1 convolution layer to reserve essential information, then we use variable group conv and pointwise conv to shrink the spatial area to 1 × 1 while saving computational cost. These settings improve the performance on face recognition tasks which shown in Section 3.</p><p>• To imporve the generalization ability of lightweight models, we propose recursive knowledge distillation which relieves the generalization gap between teacher models and student models in one generation.</p><p>• We analyse the efficiency of variable group convolution and employ an equivalence of angular distillation loss during training. Experiments conducted to show the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Variable Group Convolution</head><p>Group Convolution was first introduced in AlexNet [16] for computational cost reduction on GPUs. Then, the cardinality of group convolution demonstrated a better performance than the dimensions of depth and width in ResNext <ref type="bibr" target="#b22">[23]</ref>. Designed for mobile device, MobileNet <ref type="bibr" target="#b11">[12]</ref> and Mo-bileNetV2 <ref type="bibr" target="#b20">[21]</ref> proposed depthwise separable convolution inspired by group convolution to save computational cost while keep discriminative ability of convolution. However, depthwise separable convolution spends 95% computation time in Conv 1 × 1, which causes a large MAdds gap between two consecutive laysers (Conv 1 × 1 and Conv DW 3×3) <ref type="bibr" target="#b11">[12]</ref>. This gap is unfriendly to embedded systems who load all weights of the network to perform convolution <ref type="bibr" target="#b23">[24]</ref>: embedded systems need extra buffers for Conv 1 × 1.</p><p>To keep the balance of computational intensity inside a block, VarGNet <ref type="bibr" target="#b25">[26]</ref> sets the channel numbers in a group as a constant S. The constant channel numbers in a group lead to the variable number of groups n i in a convolution, named variable group convolution. The computational cost of a variable group convolution is:</p><formula xml:id="formula_0">k 2 × h i × w i × S × c i+1 (1) S = c i n i (2)</formula><p>The input of this layer is h i × w i × c i and the output of that is h i × w i × c i+1 . k is the kernel size. When variable group convolution is used to replace depthwise convolution in MobileNet <ref type="bibr" target="#b11">[12]</ref>, the computational cost of pointwise convolution is:</p><formula xml:id="formula_1">1 2 × h i × w i × c i+1 × c i+2<label>(3)</label></formula><p>The ratio of computational cost between variable group convolution and pointwise convolution is k 2 S ci+2 while that between depthwise convolution and pointwise convolution is</p><formula xml:id="formula_2">k 2 ci+2 . In practice, c i+2 k 2 , S &gt; 1, so k 2 S ci+2 &gt; k 2 ci+2</formula><p>. Hence, it will be more computational balanced inside a block when employs variable group convolution on the bottom of pointwise convolution instead of depthwise convolution.</p><p>Moreover, S &gt; 1 means variable group convolution has higher MAdds and larger network capacity than depthwise convoluiton (with the same kernel size), which is capable of extracting more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Blocks of Variable Group Network</head><p>Communication between off-chip memory and on-chip memory only happens on the start and the end of block computing when a block is grouped and computed together on embedded systems <ref type="bibr" target="#b23">[24]</ref>. To limit the communication cost, VarGNet sets the number of output channels to be same as the number of input channels in the normal block. Meanwhile, VarGNet expands the C channels at the start of the block to 2C channels using variable group convolution to keep the generalization ability of the block. The normal block we used is shown in <ref type="figure">Fig. 1(a)</ref>, and down sampling block is shown in <ref type="figure">Fig. 1(b)</ref>. Different from the blocks in VarGNet <ref type="bibr" target="#b25">[26]</ref>, we add SE block in normal block and employ PReLU instead of ReLU to increase the discriminative ability of the block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Lightweight Network for Face Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Head setting</head><p>The main challenge of face recognition is the large scale identities involved in testing/training phase. It requires discriminative ability as much as possible to support distinguishing millions of identities. In order to reserve this ability in lightweight networks, we use 3 × 3 Conv with stride 1 at the start of network instead of 3 × 3 Conv with stride 2 in VarGNet. It is similar to the input setting of <ref type="bibr" target="#b2">[3]</ref>. The output feature size of first conv in VarGNet will be downsampled while ours remains the same as input size, shown in <ref type="figure">Fig. 1(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Embedding setting</head><p>To obtain the embedding of faces, many work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref> employ a fully-connected layer directly on the top of last convolution. However, the parameters of this fully-connected layer will be huge when output features from last convoluiton are relatively large. For instance, in ResNet 100 <ref type="bibr" target="#b2">[3]</ref> the output of last conv is 7 × 7 × 512, and the parameters of fc layer (embedding size is 512) are 7 × 7 × 512 × 512. The overall parameters of fc layer for embedding are 12.25M, and the memory footprint is 49M (float32)! In order to design a lightweight network (memory footprint is less than 20M, FLOPs is less than 1G), we employ variable group convolution after last conv to shrink the feature maps to 1 × 1 × 512 before fc layer. Consequently, the memory footprint of fc layer for embedding is only 1M. <ref type="figure">Fig.1(d)</ref> shows the setting of embedding block. Shrinking the feature tensor to 1 × 1 × 512 before fc layer for embedding is risky since information contains by this feature tensor is limited. To avoid the derease of essential information, we expand channels after last conv to retain as much information as possible. Then we employ variable group convolution and pointwise convolution to decrease the parameters and computational cost while keep information.</p><p>Specifically, we first use a 1×1 Conv to expand the channels from 320 to 1024. Then we employ a 7 × 7 variable group convolution layer <ref type="bibr">(</ref> pointwise convolution is used to connect the channels and output the feature tensors to 1 × 1 × 512. The new embedding block setting only takes up 5.78M while the original fc layer takes up 30M (7 × 7 × 320 × 512) on the disk. Experiments of comparison between our network and VarGNet in Section 3.3 demonstrate the efficiency of our network on face recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Overall architecture</head><p>The overall architecture of our lightweight network (VarGFaceNet) is illustrated in <ref type="table">Table 1</ref>. The memory footprint of our VarGFaceNet is 20M and FLOPs is 1G. We set S = 8 in a group empirically. Benefit from variable group convolution, head settings and particular embedding settings, VarGFaceNet can achieve good performance on face recognition task with limited computational cost and parameters. In Section 3, we will demonstrate the effectiveness of our network on a million distractors face recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Angular Distillation Loss</head><p>Knowledge distillation has been widely used in lightweight network training since it can transfer the interpretation ability of a big network to a smaller network <ref type="bibr" target="#b11">[12]</ref>. Majority tasks that used knowledge distillation are close set tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref>. They apply scores/logits or embeddings/feature magnitude to compute l2 distance or cross entropy as loss. However, for open set tasks, scores/logits of training set contain limited information of testing set and the exact match of featuers maybe over-regularized in some situations. To extract useful information and avoid overregularization, <ref type="bibr" target="#b5">[6]</ref> proposes an angular distillation loss for knowledge distillation:</p><formula xml:id="formula_3">L a (F i t , F i s ) = 1 N N i=1 ||1 − F i t ||F i t || * F i s ||F i s || || 2 2<label>(4)</label></formula><p>F i t is the ith feature of teacher model, F i s is ith features of student model. m is the number of samples in a batch. Eq. 4 first computes cosine similarity between features of teacher and student, then minimizes the l2 distance between this similarity and 1. Inspired by <ref type="bibr" target="#b5">[6]</ref>, we propose to use Eq. 5 to enhance the implementation efficiency. Since cosine similarity is less than 1, minimize Eq. 4 is equivalent to minimize Eq. 5.</p><formula xml:id="formula_4">L s (F i t , F i s ) = 1 N N i=1 || F i t ||F i t || − F i s ||F i s || || 2 2<label>(5)</label></formula><p>Compared with previous l2 loss of exact features, Eq. 4 and Eq. 5 focus on angular information and the distribution of embeddings.</p><p>In addition, we employ arcface <ref type="bibr" target="#b2">[3]</ref> as our classification loss which also pays attention to angular information:</p><formula xml:id="formula_5">L Arc = − 1 N N i=1</formula><p>log e s(cos(Θy i +m)) e s(cos(Θy i +m)) + n j=1,j =yi e scosΘj (6) To sum up, the objective function we used in training is:</p><formula xml:id="formula_6">L = L Arc + αL s<label>(7)</label></formula><p>We empirically set α = 7 in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Recursive Knowledge Distillation</head><p>Knowledge distillation with one generation is sometimes difficult to transfer enough knowledge when large discrepancy exists between teacher models and student models.  For instance, in our implementation, the FLOPs of teacher model is 24G while that of student model is 1G. And the number of parameters of teacher model is 108M while that of student model is 5M. Moreover, the different architecture and block settings between teacher model and student model increase the complexity of training as well. To improve the discriminative and generalization ability of our student network, we use recursive knowledge distillation, which employs the first generation of student to initialize the second generation of student, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>In recursive knowledge distillation, we employ the same teacher model in all generations. That means the angular information of samples which guides the student model is invariable. There are two merits if we use recursive knowledge distillation:</p><p>1 It will be easier to approach guided direction of teacher when a good initialization is applied.</p><p>2 The conflicts between margin of classification loss and guided angular information in the first generation will be relieved in the next generation.</p><p>The results of our experiments in Section 3 illustrate the performance of recursive knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we first introduce the datasets and evaluation metric. Then, to demonstrate the effectiveness of our VarGFaceNet, we compare our network with y2 network(a deeper mobilefacenet <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>). After that, the investigation for the effect of different teacher models in knowledge distillation is revealed. Finally, we show the competitive performance of VarGFaceNet using recursive knowledge distillation on LFR2019 Challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and Evaluation Metric</head><p>We employ the dataset(clean from MS1M <ref type="bibr" target="#b6">[7]</ref>) provided by LFR2019 for training. All face images in this dataset are aligned by five facial landmarks predicted from RetinaFace <ref type="bibr" target="#b4">[5]</ref> then resized to 112 × 112. There are 5.1M images collected from 93K identities. For test set, Trillionpairs dataset <ref type="bibr" target="#b0">[1]</ref> is used. It contains two parts: 1) ELFW: Face images of celebrities in the LFW name list. There are 274K images from 5.7K identities; 2) DELFW: Distractors for ELFW. There are 1.58 M face images from Flickr. All test images are preprocessed and resized to 112 × 112. We refer deepglint-light to trillionpairs testing set in the following. During the training, we utilize face verification datasets (e.g. LFW <ref type="bibr" target="#b13">[14]</ref>, CFP-FP <ref type="bibr" target="#b21">[22]</ref>, AgeDB-30 <ref type="bibr" target="#b18">[19]</ref>) to validate different settings using 1:1 verification protocol. Moreover, we employ the TPR@FPR=1e-8 as evaluation metric for identification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">VarGFaceNet train from scratch</head><p>To validate the efficiency and effectiveness of VarGFaceNet, we first train our network from scratch, and compare the performance with mobilefacenet(y2) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. We employ arcface loss as the objective function of classification during training. Tabel 2 presents the comparison results of VarGFaceNet and y2. It can be observed that under the limitation of 1G FLOPs, VarGFaceNet is able to reach better face recognition performance on validation sets. Compared with y2, our verification results of AgeDB-30 , CFP-FP have increased 0.6% and 0.2% respectively, testing result of deepglint-light (TPR@FPR=1e-8) has increased 5%. There are two intuitions for the better performance: 1. our network can contain more parameters than y2 when limit FLOPs because of variable group convolution. The biggest number of channels is 256 in y2 while ours is 320 before last conv. 2. Our embedding setting can extract more essential information. y2 expands the number of channels from 256 to 512 then use 7 × 7 depthwise convolution to get the feature tensor before fc layer. We expand the number of channels from 320 to 1024 then use variable group convolution and pointwise convolution which have larger network capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">VarGFaceNet guided by ResNet</head><p>In order to achieve higher performance than train from scratch, bigger networks are applied to perform knowledge distillation using angular distillation loss. Moreover, we conduct experiments to investigate the effect of different teacher models on VarGFaceNet. We employ ResNet 100 <ref type="bibr" target="#b8">[9]</ref> with SE as our teacher model. The teacher model has 24G FLOPs and 108M parameters. The results are illus-trated in Tabel 3. It can be observed that 1. even though the architectures of teacher and student are quite different, VarGFaceNet still approaches the performance of ResNet; 2. the performance of VarGFaceNet is highly correlated with teacher model. The higher performance teacher model has, the better interpretation ability VarGFaceNet will learn.</p><p>To validate the efficiency of our settings, we conduct comparison experiments between our network and VarGNet. Using the same teacher network, we change the head setting of VarGNet to our head setting for fair comparison and use the same loss function as above. In Tabel 4, the plain VarGNet has lower accuracy in LFW, CFP-FP, AgeDB-30. There is only an average pooling between last conv and fc layer in VarGNet. The results illustrate that our embedding setting is more suitable for face recognition task since it can extract more essential information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Recursive Knowledge Distillation</head><p>As we discuss in Section 2.5, when there is a large discrepancy between teacher model and student, knowledge distillation for one generation may not enough for knowledge transfer. To validate it, we use ResNet 100 model as our teacher model, and conduct recursive knowledge distillation on VarGFaceNet. A performance improvement shown in <ref type="table">Table 5</ref> when we train the model in next generation. The varification result of LFW and CFP-FP is increased by 0.1% while testing result of deepglintlight(TPR@FPR=1e-8) is 0.4% higher than pervious generation. Furthermore, we believe that it will lead to better performance if we continue to conduct training in more generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LFW</head><p>CFP-FP AgeDB-30 deepglint-light (TPR@FPR=1e-8) recursive=1</p><p>0.99783 0.98400 0.98067 0.88334 recursive=2 0.99833 0.98271 0.98050 0.88784 <ref type="table">Table 5</ref>. Performance of recursive knowledge distillation. Performance is recorded within the same epoch.</p><p>Verification results of LFW, AgeDB-30 are increased in the second generation. Performance of testing set deepglintlight(TPR@FPR=1e-8) is increased by 0.4% the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we propose an efficient lightweight network called VarGFaceNet for large scale face recognition. Benefit from variable group convolution, VarGFaceNet is capable of finding a better trade-off between efficiency and performance. The head setting and embedding setting specific to face recogniton help preserve information while reduce parametes. Moreover, to improve the interpretation ability of lightweight network, we employ an equivalence of angular distillation loss as our objective function and present a recursive knowledge distillation strategy. The state-of-the-art performance on LFR challenge demonstrates the superiority of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The process of recursive knowledge distillation. We apply the first generation of student to initialize the second generation of student while the teacher model is remained. Angular distillation loss and arcface loss are used to guide training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>To decrease parameters of network, we apply variable group arXiv:1910.04985v4 [cs.CV] 24 Nov 2019</figDesc><table><row><cell>(a) Normal block</cell><cell>(b) Down sampling block</cell></row><row><cell>(c) Head setting</cell><cell>(d) Embedding setting</cell></row></table><note>Figure 1. Settings of VarGFaceNet. a) is the normal block of VarGFaceNet. We add SE block on normal block of VarGNet. b) is the down sampling block. c) is head setting of VarGFaceNet. We do not use downsample in first conv in order to keep enough information. c) is the embedding setting of VarGFaceNet. We first expand channels from 320 to 1024. Then we employ variable group convolution and pointwise convolution to reduce the parameters and computational cost while remain essential information.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>VarGFaceNet vs. VarGNet. We show the highest performance of every validation dataset. The performance of VarGFaceNet is higher than VarGNet on LFW, AgeDB-30 and CFP-FP.</figDesc><table><row><cell></cell><cell>LFW</cell><cell>CFP-FP</cell><cell>AgeDB-30</cell><cell cols="2">deepglint-light (TPR@FPR=1e-8)</cell></row><row><cell>teacher</cell><cell>0.99683</cell><cell>0.98414</cell><cell>0.98083</cell><cell></cell><cell>0.86846</cell></row><row><cell>student</cell><cell>0.99683</cell><cell>0.98171</cell><cell>0.97550</cell><cell></cell><cell>0.84341</cell></row><row><cell>teacher</cell><cell>0.99817</cell><cell>0.98729</cell><cell>0.98133</cell><cell></cell><cell>0.90231</cell></row><row><cell>student</cell><cell>0.99733</cell><cell>0.98200</cell><cell>0.98100</cell><cell></cell><cell>0.85461</cell></row><row><cell>teacher</cell><cell>0.99833</cell><cell>0.99057</cell><cell>0.98250</cell><cell></cell><cell>0.93315</cell></row><row><cell>student</cell><cell>0.99783</cell><cell>0.98400</cell><cell>0.98067</cell><cell></cell><cell>0.88334</cell></row><row><cell cols="6">Table 3. Performance of VarGFaceNet with the guide of different teacher models. Performance is recorded within the same epoch. Results</cell></row><row><cell cols="6">of CFP-FP(validation set) and deepglint-light(TPR@FPR=1e-8) (testing set) show that the higher performance of teacher model leads to</cell></row><row><cell cols="2">the better results of student model.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Network</cell><cell>LFW</cell><cell>CFP-FP</cell><cell>AgeDB-30</cell><cell>Flops</cell></row><row><cell></cell><cell>r100(teacher)</cell><cell>0.9987</cell><cell>0.9917</cell><cell>0.9852</cell><cell>24G</cell></row><row><cell></cell><cell>VarGNet(student)</cell><cell>0.9977</cell><cell>0.9810</cell><cell>0.9810</cell><cell>1029M</cell></row><row><cell></cell><cell>VarGFaceNet(student)</cell><cell>0.9985</cell><cell>0.9850</cell><cell>0.9815</cell><cell>1022M</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We would like to thank Xin Wang, Helong Zhou, Zhichao Li, Xiao Jiang, Yuxiang Tuo for their helpful discussion, especially Helong for his advice and discussion on recursive knowledge distillation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://trillionpairs.deepglint.com/overview.Accessed" />
		<title level="m">Trillionpairs</title>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="428" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lightweight face recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00641</idno>
		<title level="m">Retinaface: Single-stage dense face localisation in the wild</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Shrinkteanet: Million-scale lightweight face recognition via shrinking teacher-student networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10620</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Knowledge transfer via distillation of activation boundaries formed by hidden neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03233</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database forstudying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Agedb: the first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07463</idno>
		<title level="m">Dnnvm: End-to-end compiler leveraging heterogeneous optimizations on fpga-based cnn accelerators</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Vargnet: Variable group convolutional neural network for efficient embedded computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05653</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
