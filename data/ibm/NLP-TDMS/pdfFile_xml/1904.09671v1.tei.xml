<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DDGK: Learning Graph Representations for Deep Divergence Graph Kernels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>May 13-17, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Zelle</surname></persName>
							<email>dzelle@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
							<email>bperozzi@acm.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Google AI</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Google AI</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DDGK: Learning Graph Representations for Deep Divergence Graph Kernels</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">May 13-17, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3308558.3313668</idno>
					<note>ACM Reference Format: Rami Al-Rfou, Dustin Zelle, and Bryan Perozzi. 2019. DDGK: Learning Graph Representations for Deep Divergence Graph Kernels. In Proceedings of the 2019 World Wide Web Conference (WWW&apos;19),</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Kernels</term>
					<term>Graph Neural Networks</term>
					<term>Representation Learning</term>
					<term>Similarity and Search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Can neural networks learn to compare graphs without feature engineering? In this paper, we show that it is possible to learn representations for graph similarity with neither domain knowledge nor supervision (i.e. feature engineering or labeled graphs). We propose Deep Divergence Graph Kernels, an unsupervised method for learning representations over graphs that encodes a relaxed notion of graph isomorphism. Our method consists of three parts. First, we learn an encoder for each anchor graph to capture its structure. Second, for each pair of graphs, we train a cross-graph attention network which uses the node representations of an anchor graph to reconstruct another graph. This approach, which we call isomorphism attention, captures how well the representations of one graph can encode another. We use the attention-augmented encoder's predictions to define a divergence score for each pair of graphs. Finally, we construct an embedding space for all graphs using these pair-wise divergence scores.</p><p>Unlike previous work, much of which relies on 1) supervision, 2) domain specific knowledge (e.g. a reliance on Weisfeiler-Lehman kernels), and 3) known node alignment, our unsupervised method jointly learns node representations, graph representations, and an attention-based alignment between graphs.</p><p>Our experimental results show that Deep Divergence Graph Kernels can learn an unsupervised alignment between graphs, and that the learned representations achieve competitive results when used as features on a number of challenging graph classification tasks. Furthermore, we illustrate how the learned attention allows insight into the the alignment of sub-structures across graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning methods have achieved tremendous success in domains where the structure of the data is known a priori. For example domains like speech and language have intrinsic sequential structure to exploit, while computer vision applications have spatial structure (images) and perhaps temporal structure (videos). In all these cases, our intuition guides us to build models and learning algorithms based on the structure of the data. For example, translation invariant convolution networks might search for shapes regardless of their physical position in an image, or recurrent neural networks might share a common latent representation of a concept across distant time steps or diverse domains such as languages. In contrast, graph learning represents a more general class of problems because the structure of the data is free from any constraints. A neural network model must learn to solve both the desired task at hand (e.g. node classification) and to represent the structure of the problem itself -that of the graph's nodes, edges, attributes, and communities.</p><p>Despite the challenges, there has been a recent surge of interest in applying neural network models to such graph-structured data arXiv:1904.09671v1 <ref type="bibr">[cs.</ref>LG] 21 Apr 2019 <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b63">63]</ref>. While initial approaches like DeepWalk <ref type="bibr" target="#b40">[40]</ref> focused on generic representations of graph primitives (e.g. a graph's nodes <ref type="bibr" target="#b40">[40]</ref> or edges <ref type="bibr" target="#b0">[1]</ref>), present approaches ignore learning general graph and node representations in favor of maximizing accuracy on a set of narrow classification tasks. These approaches, broadly referred to as Graph Neural Networks (GNNs), seek to leverage the structure between data items as a scaffolding to perform computation (e.g. message passing, gradient updates, etc). The parameters and the activations, use the structure during training, but are tuned primarily to classify the graph's nodes, edges, and/or attributes.</p><p>While much effort has focused on unsupervised learning of node representations <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b48">48]</ref>, edge representations <ref type="bibr" target="#b0">[1]</ref>, or latent community structure <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b62">62]</ref>, relatively little work has focused on the unsupervised learning of representations for entire graphs -a problem of practical interest in domains such as information retrieval, biology, and natural language processing <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b23">23]</ref>. In cases where GNNs have been applied to the task of learning similarity between graphs, the approaches considered generally come in two flavors: an end-to-end supervised graph classification or graph representation learning.</p><p>In supervised graph classification, the task is to solve an endto-end whole-graph classification problem (i.e. the problem of assigning a label to the entire graph). These supervised approaches <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b61">61]</ref> learn an intermediate representation of an entire graph as a precondition in order to solve the classification task. This learned representation can be used to compare similarity between graphs, but is heavily biased towards maximizing performance on the classification task of interest.</p><p>The second class of approaches focuses on the more general problem of learning graph representations <ref type="bibr" target="#b43">[43]</ref>. While much exciting progress has been made in this area, the existing approaches suffer from one or more of the following limitations. First, many existing methods rely on feature engineering, such as the graph's clustering coefficient, its motif distribution, or its spectral decomposition, to represent graphs <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b56">56]</ref>. By limiting the features that they consider, these methods are limited to composing only known graph signals. Second, many of these approaches <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b61">61]</ref> have sought to encode algorithmic heuristics from the graph isomorphism literature (especially the intuition encoded in the Weisfeiler Lehman algorithm <ref type="bibr" target="#b42">[42]</ref>). Relying heavily on existing heuristics to solve a hard problem raises an important question: how well can a learningonly approach solve a classic algorithmic problem? Finally, other work in this area of graph similarity assumes that identical nodes in both graphs share the same id (i.e. the alignment is already given). While this can be useful for calculating a similarity score, we find the general problem more compelling.</p><p>In this work, we propose a method of learning graph representations driven by the similarity between a pair of graphs as measured by the divergence in their structures. We show the representations learned through our method, Deep Divergence Graph Kernels (DDGK), capture the attributes of graphs by using them as features for several classification problems. In addition, we show that our representations capture the local similarity of graph pairs and the global similarity across families of graphs.</p><p>DDGK has three key differentiators. First, it makes no assumptions about the structure of the matching problem. In order to solve the matching problem, we propose an attention mechanism: isomorphism attention to align the nodes across graph pairs. Second, DDGK does not rely on any existing heuristics for graph similarity. Instead, we learn the kernel method jointly with the node representation and alignment networks. This allows the model the freedom to learn representations that best preserve the graph, and does not impose artificial oversights. Finally, as an unsupervised method, the representations it learns emphasize structural similarity, and does not correlate with a downstream labeling tasks. This is especially useful for ranking tasks where labeling may not be available. To summarize, our main contributions are:</p><p>• Deep Divergence Graph Kernels: A novel method for learning unsupervised representations of graphs and their nodes. Our kernel is learnable and does not depend on feature engineering or domain knowledge. • Isomorphism Attention: A cross-graph attention mechanism to probabilisticly align representations of nodes between graph pairs. These attention networks allow for great interpretablity of graph structure and discoverablilty of similar substructures. • Experimental results: We show that DDGK both encodes graph structure to distinguish families of graphs, and when used as features, the learned representations achieve competitive results on challenging graph classification problems like predicting the functional roles of proteins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LEARNING GRAPH REPRESENTATIONS</head><p>In this section, we lay out the problem definition of representing graphs and the connection between our representations and the kernel framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>A graph is defined to be a tuple G = (V , E), where V is the set of vertices and E is the set of edges, E ⊆ (V × V ). A graph G can have an attribute vector Y for each of its nodes or edges. We denote the attributes of node v i as y i , and denote the attributes of an edge (v i , v j ) as y i j .</p><p>Given a family of graphs G 0 , G 1 , . . . , G N we aim to learn a continuous representation for each graph Ψ(G) ∈ R N that encodes its attributes and its structure. For this representation to be useful, it has to be comparable to other graph representations. However, it is likely that our method of graph encoding will produce one of many equally good representations each time we run it. For example we can get two different, but equal, representations by permuting the dimensions of the first one. Those representations are not comparable given they exist in two different spaces.</p><p>To avoid this problem, we seek to develop an equivalence class across all possible encodings of a graph. Essentially, two encodings of a graph are equivalent if they lead to the same pair-wise similarity scores when used to compare the graph to all other graphs in the set. We note that this issue arises when working with embedding based representations across domains, and several equivalence methods have been proposed <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b58">58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Embedding Based Kernels</head><p>In this work, we study the development of graph kernels, which are functions to compute the pairwise similarity between graphs. Specifically, given two graphs G 1 , G 2 , a classic example of a kernel defined over graph pairs is the geometric random walk kernel <ref type="bibr" target="#b9">[9]</ref> as shown in Eq. 1:</p><formula xml:id="formula_0">k × (G 1 , G 2 ) = e T (I − λA × ) −1 e,<label>(1)</label></formula><p>where A × is the adjacency matrix of the product graph of G 1 and G 2 , and λ is a hyper-parameter which encodes the importance of each step in the random walk. We aim to learn an embedding based kernel function k() as a similarity metric for graph pairs, defined as the following:</p><formula xml:id="formula_1">k(G 1 , G 2 ) = ||Ψ(G 1 ) − Ψ(G 2 )|| 2<label>(2)</label></formula><p>For a dataset of N source 1 graphs S and M target graphs (T ), for any member of the target graph set we define the i t h dimension of the representation Ψ(G ∈ T ) ∈ R N to be:</p><formula xml:id="formula_2">Ψ(G) i = v j ∈V T f д i (v j ),<label>(3)</label></formula><p>where д i ∈ S and f д i () is a predictor of some structural property of the graph G but parameterized by the graph д i . We note that the source and target graphs sets (S, T ) could be disjoint, overlapping, or equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING TO ALIGN GRAPH REPRESENTATIONS</head><p>We propose to learn a graph representation by comparing it to a population of graphs. To compare the similarity of a pair of graphs (source, target), we rely on deep neural networks to measure the divergence between their structure and attributes. First, we learn the structure of the source graph by passing it through a graph encoder. Second, to measure how much the target graph diverges from the source graph, we use the source graph encoder to predict the structure of the target graph. If the pair is similar, we expect the source graph encoder to correctly predict the target graph's structure. In this section, we develop the three key components necessary to learn the similarity between a pair of graphs. First, in Section 3.1, we discuss encoding graphs. The quality of the graph representation depends on the extent to which the encoder of each source graph is able to discover its structure.</p><p>Second, in Section 3.2, we propose a cross-graph attention mechanism to learn a soft alignment between graphs. This is necessary because a target graph may not share its vertex ids with any of the source graphs -indeed, they could even have differing number of nodes! Therefore, we need to learn an alignment between the nodes of the target graph and each source graph. This leads to an alignment that is not necessarily a one-to-one correspondence.</p><p>Third, in Section 3.3 we introduce additional constraints on the cross-graph attention learning. For example, let us assume that v i ∈ V G 1 is assigned to u j ∈ V G 2 . While both v i and u j may be structurally similar, they may belong to different node classes as indicated by their attributes. These attributes may be of significant ...  importance to the nature of the graph. For instance, swapping one element for another in a graph representing a molecule could drastically change its chemical structure.</p><p>We will see how these pairwise alignments can produce divergence scores suitable for Graph Kernels in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Encoding</head><p>To learn the structure of a graph, we train an encoder capable of reconstructing such structure given partial or distorted information. In this paper, we choose a Node-To-Edges encoder ( <ref type="figure" target="#fig_1">Figure 2</ref>) for its simplicity, but we note that additional choices are certainly possible (see Section 7 for more discussion).</p><p>Node-To-Edges Encoder. -In this setup, an encoder is given a single vertex and it is expected to predict its neighbors. This can be modeled as a multilabel classification task since the predictions are not mutually exclusive. Specifically, we are maximizing the following objective function J (θ ),</p><formula xml:id="formula_3">J (θ ) = i j e i j ∈E log Pr(v j | v i , θ ).<label>(4)</label></formula><p>Each vertex v i in the graph is represented by one-hot encoding vector ì v i . Then to embed the vertex we multiply its encoding vector with a linear layer E ∈ R |V |×d resulting in an embedded vertex e v i ∈ R d , where |V | is the number of vertices in the graph, and d is the size of the embedding space.</p><p>For graphs with a large number of nodes, we can replace this multiplication with a table lookup, extracting one row from the embedding matrix. This embedding vector represents the feature set given to the encoder tasked with predicting all adjacent vertices. Our encoder H , is implemented as a fully connected deep neural network (DNN) with an output layer of size |V | and trained as a multilabel classifier.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frozen Parameters One Hot Encoding</head><p>Neighbors of v 1 <ref type="figure">Figure 3</ref>: Attention layers mapping the target graph nodes onto the source graph. The augmented encoder has to predict the neighbors of node 1 in the target graph. First, node 1 is passed to the attention layer which assigns it mainly to node 3 of the source graph. Second, the source graph encoder learned earlier (in <ref type="figure" target="#fig_1">Figure 2</ref>) that the neighbors of node 3 are {2, 4}. Finally, the reverse attention network maps nodes {2, 4} of the source graph to nodes {2, 3, 6} of the target graph which are the neighbors of node 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-Graph Attention</head><p>So far, we have developed a utility to encode individual graphs. However, we seek to develop a method which can compare pairs of graphs, which may differ in size (differing node sets) and structure (differing edge sets). For this to happen we need a method of learning an alignment between the graphs. Ideally this method will operate in the absence of a direct mapping between nodes. In other areas, attention models have been proposed to align structured data. For example, attention models have been proposed to align pairs of images and text <ref type="bibr" target="#b55">[55]</ref>, pairs of sentences for translation <ref type="bibr" target="#b49">[49]</ref>, and pairs of speech and transcription <ref type="bibr" target="#b16">[16]</ref>. Inspired by these efforts, we formalize the problem of aligning two graphs as that of attention. We propose an attention mechanism, isomorphism attention, that aligns the nodes of a target graph against those of a source graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Isomorphism Attention.</head><p>Given two graphs S (source graph) and T (target graph), we propose a model that allows bi-directional mapping across the pair's nodes. This requires two separate attention networks. The first network allows nodes in the target graph to attend to the nodes in the source graph. The second network, allows neighborhood representations in the source graph to attend to neighborhoods in the target graph.</p><p>We denote the first attention network as (M T →S ), which assigns every node in the target graph (u i ∈ T ) a probability distribution over the nodes of the source graph (v j ∈ S). This attention network will allow us to pass the nodes of the target graph as an input to the source graph encoder. We implement this attention network using a multiclass classifier,</p><formula xml:id="formula_4">Pr(v j | u i ) = e M T →S (v j ,u i ) v k ∈V S e M T →S (v k ,u i ) .<label>(5)</label></formula><p>The second network is a reverse attention network (M S →T ) which aims to learn how to map a neighborhood's representation in the source graph to a neighborhood in the target graph. By adding both attention networks to the source graph encoder, we will be able to construct a target graph encoder that is able to predict the neighbors of each node -but utilizing the structure of the source graph. We implement the reverse attention as a multilabel classifier,</p><formula xml:id="formula_5">Pr(u j | N (v i )) = 1 1 + e −M S →T (u j , N(v i ))</formula><p>.</p><p>(6) <ref type="figure">Figure 3</ref> shows the attention network (M T →S ) receiving a onehot encoding vector representing a node (u i ) in the target graph and mapping it onto the most structurally similar node (v j ) from the source graph. The source graph encoder, then, predicts the neighbors of v j , N (v j ). The reverse attention network (M S →T ), takes N (v j ) and maps them to the neighbors of u i , N (u i ).</p><p>Both attention networks may be implemented as linear transformations W A ∈ R |V Q |×|V P | . In the case that either |V P | or |V Q | are prohibitively large, the attention network parameters can be decreased by substituting a DNN with hidden layers of fixed size. This will reduce the attention network size from Θ(|V P | × |V Q |) to Θ(|V P | + |V Q |).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attributes Consistency</head><p>Labeled graphs are not defined only by their structures, but also by the attributes of their nodes and edges. The attention network assigns each node in the target graph a probability distribution over the nodes of the source graph. There might be several, equally good, nodes in the source graph with similar structural features. However, these nodes may differ in their attributes. To learn an alignment that preserves nodes and edges attributes, we add regularizing losses to the attention and reverse-attention networks.</p><p>More specifically, we refer to the nodes as v and u for the source and target graphs, respectively. We refer to the set of attributes as Y and the distribution of attributes over the graph nodes as (Q n = Pr(y i | u)). Given that the attention network M T →S learns the distribution Pr(u k | v j ), we can calculate a probability distribution over the attributes as inferred by the attention process as the following:</p><formula xml:id="formula_6">Q n (y i |u j ) = k M T →S (y i |v k ) Pr(v k | u j ).<label>(7)</label></formula><p>We define, the attention regularizing loss over the nodes attributes to be the average cross entropy loss between the observed distribution of attributes and the inferred one (See Eq. 8).</p><formula xml:id="formula_7">L = 1 |V T | |V T | j i Pr(y i | u j ) log(Q n (y i |u j )),<label>(8)</label></formula><p>where |V T | is the number of nodes in the target graph. For preserving edge attributes over nodes, we define Q e (y i | u) = Pr(y i | u) to be the normalized attributes count over all edges connected to the node u. For instance, if a node u has 5 edges with 2 of them colored red and the other three colored yellow, Q e (red | u) = 0.4 By replacing Q n with Q e in Equations 7 and 8, we create a regularization loss for edge attributes.</p><p>We also introduce these regularization losses for reverse attention networks. Reverse attention networks maps a neighborhood in the source graph to a neighborhood in the target graph. The distribution of attributes over a node's neighborhood will be the frequency of each attribute occurrence in the neighborhood normalized by the number of attributes appearing in the neighborhood. For edges, the node's neighborhood edges are the edges appearing at 2-hops distance from the node. Similarly, we can define the probability of the edges attributes by normalizing their frequencies over the total number of attributes of edges connected to the neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEEP DIVERGENCE GRAPH KERNELS</head><p>So far, we have proposed a method for learning representations of graphs, and an attention mechanism for aligning graphs based on a set of encoded graph representations. Here we discuss our proposed method for using the alignment to construct a graph kernel based on divergence scores. First, in Section 4.1, we show how we can utilize the divergence scores to construct a full graph representation. Divergence is driven by the target graph structure and attribute prediction error as calculated using a source graph encoder. Next we introduce DDGK, our method for learning graph representations for Deep Divergence Graph Kernels in Section 4.3. Then in Section 4.4 we discuss how we train these representations. Finally we discuss the scalability of this approach in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Divergence</head><p>In Section 3 we presented a method to align two graphs by using a source graph encoder, augmented with attention layers, to encode a target graph. Here, we propose to use the ability of the augmented encoder at predicting the structure of the target graph as a measure of those graphs similarity. To explain, let us assume the trivial case where both the source and target graphs are identical. First, we train the source graph encoder. Second, we augment it with attention networks and train it to predict the structure of the target graph. The attention networks will (ideally) learn the identity function. Therefore, the source graph encoder is able to encode the target graph as accurately as encoding itself. We would reasonably conclude that these graphs are similar.</p><p>We aim to learn a metric that measures the divergence score between a pair of graphs {S,T }. If two graphs are similar, we expect their divergence to be correspondingly low. We refer to the encoder trained on a graph S as H S and the divergence score given to the target graph T to be</p><formula xml:id="formula_8">D ′ (T ∥ S) = v i ∈V T j e ji ∈E T − log Pr(v j | v i , H S )<label>(9)</label></formula><p>Given that H S is not a perfect predictor of the graph S structure, we can safely assume that D ′ (S ∥ S) 0. To rectify this problem we define</p><formula xml:id="formula_9">D(S ∥ T ) = D ′ (S ∥ T ) − D ′ (S ∥ S),<label>(10)</label></formula><p>which sets D(S ∥ S) to zero. We note that this definition is not symmetric (as D(T ∥ S) might not necessarily equal to D(S ∥ T )). If symmetry is required, we can define D(S,T ) = D(S ∥ T ) + D(T ∥ S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Embedding</head><p>Given a set of source graphs, we can establish a vector space where each dimension corresponds to one graph in the source set. Target input : Set of N source graphs S Set of M target graphs T Learning rate α Encoding epochs τ Scoring epochs ρ output :  graphs are represented as points in this vector space where the value of the i th dimension for a given target graph T j is D(T j ∥ S i ).</p><formula xml:id="formula_10">All graph representations Ψ ∈ R M ×N 1 // learn graph encodings 2 foreach д i ∈ S do 3 V , E ← д i 4 for step ← 0 to τ do 5 J (θ ) = − s t e s t ∈E log Pr(v t | v s , θ ) 6 θ = θ − α * ∂ J</formula><formula xml:id="formula_11">(M T →S , M S →T ) = − s t e s t ∈E log Pr(v t |v s ,θ j , M T →S , M S →T ) 16 M T →S = M T →S − α * ∂ J ∂M T →S 17 M S →T = M S →T − α * ∂ J ∂M S →T</formula><p>More formally, for a set of N source graphs we can define our target graph representation to be:</p><formula xml:id="formula_12">Ψ(G T ) = [D(T ∥ S 0 ), D(T ∥ S 1 ), . . . , D(T ∥ S N )]<label>(11)</label></formula><p>To create a kernel out of our graph embeddings, we use the Euclidean distance measure as outlined in Eq 2. This distance measure will guarantee a positive definite kernel <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b54">54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Algorithm : DDGK</head><p>We present pseudo-code for DDGK in Algorithm 1. The algorithm has two parts. First, a Node-To-Edges encoder is trained for all source graphs (Algorithm 1 line 5 and line 6). Second, cross-graph attentions are learned for all target-source graph pairs (Algorithm 1 line 15, line 16 and line 17). We implement DDGK using a deep neural network for its Node-To-Edges encoder and linear transformations for its isomorphism attention.    <ref type="figure">Figure 4</ref>: The effect of attributes preserving losses on the attention networks. Our method is given a pair of identical graphs, the upper graph represents the target and the other represents the source graph. Each graph consists of two rings of size 5 connected with one edge ((0, 5) and (10, 15) respectively). We visualize the strongest attention weights as cross-graph edges. On the right of each figure we visualize the rest of the attention weights as a heatmap. When the graph attends to itself without attribute preserving losses, there are several solutions that are equally good because of several symmetries available. Once we add the nodes attributes, we can see an immediate effect where the nodes from the same label class attend only to each other. This behavior further intensifies after also adding the edge attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>We implement our models using TensorFlow <ref type="bibr" target="#b44">[44]</ref>, calculate our gradients using backpropagation, and update our parameters using Adam <ref type="bibr" target="#b29">[29]</ref>. We train each source graph on its adjacency matrix for a constant number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Target Graph Encoding.</head><p>Here, the augmented encoder has to predict the neighboring vertices for each vertex in the target graph with the help of the attention and reverse-attention layers.</p><p>To learn the augmented target graph encoder (which consists of the source graph encoder with the additional attention layers), we use the following procedure:</p><p>(1) First, freeze the parameters of the source graph encoder.</p><p>(2) Second, add two additional networks, one for attention and another for reverse attention mapping between the target graph nodes to the source graph nodes and vice versa. (3) Third, add the regularizing losses to preserve the nodes or edges attributes if they are available. (4) Fourth, train the augmented encoder on the input, which is the adjacency matrix of the target graph, and a node attribute and/or edge attribute matrix (if available).</p><p>Finally, once the training of the attention layers is done, we use the augmented encoder to compute the divergence between the graph pair as discussed in 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Scalability</head><p>We start by defining the following quantities: N the number of source graphs in the dataset, M the number of target graphs in the dataset, V the average number of nodes, τ the number of epochs to encode source graphs, ρ the number of epochs to encode target graphs, l the number of encoder hidden layers, m the number of attention hidden layers, and d the embedding and hidden layer size Our method relies on pairwise similarity, therefore, we will have M × N computations that each involves scoring a target graph against one source graph. Training a source graph encoder requires τ steps that each involves 2×V ×d +l ×d 2 computations. In addition to running the source graph encoder, the target graph alignment learns the attention networks which represents ρ×(2×d×V +m×d 2 ). If we define T = max(ρ, τ ), k = max(l, m), and M = N then the total computation cost is Θ(</p><formula xml:id="formula_13">N 2 × T × (V × d + k × d 2 )</formula><p>. Because V is likely much larger than d 2 , we interpret the computational complexity as O(T N 2 V ).</p><p>In Section 5.4, we explore the effect of sampling to hasten DDGK's runtime on large datasets. We show that not all M × N comparisons are necessary to achieve high performance: empirically, it seems that less than 20% of source graphs are required, significantly speeding our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we demonstrate our method through a series of qualitative and quantitative experiments. First, we show how our attention based alignment works under different conditions. Then, we show how our representations are capable of capturing the structure of the space of graphs by applying hierarchical clustering on the kernel space. Finally, we show that the learned graph embeddings represent a sufficient feature set to predict the graph label on several challenging classification problems in real-world biological datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Cross-Graph Attention</head><p>In this qualitative experiment, we seek to understand how two graphs are related to each other. Comparing different patterns between different graphs is an important application in domains such as biology. <ref type="figure">Figure 4a</ref> shows two identical unlabeled barbell graphs. Each graph consists of two rings of size 5 connected with the edge (0, 5) and <ref type="bibr" target="#b10">(10,</ref><ref type="bibr" target="#b15">15)</ref>. The upper graph represents the target graph while the lower one represents the source graph. The edges connecting the source and target graphs represent the strongest attention weights for each node in the target graph. The heatmap shows the full attention matrix for more thorough analysis. Aligning these identical graphs is an easy task for the naked eye. However, our method can find many possible symmetries to exploit while still achieving perfect predictions. For example, nodes in the left ring can attend to the right ring of the source graph and vice versa. <ref type="figure">Figure 4b</ref> shows the previous setup with labeled graph nodes. This introduces a regularizing loss to preserve the node attributes. The attention heat map shows significant weights for the upper left and lower rights quadrants. The right ring does not attend to nodes in the left ring anymore, and vice versa. Still we can see the method exploiting symmetries within the same ring.</p><p>Finally, by also adding edge labels, the alignment problem is constrained enough that the attention heatmap is concentrated along the diagonal (See <ref type="figure">Figure 4c)</ref>. We can observe that the attention edges correspond in a one-to-one relationship between the target and source graphs. This synthetic experiment shows the effect of attribute preserving losses on learning the alignment between graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hierarchical Clustering</head><p>To understand the global structure of the graph embedding space, we explore it qualitatively using hierarchical clustering. First, we create a dataset which is a composition of 6 different families of graphs. Three graph families are mutated graphs and three families are subset of a larger set of realistic graphs. From each family we sample 5 graphs, creating a universe of 30 graphs. Then, we embed the graphs using our method constructing a graph embedding space. Finally, we cluster the embeddings according to their pairwise euclidean distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Mutated Graphs.</head><p>For these datasets we start with a known graph and generate a sequence of mutations to produce a family of graphs. In particular, we consider the following graphs.</p><p>• C. Elegans <ref type="bibr" target="#b53">[53]</ref>: represents the neural network of the C. Elegans worm. • Karate Club <ref type="bibr" target="#b60">[60]</ref>: social network of friendships between 34 members of a karate club. • Word Network <ref type="bibr" target="#b35">[35]</ref>: adjacency network of common adjectives and nouns in the novel David Copperfield by Charles Dickens. In order to generate a family G 1 · · · G k for each original graph G 0 , we employ the following mutation procedure. At each of the k time steps, there is a p = 0.5 chance of performing an edge deletion or addition. For additions, we select the two nodes to connect from any unlinked nodes according to the preferential attachment model characterized by G 0 <ref type="bibr" target="#b17">[17]</ref>. For deletions, we select an edge at random and remove it. We run this procedure for 4 times with k = 50 time steps, creating a family of 5 related graphs. The initial seed for any of these mutations is denoted by the suffix "-0".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Realistic</head><p>Graphs. We randomly pick 5 graphs from three of the real-world families of graphs we consider (D&amp;D, PTC, and NCI1). See Section 5.3.2 for more information about these graphs. <ref type="figure" target="#fig_9">Figure 5</ref> shows the result of clustering the pairwise distances between our graph embeddings. We are able to retrieve perfect clusters of {c-elegans, words} where there are clusters of size of 5 that consist only of graphs of the same type. For {NCI1, D&amp;D}, we can cluster 4 graphs out of 5 before adding a graph which is out of the family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Graph Classification</head><p>Our learned graph representations respect both attributes and structure. They can be used for graph classification tasks where the graph structure, node attributes, and/or edge attributes convey meaning or function. To demonstrate this, we use DDGK representations of  several chemo-and bio-informatics datasets as features for classification tasks. We report our results against both unsupervised and supervised methods. <ref type="table" target="#tab_0">Table 1</ref>), we perform grid searches for each dataset. We create splits of each dataset to avoid over-fitting, they are: {train, dev, test}. We use the scikit-learn SVM <ref type="bibr" target="#b39">[39]</ref> as our classifier, and we vary the kernel choices between {linear, rbf, poly, sigmoid} and the regularization coefficient C between 10 and 10 9 . We choose the hyper-parameters of both DDGK and the classifier that maximize the accuracy on the dev dataset.  <ref type="table" target="#tab_0">D&amp;D  1178  284  716  2  89  −  NCI1  4110  30  32  2  37  −  PTC  344  14  15  2  18  4  MUTAG  188  18  20  2  7  4   Table 2</ref>: Statistics of the chemo-and bio-informatics datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Hyper-parameters Search. To choose DDGK hyperparameters (See</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Datasets.</head><p>Four benchmark graph classification datasets from chemo-and bio-informatics domains are used. The datasets include D&amp;D, NCI1, PTC and MUTAG. All datasets include node labels. The PTC and MUTAG datasets also include edge labels. <ref type="table">Table 2</ref> shows network statistics for each dataset. The datasets:</p><p>• D&amp;D <ref type="bibr" target="#b20">[20]</ref>: contains 1178 proteins labeled as enzymes or non-enzymes. • NCI1 [51]: contains 4110 chemical compounds labeled as active or inactive against non-small cell lung cancer. • PTC <ref type="bibr" target="#b46">[46]</ref>: contains 344 chemical compounds labeled according to their carcinogenicity in male rats. • MUTAG <ref type="bibr" target="#b19">[19]</ref>: contains 188 mutagenic aromatic and heteroaromatic compounds labeled according to their mutagenic effect on a specific gram negative bacterium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.3</head><p>Results. The results of these experiments are presented in <ref type="table">Table 3</ref>. We see that DDGK is quite competitive, with higher average performance on both the D&amp;D and MUTAG datasets than any of the baselines. This is especially surprising given that the supervised methods have additional information available to them. We note that DDGK achieves its strong results without engineered features, or access to information from Weisfeiler-Lehman kernels. For PTC, we also see that DDGK attains competitive performance against all other methods, only being outperformed by 2 of the 9 baselines. Finally, on NCI1, we see that DDGK performs better than the method using the most similar kind of information (node2vec), but find that baselines using the WL kernel perform best on this dataset (indeed, the WL kernel itself takes the top two spots). We find this dependence quite interesting, and will seek to characterize it better in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Dimension Sampling</head><p>So far, we have been setting the source graphs set to be equal to the target graphs set. This pairwise computation is quite expensive for large datasets. To reduce the computational complexity of our method, we study the effect of sub-sampling the dimensions of our graph embedding space on the quality of graph classification.</p><p>To do that, we construct a source graph set that is a subset of the original graph set. We learn divergence scores for all target graphs against this subset. We use the reduced embeddings as features to predict graph categories. <ref type="figure" target="#fig_11">Figure 6</ref> shows that we are able to achieve stable and competitive results with less than 20% of the graphs being used as source graphs. Here we vary the number of source graphs available to each method, and observe that very few dimensions are needed to achieve our final classification performance (less than &lt; 20% of the dimensions for the datasets considered).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>The main differences between our proposed method and previous work can be summarized as follows:</p><p>(1) We are an unsupervised method, taking only a graph as input. <ref type="bibr" target="#b2">(2)</ref> We use no domain-specific information about what primitives are important in a graph, using only the edges. (3) We use no algorithmic insights from the literature in graph isomorphism (e.g. the Weisfeiler-Lehman kernel). <ref type="bibr" target="#b4">(4)</ref> We assume nothing about the mapping of node ids between graphs, instead learning the alignment.</p><p>While many approaches exist that contain at least one of these differentiators, we are, to the best of our knowledge, the only proposed method that meets all four of these conditions. In this section we will briefly cover related work in graph similarity and other applications of neural networks to graph representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Unsupervised Graph Similarity</head><p>We divide our brief survey of the literature into three kinds of unsupervised methods for graph similarity. The first seeks to explicitly define a kernel over graph features, or use the intuition from such a kernel as part of the representation learning process. The second focuses on the representation of individual elements of the graph, learning primitives that maximize some kind of reconstruction of the graph. The third group of work constructs a similarity function between graphs by an explicit vector of statistical features constructed by the graph. Traditional Graph Kernels: There has been considerable work done on unsupervised methods for graph kernel learning. Initial efforts in the area focused on theoretical views of the problem, defining graph similarity via the Graph Edit Distance <ref type="bibr" target="#b22">[22]</ref> or the size of the Maximum Common Subgraph <ref type="bibr" target="#b11">[11]</ref> between graphs.  <ref type="table">Table 3</ref>: Average accuracy in ten-fold cross validation on our graph classification task. Methods are grouped by their level of supervision during the similarity metric learning, whether they use algorithm insights the Weisfeiler-Lehman algorithm, and whether they use feature engineering (e.g. graph motifs, random walks, etc.). Baseline results taken from <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b43">43]</ref> (missing results are missing from these works). We note that DDGK performs surprisingly competitively for an unsupervised method with no hard-coded insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Unfortunately these problems are both NP-Complete in the general case, require a known correspondence between the nodes of the two graphs of interest. Many approaches are built around the graph similarity measure computed by the Weisfeiler-Lehman (WL) subtree graph kernel <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b42">42]</ref>. At its core, the WL algorithm collapses the labels of a node's neighbors into a ordered sequence, and then hashes that sequence into a new label for the node. This process repeats iteratively to average information over the neighborhood together. Other functions that use different types of predefined features for graph similarity, such as shortest-paths kernels <ref type="bibr" target="#b8">[8]</ref>, and random walk kernels <ref type="bibr" target="#b28">[28]</ref> have also been proposed, but their naive implementations suffer from high asymptotic complexity (O(n 4 ) and O(n 6 ), respectively). Faster implementations of these kernels have been proposed <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b27">27]</ref>. Some unsupervised methods also focus on extending the algorithm intuition of these classic approaches to the problem. For instance <ref type="bibr" target="#b43">[43]</ref> learns a representation for each position in a WL ordering jointly while learning a graph representation.</p><p>Unlike all of these approaches, our method deliberately avoids algorithmic insights. Our proposed isomorphism attention mechanism allows capturing higher-order structure between graphs (beyond immediate neighborhoods). Node embedding methods: Since DeepWalk <ref type="bibr" target="#b40">[40]</ref> proposed embedding the nodes via a sequence of random walks, the problem of node representation learning has received considerable attention <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b48">48]</ref>. In general, all of these methods utilize insights about similarity functions which are important to the graph. While these methods seek the best way to represent nodes, the representations are learned independently between graphs, which makes them generally unsuitable for graph similarity computations. For more information on this area, we recommend recent surveys in the area <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b18">18]</ref>. Unlike these methods, our goal is to learn representations of graphs, not of nodes.</p><p>Graph statistics: Finally, another family of unsupervised graph similarity measures define a hand-engineered feature vector to compute graph similarity. The NetSmilie method <ref type="bibr" target="#b6">[6]</ref> operates by constructing a fixed size feature value of graph statistics and uses this as a similarity embedding over graphs. Similarly, DetlaCon <ref type="bibr" target="#b31">[31]</ref> defines the similarity over two graphs with known node-to-node mapping via the similarity in their propagation of belief, and <ref type="bibr" target="#b38">[38]</ref> proposes a number of similarity measures over directed web graphs.</p><p>Unlike these methods, DDGK does not explicitly engineer its features for the problem. Instead, the similarity is learned function directly from the edges present in the adjacency matrix, with no assumptions about which features are important for the application task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Supervised Graph Similarity</head><p>The first class of supervised methods uses some supervision to inform a similarity function constructed over different hand-engineered graph features.</p><p>A number of supervised approaches also utilize intuitions from the Weisfeiler-Lehman graph kernel. Patchy-SAN <ref type="bibr" target="#b37">[37]</ref> proposes an approach for convolutional operations on graph structured data. The core of their method uses the ordering from the WL kernel to order the nodes of a rooted subgraph into a sequence, and then apply standard 1-dimensional convolutional filters. This approach is further generalized by <ref type="bibr" target="#b61">[61]</ref>, who use the WL ordering to sort a graph sample in a pooling layer. Another branch of work has focused on extending the Graph Convolutional Networks (GCNs) proposed by <ref type="bibr" target="#b30">[30]</ref> to perform supervised classification of graphs. Proposed extensions include a pooling architecture that learns a soft clustering of the graph <ref type="bibr" target="#b59">[59]</ref>, or a two-tower model which frames graph similarity as link prediction between GCN representations <ref type="bibr" target="#b4">[4]</ref>. Interestingly, it has been shown that many of these methods are not necessarily more expressive than the original Weisfeiler-Lehman subtree kernel itself <ref type="bibr" target="#b33">[33]</ref>.</p><p>Unlike all of these approaches, our method learns representations of graphs without supervision -we use no labels about the class label of a graph, and no external information about which pairs of graphs are related. Our proposed isomorphism attention mechanism allows capturing higher-order structure between graphs (beyond immediate neighborhoods).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXTENSIONS &amp; FUTURE WORK</head><p>Here we briefly discuss a number of areas of future investigation for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Graph Encoders</head><p>Given the choice of input and reconstructed output, several additional graph encoders are possible, in addition to the Nodes-To-Edges encoder which we used in this work. To mention a few options:</p><p>Edge-To-Nodes Encoder. -This encoder is trained to predict the source and destination vertices given a specific edge. Similar to the Node-To-Edges encoder, this could be expressed as a multilabel classification task with the following objective function,</p><formula xml:id="formula_14">J (θ ) = e i j ∈E log Pr(v i | e i j , θ ) + log Pr(v j | e i j , θ )<label>(12)</label></formula><p>Note that the number of edges in a graph could grow quadratically, therefore, iterating over the edges is more expensive than the nodes.</p><p>Neighborhood Encoder. -In this case, the encoder is trained to predict a set of vertices or edges that are beyond the immediate neighbors. Random walks could serve as a mechanism to calculate a neighborhood around a specific node or edge. Given a partial random walk, the encoder has to predict the vertices that could have been visited within a specific number of hops.</p><formula xml:id="formula_15">J (θ ) = (v 1 ,v 2 , ··· ,v i ) ∼RandomW al k (G, E,V )</formula><p>log Pr v j | (v 1 , v 2 , · · · , v i , θ ) (13)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Attention Mechanism</head><p>We proposed a simple attention mechanism which uses node-tonode alignment. As we discussed in Section 4.5, we could replace the linear layer with a deep neural network to reduce the size of the model if scability is an issue. While node-to-node alignment enhances the interpretability of our models, subgraph alignment could lead to better and easier understanding of how two graphs are similar. Hierarchical attention models <ref type="bibr" target="#b57">[57]</ref> could lead to higher levels of abstractions which could learn community structure and which communities are similar across a pair of graphs. Hierarchy has already been used within the context of learning better node embeddings, for example <ref type="bibr" target="#b59">[59]</ref> showed that a better understanding of the graph substructure can lead to better graph classification. Therefore, we believe extending the work beyond node-to-node alignment will significantly improve our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Regularization</head><p>We proposed attribute based losses to regularize our isomorphism attention mechanism. The graph encoder capacity was adjusted according to the source graph size. However, the source graph encoder could still suffer from overfitting which would reduce its utility in recognizing similar target graphs. Therefore, further research is necessary to understand the relation between the encoder training characteristics and the quality of the generated divergence scores</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Feature Engineering</head><p>In this work we have focused on developing an approach for representing graphs that operated without any feature engineering or algorithmic insights. While this willful ignorance has allowed us to design a new paradigm for graph similarity, we suspect that there are many fruitful combinations of this idea with other approaches for graph classification. For example, the graph embeddings we learn could be used as additional features for approaches based on learning supervised classifiers over graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this work, we have shown that neural networks can learn powerful representations of graphs without explicit feature engineering. Our proposed method, Deep Divergence Graph Kernels, learns an encoder for each graph to capture its structure, and uses a novel isomorphism preserving attention mechanism to align node representations across graphs without the use of supervision. We show that representing graphs by their divergence from different source graphs provides a powerful embedding space over families of graphs. Our proposed model is both flexible and amenable to extensions. We illustrate this by proposing extensions to handle many commonly occurring varieties of graphs, including graphs with attributed nodes, and graphs with attributed edges.</p><p>Our experimental analysis shows that despite being trained with only the graph's edges (and no feature engineering) the learned representations encode a variety of local and global information. When the representations produced by DDGK are used as features for graph classification methods, we find them to be competitive with challenging baselines which use at least one of graph labels, engineered features, or the Weisfeiler-Lehman framework. In addition to being powerful, DDGK models are incredibly informative. The learned isomorphism attention weights allow a level of insight into the alignment between a pair of graphs, which is not possible with other deep learning methods developed for graph similarity.</p><p>Unsupervised representation learning for graphs is an important problem, and we believe that the method of Deep Divergence Graph Kernels we have introduced here is an exciting step forward in this area. As future work, we will investigate 1) enhanced method for choosing informative source from the space of all graphs, 2) improving the architecture of our encoders and attention models, 3.) making it easier to reproduce research results in the area of graph similarity, and 4) making graph similarity models even easier to understand.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A Node-To-Edges Encoder. Here the input graph contains 4 vertices, and the encoder has to predict the neighbors of vertex v 3 . First, v 3 is represented by a one-hot encoding ì v 3 . Second, ì v 3 is multiplied by a linear embedding layer. Third, this embedding e v 3 is passed to a DNN which produces scores for each vertex in V . Finally, these scores are normalized using the sigmoid function to produce the final predictions, in this case, {v 2 , v 4 }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>foreach д i ∈ T do 11 V , E ← д i 12 foreach θ j ∈ encodings do 13 // learn cross-graph attention M T →S and M T →S 14 for step ← 0 to ρ do 15 J</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>18 end 19 // calculate graph divergences 20 ΨAlgorithm 1 :</head><label>1819201</label><figDesc>[i, j]← J (M T →S , M S →T ) DDGK: An unsupervised algorithm for learning graph representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Labeled nodes and edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>A hierarchical clustering of the graph kernel space for several different graph families. It shows 30 graphs that belong to 6 different families. The values of the matrix are the pairwise Euclidean distances between the graph embeddings. Node preserving loss coefficient 0, 0.25, 0.5, 1.0, 1.5, 2.0Edge preserving loss coefficient 0, 0.25, 0.5, 1.0, 1.5, 2.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Effect of sub-sampling source graphs on graph classification tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Values used during our grid search for DDGK graph representations learning hyper-parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>83.14 ± 2.72 68.10 ± 2.30 63.14 ± 6.57 91.58 ± 6.74</figDesc><table><row><cell></cell><cell></cell><cell>Unsupervised</cell><cell>No Weisfeiler-Lehman</cell><cell>No Feature Engineering</cell><cell>D&amp;D</cell><cell>NCI1</cell><cell>PTC</cell><cell>MUTAG</cell></row><row><cell>PSCN</cell><cell>[36]</cell><cell></cell><cell></cell><cell></cell><cell cols="3">76.27 ± 2.64 76.34 ± 1.68 62.29 ± 5.68 88.95 ± 4.37</cell></row><row><cell>DGCNN</cell><cell>[61]</cell><cell></cell><cell></cell><cell></cell><cell>−</cell><cell cols="2">74.44 ± 0.40 58.59 ± 2.40 85.83 ± 1.60</cell></row><row><cell>SP Kernel</cell><cell>[8]</cell><cell>✓</cell><cell></cell><cell></cell><cell cols="3">79.00 ± 0.60 74.50 ± 0.30 58.90 ± 2.20 83.00 ± 1.40</cell></row><row><cell>WL Kernel</cell><cell>[32]</cell><cell>✓</cell><cell></cell><cell></cell><cell cols="3">79.00 ± 0.40 85.80 ± 0.20 61.30 ± 1.40 86.00 ± 1.70</cell></row><row><cell>WL-OA</cell><cell>[32]</cell><cell>✓</cell><cell></cell><cell></cell><cell cols="3">79.20 ± 0.40 86.10 ± 0.20 63.60 ± 1.50 86.00 ± 1.70</cell></row><row><cell>DGK</cell><cell>[56]</cell><cell>✓</cell><cell></cell><cell></cell><cell>−</cell><cell cols="2">80.30 ± 0.40 60.10 ± 2.50 87.40 ± 2.70</cell></row><row><cell>graph2vec</cell><cell>[34]</cell><cell>✓</cell><cell></cell><cell></cell><cell>−</cell><cell cols="2">73.22 ± 1.90 60.17 ± 6.90 83.15 ± 9.20</cell></row><row><cell cols="2">S2S-N2N-PP [43]</cell><cell>✓</cell><cell></cell><cell></cell><cell>−</cell><cell cols="2">83.72 ± 0.40 64.54 ± 1.10 89.86 ± 1.10</cell></row><row><cell>node2vec</cell><cell>[24]</cell><cell cols="2">✓ ✓</cell><cell></cell><cell>−</cell><cell cols="2">61.91 ± 0.30 55.60 ± 1.40 82.01 ± 1.00</cell></row><row><cell>DDGK</cell><cell cols="4">(this paper) ✓ ✓ ✓</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper, we use source and anchor interchangeably when referring to the encoded graph.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Edge Representations via Low-Rank Asymmetric Projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (CIKM &apos;17)</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management (CIKM &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3132847.3132959</idno>
		<ptr target="https://doi.org/10.1145/3132847.3132959" />
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1787" to="1796" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Watch Your Step: Learning Node Embeddings via Graph Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9198" to="9208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1250</idno>
		<ptr target="https://doi.org/10.18653/v1/D16-1250" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2289" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05689</idno>
		<title level="m">Graph Edit Distance Computation via Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Netsimile: A scalable approach to size-independent network similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Berlingerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1209.2684</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03815</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, Fifth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast computation of graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svn</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1449" to="1456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comparison of algorithms for maximum common subgraph on randomly connected graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corrado</forename><surname>Guidobaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSPR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Learning Graph Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence</title>
		<meeting>the Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning community embedding with community detection and node embedding on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Cavallari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.02590</idno>
		<title level="m">A Tutorial on Network Embeddings</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Harp: Hierarchical representation learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention-Based Models for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Generative Model -the Preferential Attachment Scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><forename type="middle">Chung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kian</forename><forename type="middle">Fan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complex graphs and networks. Number 107</title>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Chapter 3</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kumar Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosa</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corwin</forename><surname>Hansch</surname></persName>
		</author>
		<idno type="DOI">http:/arxiv.org/abs/https:/doi.org/10.1021/jm00106a046</idno>
		<ptr target="https://doi.org/10.1021/jm00106a046" />
	</analytic>
	<monogr>
		<title level="j">Journal of Medicinal Chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Is a Single Embedding Enough? Learning Node Representations that Capture Multiple Social Contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Epasto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey of graph edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="113" to="129" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning with distance substitution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Haasdonk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claus</forename><surname>Bahlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="220" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast random walk graph kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 SIAM International Conference on Data Mining. SIAM</title>
		<meeting>the 2012 SIAM International Conference on Data Mining. SIAM</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on machine learning (ICML-03</title>
		<meeting>the 20th international conference on machine learning (ICML-03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deltacon: A principled massive-graph similarity function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">T</forename><surname>Vogelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 SIAM International Conference on Data Mining. SIAM</title>
		<meeting>the 2013 SIAM International Conference on Data Mining. SIAM</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Louis</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02244</idno>
		<title level="m">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">2017. graph2vec: Learning Distributed Representations of Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annamalai</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahinthan</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajasekar</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shantanu</forename><surname>Jaiswal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05005</idno>
		<ptr target="http://arxiv.org/abs/1707.05005" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Finding community structure in networks using the eigenvectors of matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.74.036104</idno>
		<ptr target="https://doi.org/10.1103/PhysRevE.74.036104" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">36104</biblScope>
			<date type="published" when="2006-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Convolutional Neural Networks for Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Web graph similarity for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Dasdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Internet Services and Applications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Don&apos;t Walk, Skip!: Online Learning of Multi-scale Network Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</title>
		<meeting>the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="258" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning Graph Representations with Recurrent Neural Network Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aynaz</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Berger-Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;18 Deep Learning Day</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tensorflowteam</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/Softwareavailablefromtensorflow.org" />
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph Classification with 2D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-P</forename><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tixier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Giannis Nikolentzos, Polykarpos Meladianos, and Michalis Vazirgiannis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Statistical evaluation of the predictive toxicology challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Hannu Toivonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Helma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1183" to="1193" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">NetLSD: Hearing the Shape of a Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">VERSE: Versatile Graph Embeddings from Similarity Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikil</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2006.39</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2006.39" />
	</analytic>
	<monogr>
		<title level="m">Proceedings -Sixth International Conference on Data Mining, ICDM 2006</title>
		<meeting>-Sixth International Conference on Data Mining, ICDM 2006</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="678" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Community Preserving Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="203" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Collective dynamics of âĂŸsmall-worldâĂŹnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page">440</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>En-Hsu Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Ravikuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04956</idno>
		<title level="m">D2KE: From Distance to Kernel and Embedding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On the Dimensionality of Word Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="887" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<title level="m">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An Information Flow Model for Conflict and Fission in Small Groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">W</forename><surname>Zachary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Anthropological Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="452" to="473" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Vincent W Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Cavallari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09950</idno>
		<title level="m">From node embedding to community embedding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep Variational Network Embedding in Wasserstein Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2827" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
