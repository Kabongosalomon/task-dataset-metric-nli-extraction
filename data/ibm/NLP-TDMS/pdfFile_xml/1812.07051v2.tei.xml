<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Single Image Dehazing Using Dark Channel Prior Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20181">OCTOBER 2018 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alona</forename><surname>Golts</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Freedman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow</roleName><forename type="first">Michael</forename><surname>Elad</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Single Image Dehazing Using Dark Channel Prior Loss</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING</title>
						<imprint>
							<biblScope unit="issue">Y</biblScope>
							<date type="published" when="20181">OCTOBER 2018 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single image dehazing is a critical stage in many modern-day autonomous vision applications. Early prior-based methods often involved a time-consuming minimization of a handcrafted energy function. Recent learning-based approaches utilize the representational power of deep neural networks (DNNs) to learn the underlying transformation between hazy and clear images. Due to inherent limitations in collecting matching clear and hazy images, these methods resort to training on synthetic data; constructed from indoor images and corresponding depth information. This may result in a possible domain shift when treating outdoor scenes. We propose a completely unsupervised method of training via minimization of the well-known, Dark Channel Prior (DCP) energy function. Instead of feeding the network with synthetic data, we solely use real-world outdoor images and tune the network's parameters by directly minimizing the DCP. Although our "Deep DCP" technique can be regarded as a fast approximator of DCP, it actually improves its results significantly. This suggests an additional regularization obtained via the network and learning process. Experiments show that our method performs on par with large-scale supervised methods.</p><p>Index Terms-Energy functions, deep neural networks, unsupervised learning, single image dehazing, dark channel prior.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Haze is an atmospheric phenomenon where small particles, called aerosols, obstruct the clarity of an outdoor scene and lead to poor contrast and loss of detail. The existence of haze affects an image in two aspects. It attenuates the scene radiance with correspondence to an object's distance from the camera. Moreover, it introduces an additional ambient light component, called the airlight, which causes a "veiling effect" over the clear image. The formation of a hazy image is often described as a linear per-pixel combination of the clear scene radiance and the airlight; the effect of each component is controlled by the transmission map. To recover the scene radiance image, one has to solve a system of 3N linear equations with 4N + 3 unknowns (where N is the number of image pixels).</p><p>In order to handle the under-constrained haze creation model, many researchers suggested hand-crafted image priors, shedding additional light on the behaviour of hazy versus clean images <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. These priorbased methods often formulate the problem of dehazing as an energy minimization task, where obtaining the solution of each image is called "inference", requiring a non-trivial optimization scheme. With the increasing importance of image dehazing as an initial pre-processing stage in many computer-vision tasks (e.g., object detection, autonomous car navigation), largescale learning-based techniques have been deployed to solve it A. Golts and M.Elad are from the Department of Computer Science, Technion Institute of Technology, Technion City, Haifa 32000, Israel, corresponding e-mails: salonaz@cs.technion.ac.il, elad@cs.technion.ac.il. D. Freedman is from Google Research, Haifa, Israel, email: danielfreedman@google.com. <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. These methods, however, require thousands of input and output examples.</p><p>Since clean and hazy images of the exact same scene and lighting conditions are hard to obtain, learning-based methods commonly resort to synthetic dataset creation. Given a clean image and a corresponding depth map, one can calculate the transmission map and use the haze creation model to obtain hazy images with varying amounts of haze and airlight components. These pairs of hazy and clear images are later fed as inputs and labels in a supervised training of a DNN. Outdoor depth information, however, is incredibly imprecise. For instance, the depth information of the outdoor Make3D <ref type="bibr" target="#b14">[15]</ref> and KITTI <ref type="bibr" target="#b15">[16]</ref> datasets suffers from over 4 meters of average root-Mean-Square-Error (rMSE), while the rMSE of the indoor NYU2 <ref type="bibr" target="#b16">[17]</ref> is only 0.5. Consequently, large-scale methods either use the more reliable indoor depth information <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, or draw the depth map at random <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Either of these practices creates a domain shift when addressing real-world outdoor images.</p><p>We propose to leverage the representational power of DNNs, but instead of feeding them with inaccurate synthetic pairs of hazy and clean images, we train them in an unsupervised fashion using real-world hazy images only. We optimize the network's weights by minimizing an unsupervised loss function, essentially the Dark Channel Prior (DCP) <ref type="bibr" target="#b2">[3]</ref> energy function. Our network can be regarded as a fast feed-forward approximator of the DCP. However, by stopping the optimization early, we get a significant boost in results over the classic DCP. This implies an added regularization, stemming from the network architecture and learning process. Our network, based on the Context Aggregation Network (CAN) architecture <ref type="bibr" target="#b17">[18]</ref>, is trained end-to-end from scratch without relying on any external data apart from raw hazy images. It provides the predicted transmission maps as output, from which the dehazed image can be easily reconstructed. We perform a comprehensive quantitative evaluation of our method and present state-of-theart results on SOTS-outdoor in the recently released RESIDE dataset <ref type="bibr" target="#b18">[19]</ref>. We show qualitative results on real-world images, demonstrating that the additional regularization provided by the network reduces common artifacts of prior-based methods, such as over-saturation and high-contrast.</p><p>Our "Deep-DCP" method offers the following contributions:</p><p>1) It provides state-of-the-art results in outdoor single image dehazing, outperforming both prior-based and fullysupervised DNN methods. 2) It achieves an impressive ∼ 6.5dB boost in outdoor PSNR over classical DCP, validating an effective regularization.</p><p>3) It treats the sky successfully where DCP typically fails.</p><p>4) It is the first to perform unsupervised training in single image dehazing, reducing the need in synthetic data. 5) It does not require an explicit optimization for each image as DCP, but rather learns the underlying transformation during training, requiring a fast forward-pass during test. 6) It offers a generic methodology of unsupervised training with energy functions and can be applied to any successful energy function.</p><p>The remainder of this paper is structured as follows: Section II provides a survey of previous prior-based and data-driven approaches for dehazing; Section III describes the DCP, its use as a loss function and our CAN-based architecture; Section IV provides quantitative, qualitative and runtime experimental results; Section V includes discussions and further analysis; finally, Section VI concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prior-Based Approaches</head><p>Early attempts at image dehazing have incorporated several images of the same scene, taken at different bad weather conditions <ref type="bibr" target="#b19">[20]</ref>, or using different polarization filters <ref type="bibr" target="#b20">[21]</ref>. Kopf et al. <ref type="bibr" target="#b21">[22]</ref> later performed dehazing of outdoor images by utilizing existing geo-referenced terrain and urban models including depth, texture and GIS data.</p><p>In <ref type="bibr" target="#b1">[2]</ref>, Tan et al. unveiled the haze from a single image by maximizing the local contrast of each patch in the image using a Markov Random Field (MRF) framework. In <ref type="bibr" target="#b0">[1]</ref>, Fattal et al. suggested utilizing the lack of correlation between the transmission and shading in a localized set of pixels, as a prior to resolve the ambiguity between the scene albedo and the airlight. Tarel et al. <ref type="bibr" target="#b3">[4]</ref> provided a fast calculation of the "atmosperic veil" using a series of edge-preserving linear filter operations. In <ref type="bibr" target="#b22">[23]</ref>, Nishino et al. exploited the statistical independence between the scene albedo and depth and factorized both quantities into an MRF-based energy function.</p><p>In <ref type="bibr" target="#b2">[3]</ref>, He et al. proposed the now widely used DCP and demonstrated that in clear images the darkest pixel in an image patch is close to zero (this, however, does not hold in skyregions). Using this and the assumption that the transmission map within a small image patch is constant, a coarse map can be easily derived. They further suggested a computationally costly soft matting operation for smoothing out the transmission and reconstructing the final dehazed image. Follow-up works have improved both the quality and efficiency of DCP. Specifically, in <ref type="bibr" target="#b7">[8]</ref>, the authors proposed a general boundary constraint for the transmission map for which the DCP is a special case.</p><p>Several color-based priors have been suggested as well for boosting dehazing performance <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, Fattal used the "color-lines" assumption, stating that pixels in small image patches have a one-dimensional distribution in RGB-space <ref type="bibr" target="#b24">[25]</ref>. The offset of these straight lines from the origin in hazy images allow to estimate the transmission map. Berman et al. proposed a global approach, called non-local dehazing (NLD) <ref type="bibr" target="#b6">[7]</ref>. They observed that a haze-free image contains only several hundreds of distinct colors, clustered as points in RGB-space. In the presence of haze, these color clusters form a "haze-line" where the position of a certain pixel along the line corresponds to its initial radiance color and distance from the camera.</p><p>While prior-based methods reveal fine image details, they often suffer from increased saturation and contrast, unrealistic colors and difficulty in handling sky regions. This is due in part to assumptions not suited for all hazy image patches. In addition, each image requires a separate non-trivial optimization and solution which can be prohibitive for real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data-Driven Approaches</head><p>In <ref type="bibr" target="#b5">[6]</ref>, a Color Attenuation Prior (CAP) is suggested, mixing hand-crafted observations with a data-driven approach. CAP assumes that the image depth, the amount of haze and the difference between the brightness and saturation are linearly correlated. To find the exact correlation, the authors opt for supervised regression between synthesized hazy patches and their corresponding depth maps. This results in fast inference at test time.</p><p>One of the first works to propose single image dehazing using CNNs is <ref type="bibr" target="#b11">[12]</ref>. The method, called MSCNN, is trained by feeding a two-stage network with pairs of hazy images and corresponding transmission maps. In DehazeNet <ref type="bibr" target="#b10">[11]</ref>, Cai et al. create a novel CNN architecture (featuring maxout and BReLU layers), inspired by popular prior-based methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>. AOD-Net <ref type="bibr" target="#b9">[10]</ref> in turn, proposes a joint estimation of both the transmission map and the airlight via a unified representation. Using this representation, one can easily reconstruct the scene radiance directly in an end-to-end forwardpass computation. This helps reduce errors accumulated in the separate calculation of the two quantities.</p><p>In the recent Gated Fusion Network (GFN) <ref type="bibr" target="#b12">[13]</ref>, a dehazed image is produced as a fusion of the white balance, contrast enhanced and gamma corrected images (all derived from the hazy image). The network outputs three confidence maps which determine the effect of each component. To combat halo effects of a single scale encoder-decoder structure, a multi-scale architecture is used where a coarse output is first produced, then added as input to a finer scale network. This method provides impressive results on RESIDE's SOTS-indoor, but quadruples the size of the input during training and test, making evaluation inefficient in terms of memory. Finally, in a recent work reported in <ref type="bibr" target="#b13">[14]</ref>, the authors utilize the pre-trained VGG <ref type="bibr" target="#b25">[26]</ref> network as encoder and only train the symmetric decoder via a combination of MSE and perceptual loss.</p><p>While learning-based methods achieve impressive results, they are trained in a supervised way, relying on synthetic datasets. Some methods use more accurate indoor depth information to create labelled inputs <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. This practice, however, directs increasing research effort to optimizing indoor performance, while the predominant need for dehazing is actually outdoors. Other methods use realworld outdoor images, but compromise the accuracy of the depth information. For example, <ref type="bibr" target="#b5">[6]</ref> draws each pixel in the depth map at random from a (0, 1) uniform distribution and <ref type="bibr" target="#b10">[11]</ref> enforces an additional constraint of constant depth within 16 × 16 patches. These assumptions result in block and halo artifacts in the reconstructed image and require additional postprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR METHOD</head><p>In the following we will describe our method for single image dehazing, including the driving force of our unsupervised loss function, the Dark Channel Prior <ref type="bibr" target="#b2">[3]</ref>, its implementation as a loss function for training a CNN and the architecture we choose for the task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Haze Model</head><p>The popular haze formation model in <ref type="bibr" target="#b26">[27]</ref> is given as:</p><formula xml:id="formula_0">I(x) = t(x)J(x) + (1 − t(x))A, t(x) = e −βd(x) .<label>(1)</label></formula><p>According to the above, the observed hazy image, I(x) ∈ R N ×3 , is a convex linear combination of the haze-free scene radiance, J(x), and the atmospheric light component, A, called the airlight; usually represented as a constant 3-vector in RGB-</p><formula xml:id="formula_1">space, A = A r , A g , A b . The transmission map coefficients, t(x) ∈ R N control the relative force of each component, in each pixel in the image, x ∈ R N . The transmission is a function of the depth, d(x)</formula><p>, of the scene from the observer.</p><p>Our goal in single-image-dehazing is to obtain the haze-free scene radiance, J(x). To do so, however, one needs to solve a set of 3N equations (only I(x) is given), with 4N + 3 unknowns (J(x), t(x), A). Thus, additional prior knowledge of the images in question is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dark Channel Prior</head><p>The dark channel prior is an image statistical property, indicating that in small patches of haze-free outdoor images, the darkest pixel across all color channels is very dark, and close to zero. The "dark channel" of the image is defined as</p><formula xml:id="formula_2">J dark (x) = min c∈{r,g,b} ( min y∈Ω(x) (J c (y))),<label>(2)</label></formula><p>where Ω(x) is a small patch, centered around x. This observation is contributed by three factors which appear in outdoor images: (1) shadows -induced by cars, buildings and trees;</p><p>(2) colorful objects -where one color channel is dominant, and the others are close to zero, e.g., red flowers, green leaves, blue sea; and (3) naturally dark objects -such as tree trunks and rocks. Assuming that A is known and the transmission within a small image patch, denoted ast(x), is constant, one can apply a minimum operation across channels and pixels in the haze formation equation in (1) (effectively zeroing J c (y)) and get a prediction for the transmittance <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_3">t(x) = 1 − ω · min c min y∈Ω(x) I c (y) A c ,<label>(3)</label></formula><p>where ω = 0.95 leaves a small amount of haze for naturallooking results. In sky regions although the dark channel does not always hold, it is assumed that I/A → 1, thust(x) → 0.</p><p>The resulting coarse transmission map requires an additional step of refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Soft Matting</head><p>The haze formation model in <ref type="formula" target="#formula_0">(1)</ref> is very similar to the composition model in image matting <ref type="bibr" target="#b27">[28]</ref>, where an output image is a convex linear combination of foreground and background images; controlled by the alpha matte, α. If one replaces the α-matte with the coarse transmission map,t(x), the following energy function suggested in <ref type="bibr" target="#b27">[28]</ref> can be used to acquire the refined map, t(x):</p><formula xml:id="formula_4">E(t,t) = t T Lt + λ(t −t) T (t −t),<label>(4)</label></formula><p>where the first term promotes successful image matting, and the second, fidelity to the dark channel solution. The parameter λ, controlling the force between the two, is set to λ = 10 −4 <ref type="bibr" target="#b2">[3]</ref>. The matrix L is a Laplacian-like matrix, dedicated to image matting and given by <ref type="bibr" target="#b27">[28]</ref>:</p><formula xml:id="formula_5">L ij = n|(i,j)∈pn (δ ij − w n ij ), ∀i, j = 1...N w n ij = 1 |pn | (1+(Ii−µ n ) T (Σn+ ε |pn | U3) −1 (Ij −µ n )),<label>(5)</label></formula><p>where i, j are two pixels within a small patch p n around pixel n; |p n | is the size of the patch and equal to 3 × 3 = 9 as suggested in <ref type="bibr" target="#b27">[28]</ref>; µ n ∈ R 3 and Σ n ∈ R 3×3 are the mean and covariance of the patch; U 3 is the identitly matrix; and ε is a smoothing parameter set to ε = 10 −6 <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation as a Loss Function</head><p>We rewrite the energy function in equation <ref type="formula" target="#formula_4">(4)</ref> in a tensorfriendly format by using a known decomposition of Laplacian matrices via their weights, given in <ref type="bibr" target="#b4">(5)</ref>. Rephrasing the first term in (4) in terms of weights, we have that</p><formula xml:id="formula_6">E 1 (t,t) = t T Lt = N n=1 9 i=1 9 j=1 w n ij (t i − t j ) 2 ,<label>(6)</label></formula><p>where we sum over all overlapping patches around N pixels in the resulting transmission map, t, as well as over all possible combinations of pixel pairs, i, j, in a given 3 × 3 patch. The maximum number of combinations is (3 2 ) · (3 2 ) = 81. We can vectorize this term, along with the data fidelity term</p><formula xml:id="formula_7">E(t,t) = N n=1 K k=1 W (T I − T J ) 2 + λ N n=1 (t −t) 2 ,<label>(7)</label></formula><p>where denotes elementwise multiplication; k ∈ [1..81] indexes all possible pairs of pixels in a 3 × 3 patch, and W ∈ R N ×81 is the vectorized version of the weights. T I , T J ∈ R N ×81 are repetitions of the output transmission map. The first representing the transmission patches (9 pixels in total) arranged in I → (1, .., 1, 2, ..., 2, ..., 9, ..., 9) ∈ R 81 , and the second arranged in J → (1, 2, ..., 9, 1, 2, ..., 9, ..., 1, 2, ...9) ∈ R 81 .</p><p>Above is the loss function with which we train our network, whose predicted transmission map is parametrized by t θ . We tune the parameters, θ, by minimizing the loss function in <ref type="bibr" target="#b6">(7)</ref> over the training set of hazy images, {I m } M m=1 :  where M is the number of images. Note that we do not use the "labels", i.e., the clear images, at any point, only the original hazy ones. A schematic diagram of the inputs and outputs of our loss module is given in <ref type="figure">Figure.</ref> 2.</p><formula xml:id="formula_8">θ * = arg min θ 1 M M m=1 E(t θ ,t(I m )) ,<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Computing the Scene Radiance</head><p>Once the network has finished training, the transmission map, t θ (x), of a new hazy image can be obtained by a forward-pass operation. This is used to recover the scene radiance via the haze formation model in (1):</p><formula xml:id="formula_9">J(x) = I(x) − A max(t θ (x), t 0 ) + A,<label>(9)</label></formula><p>where t 0 , which discourages division by numbers close to zero, is set to t 0 = 0.1 as suggested in <ref type="bibr" target="#b2">[3]</ref>. In order to recover the missing airlight component, A, we follow the method suggested in <ref type="bibr" target="#b2">[3]</ref>: we first pick the 0.1% brightest pixels in the dark channel of the hazy image. Then, out of these locations we pick the brightest pixel in the hazy image, I. That is the final chosen atmospheric light, A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Architecture</head><p>Our fully-convolutional, "Dilated Residual Network", shown in <ref type="figure">Figure 1</ref>, is inspired by the Context Aggregation Network (CAN) <ref type="bibr" target="#b17">[18]</ref>, which has shown impressive results in denseoutput applications. Similarly to CAN, we keep the resolution of all layers intact and identical to that of the input and output. In order to get an accurate prediction we avoid pooling and upsampling, and instead increase the receptive field via dilated convolutions with exponentially increasing dilation factors. Contrary to <ref type="bibr" target="#b17">[18]</ref>, between each dilated convolution we add another two regular convolution layers to create a richer nonlinear representation.</p><p>Our network is thus built as a cascade of 6 dilated residual blocks; each made up of two regular convolutions, followed by a single dilated convolution. The dilation factors increase by a power of two from one block to the next. The filter size and width of all convolution layers (apart from the output) is 3 × 3 × 32. All regular convolutions are followed by batch normalization <ref type="bibr" target="#b28">[29]</ref> and ReLU nonlinearity <ref type="bibr" target="#b29">[30]</ref>, and all dilated ones are followed by batch-norm only. The final layer is a linear transformation to the output dimension of the transmission map 1 × 1 × 1. To improve gradient flow and propagate finer details to the output, we incorporate additional Resnet-style <ref type="bibr" target="#b30">[31]</ref> skip connections between the input and output of each block. The skip connection is a simple addition of the input to the output of each block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>In order to train and evaluate the performance of our network, we use the recent large-scale RESIDE (REalistic Single Image DEhazing) dataset <ref type="bibr" target="#b18">[19]</ref>. RESIDE's training set, called "ITS", includes 13, 990 synthetic indoor images, created from the NYU2 <ref type="bibr" target="#b16">[17]</ref> and Middlebury stereo datasets <ref type="bibr" target="#b31">[32]</ref>. The test set includes both indoor and outdoor sections, called "SOTS-indoor" and "SOTS-outdoor" 1 , each containing 500 synthetic images. DCP <ref type="bibr" target="#b2">[3]</ref> BCCR <ref type="bibr" target="#b7">[8]</ref> NLD <ref type="bibr" target="#b6">[7]</ref> CAP <ref type="bibr" target="#b5">[6]</ref> MSCNN <ref type="bibr" target="#b11">[12]</ref> DehazeNet <ref type="bibr" target="#b10">[11]</ref>   A smaller test set of 20 outdoor images, called "HSTS", is also suggested. HSTS has a mix of 10 synthetic images (where ground truth is known) and 10 real-world images. All synthetic hazy images are created by first collecting ground-truth clean images with their corresponding depth maps and applying the haze formation model with different configurations of the A, β parameters in <ref type="bibr" target="#b0">(1)</ref>. The beta version of RESIDE provides an additional collection of 4, 322 real-world images, mined from the web, called "RTTS". Instead of using the synthetic indoor database of ITS (or its variations based on NYU2 and Middlebury), as in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, we train our network on the real-world images of RTTS. For the evaluation of PSNR (Peak Signal to Noise Ratio) and SSIM (Structural Similarity) criteria during training, we use as validation a subset of 500 images from RESIDE beta's "OTS" synthetic outdoor training set. The 500 images are selected at random from Part-I of OTS, where we make sure that no images from the SOTS-outdoor and HSTS test sets are selected for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>To enrich the RTTS training set, we perform data augmentation. The first augmentation is simply resizing the original hazy images to size 128 × 128 using bilinear interpolation. The second, third and fourth augmentations are performed randomly. Each image can be flipped horizontally or kept as is; randomly cropped to 256 × 256 or 512 × 512, and rotated at 0, 45, 90, or 135 degrees. If rotated, only the valid center of the image is taken. All augmented images are then resized to 128 × 128. The final number of training images is therefore: 4322 × 4 = 17, 288.</p><p>The parameters of our loss function are taken exactly (no additional tuning) as suggested in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b27">[28]</ref>: λ = 10 −4 , ω = 0.95, t 0 = 0.1, ε = 10 −6 , DCP patch size: 15 × 15, and soft matting patch size: 3 × 3. We use the Adam optimizer <ref type="bibr" target="#b32">[33]</ref> with batch size of 24; initial learning rate of l r = 3 · 10 −4 , and exponential decay with factor 0.96 every 3 epochs. The network weights are initialized using random initialization with zero mean and variance of 0.1. Our method is implemented in TensorFlow on a GTX Titan-X Nvidia GPU. Training time to get the optimal solution (about 30 epochs) takes 8 hours. For outdoor results we stop the training at epoch 27, whereas for SOTS-indoor, we keep training until we reach 30 epochs. Our stopping criterion is explained further in section V-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative Evaluation</head><p>We evaluate the performance of our method on the SOTSindoor, SOTS-outdoor and HSTS test sets. These test sets are created synthetically, therefore featuring both the clean images and their hazy versions. We measure the quality of our solution in terms of the PSNR and SSIM metrics. We obtain the original code and compare our results to the following prior-based approaches: DCP [3] 2 , BCCR <ref type="bibr" target="#b7">[8]</ref> and NLD <ref type="bibr" target="#b6">[7]</ref>, and the following data-driven methods: CAP <ref type="bibr" target="#b5">[6]</ref>, MSCNN <ref type="bibr" target="#b11">[12]</ref>, DehazeNet <ref type="bibr" target="#b10">[11]</ref>, AOD-Net <ref type="bibr" target="#b9">[10]</ref> and GFN <ref type="bibr" target="#b12">[13]</ref>. In case the dehazed images are out of the range [0, 1], we normalize them to [0, 1] only if it improves PSNR and SSIM values.</p><p>The numeric results 3 are given in <ref type="table" target="#tab_2">Table I</ref>. We get the highest PSNR and SSIM scores among all other methods in the larger SOTS-outdoor, and the highest SSIM in the smaller HSTS. Our method, represented by a rich neural network and trained to accommodate numerous images, obtains better results compared to prior-based methods. Specifically, compared to DCP, our method strives to approximate the solution of the same energy function, but we stop it before reaching an absolute minimum in order to get further regularization. This is particularly noticed in outdoor images where DCP often over-saturates the sky.</p><p>With regard to data-driven approaches, our high score is attributed to the fact that we train on real-world outdoor images, whereas competing methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref> concentrate on synthetic indoor images and suffer from a certain domain shift when addressing outdoor data. In addition, the synthetic hazy and clean pairs are created from coarse depth data for which training creates a negative bias towards data-driven approaches. An example of an indoor training image in ITS is given in <ref type="figure">Figure 5</ref>. Notice the rough misplaced edges in the transmission map which later translate to inaccurate hazy images. Indeed, our closest competitor in terms of outdoor results is DehazeNet <ref type="bibr" target="#b10">[11]</ref>. Recall that this method is trained on a large variety of clean image patches of outdoor scenes, making it more robust compared to methods trained on ITS.</p><p>We include the results of our method on SOTS-indoor in which it performs favourably, but gets a lower score compared to other data-driven methods and even DCP. This is expected since we train on outdoor images, creating a tradeoff between indoor and outdoor performance. As for DCP, it behaves more agreeably on indoor images which coincide better with the haze formation model and do not include sky regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>We present qualitative results on HSTS in <ref type="figure" target="#fig_1">Figure 3</ref>. In the top part of <ref type="figure" target="#fig_1">Figure 3</ref>, it can be seen that our method maintains the true colors of the original image, whereas DCP <ref type="bibr" target="#b2">[3]</ref>, BCCR <ref type="bibr" target="#b7">[8]</ref> and NLD <ref type="bibr" target="#b6">[7]</ref> tend to produce exaggerated sky regions. Our results are similar to those produced by CAP <ref type="bibr" target="#b5">[6]</ref>, however slightly closer to the true colors exhibited in the ground truth Hazy DCP <ref type="bibr" target="#b2">[3]</ref> BCCR <ref type="bibr" target="#b7">[8]</ref> NLD <ref type="bibr" target="#b6">[7]</ref> CAP <ref type="bibr" target="#b5">[6]</ref> Ours Clear</p><p>Hazy MSCNN <ref type="bibr" target="#b11">[12]</ref> AOD-Net <ref type="bibr" target="#b9">[10]</ref> GFN <ref type="bibr" target="#b12">[13]</ref> Dehaze-Net <ref type="bibr" target="#b10">[11]</ref> Ours Clear image. In the bottom half of <ref type="figure" target="#fig_1">Figure 3</ref> we provide a comparison to deep-learning based methods. In most images we maintain the true contrast and colors, whereas MSCNN <ref type="bibr" target="#b11">[12]</ref> and GFN <ref type="bibr" target="#b12">[13]</ref> provide more contrast-enhanced images. At times, we slightly change the color of the sky, which is to be expected since our method is unsupervised and does not witness the clear images at any stage. In <ref type="figure">Figure 4</ref>, one can see a real-world image comparison of our results with both prior-based and data-driven methods. We display the output of our network after 27 epochs (the optimal results for the OTS validation set we use) and after 30 epochs, where the produced images are more similar to DCP (see discussion on sec. V-A). One can see that after 27 epochs we do not remove all of the haze, perhaps indicating that the outdoor images in RESIDE are less hazy than real-world hazy images. For 30 epochs, our result is more saturated and of higher contrast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Runtime Comparison</head><p>Apart from improving the overall PSNR and SSIM performance of DCP, we hereby show that we are as efficient as fast implementations of DCP. Our inference procedure consists of two parts: a forward-pass over the trained network to obtain the predicted transmission map (performed in TensorFlow), and reconstruction using Equation 9 (performed in Numpy). We compare ourselves to a Matlab implementation of soft matting DCP <ref type="bibr" target="#b2">[3]</ref>, denoted as "slow-DCP", and guided image filter DCP <ref type="bibr" target="#b33">[34]</ref>, denoted as "fast-DCP". Note that fast-DCP is an approximation of slow-DCP and though being very efficient, achieves inferior results. Although Matlab is more efficient than Numpy and TensorFlow, we do get the benefit of using the GPU. Thus for fair comparison, we include both GPU and Intel(R) i7-5930k 3.5GHz CPU runtimes of our solution.</p><p>In <ref type="table" target="#tab_2">Table II</ref>  of slow-DCP, our feed-forward inference is much faster (×30 in GPU and ×12 in CPU). We perform on par with fast-DCP (faster for GPU and slower for CPU), but we supply results of much better visual quality which translate to a ∼ 9.5 dB increase in PSNR. Additional speed-up of our method can be performed by joint estimation of t, A during training, but we leave this for future work. To conclude, as efficient as the DCP explicit solution may be, it will lack the additional regularization obtained by our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proximity to Dark Channel Prior</head><p>During training, our network strives to approximate the DCP energy function. Since it optimizes the loss for the entire corpus of images, it may output different results from DCP <ref type="bibr" target="#b2">[3]</ref>. While DCP operates on one image at a time, our network learns a more "universal solution", suited for multiple images. In addition, as the epochs evolve and the loss value decreases, we reach closer and closer to DCP, as can be seen in the three rightmost columns in <ref type="figure">Figure 4</ref>. At earlier epochs the output images still contain a large amount of haze, whereas further on, most of the haze is lifted, but the colors appear more saturated, even non-realistic. We search for a middle-ground where most of the haze is removed and one can see the details, but the colors and contrast remain realistic and physically valid. The benefit of stopping before reaching a deeper optimum of DCP is especially noticeable in sky regions where DCP would output an exaggerated and amped-up version of the sky, whereas we produce a more natural color. In our case this "sweet-spot" is reached after 27 epochs over the training data. Nonetheless, we can keep training for a few more epochs to get more vivid results which may be more pleasant to the human eye.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unsupervised Training Regime</head><p>Although our training is completely unsupervised, we do need a stopping criterion since reaching the minimum of the loss function is not always beneficial in terms of the visual and quantitative results. To do so, we evaluate the averge loss value, PSNR and SSIM of a small supervised set of 500 images from OTS (not part of SOTS-outdoor or HSTS). A typical behaviour of the results is a decrease of the average loss; an increase in performance in PSNR and SSIM; reaching a maximum, and then, a decrease of these criteria. We choose the epoch/model that gave the best performance on the validation set from OTS. The learning parameters are chosen using a similar technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We have presented a method of unsupervised training of deep neural networks for the purpose of single image dehazing. Our method relies on the well-known Dark Channel Prior (DCP) <ref type="bibr" target="#b2">[3]</ref> and manages to improve it considerably. In addition to providing state-of-the-art performance in outdoor scenarios, our method also eliminates the need for synthetic training sets. While our focus here is DCP, we could have incorporated any other successful energy function, using it as our unsupervised loss. Our future research is focused on finding an even better combination of energy functions, or incorporating some amount of supervision to benefit from both worlds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>&amp;Fig. 2 :</head><label>2</label><figDesc>Our loss module, which receives the prediction of the network, t θ , along with the hazy image, I, and outputs the value of the DCP [3] energy loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative results on RESIDE's HSTS. Upper half: comparison to prior-based methods; bottom half: comparison to deep-learning-based methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>System architecture. Our fully-convolutional network receives real-world hazy images. Apart from the input and output layers, our network is a cascade of dilated residual blocks (dilation written above each block), which gradually increase the receptive field. The network's predicted transmission and the input image, are fed to the unsupervised, DCP loss.</figDesc><table><row><cell>x1</cell><cell>x2</cell><cell>x4</cell><cell>x8</cell><cell>x16</cell><cell>x32</cell></row><row><cell>Input:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Prediction:</cell></row><row><cell>hazy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>transmission</cell></row><row><cell>image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>map</cell><cell>Dark</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Channel</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Prior</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Loss</cell></row><row><cell></cell><cell>conv 3x3x32</cell><cell></cell><cell cols="2">dilated conv 3x3x32</cell><cell>conv 1x1x1</cell></row><row><cell></cell><cell>batch-norm</cell><cell></cell><cell>batch-norm</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ReLU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 1: Matting Term:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>calc E1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>calc W</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data Term:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>calc E2</cell><cell cols="2">+</cell><cell></cell><cell></cell></row><row><cell>Dark Channel:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>calc airlight</cell><cell>calc dark channel</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.822 15.49/0.781 18.07/0.802 22.30/0.914 19.56/0.863 22.72/0.858 21.34/0.924 21.49/0.838 24.08/0.933 SOTS-indoor 20.15/0.872 16.88/0.791 17.29/0.749 19.05/0.836 17.11/0.805 21.14/0.847 19.38/0.849 22.32/0.880 19.25/0.832</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>AOD-Net [10]</cell><cell>GFN [13]</cell><cell>Ours</cell></row><row><cell>HSTS</cell><cell>17.22/0.798 15.09/0.738 17.62/0.792 21.54/0.867</cell><cell>18.29/0.841</cell><cell>24.49/0.915</cell><cell>21.58/0.922</cell><cell cols="2">22.94/0.874 24.44/0.933</cell></row><row><cell>SOTS-outdoor</cell><cell>17.56/0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Quantitative PSNR/SSIM results of our approach (higher is better). For both SOTS-outdoor and HSTS we report the result of epoch 27, whereas in SOTS-indoor we report the result of epoch 30.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, we report the average runtimes (lower is better) over the 500 images in SOTS-outdoor which feature varying widths of ∼ 500 pixels. Compared to the explicit optimization Qualitative results of single image dehazing on real-world images. The numbers in parenthesis are the number of epochs spent training our network.</figDesc><table><row><cell>Hazy</cell><cell>NLD</cell><cell>CAP</cell><cell>MSCNN Dehaze-Net AOD-Net</cell><cell>GFN</cell><cell>Ours (27) Ours (30)</cell><cell>DCP</cell></row><row><cell>Fig. 4: Original</cell><cell></cell><cell>Hazy</cell><cell>Transmission</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Fig. 5: Example of a synthetic image and its coarse transmission</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">map from RESIDE's ITS training dataset [19].</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Average runtime and performance of SOTS-outdoor</figDesc><table><row><cell></cell><cell>slow-DCP [3]</cell><cell>fast-DCP [34]</cell><cell>ours-CPU</cell><cell>ours-GPU</cell></row><row><cell>PSNR/SSIM</cell><cell>17.56/0.822</cell><cell>14.62/0.752</cell><cell cols="2">24.07/0.933 24.07/0.933</cell></row><row><cell>runtime[sec]</cell><cell>21.67</cell><cell>1.08</cell><cell>1.71</cell><cell>0.67</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Although in the latest published paper of RESIDE, SOTS-outdoor is not officially featured, the selection of 500 specific outdoor images are still available (as well as in earlier Arxiv versions) in RESIDE's website in: https://www. dropbox.com/s/y6jupfvitv0dx5w/SOTS.zip?dl=0&amp;file_subpath=%2FSOTS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Implementation in https://github.com/sjtrny/Dark-Channel-Haze-Removal<ref type="bibr" target="#b2">3</ref> Our results slightly differ from<ref type="bibr" target="#b18">[19]</ref>. We use the original DCP<ref type="bibr" target="#b2">[3]</ref>, whereas they use the faster version<ref type="bibr" target="#b33">[34]</ref> with worse quality.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast visibility restoration from a single color or gray level image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast single image haze removal algorithm using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3522" to="3533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A fast semi-inverse approach to detect and remove the haze from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bekaert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gated fusion network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Strong baseline for single image dehazing with deep features and instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Harbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">BMVC</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Reside: A benchmark for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04143</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Chromatic framework for vision in bad weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Polarization-based vision through haze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied optics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="511" to="525" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep photo: Model-based photograph enhancement and viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian defogging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="278" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An analysis of single image defogging methods using a color ellipsoid framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Color lines: Image specific color representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Omer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vision through the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Middleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geophysik II/Geophysics II</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1957" />
			<biblScope unit="page" from="254" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A closed form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High-accuracy stereo depth maps using structured light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
