<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Background Matting: The World is Your Green Screen</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Jayaram</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Background Matting: The World is Your Green Screen</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Using a handheld smartphone camera, we capture two images of a scene, one with the subject and one without. We employ a deep network with an adversarial loss to recover alpha matte and foreground color. We composite the result onto a novel background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose a method for creating a matte -the per-pixel foreground color and alpha -of a person by taking photos or videos in an everyday setting with a handheld camera. Most existing matting methods require a green screen background or a manually created trimap to produce a good matte. Automatic, trimap-free methods are appearing, but are not of comparable quality. In our trimap free approach, we ask the user to take an additional photo of the background without the subject at the time of capture. This step requires a small amount of foresight but is far less timeconsuming than creating a trimap. We train a deep network with an adversarial loss to predict the matte. We first train a matting network with supervised loss on ground truth data with synthetic composites. To bridge the domain gap to real imagery with no labeling, we train another matting network guided by the first network and by a discriminator that judges the quality of composites. We demonstrate results on a wide variety of photos and videos and show significant improvement over the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Imagine being able to easily create a matte the per-pixel color and alpha of a person by taking photos or videos in an everyday setting with just a handheld smartphone. Today, the best methods for extracting (pulling) a good quality matte require either a green screen studio, or the manual creation of a trimap (foreground/background/unknown seg-mentation), a painstaking process that often requires careful painting around strands of hair. Methods that require neither of these are beginning to appear, but they are not of comparable quality. Instead, we propose taking an additional photo of the (static) background just before or after the subject is in frame, and using this photo to perform background matting. Taking one extra photo in the moment requires a small amount of foresight, but the effort is tiny compared to creating a trimap after the fact. This advantage is even greater for video input. Now, the world is your green screen.</p><p>We focus on a method that is tuned to human subjects. Still, even in this setting pulling the matte of a person given a photo of the background the problem is ill-posed and requires novel solutions.</p><p>Consider the compositing equation for image I given foreground F , background B, and mixing coefficient α:</p><formula xml:id="formula_0">I = αF + (1 − α)B.</formula><p>For color images and scalar α, and given B, we have four unknowns (F and α), but only three observations per pixel (I). Thus, the background matting problem is underconstrained. Background/foreground differences provide a signal, but the signal is poor when parts of the person are similar in color to the background. Furthermore, we do not generally have an image of the ideal background: the subject can cast shadows and cause reflections not seen in the photo taken without the subject, and exact, pixel-level alignment with no resampling artifacts between handheld capture of two photos is generally not attainable. In effect, rather than the true B that produced I, we have some perturbed version of it, B . Finally, we can build on person segmentation algorithms to make the problem more tractable to identify what is semantically the foreground. However current methods, exhibit failures for complex body poses and fine features like hair and fingers.</p><p>Given these challenges and recently published successes in solving matting problems, a deep learning approach is a natural solution. We propose a deep network that estimates the foreground and alpha from input comprised of the original image, the background photo, and an automatically computed soft segmentation of the person in frame. The network can also utilize several frames of video, useful for bursts or performance capture, when available. However, the majority of our results, including all comparisons to single-image methods, do not use any temporal cues.</p><p>We initially train our network on the Adobe Matting dataset <ref type="bibr" target="#b35">[36]</ref>, comprised of ground truth mattes that can be synthetically composited over a variety of backgrounds. In practice, we found the domain gap between these synthetic composites and real-world images did not lead to good results using standard networks. We partially close this gap in two ways: by augmenting the dataset and by devising a new network a Context Switching Block that more effectively selects among the input cues. The resulting mattes for real images can still have significant artifacts, particularly evident when compositing onto a new background. We thus additionally train the network in a self-supervised manner on real unlabelled input images using an adversarial loss to judge newly created composites and ultimately improve the matting process.</p><p>Our method has some limitations. First, we do require two images. Trimap-based methods arguably require two images as well for best results -the trimap itself is a handmade second image -though they can be applied to any input photo. Second, we require a static background and small camera motion; our method would not perform well on backgrounds with people walking through or with a camera that moves far from the background capture position. Finally, our approach is specialized to foregrounds of (one or more) people. That said, person matting without big camera movement in front of a static background is, we argue, a very useful and not uncommon scenario, and we deliver state-of-the-art results under these circumstances.</p><p>Our contributions include: • The first trimap-free automatic matting algorithm that utilizes a casually captured background. • A novel matting architecture (Context Switching Block) to select among input cues. • A selfsupervised adversarial training to improve mattes on real images. • Experimental comparisons to a variety of competing methods on wide range of inputs (handheld, fixedcamera, indoor, outdoor), demonstrating the relative success of our approach. Our code and data is available at http://github.com/senguptaumd/Background-Matting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Matting is a standard technique used in photo editing and visual effects. In an uncontrolled setting, this is known as the "natural image matting" problem; pulling the matte requires solving for seven unknowns per pixel (F, B, α) and is typically solved with the aid of a trimap. In a studio, the subject is photographed in front of a uniformly lit, constant-colored background (e.g., a green screen); reasonable results are attainable if the subject avoids wearing colors that are similar to the background. We take a middle ground in our work: we casually shoot the subject in a natural (non-studio) setting, but include an image of the background without the subject to make the matting problem more tractable. In this section, we discuss related work on natural image matting, captured without unusual hardware.</p><p>Traditional approaches. Traditional (non-learning based) matting approaches generally require a trimap as input. They can be roughly categorized into sampling-based techniques and propagation-based techniques. Samplingbased methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b1">2]</ref> use sampling to build the color statistics of the known foreground and background, and then solve for the matte in the 'unknown' region. Propagation-based approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref> aim to propagate the alpha matte from the foreground and the background region into the 'unknown' region to solve the matting equation. Wang and Cohen <ref type="bibr" target="#b34">[35]</ref> presents a nice survey of many different matting techniques.</p><p>Learning-based approaches.</p><p>Deep learning approaches showed renewed success in natural image matting, especially in presence of user-generated trimaps. Some methods combine learning-based approaches with traditional techniques, e.g., KNN-matting <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b6">7]</ref>. Xu et al. <ref type="bibr" target="#b35">[36]</ref> created a matting dataset with real mattes and composited over a variety of backgrounds and trained a deep network to predict the alpha matte; these results were further improved by Lutz et al. <ref type="bibr" target="#b22">[23]</ref> using an adversarial loss. Recently Tang et al. <ref type="bibr" target="#b31">[32]</ref> proposed a hybrid of a samplingbased approach and learning to predict the alpha matte. Lu et.al <ref type="bibr" target="#b21">[22]</ref> proposed a new index-guided upsampling and unpooling operation that helps the network predict better alpha mattes. Cai et al. <ref type="bibr" target="#b2">[3]</ref> showed robustness to faulty user-defined trimaps. All of these methods only predict the alpha matte and not the foreground, leaving open the (nontrivial) problem of recovering foreground color needed for composites. Recently Hou et al. <ref type="bibr" target="#b15">[16]</ref> introduced Context-Aware Matting (CAM) which simultaneously predicts the alpha and the foreground, thus solving the complete matting problem, but is not robust to faulty trimaps. In contrast to these methods (and the traditional approaches), our work jointly predicts alpha and foreground using an image of the background instead of a trimap.</p><p>Recently, researchers have developed algorithms that perform matting without a trimap, focusing mostly on hu-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Overview of our approach. Given an input image I and background image B , we jointly estimate the alpha matte α and the foreground F using soft segmentation S and motion prior M (for video only). We propose a Context Switching Block that efficiently combines all different cues. We also introduce self-supervised training on unlabelled real data by compositing into novel backgrounds. mans (as we do). Aksoy et.al. <ref type="bibr" target="#b0">[1]</ref> introduced fully automatic semantic soft segmentation for natural images. In <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30]</ref> the authors perform portrait matting without trimap, utilizing segmentation cues. Trimap-free matting has also been extended to handle whole bodies in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b4">5]</ref>. These methods aim to perform trimap prediction, followed by alpha prediction. Our work is also human-focused; we compare our approach with the recent state-of-the-art automatic human matting algorithm <ref type="bibr" target="#b36">[37]</ref> and obtain significantly better performance with the aid of the background image.</p><p>Matting with known natural background. Difference matting proposed by Qian and Sezan <ref type="bibr" target="#b25">[26]</ref> attempts to solve matting with a natural background by simple background subtraction and thresholding but is very sensitive to the threshold and produces binary mattes. Similarly, change detection via background subtraction <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b9">10]</ref> generally does not produce alpha mattes with foreground and considers shadows to be part of the foreground. Some traditional approaches like Bayesian matting <ref type="bibr" target="#b8">[9]</ref> and Poisson matting <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref> can handle known background in their framework, but additionally require trimaps.</p><p>Video Matting. Researchers have also focused on video-specific methods.</p><p>Chuang et.al. <ref type="bibr" target="#b7">[8]</ref> extended Bayesian Matting to videos by utilizing the known background and optical flow, requiring trimaps for keyframes. Flow-based temporal smoothing can be used <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref> (again with trimaps) to encourage temporal coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>The input to our system is an image or video of a person in front of a static, natural background, plus an image of just the background. The imaging process is easy, just requiring the user to step out of the frame after the shot to capture the background, and works with any camera with a setting to lock the exposure and focus (e.g., a smartphone camera). For handheld capture, we assume camera motion is small and align the background to a given input image with a homography. From the input, we also extract a soft segmentation of the subject. For video input, we can additionally utilize nearby frames to aid in matting.</p><p>At the core of our approach is a deep matting network G that extracts foreground color and alpha for a given input frame, augmented with background, soft segmentation, and (optionally nearby video frames), and a discriminator network D that guides the training to generate realistic results. In Section 3.1, we describe the matting network, which contains a novel architecture -a "Context-switching block"that can combine different input cues selectively. We first train a copy of this network G Adobe with supervision using the Adobe Matting Dataset <ref type="bibr" target="#b35">[36]</ref>. We use known foreground and alpha mattes of non-transparent objects, which are then composited over a variety of backgrounds (i.e., real source images, but synthetic composites). Our matting network, along with some data augmentation, help overcome some of the domain gap between the synthetically composited imagery and real data that we later capture with a consumer camera (e.g., a smartphone).</p><p>In Section 3.2, we describe a self-supervised scheme to bridge the domain gap further and to generally improve the matting quality. The method employs an adversarial network comprised of a separate copy of the deep matting network, G Real , that tries to produce a matte similar to the output of G Adobe and a discriminator network D that scores the result of compositing onto a novel background as real or fake. We train G Real and D jointly on real inputs, with supervision provided by (the now fixed) G Adobe network applied to the same data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Supervised Training on the Adobe Dataset</head><p>Here we describe our deep matting network, which we first train on the Adobe Matting Dataset, restricted to the subset of non-transparent objects. The network takes as input an image I with a person in the foreground, an image of the background B registered to I (as noted earlier, B is not the same as the true B with subject present), a soft segmentation of the person S, and (optionally for video) a stack of temporally nearby frames M , and produces as output a foreground image F and alpha matte α.</p><p>To generate S, we apply person segmentation <ref type="bibr" target="#b3">[4]</ref> and then erode (5 steps), dilate (10 steps), and apply a Gaussian blur (σ = 5). When video is available, we set M to be the concatenation of the two frames before and after I, i.e., {I −2T , I −T , I +T , I +2T } for frame interval T ; these images are converted to grayscale to ignore color cues and focus more on motion cues. In the absence of video, we simply set M to {I, I, I, I}, also converted to grayscale. We denote the input set as X ≡ {I, B , S, M }. The network with weight parameters θ thus computes:</p><formula xml:id="formula_1">(F, α) = G(X; θ).<label>(1)</label></formula><p>In designing and training the network, the domain gap between the Adobe dataset and our real data has proven to be a significant driver in our choices as we describe below.</p><p>A natural choice for G would be a residual-block-based encoder-decoder <ref type="bibr" target="#b38">[39]</ref> operating on a concatenation of the inputs {I, B , S, M }. Though we would expect such a network to learn which cues to trust at each pixel when recovering the matte, we found that such a network did not perform well. When training on the Adobe synthetic-composite data and then testing on real data, the resulting network tended to make errors like trusting the background B too much and generating holes whenever F was too close in color; the network was not able to bridge the domain gap.</p><p>Instead, we propose a new Context Switching block (CS block) network ( <ref type="figure">Figure 2</ref>) to combine features more effectively from all cues, conditioned on the input image. When, e.g., a portion of the person matches the background, the network should focus more on segmentation cue in that region. The network has four different encoders for I, B , S, and M that separately produce 256 channels of feature maps for each. It then combines the image features from I with each of B , S and M separately by applying 1x1 convolution, BatchNorm, and ReLU ('Selector' block in <ref type="figure">Fig. 2</ref>), producing 64-channel features for each of the three pairs. Finally, these three 64-channel features are combined with the original 256-channel image features with 1x1 convolution, BatchNorm, and ReLU (the 'Combinator' block in <ref type="figure">Fig. 2</ref>) to produce encoded features which are passed on to the rest of the network, consisting of residual blocks and decoders. We observe that the CS Block architecture helps to generalize from the synthetic-composite Adobe dataset to real data ( <ref type="figure" target="#fig_1">Figure 4</ref>). More network architecture details are provided in the supplementary material.</p><p>We train the network with the Adobe Matting dataset <ref type="bibr" target="#b35">[36]</ref> which provides 450 ground truth foreground image F * and alpha matte α * (manually extracted from natural images). We select the subset of 280 images corresponding to non-transparent objects (omitting, e.g., objects made of glass). As in <ref type="bibr" target="#b35">[36]</ref>, we can compose these foregrounds over known backgrounds drawn from the MS-COCO dataset, augmented with random crops of varying resolutions, re-scalings, and horizontal flips. These known backgrounds B would not be the same as captured backgrounds B in a real setting. Rather than carefully simulate how B and B might differ, we simply perturbed B to avoid training the network to rely too much on its exact values. In particular, we generated each B by randomly applying either a small gamma correction γ ∼ N (1, 0.12) to B or adding gaussian noise η ∼ N (µ ∈ [−7, 7], σ ∈ [2, 6]) around the foreground region. Further, to simulate imperfect segmentation guidance S we threshold the alpha matte and then erode (10-20 steps), dilate (15-30 steps) and blur (σ ∈ [3, 5, 7]) the result. For the motion cue M , we applied random affine transformations to foreground+alpha before compositing onto the background, followed by conversion to grayscale. To compute I and M we used the compositing equation with B as the background, but we provided B as the input background to the network.</p><p>Finally, we train our network G Adobe ≡ G(·; θ Adobe ) on the Adobe dataset with supervised loss: min</p><formula xml:id="formula_2">θ Adobe E X∼p X [ α − α * 1 + ∇(α) − ∇(α * ) 1 + 2 F − F * 1 + I − αF − (1 − α)B 1 ],<label>(2)</label></formula><p>where (F, α) = G(X; θ Adobe ), and the gradient term on α encourages sharper alpha mattes <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adversarial Training on Unlabelled Real data</head><p>Although our proposed Context Switch block (CS block) combined with data augmentation significantly helps in bridging the gap between real images and synthetic composites created with the Adobe dataset, it still fails to handle all difficulties present in real data. Theses difficulties include (1) traces of background around fingers, arms, and hairs being copied into the matte; (2) segmentation failing;</p><p>(3) significant parts of the foreground color matching the background color; (4) misalignment between the image and the background (we assume only small misalignment). To handle these cases, we aim to learn from unlabelled, real data (real images + backgrounds) with self-supervision.</p><p>The key insight is that significant errors in the estimated matte typically result in unrealistic composites over novel backgrounds. For example, a bad matte might contain a chunk of the source background, which, when composited over a new background, will have a piece of the original background copied over the new background, a major visual artifact. Thus, we can train an adversarial discriminator to distinguish between fake composites and (already captured) real images to improve the matting network.</p><p>The matting network (G Real ≡ G(·; θ Real )) and discriminator network D can be trained end-to-end based on just a standard discriminator loss. However, G Real could settle on setting α = 1 everywhere, which would result in simply copying the entire input image into the composite passed to D. This solution is "optimal" for G Real , since the input image is indeed real and should fool D. Initializing with G Adobe and fine-tuning with a low learning rate (was necessary for stable training with a discriminator) is not very effective. It does not allow significant changes to network weights needed to generate good mattes on real data.</p><p>Instead, we use G Adobe for teacher-student learning. In particular, for a real training image I and associated inputs comprising X, we obtain (F ,α) = G(X; θ Adobe ) to serve as "pseudo ground-truth". We can now train with an adversarial loss and a loss on the output of the matting network G(X; θ Real ) when compared to "pseudo ground-truth", following <ref type="bibr" target="#b26">[27]</ref>; this second loss is given small weight which is reduced between epochs during training. Though we initialize θ Real in the standard randomized way, the network is still encouraged to stay similar to the behavior of G Adobe while having the flexibility to make significant changes that improve the quality of the mattes. We hypothesize that this formulation helps the network to avoid getting stuck in the local minimum of G Adobe , instead finding a better minimum nearby for real data.</p><p>We use the LS-GAN <ref type="bibr" target="#b23">[24]</ref> framework to train our generator G Real and discriminator D. For the generator update we minimize:</p><formula xml:id="formula_3">min θ Real E X,B∼p X,B [(D(αF + (1 − α)B) − 1) 2 + λ{2 α −α 1 + 4 ∇(α) − ∇(α) 1 + F −F 1 + I − αF − (1 − α)B 1 }],<label>(3)</label></formula><p>where (F, α) = G(X; θ Real ),B is a given background for generating a composite seen by D, and we set λ to 0.05 and reduce by 1/2 every two epochs during training to allow the discriminator to play a significant role. We use a higher weight on the alpha losses (relative to Equation 2), especially the gradient term to encourage sharpness.</p><p>For the discriminator, we minimize:</p><formula xml:id="formula_4">min θDisc E X,B∼p X,B [(D(αF + (1 − α)B)) 2 ] + E I∈p data [(D(I) − 1) 2 ],<label>(4)</label></formula><p>where θ Disc represents the weights of the discriminator network and again (F, α) = G(X; θ Real ).</p><p>As a post-process, we threshold the matte at α &gt; 0.05, extract the largest N connected components, and set α = 0 for pixels not in those components, where N is the number of disjoint person segmentations in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>We compared our approach with a variety of alternative methods, esp. recent deep matting algorithms that have performed well on benchmarks: BM: Bayesian Matting [9]traditional, trimap-based method that can accept a known background <ref type="bibr" target="#b7">[8]</ref>. (An alternative, Poisson Matting <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref> with known background, performed much worse. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on Synthetic-Composite Adobe Dataset</head><p>We train G Adobe on 26.9k exemplars: 269 objects composited over 100 random backgrounds, plus perturbed versions of the backgrounds as input to the network. We train with batch-size 4, learning rate 1e −4 with Adam optimizer.</p><p>We compare results across 220 synthetic composites from the Adobe Dataset <ref type="bibr" target="#b35">[36]</ref>: 11 held-out mattes of human subjects composed over 20 random backgrounds, in <ref type="table">Table 1</ref>. We computed a trimap for each matte through a process of alpha matte thresholding and dilation as described in <ref type="bibr" target="#b35">[36]</ref>. We dilated by 10 and 20 steps to generate two different trimaps (more steps gives wider unknown region). We additionally computed a perturbed background B by applying small random affine transformation (translate ∈ N (0, 3), rotate ∈ N (0, 1.3 • ) and small scaling and shear) followed by gamma correction γ ∼ N (1, 0.12) and gaussian noise η ∼ N (µ ∈ [−5, 5], σ ∈ <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>). For our approach, we only evaluated the result of applying the G Adobe network ('Ours-Adobe'), since it was trained only on the Adobe data, as were the other learning-based approaches we compare to. We rescaled all images to 512 × 512 and measure the SAD and MSE error between the estimated and ground truth (GT) alpha mattes, supplying algorithms with the two different trimaps and with backgrounds B and B as needed. We omitted LFM from this comparison, as the released model was trained on additional training data, along with the training set of Adobe dataset. That said, it produces a SAD and MSE of 2.00, 1.08e −2 , resp., while our method achieves error of 1.72, 0.97e −2 .</p><p>We observe that our approach is more robust to background perturbation when compared to BM, and it improves on all other trimap-based matting algorithms (BM, CAM, IM). As trimaps get tighter, the trimap-based matting algorithms get better, but tight trimaps are time-consuming to create in practice. The goal of our work is to fully eliminate the need for manually created trimaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on Real Data</head><p>We captured a mix of handheld and fixed-camera videos, taken indoors and outside using a smartphone (iPhone 8). The fixed-camera setup consisted of an inexpensive selfie stick tripod. In each case, we took a video with the subject moving around, plus a shot of the background (single video frame) with no subject. All frames were captured in HD (1920×1080), after which they were cropped to 512×512 (input resolution to our network) around the segmentation mask for one person or multiple. We retrain G Adobe on 280k composites consisting of 280 objects from Adobe Dataset <ref type="bibr" target="#b35">[36]</ref>. We then train separate copies of G Real , one each on handheld videos and fixed camera videos, to allow the networks to focus better on the input style. For handheld videos we account for small camera shake by aligning the captured background to individuals frames through homography. In total, we trained on 18k frames for hand-held camera and 19k frames for fixed camera. We captured 3390 additional background frames forB. We use a batch-size of 8, learning rate of 1e −4 for G Real and 1e −5 for D and update D with Adam optimizer. We also update the weights of D after 5 successive updates of G Real .   <ref type="table">Table 3</ref>: User study on 10 real world videos (handheld). To compare algorithms on real data, we used 10 handheld videos and 10 fixed-camera videos as our (held-out) test data. The BM, CAM, and IM methods each require trimaps. We did not manually create trimaps (esp. for video sequences which is infeasible). Instead, we applied segmentation <ref type="bibr" target="#b3">[4]</ref>, and labeled each pixel with person-class probability &gt; 0.95 as foreground, &lt; 0.05 as background, and the rest as unknown. We tried alternative methods, including background subtraction, but they did not work as well.</p><p>To evaluate results, we could not compare numerically to ground truth mattes, as none were available for our data. Instead, we composited the mattes over a green background and performed a user study on the resulting videos. Since IM and LFM do not estimate F (needed for compositing), we set F = I for these methods. We also tried estimating F directly from the matting equation (given α and B ), but the results were worse (see supplementary material). We do not use any temporal information and set M = {I, I, I, I} for all comparisons to prior methods.  In the user study, we compared the composite videos produced by G Real network ('Ours-Real') head-to-head with each of the competing algorithms. Each user was presented with a web page showing the original video, our composite, and a competing composite; the order of the last two was random. The user was then asked to rate composite A relative to B on a scale of 1-5 (1 being 'much worse', 5 'much better'). Each video pair was rated ∼ 10 users.</p><p>The results of the user study, with scores aggregated over all test videos, are shown in <ref type="table" target="#tab_3">Tables 2 and 3</ref>. Overall, our method significantly outperformed the alternatives. The gains of our method are somewhat higher for fixed-camera results; with handheld results, registration errors can still lead to matting errors due to, e.g., parallax in non-planar background scenes (see <ref type="figure" target="#fig_3">Fig 6(f)</ref>).</p><p>Single image results are shown in <ref type="figure" target="#fig_3">Figure 6</ref>, again demonstrating improvement of our method over alternatives. We note that LFM in particular has difficulty zeroing in on the person. More results generated by our approach with handheld camera in natural backgrounds are shown in <ref type="figure" target="#fig_0">Figure 3</ref>. In (c), (d) we show examples of multiple people interacting in a single image, and in (e) we show a failure case with a dynamic background, the fountain. Please see supplementary material for video results and more image results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Studies</head><p>Role of motion cues. As shown in <ref type="figure" target="#fig_2">Figure 5</ref>, video motion cues M can help in predicting a cleaner matte when foreground color matches the background. (Note: we did not use motion cues when comparing to other methods, regardless of input source.) much better better similar worse much worse  'Ours-Real' vs 'Ours-Adobe'. As expected, 'Ours-Adobe' outperformed 'Ours-Real' on the syntheticcomposite Adobe dataset on which 'Ours-Adobe' was trained. 'Ours-Real' achieved a SAD score of 3.50 in comparison to 1.73 of 'Ours-Adobe'. However 'Ours-Real' significantly outperformed 'Ours-Adobe' on real data as shown by qualitative examples in <ref type="figure" target="#fig_3">Figure 6</ref> and by an additional user study ( <ref type="table" target="#tab_5">Table 4</ref>). The gain of 'Ours-Real' in the user study (∼ 10 users per pair-wise comparison) was larger for handheld captures; we suspect this is because it was trained with examples having alignment errors. (We did try training 'Ours-Adobe' with alignment errors introduced into B but found the results degraded overall.)</p><p>Role of Context Switching Block (CS Block). We compare our CS Block architecture to a standard residualblock-based encoder-decoder <ref type="bibr" target="#b38">[39]</ref> scheme that was run on a naive concatenation of I, B , S, and M . We find that the concatenation-based network learns to focus too much on color difference between I and B and generates holes when their colors are similar. The CS Block architecture effectively utilizes both segmentation and color difference cues, along with motion cues when present, to produce better matte, as shown in <ref type="figure" target="#fig_1">Figure 4 (more in supplementary)</ref>. Empirically, we observe that the CS block helps significantly in 9 out of 50 real videos, especially when foreground color is similar to the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a background matting technique that enables casual capture of high quality foreground+alpha mattes in natural settings. Our method requires the photographer to take a shot with a (human) subject and without, not moving much between shots. This approach avoids using a green screen or painstakingly constructing a detailed trimap as typically needed for high matting quality. A key challenge is the absence of real ground truth data for the background matting problem. We have developed a deep learning framework trained on synthetic-composite data and then adapted to real data using an adversarial network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>We provide additional details and results in this Appendix. In Sec. B we describe the details of our network architecture. In Sec. C.1 we clarify our choice of automatic trimap generation, especially to show why background subtraction is not good enough for this problem. In Sec. C.2, we show why algorithms that do not predict foreground F introduce additional artifacts in compositing. In Sec. D, we provide additional qualitative examples for ablation studies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architectures</head><p>Our proposed matting network G Adobe , shown again for reference in <ref type="figure">Figure 2</ref> (same as <ref type="figure">Figure 2</ref> in the main paper) consists of the Context Switching Block (CS Block) followed by residual blocks (ResBLKs) and decoders to predict alpha matte α and foreground layer F . Below we describe the network architecture in details. Most of our Generator architecture, especially residual blocks and decoders, and Discriminator architecture are based on that of <ref type="bibr" target="#b38">[39]</ref>. Generator G Adobe 'Image Encoder' and 'Prior Encoder' (CS Block): C64(k7) -C*128(k3) -C*256(k3) 'CN(kS)' denotes convolution layers with N S × S filters with stride 1, followed by Batch Normalization and ReLU. 'C*N(kS)' denotes convolution layers with N S × S filters with stride 2, followed by Batch Normalization and ReLU. The output of 'Image Encoder' layer produces a blob of spatial resolution 256 × 128 × 128. All convolution layers do not have any bias term. 'Selector' (CS Block): C64(k1) 'CN(kS)' denotes convolution layers with N S × S filters with stride 1, followed by Batch Normalization and ReLU. The 'Selector' block takes as input the concatenation of image feature and a prior feature as a blob of spatial resolution 512 × 128 × 128. The output of the 'Selector' network is a blob of spatial resolution 64 × 128 × 128. The goal of the 'Selector' block is to generate prior features conditioned on the image. This will help the network to generalize from synthetic-composite dataset (on which it was trained) to real images by not over-relying on one kind of features (e.g. color difference with background). 'Combinator' (CS Block): C256(k1) 'CN(kS)' denotes convolution layers with N S × S filters with stride 1, followed by Batch Normalization and ReLU. The 'Combinator' block takes as input the concatenation of image features of spatial resolution 256 × 128 × 128, along with 3 other prior features from the 'Selector' network of spatial resolution 64×128×128 each. Thus the input to the 'Combinator' block is of spatial resolution 448 × 128 × 128 and the output is of spatial resolution 256 × 128 × 128. The 'Combinator' block learns to combine the individual priors features with the original image feature for the goal of improving matting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>'ResBLKs': K ResBLK</head><p>The output of the combinator is first passed through K = 7 'ResBLK's and then provided as input to 2 separate 'ResBLK's of K = 3 for alpha matte and foreground layer separately. All 'ResBLK's operate at a spatial resolution of 256 × 128 × 128. Each 'ResBLK' consists of Conv256(k3) -BN -ReLU -Conv256(k3) -BN, where 'BN' denote Batch Normalization.</p><p>'Decoder' for alpha matte α: CU*128(k3)-CU*64(k3)-Co1(k7)-Tanh The input to the 'Decoder' is of resolution 256 × 128 × 128.'CU*N(kS)' denotes bilinear upsampling by factor of 2, followed by convolution layer with N S × S filters with stride 1, Batch Normalization and ReLU. The last layer Co3k <ref type="bibr" target="#b6">(7)</ref> consists of only convolution layers of 1 7 × 7 filters, followed by Tanh layer. This scales the output alpha between (−1, 1).</p><p>'Decoder' for foreground F : CU*128(k3)-CU*64(k3)-Co1(k7) The input to the 'Decoder' is of resolution 256 × 128 × 128.'CU*N(kS)' denotes bilinear upsampling by factor of 2, followed by convolution layer with N S × S filters with stride 1, Batch Normalization and ReLU. There is also a skip connection from the image input features of resolution 128 × 256 × 256 which is combined with the output of CU*128(k3) and passed on to CU*64(k3). The last layer Co3k <ref type="bibr" target="#b6">(7)</ref> consists of only convolution layers of 1 7 × 7 filters.</p><p>Discriminator D: C*64(k4) -C*I128(k4) -C*I256(k4) -C*I512(k4) We use 70 70 PatchGAN <ref type="bibr" target="#b16">[17]</ref>. 'C*N(kS)' denotes convolution layers with N S × S filters with stride 2, followed by leaky ReLUs of slope 0.2. 'I' denotes the presense of Instance Norm before leaky ReLU, in all layers except the first one. After the last layer a convolution filter of kernel 4 × 4 is applied to produce a 1 dimensional output. The PatchGAN is applied over the whole composite image by convolving with every 70 × 70 patch to determine if it is real or fake. <ref type="figure">Figure 7</ref>: Choice of Foreground layer. For baseline algorithms, IM and LFM, that do not predict the foreground layer F , we observe that F = I produces less visible artifacts compared to predicting F from the matting equation using the captured background B . Notice how some of the brick texture creeps into the foreground when solving for F with the matting equation. We also show that our approach, which jointly estimates F and α, produces less artifacts in compositing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Automatic Trimap Generation</head><p>We compare our method with algorithms that require user defined trimaps (CAM, IM, BM). It is extremely time consuming to annotate trimaps for every frame of a video, or even for a bunch of keyframes whose trimaps then need to be propagated to the remaining frames and then touched up. As described in the paper, we instead created trimaps automatically by applying segmentation <ref type="bibr" target="#b3">[4]</ref>, and labelling each pixel with person-class probability &gt; 0.95 as foreground, &lt; 0.05 as background, and the rest as unknown. We tried, and rejected, alternative methods, including background subtraction and erosion-dilateion of the segmentation mask, which we now describe and illustrate here for completeness.</p><p>Background subtraction is popularly used for change de-tection, but is extremely sensitive to color differences and produces only a binary matte. Thus it is not in itself a suitable candidate for matting. However background subtraction could in principle be used to generate a trimap. In our experiments, we observed that the best thresholds varied from image to image and even then produced mediocre results (see hand-tuned example in <ref type="figure" target="#fig_5">Figure 8</ref>). We also tried erosion-dilation of the segmentation mask (erode 5 steps, dilate 15 steps) to try to produce a fixed width 'unknown' band but we often ended up with mattes that pulled in the background as a part of the foreground. <ref type="figure" target="#fig_5">Figure 8</ref> shows examples of trimaps and resulting alpha mattes using erosiondilation, hand-tuned thresholding of background subtraction, and our probability-thresholded 'Automatic Trimap' method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Predicting Foreground layer F</head><p>To produce composites -the primary reason for extracting a matte in the first place -we require both α and F . Since IM and LFM do not estimate F , we are still left with the task of estimating it ourselves. Why is this non-trivial? From the matting equation (I = αF + (1 − α)B) after estimating only α, we can say that observed pixel color I must be α of the way along a line segment from B to F . Clearly, there is an infinite family of B's and F 's that satisfy this constraint; thus, given just I and α, we cannot readily infer F . Our seemingly naive solution is to set F = I for these methods. We also tried estimating F directly from the matting equation given B -i.e., F = (I − (1 − α)B )/α when α = 0, with F clamped so that each color channel is in [0, 1] -but the results were worse than F = I, largely due to discrepancies between B and B, particularly in the handheld camera case, where small misalignments can arise. We show a comparison of these two options for IM matting, plus a comparison to our result, in <ref type="figure">Figure 7</ref>. The figure shows that matting-equation based estimation of F pulls some of the background texture into the matte. The F = I solution is better, but picks up some of the background colors (a bit of green and yellow in this case), since it is just copying foreground-background mixed pixels and blending them over another background. Our method, which estimates F directly, shows fewer artifacts. Our matte is a bit softer, but it captures structure like the curls on the top of the head; further, some of the apparent sharpness of the other composites comes from copying over too much of the original image rather (which has detail) rather than fully separating F from the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Results on Real Data</head><p>After conducting the user study, we realized that we had given a slight advantage to our method by retaining only the largest α &gt; 0 connected component for our background matting approach but not for the competing approaches. We noted that the CAM and IM methods had small "floaters" after matting, so applied the connected component removal to these videos and re-ran the study. We did not observe that BM had floaters in our examples; any that appeared to remain, were actually connected to the largest component by "bridges" of small α. LFM was more problematic. We found that LFM would at times pull in pieces of the background that were larger than the foreground person; the result of retaining the largest connected component would then mean losing the foreground subject altogether, an extremely objectionable artifact. Rather than continuing to refine the post-process for LFM, we simply did not apply a post-process for its results. As seen in the videos, LFM, in any case, had quite a few other artifacts that made it not competitive with the others. <ref type="table" target="#tab_7">Table 5</ref> and 6 shows the result of the updated user study. We observe that the results are similar to the ones reported in the main paper (i.e., Tables 2 and 3 in the main paper); the excess connected components for CAM and IM did not have a significant impact relative to other errors in matte estimation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>In this section, we provide additional details and more results for the ablation studies already presented in the main paper. Specifically, we analyze (i) Role of Context Switching Block (ii) Role of motion cues and (iii) Compare 'Ours-Real' to 'Ours-Adobe'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Role of Context Switching Block</head><p>Here we go in more depth on the paper's CS block ablation study, again showing that the CS Block network is largely effective in utilizing all cues and in generalizing better from the synthetic-composite Adobe dataset <ref type="bibr" target="#b35">[36]</ref> to real world images. To this end we train G Adobe with CS Block on the Adobe dataset which we term as 'Ours-Adobe' (with Context Switching). Additionally, we construct another network G concat (No Context Switching), where we remove the CS block. The input to this network is the concatenation of the input image, background, soft segmentation, and motion cues {I, B , S, M } , which is passes through the 'Image Encoder' architecture to produce a 256 × 128 × 128 dimensional feature. Since there is no CS Block, we directly pass this feature to the ResBLKs and then continue through the same architecture presented in <ref type="figure">Figure 2</ref>. We also train this network on the Adobe dataset, following the exact protocol of training 'Ours-Adobe'.</p><p>We then test both G Adobe and G concat on our real video dataset. Note that for this experiment we use the motion cue M = {I −2T , I −T , I +T , I +2T } for both G Adobe and G concat . We captured videos at 60fps and set T = 20 frames.</p><p>In <ref type="figure" target="#fig_6">Figure 9</ref> of this document and in <ref type="figure" target="#fig_1">Figure 4</ref> of the main paper, we show multiple examples from different videos where a part of the foreground person matches the background color. G concat ('No Context Switching') fails in these situations, this is because while training on the synthetic-composite dataset it learns to focus too much on the color differences to perform matting and fails when colors coincide. On the other hand G Adobe ('With Context Switching') handles color coincidences better by utilizing the soft segmentation cues provided in the input. Thus the CS Block learns to properly utilize the cues at hand, when compared to 'No Context Switching', which focuses more on the color differences to produce a matte. Note that, the holes produced by 'No Context Switching' as shown in Figure 9 appears in multiple frames of that video where the color coincides significantly; we show only 1 sample from all of these frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Role of Motion Cues</head><p>Here we provide more detail and examples on the paper's ablation study for motion cues. When the input is video, we have the option of setting the motion cue to M = {I −2T , I −T , I +T , I +2T }, where T=20 for a 60fps video. We train another network G still , by removing the motion cue input and its related 'Prior Encoder' and 'Selector' from the architecture presented in <ref type="figure">Figure 2</ref>. Thus G still is the same as G Adobe , but without the motion cue block. We train both G Adobe and G still on the synthetic-composite Adobe dataset, following the same training protocol as described in the paper, for both networks. We then test both G Adobe and G concat on our real video dataset.</p><p>In <ref type="figure" target="#fig_7">Figure 10</ref> of this document and in <ref type="figure" target="#fig_2">Figure 5</ref> of the main paper, we show the role motion cue plays in improving the alpha matte estimation. Specifically, the motion cue helps when foreground color matches the background, and when the foreground is moving significantly. Due to the foreground motion, the network can utilize additional frames (4 in this case) to determine that the regions which move are more likely to be foreground than the background, even though the color matches with the background. Note that, this may not be always true, e.g. a shadow cast on the background also moves with the foreground. Small camera motion with a handheld camera can also effectively cause motion in the background due to misregistration. Additionally, since we consider only a small time window of 1.33 secs for the motion cue, often there is lack of information as the foreground appears to be almost static during that time.</p><p>To reiterate: whenever comparing to competing methods, we set the motion cue to M = {I, I, I, I} and treat all images (including video frames) independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. 'Ours-Real' vs 'Ours-Adobe'</head><p>Here we show more comparisons between using just the GAdobe network for matting ('Ours-Adobe') and using the full network G Real guided by G Adobe and discriminator D ('Ours-Real'). In the main paper, we present an ablation study comparing 'Ours-Real' with 'Ours-Adobe' as both a user study in <ref type="table">Table 3</ref> and with qualitatively comparisons in <ref type="figure" target="#fig_3">Figure 6</ref>. Additional visual comparison on our test videos are presented in our project webpage. In <ref type="figure" target="#fig_8">Figure 11</ref> of this document, we provide additional qualitative comparison between 'Ours-Real' and 'Ours-Adobe'. We find that 'Ours-Real' is generally better though on occasion it is not; (k) and (l) are instances where 'Ours-Real' produces an inferior matte compared to 'Ours-Adobe'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Role of background and segmentation</head><p>The captured background image without the subject and the estimated soft segmentation map are two key additional inputs that helps in estimating the foreground and alpha matte. We found that omitting the background image from the baseline G Adobe model degrades results substantially: SAD error of 8.33 without background vs. 1.73 with background on synthetic-composite Adobe dataset. For backgrounds that are relatively distant or roughly planar, homography-based alignment is accurate, and the network learns to handle remaining mis-registrations by training on hand-held videos. Alignment fails when the background, e.g., has two planes <ref type="figure" target="#fig_1">(Fig 4f,</ref> with two orthogonal walls).</p><p>Soft segmentation is obtained by eroding and dilating the segmentation predicted by Deeplabv3+. We observe that eroding and dilating the segmentation by 20 steps only increase the SAD error from 1.73 to 1.76 on syntheticcomposite Adobe dataset. Hence our method is quite robust to errors in segmentation, and soft segmentation only indicates which is the foreground subject in the image. In contrast, the captured background plays more crucial role in the performance of the method. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>(a-e) Resulted alphas and foregrounds for photos taken with handheld camera against natural backgrounds; (e) is an example failure case with dynamic background (fountain). See video results in the supplementary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Role of Context Switching Block (CS Block).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Role of motion cues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of matting methods with camera fixed (a,b,c) and handheld (d,e,f). Our method fails in (f) due to misregistration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Specifically, we show the role of Content Switching Block in Sec. D.1, motion cue in Sec. D.2, self-supervised adversarial training on real data in Sec. D.3 and robustness w.r.t. segmentation and background in Sec. D.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Automatic Trimap generation. Our choice of automatic trimap generation from the probability estimates of the segmentation network performs better than background subtraction or erosion-dilation of the segmentation mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Role of CS Block. When foreground color coincides with the background color, Context Switching Block utilizes soft segmentation to predict the correct matte. 'No Context Switching' produces holes when foreground color matches strongly with the background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Role of Motion Cues. Motion cue helps in predicting better matte when foreground color coincides with the background and foreground moves in front of the background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Ours-Real vs Ours-Adobe. 'Ours-Real' is trained on real data guided by 'Ours-Adobe' (trained on synthetic-composite dataset) along with an adversarial loss. (k) and (l) are instances where 'Ours-Real' produces worse result compared to 'Ours-Adobe'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm Additional Inputs SAD MSE(10 −2 )</figDesc><table><row><cell>BM</cell><cell cols="2">Trimap-10, B 2.53</cell><cell>1.33</cell></row><row><cell>BM</cell><cell cols="2">Trimap-20, B 2.86</cell><cell>1.13</cell></row><row><cell>BM</cell><cell cols="2">Trimap-20, B 4.02</cell><cell>2.26</cell></row><row><cell>CAM</cell><cell>Trimap-10</cell><cell>3.67</cell><cell>4.50</cell></row><row><cell>CAM</cell><cell>Trimap-20</cell><cell>4.72</cell><cell>4.49</cell></row><row><cell>IM</cell><cell>Trimap-10</cell><cell>1.92</cell><cell>1.16</cell></row><row><cell>IM</cell><cell>Trimap-20</cell><cell>2.36</cell><cell>1.10</cell></row><row><cell>Ours-Adobe</cell><cell>B</cell><cell>1.72</cell><cell>0.97</cell></row><row><cell>Ours-Adobe</cell><cell>B</cell><cell>1.73</cell><cell>0.99</cell></row></table><note>Table 1: Alpha matte error on Adobe Dataset (lower is better).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). CAM: Context-Aware Matting [16] -trimap-based deep matting technique that predicts both alpha and foreground. IM: Index Matting [22] -trimap-based deep matting technique that predicts only alpha. LFM: Late Fusion Matting [37]trimap-free deep matting algorithm that predicts only alpha.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>User study on 10 real world videos (fixed camera).</figDesc><table><row><cell cols="4">Ours vs. much better better similar worse much worse</cell></row><row><cell>BM</cell><cell>61.0%</cell><cell>31.0% 3.0% 4.0%</cell><cell>1.0%</cell></row><row><cell>CAM</cell><cell>43.3%</cell><cell>37.5% 5.0% 4.2%</cell><cell>10.0%</cell></row><row><cell>IM</cell><cell>33.3%</cell><cell>47.5% 5.9% 7.5%</cell><cell>5.8%</cell></row><row><cell>LFM</cell><cell>65.7%</cell><cell>27.1% 4.3% 0%</cell><cell>2.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>User Study: Ours-Real vs Ours-Adobe.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>User study on 10 real world videos (fixed camera).</figDesc><table><row><cell cols="4">Ours vs. much better better similar worse much worse</cell></row><row><cell>BM</cell><cell>61.0%</cell><cell>31.0% 3.0% 4.0%</cell><cell>1.0%</cell></row><row><cell>CAM</cell><cell>45.0%</cell><cell>35.0% 5.0% 5.0%</cell><cell>10.0%</cell></row><row><cell>IM</cell><cell>34.2%</cell><cell>46.6% 6.7% 2.5%</cell><cell>10.0%</cell></row><row><cell>LFM</cell><cell>65.7%</cell><cell>27.1% 4.3% 0%</cell><cell>2.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>User study on 10 real world videos (handheld).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by NSF/Intel Visual and Experimental Computing Award #1538618 and the UW Reality Lab.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic soft segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Designing effective inter-pixel information flow for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tunc Ozan Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Disentangled image matting. International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic human matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiezheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knn matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2175" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inso</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="626" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video matting of complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2002" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonparametric model for background subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="751" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shared sampling for real-time alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel M</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Near-real-time image matting with known background</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Hong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Conference on Computer and Robot Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random walks for interactive alpha-matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schiwietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aharon</forename><surname>Shmuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rüdiger</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VIIP</title>
		<meeting>VIIP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast matting using large kernel matting laplacian matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2165" to="2172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context-aware image matting for simultaneous foreground and alpha estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonlocal matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2193" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporally coherent video matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Chul</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In-Kwon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphical Models</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="228" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<title level="m">Spectral matting. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1699" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Indices matter: Learning to index for deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Amplianitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Smolic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alphagan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10088</idno>
		<title level="m">Generative adversarial networks for natural image matting</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Background subtraction techniques: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Piccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3099" to="3104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video background replacement without a blue screen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M Ibrahim</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sezan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)</title>
		<meeting>1999 International Conference on Image Processing (Cat. 99CH36348)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="143" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sfsnet: Learning shape, refectance and illuminance of faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Regognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporally coherent and spatially accurate video matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Shahrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepu</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="381" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving image matting using comprehensive sampling sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Shahrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepu</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="636" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Poisson matting. In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2003" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning-based sampling for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tunc Ozan</forename><surname>Aydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An iterative optimization approach for unified image segmentation and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="936" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimized color sampling for robust matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Image and video matting: a survey. Foundations and Trends® in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="97" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A late fusion cnn for digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast deep matting for portrait animation on mobile phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
