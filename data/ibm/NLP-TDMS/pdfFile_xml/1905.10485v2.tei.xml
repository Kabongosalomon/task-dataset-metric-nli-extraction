<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Latent Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
							<email>zxiao@uchicago.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yan</surname></persName>
							<email>yanq@uchicago.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Amit</surname></persName>
							<email>amit@marx.uchicago.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Chicago Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chicago Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Latent Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose the Generative Latent Flow (GLF), an algorithm for generative modeling of the data distribution. GLF uses an Auto-encoder (AE) to learn latent representations of the data, and a normalizing flow to map the distribution of the latent variables to that of simple i.i.d noise. In contrast to some other Auto-encoder based generative models, which use various regularizers that encourage the encoded latent distribution to match the prior distribution, our model explicitly constructs a mapping between these two distributions, leading to better density matching while avoiding over regularizing the latent variables. We compare our model with several related techniques, and show that it has many relative advantages including fast convergence, single stage training and minimal reconstruction trade-off. We also study the relationship between our model and its stochastic counterpart, and show that our model can be viewed as a vanishing noise limit of VAEs with flow prior. Quantitatively, under standardized evaluations, our method achieves state-of-the-art sample quality among AE based models on commonly used datasets, and is competitive with GANs' benchmarks. * equal contribution Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep generative models have recently attracted much attention in the deep learning literature. These models are used to formulate the distribution of complex data as a function of random noise passed through a network, so that rendering samples from the distribution is particularly easy. Deep generative models can be roughly classified into explicit and implicit models. The former class assumes explicit parametric specification of the distribution, whereas the latter does not. Implicit models are dominated by Generative Adversarial Networks (GANs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>. GANs have exhibited impressive performance in generating high quality images <ref type="bibr" target="#b12">[13]</ref> and other vision tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Despite their successes, training GANs can be challenging, partly because they are trained by solving a saddle point optimization problem formulated as an adversarial game. It is well known that training GANs is unstable and extremely sensitive to hyper-parameter settings <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b55">56]</ref>, and sometimes training leads to mode collapse <ref type="bibr" target="#b16">[17]</ref>, where most of the samples share some common properties. Although there have been multiple efforts to overcome the difficulties in training GANs, by modifying the objective functions or introducing normalization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, researchers are also actively studying non-adversarial methods that are known to be less affected by these issues.</p><p>Some explicit methods directly model p(x), the distribution of data, and training is guided by maximizing the data likelihood. For example, auto-regressive models <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, which assume the data distribution can be expressed in an auto-regressive pattern, have a simple and stable training process, and currently give the best likelihood results; however, they cannot provide low-dimensional representations of images, and their sampling procedure is inefficient. Normalizing flows <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref> model p(x) as an invertible transformation from a simple distribution through a change of variables. While being mathematically clear, normalizing flows have one major drawback: computational complexity. Flows have to keep the dimensionality of the original data in order to maintain bijectivity, and this makes training computationally expensive. Considering the prohibitively long training time and advanced hardware requirements in training large scale flow models such as <ref type="bibr" target="#b11">[12]</ref>, we believe that it is worth exploring the application of flows in the low dimensional representation spaces rather than in the original data.</p><p>Other explicit generative models often adopt low dimensional latent representations, which are usually obtained from auto-encoders, and generate samples by decoding z's sampled from a pre-defined prior distribution p(z). We call this type of method AE based models. Variational Auto-encoders (VAEs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> are perhaps the most influential AE based models. VAEs are trained to minimize a variational bound of the data log likelihood, which is composed of the reconstruction loss plus the KL divergence between q(z|x), the approximate posterior distribution returned by the probabilistic encoder, and the prior p(z). AE based models are easy to train, and they provide low dimensional codes for the data, but unfortunately, their generation quality still lies far below that of GANs, especially on large datasets. For example, it is observed that VAEs tend to generate blurry images, an effect that is usually attributed to the failure to match the marginal distribution in the latent space <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b45">46]</ref>. Some modifications to VAEs <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b30">31]</ref> improve the estimated test data likelihood. However, it is known that higher likelihood is not directly related to better sample quality <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b56">57]</ref>. Some other modifications have recently been proposed to better match the distribution of latent variables and the prior distribution <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7]</ref>, and they are shown to have the potential to generate high quality samples. These are discussed in greater detail in Section 2.</p><p>Our work pursues the same goal of improving the generation quality of AE based models. To this end, we propose Generative Latent Flow (GLF), which uses a deterministic auto-encoder to learn a mapping to and from a latent space, and a normalizing flow that serves as an invertible transformation between the latent space distribution and a simple noise distribution. Our contributions are summarized as follows: i) we propose Generative Latent Flow, which is an AE based generative model that can generate high quality samples. ii) through standardized evaluations, we show that our model achieves state-of-the-art sample quality among competing models, and can match the benchmarks of GANs. Moreover it has the advantage of one stage training and faster convergence. iii) we carefully study some variants of our method and show its relationship to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation and related works</head><p>Consider an AE based generative model that can generate samples from the data space X . Ideally, the auto-encoder defines a low dimensional latent space Z, where each image x i ∈ X is associated with a latent vector z i ∈ Z through the decoder x i = G(z i ). The marginal distribution over Z, denoted bỹ p(z), is unknown and possibly complicated, and depends on the encoder E. In some cases, the model also has a predefined prior distribution p(z) on Z. Since samples are generated by sampling ε ∼ p(ε) from the prior and feeding ε to the decoder, in order to generate high quality samples, AE based models need to have: (a) a good decoder G that can output realistic images given latent variables sampled fromp(z), and (b) a good match betweenp(z) and p(z).</p><p>Criterion (a) is easily ensured by minimizing the reconstruction loss of the auto-encoders, and there are different ways to ensure criterion (b). Intuitively, criterion (b) can be achieved by either modifying the encoder so thatp(z) is close to p(z), or conversely modifying p(z) to match some observed distribution on the latent space. The classic VAE model adopts the first approach indirectly using an approximation q(z) forp(z). It assumes a simple prior, and the KL regularizer in the ELBO objective penalizes KL [q(z) p(z)] plus a mutual information term as shown in <ref type="bibr" target="#b46">[47]</ref>. The approximation q(z) = E x∼p data [q(z|x)] is called the aggregated approximate posterior. Several modifications to VAE's objective <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>, which are designed for the task of unsupervised disentanglement, further decompose the KL term in ELBO, put a stronger penalty specifically on the mismatch between q(z) and p(z). There are also attempts to use normalizing flows as VAEs' posterior distributions <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Although similar in name, they are completely different from our models, as they aim to complicate the approximate posterior q(z|x) thus modifying the distribution q(z). As of yet, these modifications to VAEs have not been shown to improve generation quality. In particular, empirically VAEs with flow posterior have been shown to improve neither the matching of q(z) and p(z) <ref type="bibr" target="#b45">[46]</ref>, nor the generation quality <ref type="bibr" target="#b9">[10]</ref>. Adversarial auto-encoders <ref type="bibr" target="#b51">[52]</ref> and Wasserstein auto-encoders <ref type="bibr" target="#b29">[30]</ref> use adversarial regularizer or MMD regularizers <ref type="bibr" target="#b54">[55]</ref> to force the q(z) to be close to p(z). These regularizations can be applied to both deterministic and probabilistic auto-encoders, and are shown to improve generation quality, as they generate sharper images than VAEs do.</p><p>One problem with regularizing q(z) is that it introduces a trade off with reconstruction, i.e. criterion (a). This motivates the use of learnable priors optimized to match q(z). <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b28">29]</ref> propose different ways to approximate the aggregated posterior q(z) during training, and use the approximated q(z) as their VAEs' prior distributions. This is a natural way to modify the prior to match q(z), however, these methods have not been shown to improve generation quality. Two-stage-VAE <ref type="bibr" target="#b9">[10]</ref> introduces another VAE on the latent space defined by the first VAE to learn the distribution of its latent variables. GLANN <ref type="bibr" target="#b6">[7]</ref> learns a latent representation by GLO <ref type="bibr" target="#b4">[5]</ref> and matches the densities of the latent variables with an implicit maximum likelihood estimator <ref type="bibr" target="#b5">[6]</ref>. VQ-VAE <ref type="bibr" target="#b52">[53]</ref> first learns an AE with discrete latent variables that are stored in a code-book, and then fits an auto-regressive prior on the latent space. RAE+GMM <ref type="bibr" target="#b27">[28]</ref> trains a regularized auto-encoder <ref type="bibr" target="#b57">[58]</ref> and fits a mixture of Gaussian distribution on the latent space. These methods significantly improve the generation quality, but they all involve two-stage training procedure, which adds a computational overhead.</p><p>Motivated by above works, we wish to design an AE based generative model that enjoys the best of both worlds: it can be trained end-to-end in a single stage, and it can greatly improve the generation quality without over regularizing the latent variables. We accomplish these goals by using normalizing flow on the latent space of a deterministic AE. More details will be presented in Section 3. Note that our method is closely related VAEs with normalizing flow as a learnable prior and the connections are discussed in Section 3.4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Generative latent flow (GLF) model</head><p>Our model uses a deterministic auto-encoder composed of an encoder E η : X → Z and decoder (generator) G φ : Z → X . In addition we have a transformation F θ from the distribution on the noise space E, which is assumed to be the standard Gaussian distribution, to the distribution on Z. The transformation F θ is defined in terms of a normalizing flow and all three components are learned simultaneously end to end using a loss that combines the reconstruction quality and the likelihood of the encoded data z i with respect to the transformation F θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Normalizing flows for the transformation F</head><p>The core of normalizing flows is carefully-designed invertible networks that map the training data to a simple distribution. Let z ∈ Z be an observation from an unknown target distribution z ∼ p(z) and p ε be the unit Gaussian prior distribution on E. Given a bijection f θ : Z → E, we define a model p θ (z) with parameters θ on Z, and we can compute the negative log likelihood (NLL) of z by the change of variable formula:</p><formula xml:id="formula_0">− log(p θ (z)) = L NLL (f θ (z)) = − log p ε (f θ (z)) + log det ∂f θ (z) ∂z<label>(1)</label></formula><p>where ∂f θ (z) ∂z is the Jacobian matrix of f θ . In order to learn the flow f θ , the NLL objective of z is minimized, which is equivalent to maximize the likelihood of z. Since the mapping is a bijection, sampling from the trained model p θ (z) is trivial: simpy sample ε ∼ p ε and compute z = f −1 θ (ε). In our method, we use the normalizing flow to model the transformation F θ = f −1 θ . The key to designing a tractable flow model is defining the transformation f θ so that the inverse transformation and the determinant of the Jacobian matrix can be efficiently computed. Based on <ref type="bibr" target="#b3">[4]</ref>, we adopt the following layers to form the flows used in our model. Affine coupling layer: Given a D dimensional input data z and d &lt; D, we partition the input into two vectors z 1 = z 1:d and z 2 = z d+1:D . The output of one affine coupling layer is given by</p><formula xml:id="formula_1">y 1 = z 1 , y 2 = z 2 exp(s(z 1 )) + t(z 1 ) where s and t are functions from R d → R D−d and</formula><p>is the element-wise product. The inverse of the transformation is explicitly given by z 1 = y 1 , z 2 = (y 2 − t(y 1 )) exp(−s(y 1 )). The determinant of the Jacobian matrix of this transformation is simply det</p><formula xml:id="formula_2">∂y ∂z = d j=1 (exp[s(z 1 ) j ]).</formula><p>Since computing both the inverse and the Jacobian does not require computing the inverse and Jacobian of s and t, both functions can be arbitrarily complex.</p><p>Combining coupling layers with random permutation: Affine coupling layers leave some components of the input data unchanged. In order to transform all the components, two coupling layers are combined in an alternating pattern to form a coupling block, so the unchanged components in the first layer can be transformed in the second layer. In particular, we add a fixed random permutation of dimensions of the input data after each coupling layer. See <ref type="figure" target="#fig_0">Figure 1b</ref> for an illustration of a coupling block used in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The objective function</head><p>Having defined the invertible flow F θ : E → Z, we also need to train a deterministic auto-encoder composed of an encoder E η and a decoder G φ . The auto-encoder is trained to minimize the reconstruction loss, which we set to be the common MSE loss. The overall training objective is a combination of the reconstruction loss and the NLL loss for the flow transformation:</p><formula xml:id="formula_3">L(η, φ, θ) = 1 N N i=1 βL recon x i , G φ (E η (x i )) + L NLL f θ (sg [E η (x i )])<label>(2)</label></formula><p>where η, φ are the parameters of the encoder and decoder respectively, θ is the parameter of the flow, sg[·] is the stop gradient operation, and β is a hyper-parameter that controls the relative weight of the reconstruction loss and the NLL loss in equation <ref type="bibr" target="#b0">(1)</ref>. Note that when assuming the decoder distribution is Gaussian with variance γ 2 at each pixel, β can be viewed as the inverse variance.</p><p>After training the model, the generating process is easy: first sample a noise ∼ N (0, I) and then obtain a generated samplex = G φ (F ( )), where F = f −1 θ . Since the highlight of our model is applying a flow on latent variables, we name it Generative Latent Flow (GLF). See <ref type="figure" target="#fig_0">Figure 1a</ref> for an illustration of the GLF model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Why Stop The Gradients?</head><p>The stop gradient operation in <ref type="formula" target="#formula_3">(2)</ref> is important. If we let gradients of the NLL loss back propagate into the latent variables, it can lead to degenerate z's. This is because f θ has to transform the z's to unit Gaussian noise, so the smaller the scale of the z's, the more negative the log-determinant of the Jacobian becomes. Since there is no constraint on the scale of latent variables, the Jacobian term can dominate the entire objective, driving the NLL loss to negative infinity through shrinking z towards 0. While the latent variables cannot become exactly 0 because of the presence of reconstruction loss in the objective, the extremely small scale of z may cause numerical issues that cause severe fluctuations. Therefore, we propose to stop the gradient of the NLL loss at the latent variables so that it cannot modify the values of z or affect the parameters of the encoder. We experimentally verify the problems of latent regularization in Section 4.2.1.</p><p>We call our original model with stopped gradients GLF and without stopped gradients regularized GLF, since the flow acts as a regularizer on the auto-encoder. Note that for GLF, the value of β in <ref type="formula" target="#formula_3">(2)</ref> does not matter, since the reconstruction loss and the NLL loss are independent. Note also that GLF can also be trained two stages, namely an auto-encoder is trained first, and then the flow is trained to map the distributions. Empirically, we find that the two-stage training strategy leads to similar performance, so we only focus on one-stage training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Connection to VAEs with Flow Prior</head><p>Our method is closely related to VAEs with normalizing flow priors. To see this, consider the ELBO objective of plain VAEs with Gaussian prior and posterior (η, φ denote the parameters of encoder and decoder, respectively):</p><formula xml:id="formula_4">ELBO(η, φ) = E p data (x) E qη(z|x) [log p φ,β (x|z) + log p(z) − log q η (z|x)]<label>(3)</label></formula><p>The first term is related to the reconstruction loss and depends on the precision β of the observationd at each pixel, while the last two terms can be combined as KL [q(z|x) p(z)].</p><p>If we introduce a normalizing flow f θ for the prior distribution, then the prior</p><formula xml:id="formula_5">p θ becomes p θ (z) = p ε (f θ (z)) det ∂f θ (z) ∂z ,</formula><p>where p ε is the standard Gaussian density. Substituting this prior into <ref type="formula" target="#formula_4">(3)</ref>, we obtain the ELBO(η, φ, θ) for VAEs with flow prior:</p><formula xml:id="formula_6">E p data (x) E qη(z|x) log p φ,β (x|z) + log p ε (f θ (z)) + log det ∂f θ (z) ∂z − log q η (z|x)<label>(4)</label></formula><p>Comparing <ref type="formula" target="#formula_6">(4)</ref> and <ref type="formula" target="#formula_3">(2)</ref>, we observe that if the expectation over q η (z|x) is estimated by sampling, ELBO(η, φ, θ) is precisely the negative of GLF's objective (without stopping gradients) plus an additional entropy term that corresponds to the entropy of encoder distribution. As β increases two things occur as demonstrated empirically in Section 4.2.1. First the estimated variances from the encoder decrease, and second the contribution of the reconstruction loss to the gradient of the encoder parameters becomes larger than the contribution of the flow's likelihood loss. Thus as β increases the VAE+flow converges to GLF. Furthermore Gaussian VAEs with flow prior does not suffer from the degeneracy of regularized GLF because of the presence of the entropy term. It is the negative sum of the log variances of the latent variables, and thus it encourages the encoder to output large posterior variance, preventing latent variables from collapsing to 0.</p><p>VAEs with flow prior have attracted very little attention <ref type="bibr" target="#b53">[54]</ref>, and they have only focused on improvement of the data likelihood. Our work is differs in two ways: 1. we are the first to evaluate the effects of normalizing flow prior on generation quality; 2. we use deterministic AEs rather than VAEs, thus avoiding the need to choose β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To demonstrate the performance of our method, we present both quantitative and qualitative evaluations on four commonly used datasets for generative models: MNIST <ref type="bibr" target="#b34">[35]</ref>, Fashion MNIST <ref type="bibr" target="#b35">[36]</ref>, CIFAR-10 <ref type="bibr" target="#b36">[37]</ref> and CelebA <ref type="bibr" target="#b7">[8]</ref>. Throughout the experiments, we use 20-dimensional latent variables for MNIST and Fashion MNIST, and 64-dimensional latent variables for CIFAR-10 and CelebA.</p><p>[38] adopted a common network architecture based on InfoGAN <ref type="bibr" target="#b38">[39]</ref> to evaluate GANs. In order to make fair comparisons without designing arbitrarily large networks to achieve better performance, we use the generator architecture of InfoGAN as our decoder's architecture, and we make the encoder to be symmetric to the decoder. For details of the AE network structures, see Appendix A. For the flow applied on latent variables, we use 4 affine coupling blocks defined as in <ref type="figure" target="#fig_0">Figure 1b</ref>, where each block contains 3 fully connected layers each with k hidden units. For MNIST and Fashion MNIST, k = 64, while for CIFAR-10 and CelebA, k = 256. Note that the flow only adds a small parameter overhead on the auto-encoder (less than 3%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Metrics</head><p>We use the Fréchet Inception Distance (FID) <ref type="bibr" target="#b40">[41]</ref> as a metric for image generation quality. FID is computed by first extracting features of a set of real images x and a set of generated images g from an intermediate layer of the Inception network <ref type="bibr" target="#b8">[9]</ref>. Each set of features is fitted with a Gaussian distribution, yielding means µ x , µ g and co-variances matrices Σ x , Σ g . The FID score is defined to be the Fréchet distance between these two Gaussians:</p><formula xml:id="formula_7">FID(x, g) = µ x − µ g 2 2 + Tr Σ x + Σ g − 2 (Σ x Σ g ) 1 2</formula><p>It is claimed that the FID score is sensitive to mode collapse and correlates well with human perception of generator quality. Recently, <ref type="bibr" target="#b41">[42]</ref> proposed using Precision and Recall for Distributions (PRD) which can assess both the quality and diversity of generated samples. We also include PRD in our studies. See Appendix F.  <ref type="table" target="#tab_0">Table 1</ref> summarizes the main results of this work. We compare the FID scores obtained by our method with the scores of the VAE baseline and several existing AE based models that are claimed to produce high quality samples. Instead of directly citing their reported results, we re-ran the experiments because we want to evaluate them under standardized settings so that all models adopt the same AE architectures, latent dimensions and image pre-processing. We use GLF and VAE+flow prior with β = 1 to report the results in the table. For other methods, we largely follow their proposed experimental settings. Details of each experiment are presented in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Note that the authors of WAE propose two variants, namely WAE-GAN and WAE-MMD. We only report the results of WAE-GAN, as we found it consistently outperforms WAE-MMD. Note also that, GLANN <ref type="bibr" target="#b6">[7]</ref> obtains impressive FID scores, but it uses perceptual loss <ref type="bibr" target="#b24">[25]</ref> as the reconstruction loss. The perceptual loss is obtained by feeding both training images and reconstructed images into a pre-trained network such as VGG <ref type="bibr" target="#b25">[26]</ref>, and computing the L 1 distance between some of the intermediate layers' activation. We also train our method with perceptual loss and compare with GLANN in the last two rows of <ref type="table" target="#tab_0">Table 1</ref>.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, our method obtains significantly lower FID scores than competing AE based models across all four datasets. In particular, GLF greatly outperforms VAE+flow prior in the default setting. A more detailed analysis and comparison between the two methods will be done in Section 4.2.1. We also confirm that VAE+flow posterior cannot improve generation quality. Perhaps the competing model with the closest performances to ours is RAE+GMM, which shares some similarity with GLF in that both methods fit the density of the latent variables of an AE explicitly. To compare our method with GANs, we also include the results from <ref type="bibr" target="#b37">[38]</ref> in Appendix C. In <ref type="bibr" target="#b37">[38]</ref>, the authors conduct standardized and comprehensive evaluations of representative GAN models with large-scale hyper-parameter searches, and therefore, their results can serve as a strong baseline. The results indicate that our method's generation quality is competitive with that of carefully tuned GANs.</p><p>Qualitative results are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Besides samples of the datasets used for quantitative evaluation, samples of CelebA-HQ <ref type="bibr" target="#b58">[59]</ref> with the larger size of 256 × 256 are also included in <ref type="figure" target="#fig_1">Figure 2e</ref> to show our method's potential of scaling up to images with higher resolution. Qualitative results show that our model can generate sharp and diverse samples in each dataset. In <ref type="figure" target="#fig_1">Figure 2f</ref>, we show CelebA images generated by linearly interpolating two samples of random noise. The smooth interpolation indicates that our method fits the distribution of latent variables well. For more qualitative results, including samples from the models trained with perceptual loss, see Appendix G. We see that samples from models trained with perceptual loss have higher quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparisons: GLF vs. Regularized GLF and VAE+flow Prior.</head><p>As discussed in Section 3.3, regularized GLF is unstable because of the degeneracy of latent variables created by the NLL loss. We empirically study the effect of latent regularization as a function of β on CIFAR-10. For low values of β = 1 and 10, the NLL loss completely dominates the learning signal and the reconstruction loss quickly diverges. Even for larger values of β = 50, 100, 400 the NLL loss decreases to a very small negative value, and although overall performance is reasonable it oscillates quite strongly as training proceeds. The relevant plots are shown in <ref type="figure" target="#fig_3">Figure 4</ref> in Appendix D.</p><p>In contrast, for GLF, where the flow does not modify z, the NLL loss does not degenerate, resulting in stable improvements of FID scores as training progress.</p><p>We also trained VAEs+flow prior with different choices of decoder variances (equivalently, different choices of β), plus one with learnable decoder variance as done in <ref type="bibr" target="#b9">[10]</ref>. We record the progression of FID scores of these models on CIFAR-10 in <ref type="figure" target="#fig_2">Figure 3a</ref>. In <ref type="figure" target="#fig_2">Figure 3b</ref>, we plot the entropy loss, which is one term in VAE+flow prior's minimization objective. The entropy loss is expressed as − d j=1 log(σ j )/2, where σ j is the standard deviation of the j th latent variable. Higher entropy loss means that the latent variables have lower variances. In <ref type="figure" target="#fig_2">Figure 3c</ref>, we plot the NLL loss.</p><p>From <ref type="figure" target="#fig_2">Figure 3a</ref>, we see that GLF converges faster and obtains lower FID score than VAEs+flow prior. The performance gap closes as β increases, however, even with large β, GLF still slightly outperforms VAE+flow prior. We also find that the learnable β for VAE+flow prior is not effective, probably due to relatively small values of β in early time. When β is large, as indicated before, the posterior variances become very small, so that effectively we are training an AE. For example, as shown in <ref type="figure" target="#fig_2">Figure 3b</ref>, when β = 400, the corresponding average posterior variance is around 10 −4 .</p><p>In contrast to regularized GLF, there is no degeneracy of latent variables observed thanks to the noise introduced by VAEs and the corresponding entropy term. Indeed, <ref type="figure" target="#fig_2">Figure 3c</ref> shows that the training of VAE+flow prior does not over-fit the NLL loss, as opposed to regularized GLF where severe over-fitting to NLL loss occurs as shown in <ref type="figure" target="#fig_3">Figure 4c</ref>. Comparing <ref type="figure" target="#fig_2">Figure 3a</ref> and 4a, we observe that unlike regularized GLF, VAE+flow prior does not suffer from divergence or fluctuations in FID scores, even with relatively small β. In general, the results of FID scores show that regularized GLF is unstable, while as β increases, the performance of VAE+flow prior converges to that of GLF, which outperforms them all.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training time</head><p>Besides better performances, our method also has the advantage of faster convergence among competing methods such as GLANN and Two-stage VAE. In <ref type="table" target="#tab_1">Table 2</ref> we compare the number of training epochs to obtain the FID scores in <ref type="table" target="#tab_0">Table 1</ref>. We also compare the per epoch training clock time in Appendix E. The results indicate that GLF requires much less training time to generate high quality samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduce Generative Latent Flow, a novel generative model which uses an autoencoder to learn a latent space from training data and a normalizing flow to match the distribution of the latent variables with the prior. Under standardized evaluations, our model achieves state-of-the-art results in image generation among several recently proposed Auto-encoder based models. Besides higher generation quality, our method also enjoys advantages such as faster training time and end-toend single stage training. While we are not claiming that our GLF model is superior to GANs, we do believe that it opens the door to realize the potential of AE based models to produce high quality samples just as GANs do. The comparison between our method and its stochastic counterparts briefly examines the question about the effects of adding noise during the training of generative models, which is a topic that deserves further studies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Network Architectures</head><p>In this section we provide <ref type="table" target="#tab_2">Table 3</ref> that summarizes the auto-encoder network structure. The network structure is adopted from InfoGAN <ref type="bibr" target="#b38">[39]</ref>, and the difference between the networks we used for each dataset is the size of the fully connected layers, which depends on the size of the image. All convolution and deconvolution layers have stride = 2 and padding = 1 to ensure the spatial dimension decreases/increases by a factor of 2. M is simply the size of an input image divided by 4. Specifically, for MNIST and Fashion MNIST, M = 7; for CIFAR-10, M = 8; for CelebA, M = 16. BN stands for batch normalization.</p><p>For VAEs, the final FC layer of the encoder will have doubled output size to return both the mean and standard deviation of latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment Settings</head><p>In this section, we present the details of our experimental settings for results in <ref type="table" target="#tab_0">Table 1</ref>. Since the settings for MNIST and Fashion MNIST are the same, we only mention MNIST for simplicity. For GLANN, we directly cite the results from <ref type="bibr" target="#b6">[7]</ref>, as their experimental settings is very similar to ours.</p><p>We use the original images in the training sets for MNIST, Fashion MNIST and CIFAR-10. For CelebA, we follow the same pre-processing as in <ref type="bibr" target="#b37">[38]</ref>: center crop to 160 × 160 and then resize to 64 × 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Settings for training GLF</head><p>For all datasets (except CelebA-HQ), we use batch size 256 and Adam <ref type="bibr" target="#b42">[43]</ref> optimizer with initial learning rate 10 −3 for the parameters of both the AE and the flow. We add a weight decay 2 × 10 −5 to the optimizers for the flow. For MNIST, we train our model for 100 epochs, with learning rate decaying by a factor of 2 after 50 epochs. For CIFAR-10, we train our model for 200 epochs, with the learning rate decaying by a factor of 2 every 50 epochs. For CelebA, we train our model for 40 epochs with no learning rate decay.</p><p>For GLF with perceptual loss as the reconstruction loss, we compute the perceptual loss as suggested in <ref type="bibr" target="#b59">[60]</ref>. See https://github.com/facebookresearch/NAM/blob/master/code/ perceptual_loss.py for their implementation. Other settings are the same.</p><p>For CelebA-HQ dataset, we adopt our AE network structure based on DCGAN <ref type="bibr" target="#b10">[11]</ref>. Note that this is a relatively simple network for high resolution imgaes. We use batch size 64, with initial learning rate 10 −3 for both the AE and the flow. We train our model for 60 epochs, with learning rate decaying by a factor of 2 after 40 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Settings for training VAEs and VAE variants</head><p>We adopt common settings for our reported results of VAE, VAE+flow prior and VAE+flow posterior. We still use batch size 256, and Adam optimizer with initial learning rate 10 −3 for both the VAE and the flow, if applicable. We find VAEs need longer time to converge, so we double the training epochs.</p><p>We train MNIST for 200 epochs, with learning rate decaying by a factor of 2 after 100 epochs. We train CIFAR-10 for 400 epochs, with the learning rate decaying by a factor of 2 every 100 epochs. We train CelebA for 80 epochs with learning rate decaying by a factor of 2 after 40 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Settings for training WAE-GAN</head><p>We follow the settings introduced in the original WAE paper <ref type="bibr" target="#b29">[30]</ref>. The adversary in WAE-GAN has the following architecture:</p><formula xml:id="formula_8">z ∈ R d → FC 512 → ReLU → FC 512 → ReLU → FC 512 → ReLU → FC 512 → ReLU → FC 1</formula><p>where d is the dimension of the latent variables.</p><p>WAE has two major hyper-parameters: λ which controls the weight coefficient of the adversarial regularizer, and σ 2 which is the variance of the prior. Batch size is 100 for all datasets. For MNIST, λ = 10 and σ 2 = 1, and the model is trained for 100 epochs. The initial learning rate is 10 −3 for the AE and 5 × 10 −4 for the adversary. After 30 epochs both learning rates decreased both by factor of 2, and after first 50 epochs further by factor of 5. For CIFAR, λ = 10 and σ 2 = 1 and the model is trained for 200 epochs. The initial learning rates are the same as training MNIST, and the learning rate decays by a factor of 2 after first 60 epochs, and further by a factor of 5 after 120 epochs. For CelebA, λ = 1 and σ 2 = 2. The model is trained for 55 epochs. The initial learning rate is 3 × 10 −4 for the AE and 10 −3 for the adversary. Both learning rates decays by factor of 2 after 30 epochs, further by factor of 5 after 50 first epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Settings for training Two stage VAE</head><p>We adopt the settings in the original paper <ref type="bibr" target="#b9">[10]</ref>. For all datasets, the batch size is set to be 100, and the initial learning rate for both the first and the second is 10 −4 . For MNIST, the first VAE is trained for 400 epochs, with learning rate halved every 150 epochs; the second VAE is trained for 800 epochs with learning rate halved every 300 epochs. For CIFAR-10, 1000 and 2000 epochs are trained for the two VAEs respectively, and the learning rates are halved every 300 and 600 epochs for the two stages. For CelebA, 120 and 300 epochs are trained for the two VAEs respectively, and the learning rates are halved every 48 and 120 epochs for the two stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Settings for training RAE+GMM</head><p>The settings of batch size, learning rate scheduling and number of epochs for training RAE are the same as those of GLF. The objective of the RAE is reconstruction loss plus a penalty on the norm of the latent variable. Since the author does not report their choices for the penalty coefficient γ, we search over γ ∈ 0.1, 0.5, 1, 2, and we find that β = 0.5 leads to the best overall performances, and therefore we let γ = 0.5. After training the RAE, we fit a 10-component Gaussian mixture distribution on the latent variables. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Comparison with GANs</head><p>In <ref type="table" target="#tab_3">Table 4</ref> we combine our reported results of AE based models and the FID scores of GANs cited from <ref type="bibr" target="#b37">[38]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Issues with latent regularization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Clock time comparisons</head><p>In <ref type="table" target="#tab_4">Table 5</ref>, we report the clock training time per epoch of our method, two-stage VAE and GLANN. Note that for methods using perceptual loss, the per epoch training time is longer because activations need to be computed. This, together with <ref type="table" target="#tab_1">Table 2</ref>, shows that we need much shorter training time while obtaining better performances. In our method, training the flow does not add much computational time due to the low dimensionality.We record the clock time on a platform with a single GTX 1080 GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Precision and Recall</head><p>In this section, we report the precision and recall (PRD) evaluation of our randomly generated samples on each dataset in <ref type="table" target="#tab_5">Table 6</ref>. The two numbers in each entry are F 8 , F 1 8 that captures recall and precision. See <ref type="bibr" target="#b41">[42]</ref> for more details. We report the PRD for our models under the setting of obtaining the results in <ref type="table" target="#tab_0">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G More qualitative results</head><p>In <ref type="figure" target="#fig_4">Figure 5</ref>, we show more samples of each dataset generated by GLF, using either MSE or perceptual loss as reconstruction loss. In <ref type="figure">Figure 6</ref>, we show samples of CelebA-HQ datasets from GLF trained with perceptual loss. In <ref type="figure">Figure 7</ref>, we show examples of interpolations between two randomly sampled noises on CelebA from GLF trained with perceptual loss.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Illustration of GLF model. The red arraw contains a stop gradient operation. See Section 3.3. (b) Structure of one flow block. It splits the input into two parts y = (y 1 , y 2 ), goes through the coupling layer C, and applies the random permutation P .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a)-(e): Randomly generated samples from our method trained on different datasets. (f): Random noise interpolation on CelebA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Record of FID scores on CIFAR-10 for VAEs+flow prior with different values of β and GLF. (b) Record of entropy losses for corresponding models. (c) Record of NLL losses for corresponding models..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a) Record of FID scores on CIFAR-10 for regularized GLF with different values of β and GLF. β = 1 and 10 are omitted because they leads to divergence in reconstruction loss. (b) Record of reconstruction losses for corresponding models. (c) Record of NLL losses for corresponding models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>(a)-(d) Randomly generated samples from our method with MSE loss. (e)-(h) Randomly generated samples from our method with perceptual loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Randomly generated samples from our method with perceptual loss on CelebA-HQ dataset Noise interpolation on CelebA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>FID scores obtained from different models. For our reported results, we executed 10 independent trials and report the mean and standard deviation of the FID scores. Each trail is computing the FID between 10k generated images and 10k real images.</figDesc><table><row><cell></cell><cell>MNIST</cell><cell>Fashion</cell><cell>CIFAR-10</cell><cell>CelebA</cell></row><row><cell>VAE</cell><cell cols="4">28.2 ± 0.3 57.5 ± 0.4 142.5 ± 0.6 71.0 ± 0.5</cell></row><row><cell>WAE-GAN</cell><cell cols="3">12.4 ± 0.2 31.5 ± 0.4 93.1 ± 0.5</cell><cell>66.5 ± 0.7</cell></row><row><cell>Two-Stage VAE</cell><cell cols="4">10.9 ± 0.7 26.1 ± 0.9 96.1 ± 0.9 2 65.2 ± 0.8</cell></row><row><cell>RAE + GMM</cell><cell cols="3">10.8 ± 0.1 25.1 ± 0.2 91.6 ± 0.6</cell><cell>57.8 ± 0.4</cell></row><row><cell>VAE+flow prior</cell><cell cols="4">28.3 ± 0.2 51.8 ± 0.3 110.4 ± 0.5 54.3 ± 0.3</cell></row><row><cell>VAE+flow posterior</cell><cell cols="4">26.7 ± 0.3 55.1 ± 0.3 143.6 ± 0.8 67.9 ± 0.3</cell></row><row><cell>GLF (ours)</cell><cell>8.2 ± 0.1</cell><cell cols="2">21.3 ± 0.2 88.3 ± 0.4</cell><cell>53.2 ± 0.2</cell></row><row><cell cols="2">GLANN with perceptual loss 8.6 ± 0.1</cell><cell cols="2">13.0 ± 0.1 46.5 ± 0.2</cell><cell>46.3 ± 0.1</cell></row><row><cell>GLF+perceptual loss (ours)</cell><cell>5.8 ± 0.1</cell><cell cols="2">10.3 ± 0.1 44.6 ± 0.3</cell><cell>41.8 ± 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">Number of training epochs for Two-stage VAE, GLANN, and GLF</cell></row><row><cell></cell><cell cols="3">MNIST/Fashion CIFAR-10 CelebA</cell></row><row><cell cols="2">Two-stage VAE First/Second 400/800</cell><cell cols="2">1000/2000 120/300</cell></row><row><cell>GLANN First/Second</cell><cell>500/50</cell><cell>500/50</cell><cell>500/50</cell></row><row><cell>GLF</cell><cell>100</cell><cell>200</cell><cell>40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Network structure for auto-encoder based on InfoGAN Conv 64 , ReLU FC nz → 1024, BN, ReLU 4 × 4 Conv 128 , BN, ReLU FC 1024 → 128 × M × M , BN, ReLU Flatten, FC 128 × M × M → 1024, BN, ReLU 4 × 4 Deconv 64 , BN, ReLU FC 1024 → nz 4 × 4 Deconv 128 , Sigmoid</figDesc><table><row><cell>Encoder</cell><cell>Decoder</cell></row><row><cell>Input x</cell><cell>Input z</cell></row><row><cell>4 × 4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>FID score comparisons of GANs and various AE based models</figDesc><table><row><cell></cell><cell>MNIST</cell><cell>Fashion</cell><cell>CIFAR-10</cell><cell>CelebA</cell></row><row><cell>MM GAN</cell><cell>9.8 ± 0.9</cell><cell cols="2">29.6 ± 1.6 72.7 ± 3.6</cell><cell>65.6 ± 4.2</cell></row><row><cell>NS GAN</cell><cell>6.8 ± 0.5</cell><cell cols="2">26.5 ± 1.6 58.5 ± 1.9</cell><cell>55.0 ± 3.3</cell></row><row><cell>LSGAN</cell><cell>7.8 ± 0.6</cell><cell cols="3">30.7 ± 2.2 87.1 ± 47.5 53.9 ± 2.8</cell></row><row><cell>WGAN</cell><cell>6.7 ± 0.4</cell><cell cols="2">21.5 ± 1.6 55.2 ± 2.3</cell><cell>41.3 ± 2.0</cell></row><row><cell>WGAN GP</cell><cell cols="3">20.3 ± 5.0 24.5 ± 2.1 55.8 ± 0.9</cell><cell>30.3 ± 1.0</cell></row><row><cell>DRAGAN</cell><cell>7.6 ± 0.4</cell><cell cols="2">27.7 ± 1.2 69.8 ± 2.0</cell><cell>42.3 ± 3.0</cell></row><row><cell>BEGAN</cell><cell cols="3">13.1 ± 1.0 22.9 ± 0.9 71.4 ± 1.6</cell><cell>38.9 ± 0.9</cell></row><row><cell>VAE</cell><cell cols="4">28.2 ± 0.3 57.5 ± 0.4 142.5 ± 0.6 71.0 ± 0.5</cell></row><row><cell>WAE-GAN</cell><cell cols="3">12.4 ± 0.2 31.5 ± 0.4 93.1 ± 0.5</cell><cell>66.5 ± 0.7</cell></row><row><cell>Two-Stage VAE</cell><cell cols="3">10.9 ± 0.7 26.1 ± 0.9 96.1 ± 0.9</cell><cell>65.2 ± 0.8</cell></row><row><cell>RAE + GMM</cell><cell cols="3">10.8 ± 0.1 25.1 ± 0.2 91.6 ± 0.6</cell><cell>57.8 ± 0.4</cell></row><row><cell cols="2">GLANN (with perceptual loss) 8.6 ± 0.1</cell><cell cols="2">13.0 ± 0.1 46.5 ± 0.2</cell><cell>46.3 ± 0.1</cell></row><row><cell>VAE+flow prior</cell><cell cols="4">28.3 ± 0.2 51.8 ± 0.3 110.4 ± 0.5 54.3 ± 0.3</cell></row><row><cell>VAE+flow posterior</cell><cell cols="4">26.7 ± 0.3 55.1 ± 0.3 143.6 ± 0.8 67.9 ± 0.3</cell></row><row><cell>GLF (ours)</cell><cell>8.2 ± 0.1</cell><cell cols="2">21.3 ± 0.2 88.3 ± 0.4</cell><cell>53.2 ± 0.2</cell></row><row><cell>GLF+perceptual loss (ours)</cell><cell>5.8 ± 0.1</cell><cell cols="2">10.3 ± 0.1 44.6 ± 0.3</cell><cell>41.8 ± 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Per-epoch training time in seconds MNIST CIFAR-10 CelebA</figDesc><table><row><cell>2-stage VAE 1st/2nd</cell><cell>5/2</cell><cell>6/2</cell><cell>60/28</cell></row><row><cell>GLF</cell><cell>10</cell><cell>13</cell><cell>108</cell></row><row><cell cols="2">GLANN with perceptual loss 14</cell><cell>16</cell><cell>292</cell></row><row><cell>GLF with perceptual loss</cell><cell>16</cell><cell>19</cell><cell>343</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of random sample quality by precision / recall. Higher numbers are better.</figDesc><table><row><cell>MNIST</cell><cell>Fashion</cell><cell>CIFAR-10</cell><cell>CelebA</cell></row><row><cell cols="4">(0.981, 0.963) (0.974, 0.955) (0.685, 0.805) (0.447, 0.726)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that there is a large discrepancy between this and the result reoprted in the original paper. See Appendix B.4 for explanation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Optimizing the latent space of generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05776</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Implicit maximum likelihood estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09087</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08985</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Diagnosing and enhancing vae models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05789</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durk</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">NIPS 2016 tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Veegan: Reducing mode collapse in gans using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Wasserstein gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02163</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep feature consistent variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianxu</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">From Variational to Deterministic Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scholkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arxiv preprint arxiv 1903.12436</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Resampled Priors for Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11428</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Wasserstein auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schoelkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01558</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durk</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sylvester normalizing flows for variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05649</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<title level="m">Importance weighted autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Are GANs created equal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10337</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">A Large-Scale Study. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01844</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Distribution matching in variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Elbo surgery: yet another way to carve up the variational evidence lower bound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Advances in Approximate Bayesian Inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">VAE with a VampPrior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07120</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning Hierarchical Priors in VAEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexej</forename><surname>Klushyn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04982</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Disentangling by factorising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05983</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learnable explicit density for continuous latent space and variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02248</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generalization and equilibrium in generative adversarial nets (gans)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Flow-gan: Bridging implicit and prescribed learning in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08868</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">What regularized auto-encoders learn from the data-generating distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3563" to="3593" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Nam: Non-adversarial unsupervised domain mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
