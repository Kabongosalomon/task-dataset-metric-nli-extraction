<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Caetano</surname></persName>
							<email>carlos.caetano@dcc.ufmg.br</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Smart Sense Laboratory</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais Belo Horizonte</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Brémond</surname></persName>
							<email>francois.bremond@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">INRIA Sophia Antipolis Valbonne</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Robson Schwartz</surname></persName>
							<email>william@dcc.ufmg.br</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Smart Sense Laboratory</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais Belo Horizonte</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the last years, the computer vision research community has studied on how to model temporal dynamics in videos to employ 3D human action recognition. To that end, two main baseline approaches have been researched: (i) Recurrent Neural Networks (RNNs) with Long-Short Term Memory (LSTM); and (ii) skeleton image representations used as input to a Convolutional Neural Network (CNN). Although RNN approaches present excellent results, such methods lack the ability to efficiently learn the spatial relations between the skeleton joints. On the other hand, the representations used to feed CNN approaches present the advantage of having the natural ability of learning structural information from 2D arrays (i.e., they learn spatial relations from the skeleton joints). To further improve such representations, we introduce the Tree Structure Reference Joints Image (TSRJI), a novel skeleton image representation to be used as input to CNNs. The proposed representation has the advantage of combining the use of reference joints and a tree structure skeleton. While the former incorporates different spatial relationships between the joints, the latter preserves important spatial relations by traversing a skeleton tree with a depth-first order algorithm. Experimental results demonstrate the effectiveness of the proposed representation for 3D action recognition on two datasets achieving state-of-the-art results on the recent NTU RGB+D 120 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human action recognition plays an important role in various applications such as surveillance systems, health care systems and robot and human-computer interaction. Significant progress on the action recognition task has been achieved due to the design of discriminative representations based on appearance information by using RGB frames. However, due to the development of cost-effective RGB-D sensors (e.g., Kinect), it became possible to employ different types of data such as depth information as well as human skeleton joints to perform 3D action recognition. Compared to RGB or depth information, skeleton based methods have demonstrated impressive results by modeling temporal dynamics in videos. These approaches have the advantage of being computationally efficient due to smaller data size and being robust to illumination changes, background noise and invariance to camera views <ref type="bibr" target="#b0">[1]</ref>.</p><p>On the last decade, many works for 3D action recognition model temporal dynamics in videos by employing Dynamic Time Warping (DTW), Fourier Temporal Pyramid (FTP) or Hidden Markov Model (HMM) in conjunction with skeleton handcrafted feature descriptors <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Nowadays, large efforts have been directed to the employment of deep neural networks to model skeleton data by using two main approaches: (i) Recurrent Neural Networks (RNNs) with Long-Short Term Memory (LSTM) <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>; and (ii) skeleton image representations used as input to a Convolutional Neural Network (CNN) <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b19">[20]</ref>. Although the former approach present excellent results in 3D action recognition task due to their power of modeling temporal sequences, such structures lack the ability to efficiently learn the spatial relations between the skeleton joints <ref type="bibr" target="#b17">[18]</ref>. On the other hand, the latter takes the advantage of having the natural ability of learning structural information from 2D arrays and is able to learn spatial relations from the skeleton joints.</p><p>As the forerunner of skeleton image representations, Du et al. <ref type="bibr" target="#b11">[12]</ref> take advantage of the spatial relations by employing a hierarchical structure. The authors represent each skeleton sequence as 2D arrays, in which the temporal dynamics of the sequence is encoded as variations in columns and the spatial structure of each frame is represented as rows. Finally, the representation is fed to a CNN to perform action prediction. Such type of representations is very compact since it encodes the entire video sequence in a single image.</p><p>In this paper, we introduce a novel skeleton image representation, named Tree Structure Reference Joints Image (TSRJI), to be used as input for CNNs. We improve the representation of skeleton joints for 3D action recognition encoding temporal dynamics by combining the use of reference joints <ref type="bibr" target="#b14">[15]</ref> and a tree structure skeleton <ref type="bibr" target="#b17">[18]</ref>. The method takes advantage of a structural organization of joints that preserves spatial relations of more relevant joint pairs and also by incorporating different spatial relationships between the joints. To perform action classification, we train a small CNN architecture with only three convolutional layers and two fully-connected layers.</p><p>Since the network is shallow and takes as input a compact representation for each video, it is extremely fast to train.</p><p>Our hypothesis is based on the assumption that the rearrangement of the structural organization of joints to be used as inputs helps on guiding the network to extract certain information, possibly complementary, that would not be extracted by using other modalities, such as RGB or depth information. Aligned with our hypothesis, other works mention that although the temporal evolution patterns can be learned implicitly with CNN using RGB data, an explicit modeling is preferable <ref type="bibr" target="#b18">[19]</ref>.</p><p>According to the experimental results, our proposed skeleton image representation can handle skeleton based 3D action recognition very well being able to recognize actions accurately on two well-known large scale datasets (NTU RGB+D 60 <ref type="bibr" target="#b8">[9]</ref> and NTU RGB+D 120 <ref type="bibr" target="#b20">[21]</ref>). We achieve the state-of-the-art performance on the large scale NTU RGB+D 120 <ref type="bibr" target="#b20">[21]</ref> dataset. Moreover, we show that our approach can be combined with a temporal structural joint representation <ref type="bibr" target="#b18">[19]</ref> to obtain state-of-the-art performance (up to 3.3 percentage points when compared to the best skeleton based method reported to date).</p><p>The code of our TSRJI representation is publicly available to facilitate future research 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we present a literature review of works that employ 3D action recognition based on skeleton image representations in conjunction with CNNs.</p><p>As one of the earliest works on skeleton image representations, Du et al. <ref type="bibr" target="#b11">[12]</ref> represent the skeleton sequences as a matrix. Each row of such matrix corresponds to a chain of concatenated skeleton joint coordinates from the frame t. Hence, each column of the matrix corresponds to the temporal evolution of the joint j. At this point, the matrix size is J × T × 3, where J is the number of joints for each skeleton, T is the total frame number of the video sequence and 3 is the number coordinate axes (x, y, z). The values of this matrix are quantified into an image (i.e., linearly rescaled to a [0, 255]) and normalized to handle the variable-length problem. In this way, the temporal dynamics of the skeleton sequence is encoded as variations in rows and the spatial structure of each frame is represented as columns. Finally, the authors use their representation as input to a CNN model composed by four convolutional layers and three max-pooling layers. After the feature extraction, a feed-forward neural network with two fully-connected layers is employed for classification.</p><p>Wang et al. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref> present a skeleton representation to represent both spatial configuration and dynamics of joint trajectories into three texture images through color encoding, named Joint Trajectory Maps (JTMs). The authors apply rotations to the skeleton data to mimicking multi-views and also for data enlargement to overcome the drawback of CNNs usually being not view invariant. JTMs are generated by projecting the trajectories onto the three orthogonal planes. To encode motion direction in the JTM, they use a hue colormap function to "color" the joint trajectories over the action period. They also encode the motion magnitude of joints into saturation and brightness claiming that changes in motion results in texture in the JMTs. Finally, the authors individually fine-tune three AlexNet <ref type="bibr" target="#b21">[22]</ref> CNNs (one for each JTM) to perform classification.</p><p>Representations based on heat map to encode spatialtemporal skeleton joints were also proposed by Liu et al. <ref type="bibr" target="#b13">[14]</ref>. Their approach considers each joint as 5D point space (x, y, z, t, j) and expresses them as a 2D coordinate space on a 3D color space. Thus, they permute elements of the 5D point space. Nonetheless, such permutation can generate very similar representations which may contain redundant information. To that end, they use ten types of ranking to ensure that each element of the point (x, y, z, t, j) can be assigned to the color 2D coordinate space. After that, the ten skeleton representations are quantified and treated as a color image. Finally, the authors employ a multiple CNN-based model, one for each of the representations. They used the AlexNet <ref type="bibr" target="#b21">[22]</ref> architecture and fused the posterior probabilities generated from each CNN for the final class score.</p><p>To overcome the problem of the sparse data generated by skeleton sequence video, Ke et al. <ref type="bibr" target="#b14">[15]</ref> represent the temporal dynamics of the skeleton sequence by generating four skeleton representation images. Their approach is closer to Du et al. <ref type="bibr" target="#b11">[12]</ref> method, however they compute the relative positions of the joints to four reference joints by arranging them as a chain and concatenating the joints of each body part to the reference joints resulting onto four different skeleton representations. According to the authors, such structure incorporates different spatial relationships between the joints. Finally, the skeleton images are resized and each channel of the four representations is used as input to a VGG19 <ref type="bibr" target="#b22">[23]</ref> pre-trained architecture for feature extraction.</p><p>To encode motion information on skeleton image representation, Li et al. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref> proposed the skeleton motion image. Their approach is created similar to Du et al. <ref type="bibr" target="#b11">[12]</ref> skeleton image representation, however each matrix cell is composed by joint difference computation between two consecutive frames. To perform classification, the authors used Du et al. <ref type="bibr" target="#b11">[12]</ref> approach and their proposed representation independently as input of a neural network with a two-stream paradigm. The CNN used was a small seven-layer network consisting of three convolution layers and four fully-connected layers.</p><p>Yang et al. <ref type="bibr" target="#b17">[18]</ref> claim that the concatenation process of chaining all joints with a fixed order turns into lack of semantic meaning and leads to loss in skeleton structural information. To that end, Yang et al. <ref type="bibr" target="#b17">[18]</ref> proposed a representation named Tree Structure Skeleton Image (TSSI) to preserve spatial relations. Their method is created by traversing a skeleton tree with a depth-first order algorithm with the premise that the fewer edges there are, the more relevant the joint pair is. The generated representation is then quantified into an image and resized before being presented to a ResNet-50 [24] CNN architecture.</p><p>As it can be seen from the reviewed methods, most of them are improved versions of Du et al. <ref type="bibr" target="#b11">[12]</ref> skeleton image representation focusing on spatial structural of joint axes while the temporal dynamics of the sequence is encoded as variations in columns. Despite the aforementioned methods produce promising results, we believe that performance can be improved by explicitly employing joints relationships, which enhances the temporal dynamics encoding. In view of that, our approach takes advantage of combining a structural organization that preserves spatial relations of more relevant joint pairs by using the skeleton tree with a depth-first order algorithm from Yang et al. <ref type="bibr" target="#b17">[18]</ref> and also by incorporating different spatial relationships by using the reference joints technique from Ke et al. <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>In this section, we introduce our proposed skeleton image representation based on reference joints and a tree structure skeleton, named Tree Structure Reference Joints Image (TSRJI). Finally, we present the CNN architecture employed in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tree Structure Reference Joints Image (TSRJI)</head><p>As reviewed in Section II, a crucial step to achieve good performance using skeleton image representations is to define how to build the structural organization of the representation preserving the spatial relations of relevant joint pairs. In view of that and due to the successful results achieved by the skeleton image representations, our approach follows the same fundamentals by representing the skeleton sequences as a matrix. Furthermore, our method is based on two premises of successful representations of the literature: (i) the fewer edges there are, the more relevant the joint pair is <ref type="bibr" target="#b17">[18]</ref>; and (ii) different spatial relationships between the joints leads to less sparse data <ref type="bibr" target="#b14">[15]</ref>.</p><p>To address the first premise, we apply the depth-first tree traversal order <ref type="bibr" target="#b17">[18]</ref> to each skeleton data from frame t to generate a pre-defined chain order C t that best preserves the spatial relations between joints in original skeleton structures (see <ref type="figure">Figure 1</ref>). The basic assumption here is that the spatially related joints in original skeletons have direct graph links between them <ref type="bibr" target="#b17">[18]</ref>. The less edges required to connect a pair of joints, the more related is the pair. In view of that, with the C t chain order, the neighboring columns in skeleton images are spatially related in original skeleton structures.</p><p>To address the second premise, we apply the reference joints technique <ref type="bibr" target="#b14">[15]</ref> to each generated C t chain. To that end, four reference joints are respectively used to compute relative positions of the other joints: (a) the left shoulder; (b) the right shoulder; (c) the left hip; and (d) the right hip. Thus, at this point, we have four C chains for each skeleton of each frame (i.e., C t a , C t b , C t c , C t d ). The hypothesis here, introduced by Ke et al. <ref type="bibr" target="#b14">[15]</ref>, is that relative positions between joints provide more useful information than their absolute locations. According to Ke et al. <ref type="bibr" target="#b14">[15]</ref>, these four joints are selected as . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=0</head><p>. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=1</head><p>. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=2</head><p>. . .</p><formula xml:id="formula_0">= .</formula><p>. <ref type="table">.   1 2  2  3 4 3  21  21   1 2  2  3 4 3  21  21   1 2  2  3 4 3  21  21   1 2  2  3 4 3  21</ref>  . . . reference joints due to the fact that they are stable in most actions, thus reflecting the motions of the other joints. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the reference joints technique computation. After dealing with the aforementioned premises, we compute four matrices S (one for each reference joint) that correspond to the concatenation of the chains C t from a video (i.e., S a , S b , S c , S d ), where each column of each matrix denotes the temporal evolution of the arranged chain joint c. At this point, the size of matrix S is J × T × 3, where J is the number of joints of the any reference joint chain C t , T is the total frame number of the video sequence and 3 is the number joint coordinate axes (x, y, z).</p><p>Finally, the generated matrices are normalized into [0, 1] and empirically resized into a fixed size of J × 100 to be used as input to CNNs, since number of frames may vary depending on the skeleton sequence of each video. <ref type="figure" target="#fig_2">Figure 3</ref> gives an overview of our method for building the skeleton image representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convolutional Neural Network Architecture Employed</head><p>To learn the features of the generated skeleton image representations, we adopted a modified version of the CNN architecture proposed by Li et al. <ref type="bibr" target="#b15">[16]</ref>. They designed a small convolutional neural network which consists of three convolution layers and four fully-connected (FC) layers. However, we modified it to a tiny version, employing the convolutional layers and only two FC layers. All convolutions have a kernel size of 3 × 3, the first and second convolutional layers with a stride of 1 and the third one with a stride of 2. Max pooling and ReLU neuron are adopted and the dropout regularization ratio. We opted for using such architecture since it demonstrated good performance and, according to the authors, it can be easily trained from scratch without any pre-training and is superior on its compact model size and fast inference speed as well. <ref type="figure" target="#fig_4">Figure 4</ref> presents an overview of the employed architecture.</p><p>To cope with actions involving multi-person interaction (e.g., shaking hands), we apply a common choice in the literature which is to stack skeleton image representations of different people as the network input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we present the experimental results obtained with the proposed Tree Structure Reference Joints Image (TSRJI) for the 3D action recognition problem. To prove that a good structural organization of joints is important to preserve the spatial relations of the skeleton data, we compare our approach with a baseline employing random joints order when creating the representation (i.e., the creation of the chains' order C t does not take into account any semantic meaning of adjacent joints). Moreover, we also compare with the classical skeleton image representations used by state-ofthe-art approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> as well as to estate-of-the-art methods on the NTU RGB+D 120 <ref type="bibr" target="#b20">[21]</ref>.</p><p>A. Datasets 1) NTU RGB+D 60 <ref type="bibr" target="#b8">[9]</ref>: it is a publicly available 3D action recognition dataset consisting of 56,880 videos from 60 action categories which are performed by 40 distinct subjects. The videos were collected by three Microsoft Kinect sensors. The dataset provides four different data information: (i) RGB frames; (ii) depth maps; (iii) 395 infrared sequences; and (iv) skeleton joints. There are two different evaluation protocols: cross-subject, which split the 40 subjects into training and testing; and cross-view, which uses samples from one camera for testing and the other two for training. The performance is evaluated by computing the average recognition across all classes.</p><p>2) NTU RGB+D 120 <ref type="bibr" target="#b20">[21]</ref>: is the most recent largescale 3D action recognition dataset captured under various environmental conditions and consists of 114,480 RGB+D video samples captured using the Microsoft Kinect sensor. As in NTU RGB+D 60 <ref type="bibr" target="#b8">[9]</ref>, the dataset provides RGB frames, depth maps, infrared sequences and skeleton joints. It is composed by 120 action categories performed by 106 distinct subjects in a wide range of age distribution. There are two different evaluation protocols: cross-subject, which split the 106 subjects into training and testing; and cross-setup, which divides samples with even setup IDs for training (16 setups) and odd setup IDs for testing (16 setups). The performance is evaluated by computing the average recognition across all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>To isolate only the contribution brought by the proposed representation to the action recognition problem, all compared skeleton image representations were implemented and tested on the same datasets and using the same network architecture. We also applied the same split of training and testing data and employed the evaluation protocols and metrics proposed by the creators of the datasets.</p><p>For the network architecture employed, we used a dropout regularization ratio set to 0.5. The learning rate is set to 0.001 and batch size is set to 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation</head><p>In this section, we present experiments for our proposed TSRJI representation and report a comparison with skeleton images baselines and methods of the literature. <ref type="table" target="#tab_1">Table I</ref> presents a comparison of our approach with skeleton image representations of the literature. For the methods that have more than one "image" per representation ( <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b14">[15]</ref>), we stacked them to be used as input to the network. The same was performed for our TSRJI (Stacked) approach considering the images for each reference joint (i.e., S a , ...    <ref type="bibr" target="#b17">[18]</ref> achieving 70.8% of accuracy 69.5%. However, it is worth noting that we achieved a close competitive accuracy of 69.3% with our TSRJI (Stacked) approach. On the other side, the best result on cross-view protocol was obtained by our TSRJI (Stacked) approach achieving 76.7% of accuracy. Compared to Ke et al. <ref type="bibr" target="#b14">[15]</ref>, we achieved an improvement of 1.2 percentage points (p.p.). Moreover, there is an improvement of 1.1 p.p. when compared to the Tree Structure Skeleton Image (TSSI) from Yang et al., <ref type="bibr" target="#b17">[18]</ref>, which was the best baseline result on this protocol. Detailed improvements are shown in <ref type="figure" target="#fig_6">Figure 5</ref>.</p><p>Comparing to the random joints order baseline <ref type="table" target="#tab_1">(Table I)</ref>, it is worth noting an improvement of 1.5 p.p. on cross-subject protocol and 1.5 p.p. on cross-view protocol obtained by our TSRJI (Stacked). This shows the importance of keeping a structural organization of joints that preserves spatial relations of relevant joint pairs, bringing semantic meaning of adjacent joints to the representation.</p><p>We also employed experiments by employing a late fusion technique with our proposed skeleton image representation.</p><p>To that end, each reference isolate joint image S is used as input to a CNN. The late fusion technique applied was a nonweighted linear combination of the prediction scores generated by each CNN output. <ref type="table" target="#tab_1">Table I</ref> presents a comparison of our TSRJI (Late Fusion) with skeleton image representations of the literature. Here, our proposed representation achieved the best results in both protocols of the NTU RGB+D 60 <ref type="bibr" target="#b8">[9]</ref> dataset. We achieved 73.3% of accuracy on cross-subject protocol, with an improvement of 2.5 p.p over the best baseline method (Ke et al. <ref type="bibr" target="#b14">[15]</ref>). Furthermore, we achieved an accuracy of 80.3% on the cross-view protocol with an improvement of 4.7 p.p. when compared to Yang et al., <ref type="bibr" target="#b17">[18]</ref>. Finally, <ref type="table" target="#tab_1">Table II</ref> presents the experiments of our proposed skeleton image representation on the recent proposed NTU RGB+D 120 <ref type="bibr" target="#b20">[21]</ref> dataset. Based on the results achieved in <ref type="table" target="#tab_1">Table I</ref>, we employed the late fusion scheme for our approach.</p><p>According to <ref type="table" target="#tab_1">Table II</ref>, we achieved good results with our TSRJI (Late Fusion) representation outperforming many skeleton based methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b33">[34]</ref>. We achieved state-of-the-art results, outperforming the best reported method (Body Pose Evolution Map <ref type="bibr" target="#b33">[34]</ref>) on cross-subject protocol (accuracy of 65.5%). On the other hand, the best result on cross-setup protocol is obtained by Liu et al. <ref type="bibr" target="#b33">[34]</ref> achieving 66.9 of accuracy.</p><p>To exploit a possible complementarity of the temporal (motion) and spatial skeleton image representations, we employed the late fusion combination scheme of our approach and Li et al. <ref type="bibr" target="#b18">[19]</ref> method that explicitly provides motion information on the representation. With such combination, we achieved state-of-the-art results outperforming the best reported method (Body Pose Evolution Map <ref type="bibr" target="#b33">[34]</ref>) by up to 3.3 p.p. on crosssubject protocol.</p><p>In comparison with LSTM approaches, we outperform the best reported method (Two-Stream Attention LSTM <ref type="bibr" target="#b31">[32]</ref>) by 4.3 p.p. using our TSRJI representation and 6.7 p.p. when combining it with Li et al. <ref type="bibr" target="#b18">[19]</ref> method on cross-subject protocol. Regarding the cross-setup protocol, we obtained similar comparative accuracy (62.8) using our TSRJI fused Ke et al. <ref type="bibr" target="#b14">[15]</ref> Yang et al. <ref type="bibr" target="#b17">[18]</ref> TSRJI (Stacked) Action Number   <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-subject Cross-setup Approach</head><p>Acc. (%) Acc. (%)</p><p>Part-Aware LSTM <ref type="bibr" target="#b8">[9]</ref> 25.5 26.3 Soft RNN <ref type="bibr" target="#b24">[25]</ref> 36.3 44.9 Dynamic Skeleton <ref type="bibr" target="#b25">[26]</ref> 50.8 54.7 Spatio-Temporal LSTM <ref type="bibr" target="#b26">[27]</ref> 55.7 57.9 Internal Feature Fusion <ref type="bibr" target="#b27">[28]</ref> 58.2 60.9 Literature GCA-LSTM <ref type="bibr" target="#b28">[29]</ref> 58.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="59.2">results</head><p>Multi-Task Learning Network <ref type="bibr" target="#b14">[15]</ref> 58.4 57.9 FSNet <ref type="bibr" target="#b29">[30]</ref> 59.9 62.4 Skeleton Visualization (Single Stream) <ref type="bibr" target="#b30">[31]</ref> 60. with Li et al. <ref type="bibr" target="#b18">[19]</ref>. This indicates that our skeleton image representation approach used as input for CNNs leads to a better learning of joint spatial relations than the approaches that employs LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>Since our proposed TSRJI representation is based on the combination of the tree structural organization from Yang et al. <ref type="bibr" target="#b17">[18]</ref> and the reference joints technique from Ke et al. <ref type="bibr" target="#b14">[15]</ref>, we better analyze our achieved results by taking a closer look at actions from NTU RGB+D dataset that our method achieved higher performance than Ke et al. <ref type="bibr" target="#b14">[15]</ref> and Yang et al. <ref type="bibr" target="#b17">[18]</ref>. <ref type="figure" target="#fig_6">Figure 5</ref>, presents the detailed improvements of our TSRJI (Stacked) representation. The actions that were most correctly classified by TSRJI (Stacked) and misclassified by the baselines are: standing up (9); writing (12); tear up paper (13); wear jacket <ref type="bibr" target="#b13">(14)</ref>; wear a shoe (16); take off glasses <ref type="bibr" target="#b18">(19)</ref>; take off a hat cap <ref type="bibr" target="#b20">(21)</ref>; make a phone call (28); playing with phone (29); typing on a keyboard (30); taking a selfie <ref type="bibr" target="#b31">(32)</ref>; check time <ref type="bibr" target="#b32">(33)</ref>; nod head bow (35); shake head (36); wipe face (37); sneeze or cough (41); point finger at the other person (54); touch other person pocket (57); and handshaking (58) <ref type="bibr" target="#b1">2</ref> . We note that the baselines usually confused such actions, which are actions involving arm and hand movements.</p><p>We also analyze our achieved results with the employed late fusion scheme. To better perform such comparison, we combined Ke et al. <ref type="bibr" target="#b14">[15]</ref> and Yang et al. <ref type="bibr" target="#b17">[18]</ref> representations with the same late fusion scheme employed by us. <ref type="figure" target="#fig_7">Figure 6</ref>, presents the detailed improvements of our TSRJI (Late Fusion) representation. For instance, some actions that were most correctly classified by TSRJI (Late Fusion) and misclassified by the baseline are: brushing teeth (3); make a phone call (28); playing with phone (29); typing on a keyboard (30); wipe face (37); and sneeze or cough (41). We note that the baseline confused actions involving arm and hand movements. It shows that our proposed representation performs better and provides a richer discriminability than a simply combination of the based methods.</p><p>The correctly classifications of the aforementioned actions by our TSRJI representation show that feeding the network with explicit structural organization of relevant joint pairs might improve the classification. We believe that the reference  <ref type="table">8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33</ref>  joints technique helped on improving such actions since the shoulders were two of the reference joints. Thus, since such joints are stable they could reflect the motions of the arms and hand joints. Furthermore, the spatial relations of adjacent joint pairs were preserved by the use of the depth-first tree traversal order algorithm bringing more semantic meaning to the representation. We also investigated the cases where our method failed. The most misclassified actions correspond to cases, such as clapping (10), rub two hands together <ref type="bibr" target="#b33">(34)</ref>, reading (11), writing <ref type="bibr" target="#b11">(12)</ref>, typing on a keyboard <ref type="bibr" target="#b29">(30)</ref>, wear a shoe <ref type="bibr" target="#b15">(16)</ref> and take off a shoe <ref type="bibr" target="#b16">(17)</ref>. Our method confused clapping (10) with rub two hands together <ref type="bibr" target="#b33">(34)</ref>, in which both actions are composed by closer movements with the hands. Furthermore, the analysis of the misclassified videos revealed that the method presented difficulties with actions with very similar movements differentiating by the object used (e.g., the action writing (12) is confused with reading (11), typing on a keyboard (30) and playing with phone <ref type="formula">(29)</ref>). Another misclassification of our approach is wear a shoe <ref type="bibr" target="#b15">(16)</ref> with take off a shoe <ref type="bibr" target="#b16">(17)</ref>. Such analysis indicates that the use of explicit motion information could help enhancing the classification. <ref type="figure" target="#fig_8">Figure 7</ref> illustrates the confusion matrix of our TSRJI representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORKS</head><p>In this work, we proposed a novel skeleton image representation to be used as input of CNNs. The method takes advantage of a structural organization of joints that preserves spatial relations of more relevant joint pairs and also by incorporating different spatial relationships between the joints. Experimental results on two publicly available datasets demonstrated the excellent performance of the proposed approach. Another interesting finding is that the combination of our representation with explicitly motion method of the literature improves the 3D action recognition outperforming the stateof-the-art on NTU RGB+D 120 dataset.</p><p>Directions to future works include the evaluation of the proposed representation with other distinct architectures. Moreover, we intend to evaluate its behavior on 2D action datasets with skeletons estimated by methods of the literature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Reference joints technique applied to skeleton data. (a) Chain C t considering 25 Kinect joints. (b) Generated chains C t a , C t b , C t c , C t d considering the reference joints (dark painted joints).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Proposed skeleton image representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Network architecture employed for 3D action recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of TSRJI (Stacked) with Ke et al.<ref type="bibr" target="#b14">[15]</ref> and Yang et al.<ref type="bibr" target="#b17">[18]</ref> on NTU RGB+D 60<ref type="bibr" target="#b8">[9]</ref> dataset for cross-view protocol. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 Ke et al. [15] + Yang et al. [18] (Late Fusion) TSRJI (Late Fusion) Action Number Comparison of TSRJI (Late Fusion) with Ke et al. [15] + Yang et al. [18] (Late Fusion) on NTU RGB+D 60 [9] dataset for cross-view protocol. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 Confusion matrix of TSRJI (Late Fusion) on NTU RGB+D 60 [9] dataset. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ACTION</head><label>I</label><figDesc>RECOGNITION ACCURACY (%) RESULTS ON NTU RGB+D 60 [9] DATASET. RESULTS FOR THE BASELINES WERE OBTAINED RUNNING EACH METHOD IMPLEMENTATION.</figDesc><table><row><cell></cell><cell></cell><cell>Cross-</cell><cell>Cross-</cell></row><row><cell></cell><cell></cell><cell>subject</cell><cell>view</cell></row><row><cell></cell><cell>Approach</cell><cell cols="2">Acc. (%) Acc. (%)</cell></row><row><cell></cell><cell>Random joints order</cell><cell>67.8</cell><cell>74.2</cell></row><row><cell></cell><cell>Du et al. [12]</cell><cell>68.7</cell><cell>73.0</cell></row><row><cell cols="2">Baseline Wang et al. [13]</cell><cell>39.1</cell><cell>35.9</cell></row><row><cell>results</cell><cell>Ke et al. [15]</cell><cell>70.8</cell><cell>75.5</cell></row><row><cell></cell><cell>Li et al. [19]</cell><cell>56.8</cell><cell>61.3</cell></row><row><cell></cell><cell>Yang et al. [18]</cell><cell>69.5</cell><cell>75.6</cell></row><row><cell>Our</cell><cell>TSRJI (Stacked)</cell><cell>69.3</cell><cell>76.7</cell></row><row><cell>results</cell><cell>TSRJI (Late Fusion)</cell><cell>73.3</cell><cell>80.3</cell></row><row><cell>S</cell><cell></cell><cell></cell><cell></cell></row></table><note>b , S c , S d ). Regarding the cross-subject protocol, the best results were obtained by Reference Joints technique from Ke et al. [15] achieving 70.8% of accuracy and the Tree Structure Skeleton Image (TSSI) from Yang et al.,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ACTION</head><label>II</label><figDesc>RECOGNITION ACCURACY (%) RESULTS ON NTU RGB+D 120 [21] DATASET. RESULTS FOR LITERATURE METHODS WERE OBTAINED FROM</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/carloscaetano/skeleton-images</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The number in parentheses represents the action index.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the National Council for Scientific and Technological Development -CNPq (Grants 311053/2016-5, 204952/2017-4 and 438629/2018-3), the Minas Gerais Research Foundation -FAPEMIG (Grants APQ-00567-14 and PPM-00540-17) and the Coordination for the Improvement of Higher Education Personnel -CAPES (DeepEyes Project).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Space-time representation of people based on 3d skeletal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Eigenjoints-based action recognition using nave-bayes-nearest-neighbor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The moving pose: An efficient 3d kinematics descriptor for low-latency action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histogram of oriented displacements (hod): Describing trajectories of human joints for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mining mid-level features for action recognition based on effective skeleton representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<editor>DICTA</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3-d human action recognition by shape analysis of motion trajectories on riemannian manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d action recognition using data visualization and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICMEW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Action recognition based on joint trajectory maps with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Knowledge-Based Systems</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Action recognition with spatiotemporal visual attention on skeleton image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Early action prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition using spatio-temporal lstm network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Skeleton-based online action prediction using scale selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kot Chichung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Pattern Recogn</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Skeletonbased human action recognition with global context-aware attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
