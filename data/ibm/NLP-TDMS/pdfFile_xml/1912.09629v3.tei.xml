<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring the Capacity of an Orderless Box Discretization Network for Multi-orientation Scene Text Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Tong He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">Xinyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjie</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaitao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Lianwen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
						</author>
						<title level="a" type="main">Exploring the Capacity of an Orderless Box Discretization Network for Multi-orientation Scene Text Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Scene text · Text detection · Orderless Box Discretization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-orientation scene text detection has recently gained significant research attention. Previous methods directly predict words or text lines, typically by using quadrilateral shapes. However, many of these methods neglect the significance of consistent labeling, which is important for maintaining a stable training process, especially when it comprises a large amount of data. Here we solve this problem by proposing a new method, Orderless Box Discretization (OBD), which first discretizes the quadrilateral box into several key edges containing all potential horizontal and vertical positions. To decode accurate vertex positions, a simple yet effective matching procedure is proposed for reconstructing the quadrilateral bounding boxes. Our method solves the ambiguity issue, which has a significant impact on the learning process. Extensive ablation studies are conducted to validate the effectiveness of our proposed method quantitatively. More importantly, based on OBD, we provide a detailed analysis of the impact of a collection of refinements, which may inspire others to build stateof-the-art text detectors. Combining both OBD and these useful refinements, we achieve state-of-the-art performance on various benchmarks, including ICDAR 2015 and MLT. Our method also won the first place in the text detection task at the recent ICDAR2019 Robust Reading Challenge for Reading Chinese Text on Signboards, further demonstrating its superior performance. The code is available at https://git.io/TextDet. * Corresponding authors.</p><p>(a) Previous regression-based methods. (b) Our proposed OBD. <ref type="figure">Fig. 1</ref>: Comparison of (a) previous methods and (b) our proposed OBD. Previous methods directly regress the vertices, which can often be adversely affected by inconsistent labeling of training data, resulting in unstable training and unsatisfactory performances. Our method tackles this problem and removes the ambiguity by discretizing a quadrilateral bounding box that is orderless.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scene text detection in arbitrary orientations has garnered significant attention in computer vision because of its numerous potential applications, including augmented reality and robot navigation. Scene text detection is also the foundation and prerequisite for text recognition, which provides a reliable and straightforward approach to scene understanding. However, this challenge remains largely unsolved because text instances in natural images are often of multiorientation, low-quality representations, having perspective distortions of various sizes and scales.</p><p>In the literature, several methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> have been developed for solving horizontal scene text detection. However, scene text in the wild is typically presented in a multi-orientation form, attracting a few recent studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">16,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b16">18]</ref> that can be roughly categorized into two groups: segmentation and regression-based methods. Segmentation-based methods often employ networks, such as fully convolution networks (FCNs) <ref type="bibr" target="#b17">[19]</ref> and Mask R-CNN <ref type="bibr" target="#b18">[20]</ref>. Segmentation-based methods have become the mainstream approach, because they are sufficiently robust in many complicated scenarios. One limitation is that segmented text instances often require additional post-processing steps. For example, the segmentation results obtained by Mask R-CNN must be fitted into rotated quadrilateral bounding boxes, which necessitates a number of heuristic settings and geometric assumptions.</p><p>On the other hand, Regression-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b23">25]</ref> are comparatively simple. For multi-orientation text, explicitly predicting the vertices obtains the four boundaries of the text instances. Thus, no additional grouping procedure is required. Although these methods can directly predict vertex positions, the significance of regression without facing inconsistent labeling has rarely been discussed. Consider the efficient and accurate scene text (EAST) detector <ref type="bibr" target="#b23">[25]</ref> method as an example. In EAST, each feature within a text instance is responsible for regressing the corresponding quadrilateral bounding box by predicting four distances to the boundaries and a rotation angle from the viewpoint. A pre-processing step to assign regression targets is required. As shown in <ref type="figure" target="#fig_17">Figure 1</ref>, the regression targets can be altered drastically, even with a minor rotation. Such ambiguities lead to an unstable training process, which considerably degrades the performance. Our experiments indicate that the accuracy of EAST <ref type="bibr" target="#b23">[25]</ref> deteriorates sharply (by more than 10%) when equipped with a random rotation technique for data augmentation, which is supposed to boost the performance.</p><p>To address this problem, we propose a novel method, (i.e., Orderless Box Discretization (OBD)), which consists of two modules: Key Edges Detection and Matching-Type Learning. The fundamental idea is to employ invariant representations (e.g., minimum x, minimum y, maximum x, maximum y, mean center point, and intersecting point of the diagonals) that are irrelevant to the label sequence to deduce the bounding box coordinates inversely. To simplify the parameterization, the OBD method first locates all discretized horizontal and vertical edges that contain a vertex. Then, a sequence labeling matching type is learned to determine the best-fit quadrilateral. By avoiding the ambiguity of the training targets, our approach successfully improves performance when a large amount of rotated data is involved.</p><p>We complement our method with a few critical technical innovations that further enhance performance. We conduct extensive experiments and ablation studies based on our method to explore the influence of six relevant issues: (namely, data arrangement, pre-processing, backbone, proposal generation, prediction head, and post-processing) to determine the significance of the various components. We thus provide useful tips for designing state-of-the-art text detectors. Leveraging OBD and these useful refinements, we won first place in the task of Text Line Detection at the IC-DAR2019 Robust Reading Challenge on Reading Chinese Text on Signboards.</p><p>Our main contributions are summarized as follows.</p><p>1. Our method addresses the inconsistent labeling issue of regression-based methods, which is of great importance for achieving good detection accuracy. 2. The flexibility of our proposed method allows us to make use of several key refinements that are critical to further boosting accuracy. Our method achieves stateof-the-art performance on various scene text detection benchmarks, including ICDAR2015 <ref type="bibr" target="#b24">[26]</ref> and MLT <ref type="bibr" target="#b25">[27]</ref>. Additionally, our method won the first place in the Text Detection task of the recent ICDAR2019 Robust Reading Challenge on Reading Chinese Text on Signboard.</p><p>Based on the detection results, we integrate advanced recognition models to achieve state-of-the-art results. 3. Our method can be generalized to ship detection in aerial images without minimum modification. The significant improvement in terms of the TIoU-Hmean metric further demonstrates the robustness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently, the emergence of new datasets <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b29">31]</ref> has propelled arbitrarily shaped scene text detection to mainstream research. Multi-orientation scene text detection is one of its most important representations, because multiorientation scene text comprises most of the text found in real-world visual scenes. The computer-driven detection task remains complex, and there is much room for improvement with regards to decoding multi-orientation text from pictures. Hence, detection benchmarks, such as the MLT <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b30">32]</ref> dataset, are leveraged to refine the process. However, using quadrilateral bounding boxes can result in some problems for both current segmentation and nonsegmentation-based methods.</p><p>Segmentation-based. Segmentation-based methods <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b36">38]</ref> usually require additional steps to group pixels into polygons.</p><p>Non-segmentation-based. Non-segmentation based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b23">25]</ref> can directly learn the exact bounding box for localizing the text instances, but they are easily affected by the label sequence. Usually, such methods use a typical sorting method of the coordinate sequence to alleviate this issue. However, the solutions are not robust because the entire sequence may change even with a small amount of interference. To clarify this, we discuss some of the previous solutions as follows:</p><p>-Given an annotation having coordinates of four points, a common sorting method of the coordinate sequence   (b) Textboxes++ <ref type="bibr" target="#b13">[14]</ref>.  to alleviate this issue is to choose the point having the minimum x as the first point, then deciding the rest of the points in a clockwise manner. However, this protocol is not robust. Considering the horizontal rectangle as an example, using this protocol, let us decide that the first point is the top-left point. Thus, the fourth point is the bottom-left point. Suppose that the bottom-left point moves leftward one pixel (which is possible because of the inconsistent labeling). In that case, the original fourth point becomes the first point, and the whole sequence changes, resulting in very unstable learning.</p><p>-As shown in <ref type="figure" target="#fig_4">Figure 2</ref>(a), DMPNet <ref type="bibr" target="#b7">[8]</ref> proposed a protocol that uses the slope to determine the sequence. However, if the diagonal is vertical, leftward, or rightward, change of a pixel can result in a completely different sequence. -As shown in <ref type="figure" target="#fig_4">Figure 2</ref>(b), given four points, Textboxes++ <ref type="bibr" target="#b13">[14]</ref> uses the distances between the annotation points and the vertices of the circumscribed horizontal rectangle to determine the sequence. However, if q 1 and q 4 have the same distance to p 1 , and one pixel rotation can completely change the whole sequence. -As shown in <ref type="figure" target="#fig_4">Figure 2</ref>(c), QRN [24] first finds the mean center point of the four given points then constructs a Cartesian coordinate system. Using the positive x axis, QRN ranks the intersection angles of the four points and chooses the point having the minimum angle as the first. However, if the first point is in the positive x axis, one pixel change upward or downward will result in an entirely different sequence.</p><p>Although these methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">24]</ref> can alleviate confusion to some extent, the results can be significantly undermined when using pseudo samples having large degrees of rotation.</p><p>Unlike these methods, our method is the first to directly produce a compact quadrilateral bounding box without complex post-processing. Moreover, it can completely avoid inconsistent labeling issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>Our proposed scene text detection system consists of three core components: an Orderless Box Discretization (OBD) block, a matching-type learning (MTL) block, and rescoring and post-processing (RPP) block. <ref type="figure" target="#fig_5">Figure 3</ref> illustrates the overall pipeline of the proposed framework, and more details are presented in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Orderless Box Discretization</head><p>The purpose of multi-orientation scene text detection is to accurately localize the textual content by generating outputs in the form of rectangular or quadrilateral bounding boxes. Compared with rectangular annotations, quadrilateral labels demonstrate an increased capability to cover effective text regions, especially for rotated texts. However, as discussed in Section 2, simply replacing rectangular bounding boxes with quadrilateral annotations can introduce inconsistency because of the sensitivity of the non-segmentationbased methods to label sequences. As shown in <ref type="figure" target="#fig_17">Figure 1</ref>, the detection model might fail to obtain accurate features for the corresponding points when facing small disturbances. One possible reason behind this is that the neural-networkbased regressor for bounding box prediction is essentially a nonlinear continuous function, which means that each input is only mapped to one output. Thus a non-function or a function with a steep gradient cannot be effectively fitted. In our case, a small disturbance may completely change the whole sequence of the vertex and thus a similar input may result in completely different output as well as a steep gradient. Therefore, instead of predicting sequencesensitive distances or coordinates, an OBD block is proposed to discretize the quadrilateral box into eight Key Edges (KE) comprising order-irrelevant points; i.e., minimum x(x min ) and y(y min )), the second-smallest x(x 2 ) and y(y 2 ), the second-largest x(x 3 ) and y(y 3 ), and the maximum x(x max ) and y(y max ) (see <ref type="figure" target="#fig_17">Figure 1</ref>). We use x-KEs and y-KEs in the following sections to represent [x min , x 2 , x 3 , x max ] and [y min , y 2 , y 3 , y max ], respectively.  Specifically, the proposed approach is based on the widely used generic object detection framework, Mask R-CNN <ref type="bibr" target="#b18">[20]</ref>. As shown in <ref type="figure" target="#fig_7">Figure 4</ref> In practice, OBD does not directly learn the x-KEs and y-KEs because of the restriction of the region of interest (RoI). Specifically, the original Mask R-CNN framework limits the prediction inside the RoI areas. Thus, if the regression bounding box is not accurate, the missing pixels outside of the bounding box will not to be restored. To solve this problem, the x-KEs and y-KEs are encoded in the form of "half lines" during training. Suppose we have x-KEs,  Then, the "half lines" are defined as follows:</p><formula xml:id="formula_0">x i ∈ [x min , x 2 , x 3 , x max ],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MTL Block</head><formula xml:id="formula_1">x i hal f = x i +x mean 2 , y i hal f = y i +y mean 2 ,<label>(1)</label></formula><p>where x mean and y mean represent the value of the mean central point of the ground-truth bounding box for the x and y axes, respectively. By employing such a training strategy, the proposed OBD block can break the RoI restriction (see <ref type="bibr">Figure 5)</ref>. Thus, it is more likely to produce accurate bounding box because x hal f and y hal f fall into the area of the RoIs in most cases, even if the border of the text instance is located outside the RoIs. Similar to Mask R-CNN, the overall detector is trained in a multi-task manner. Thus, the loss function comprises four terms:</p><formula xml:id="formula_2">L = L cls + L box + L mask + L ke ,<label>(2)</label></formula><p>where the first three terms, L cls , L box and L mask , follow the same settings as presented in <ref type="bibr" target="#b18">[20]</ref>. L ke is the cross-entropy loss, which is used for learning the Key Edges prediction task. The authors made an interesting observation in which the additional keypoint branch can harm the bounding box detection performance <ref type="bibr" target="#b18">[20]</ref>. However, based on our experiments (see <ref type="table" target="#tab_2">Tables 1 and 2)</ref>, the proposed OBD block is the key component that significantly boosts the detection accuracy. There may be two reasons for this. First, ours is different from the keypoint detection task, which has to learn M 2 classes against each other. Thus, the numbers of competitive pixels in the OBD block is only M. Second, for the keypoint detection task, neither one-hot point nor a small circled area can be used to describe the target keypoint accurately, while the KEs produced by OBD are well defined. Thus, our method may provide more accurate supervision for training the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Matching-Type Learning</head><p>It is noteworthy that the OBD block only learns to predict the numerical values of eight KEs but is unable to predict the connection between the x-KEs and y-KEs. Therefore, we need to design a proper matching procedure to reconstruct the quadrilateral bounding box from the KEs. Otherwise, the incorrect matching type may lead to completely unreasonable results (see <ref type="figure" target="#fig_10">Figure 6</ref>).</p><p>As described in Section 3.1, there are four x-KEs and four y-KEs outputted by the OBD block. Each x-KE should match one of the y-KEs to construct a corner point, such as (x min , y min ), (x 2 , y max ), and (x max , y 2 ). Then, all four constructed corner points are assembled for the final prediction, giving us the quadrilateral bounding box. It is important to note that different orders of the corners would produce different results. Hence, the total number of matching-types between the x-KEs and y-KEs can be simply calculated by  <ref type="figure" target="#fig_7">Figure 4</ref>, the feature maps that are used for predicting the x-KEs and y-KEs are used for classifying the matching-types. Specifically, the output feature of the deconvolution layer is connected to a convolutional layer having an M/2 × M/2 kernel size with 24 output channels. Thus, the matching procedure is formed as a 24-category classification task. In our method, the MTL head is trained by minimizing the cross-entropy loss, and the experiments demonstrate that the convergence speed is very fast.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Re-scoring and Post-processing</head><p>The fact that the detectors can sometimes output high confidence scores for false positive samples is a long-standing issue in the detection community for both generic objects and text. One possible reason for this may be that the scoring head used in most of the current literature is supervised by the softmax loss, which is designed for classification but not for explicit localization. Moreover, the classification score only considers whether the instance is foreground or background, and it shows less sensitivity to the compactness of the bounding box. Therefore, a confidence RPP block, is proposed to suppress unreasonable false positives. Specifically, RPP adopts a policy similar to multiple expert systems to reduce the risk of outputting high scores for negative samples. In RPP, an OBD score S OBD is first calculated based on eight KEs (four x-KEs and four y-KEs):</p><formula xml:id="formula_3">S OBD = 1 K K ∑ k=1 max v k f v k ,<label>(3)</label></formula><p>where K = 8 is the number of KEs, v k is the output score vector of the k th KE shown in (4), and f (v k ) is defined to sum up the peak value, v i , and its neighbors. As shown in <ref type="figure" target="#fig_11">Figure 7</ref>(a), the distribution of S OBD demonstrates a onepeak pattern in most cases. Nonetheless, the peak value is still significantly lower than 1. Hence, we sum up four adjacent scores that are near the peak value for each KE score to avoid a confidence score that is too low.</p><formula xml:id="formula_4">v k = [v 1 , v 2 , ..., v i−2 , v i−1 , v i , v i+1 , v i+2 f (v k )=∑ P=min(n,i+2) p=max(i−2,1) (v p ) , ...v n ].<label>(4)</label></formula><p>It is important to note that the number of adjacent values will be less than four if the peak value is located at the head or tail of the vector. Thus, only the existing neighbors should be counted. Finally, the refined confidence can be obtained by:</p><formula xml:id="formula_5">score = (2 − γ)S box + γS OBD 2 ,<label>(5)</label></formula><p>where 0 ≤ γ ≤ 2 is the weighting coefficient and S box is the original softmax confidence for the bounding box. Because both S box and S OBD are both between [0,1], the value of score(ℜ) is also between [0, 1]. Counting the S OBD into the final score enables the proposed detector to draw lessons from multiple agents (eight KE scores) while enjoying the benefits of a tightness-aware confidence supervised by the KE prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>It has been proven that the detection performance can be often boosted with the multi-task learning framework. For example, as shown in <ref type="bibr" target="#b18">[20]</ref>, simultaneously training a detection head with an instance segmentation head can significantly improve the detection accuracy. Similarly, a segmentation head is also employed in the proposed OBD network to predict the area inside the bounding box, which forces the model to regularize pixel-level features to enhance both performance and robustness. However, some issues associated with the segmentation head are highlighted in <ref type="figure">Figure 8</ref>. In (a), the segmentation mask can sometimes produce false positive pixels while the OBD prediction remains correct. In (b), the segmentation head fails to maintain some positive samples that have been successfully detected by the OBD block. Therefore, compared with some segmentation-based approaches that directly reconstruct the bounding box by exploiting the segmentation mask, the MTL block can learn geometric constraints to avoid false positives caused by an inaccurate segmentation output. This also reduces the heavy reliance on the segmentation task. Specifically, as shown in <ref type="figure" target="#fig_10">Figure 6</ref>(b), the blue dashed line matches an invalid shape that violates the definition of a quadrilateral, because the sides should only have two intersections, at the head and tail. By simply removing these abnormal results, the MTL block can further eliminate some false positives that might cheat the segmentation branch. Another interesting observation is that the RPP block exhibits a strong capability to suppress false positives, making predictions more reliable. To provide an analysis, we visualize the term S OBD , which is used in the RPP block (see Equation <ref type="formula" target="#formula_5">(5)</ref>). Doing so, we find that there are two typical patterns for the KE scores output by the OBD block, as shown in <ref type="figure" target="#fig_11">Figure 7</ref>. Sub-figure (a) shows a one-peak pattern, and subfigure (b) shows a multi-peak pattern. In normal cases, the KE scores show a regular pattern, in which there is only one peak value in the output vector (see <ref type="figure" target="#fig_11">Figure 7</ref>(a)). However, with hard negative samples, two or more peak values appear (see Figures 7(b), 7(c), and 7(d)). These multiple peaks share confidence, and the total score is normalized to one. Therefore, based on Equations <ref type="formula" target="#formula_3">(3)</ref> and <ref type="formula" target="#formula_5">(5)</ref>, the final score will be decreased such that the proposed model is less likely to output high confidence for those false-positive instances.</p><p>Based on our observation, we find that the matchingtype prediction could be wrong even if KE is accurate. An example is shown in the bottom instance of the lower-right corner image of <ref type="figure" target="#fig_7">Figure 14</ref> (b), where x min is mistakenly matched to y min . If x min and the second smallest x change their matching y key edge, the detection result can be tighter. Although such a case does not obviously affect both the detection and recognition performance, it is an underlying weakness of the MTL. It is worth mentioning that sometimes the matching type may form an irregular bounding box, i.e., the sides have self-intersection. We find that such cases are very rare and mostly occur with false negatives. For such irregular results, we simply remove them.</p><p>4 Ablation studies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Our model is implemented using PyTorch. We first evaluate the proposed components of our methods. The initial learning rate is set to 0.01, which is decreased by 10 at 10,000 iterations and 15,000 iterations. The maximum iterations is 20,000 and the image batch size is set to 4. The shorter size of the input image is randomly scaled from 680 to 1000 with the interval of 40, while the maximum size is set to 1480. The weights of KE and matching type learning are set to 0.1 and 0.01, respectively. Flip, random crop, and random rotation are used to improve the generalization ability. Unless specified otherwise, the re-scoring ratio is kept to be 1.4.</p><p>For ablation studies of refinements, each experiment uses a single network that is a variation of our baseline model (first row of <ref type="table">Table 5</ref>). Each network is trained on the official ReCTS training set unless specified otherwise. Additionally, because the test scale may significantly influence the final detection result, the testing max size is fixed at 2,000 pixels, and the scale is fixed to 1,400 pixels for strictly fair ablation experiments. The ratio of the flip is also fixed at 0.5, which is the flipping probability for deciding whether 1 <ref type="figure">Fig. 8</ref>: Compared with the segmentation head, the proposed KE head predicts more compact bounding boxes and shows a higher recall rate for instances that were missed by segmentation. Colored quadrangles are the final detection results, whereas white transparent areas are the mask predictions grouped by the minimum area rectangle.</p><p>to horizontally flip the images for data augmentation. Results are reported on the validation set of ReCTS based on the widely used main performance metric, Hmean. We also report the best confidence threshold that leads to the best performance, which can also reveal some important information.</p><p>The number of iterations for training one network is set to 80,000 iterations, with a batch size of four images per GPU on four 1080ti GPUs. The final cumulative model is trained for 160 epochs on four V100 GPUs, which takes approximately 6 days. The baseline model employs ResNet-101-FPN as the backbone, which is initialized by a model pretrained on the MLT <ref type="bibr" target="#b25">[27]</ref> data. We only use fixed batch normalization for the stem and bottleneck, i.e., the batch statistics and the affine parameters are fixed. For all prediction heads, we do not use batch normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation studies of the proposed method</head><p>In this section, we report ablation studies on the ICDAR 2015 <ref type="bibr" target="#b24">[26]</ref> dataset, to validate the effectiveness of each component of our method. First, we evaluate the influence of the proposed modules on performance. The results are presented in <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_12">Figure 9</ref>. From <ref type="table">Table 1</ref>, we can see that OBD and RPP can lead to improvements of 2.4 and 0.6%, respectively, in terms of Hmean. Additionally, <ref type="figure" target="#fig_12">figure  9</ref> shows that our method can substantially outperform the baseline Mask R-CNN under different confidence thresholds, further demonstrating its effectiveness.</p><p>Furthermore, we conduct experiments by comparing the mask and KE branches (including OBD and RPP) on the same network. Thus, we test only on one of the branches. We simply use the provided training samples of IC15 without any data augmentation. The results are presented in <ref type="table" target="#tab_2">Table 2</ref>, verifying that the proposed modules can effectively improve the scene text detection performance.</p><p>More importantly, we also conduct experiments to verify that introducing ambiguity in the training is harmful to <ref type="table">Table 1</ref>: Ablation studies demonstrating the effectiveness of the proposed method. The γ of RPP is set to 1.4 (best practice). The results on this table also adopt MLT training data and data augmentation strategies to help improve the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Algorithms Hmean</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICDAR2015</head><p>Mask R-CNN baseline 83.5% Baseline + OBD 85.9% (↑ 2.4%) Baseline + OBD + RPP 86.5% (↑ 3.0%)  achieving good results. Specifically, by using the same configuration, we first train Textboxes++ <ref type="bibr" target="#b13">[14]</ref>, EAST <ref type="bibr" target="#b23">[25]</ref>, CTD <ref type="bibr" target="#b27">[29]</ref>, APE <ref type="bibr" target="#b37">[39]</ref> (the champion method of DOAI2019 competition task1), and the proposed method with the original 1,000 training images of the ICDAR 2015 dataset. Then, we randomly rotate the training images</p><formula xml:id="formula_6">[0 • , 15 • , 30 • , ..., 360 • ]</formula><p>and randomly select additional 2,000 images from the rotated dataset to fine-tune these models. We also randomly select additional 2,000 images that are between [−30 • , 30 • ] to evaluate the difference under lower rotation degree. The results are presented in <ref type="table" target="#tab_3">Table 3</ref>. Our method can effectively address the inconsistent labeling issue without drastically degrading the accuracy. Furthermore, as shown in <ref type="table" target="#tab_4">Table 4</ref>, our proposed method exhibit higher robustness under various degrees of rotation.</p><p>Note for the resnet-50 version and the following final competition version of our method, the inference time is 4.5 FPS and 0.83 FPS, respectively. The speed is tested using a single NVIDIA GTX 2080 Ti and the short size of the input image is scaled to 1,000.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies of refinements based on our method</head><p>In this section, we provide a detailed analysis of the impact of refinements based on the proposed methods, to evaluate the limits of our method and whether it can be mutually promoted by existing modules. By combining effective refinements, our method achieves first place in the detection task of the ICDAR 2019 Robust Reading Challenge on Reading Chinese Text on Signboards.</p><p>In the following sections, we present an extensive set of experiments that rate our baseline model. Thus, we present results of OBD having alternative architectures and different strategies with respect to six relevant components for training, including data arrangement, pre-processing, backbone, proposal generation, prediction head, and post-processing.</p><p>The objective is to show that the proposed model corresponds to a local optimum in the space of architectures and parameters and to evaluate the sensitivity of the final performance to each design choice. The following discussions follow the structure of <ref type="table">Table 5</ref>. Note that the significant breadth and exhaustivity of the following experiments represent more than 3,000 GPU hours of training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Competition Dataset</head><p>The competition dataset, Reading Chinese Text on Signboards (ReCTS), is a practical and challenging multiorientation natural scene text dataset containing 25,000 signboard images. A total of 20,000 images are used for the training set, with a total of 166,952 text instances. The remaining 5,000 images are used for the test set. Examples of this dataset are shown in <ref type="figure" target="#fig_13">Figure 10</ref>. The layout and arrangement of Chinese characters in this dataset are clearly differ- ent from those in other benchmarks. Because the function of a signboard is to attract a customer base, it is very common to notice their aesthetic appearance. Thus, the Chinese characters can be arranged in any kind of layout with various fonts. Additionally, characters from one word can be in diverse orientations, diverse fonts, or diverse shapes, which complicates the challenge. This dataset provides both text lines and character annotations to inspire new algorithms that can take advantage of the arrangement of characters. To evaluate the function of each component, we split the original training set into 18,000 training images and 2,000 validation images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study of data arrangement</head><p>Considering the image diversity and the consistency and quality of annotation, we collected a 60,000-item dataset for pretraining, which consisted of 30,000 images from the LSVT <ref type="bibr" target="#b28">[30]</ref> training set, 10,000 images from the MLT 2019 <ref type="bibr" target="#b30">[32]</ref> training set, and 5,603 images from ArT <ref type="bibr" target="#b29">[31]</ref>, which contained all the images from SCUT-CTW1500 <ref type="bibr" target="#b27">[29]</ref> and Total-text <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b38">40]</ref>. The remaining 14,859 images were selected from RCTW-17 <ref type="bibr" target="#b39">[41]</ref>, ICDAR 2015 <ref type="bibr" target="#b24">[26]</ref>, IC-DAR 2013 <ref type="bibr" target="#b40">[42]</ref>, MSRA-TD500 <ref type="bibr" target="#b41">[43]</ref>, COCO-Text <ref type="bibr" target="#b42">[44]</ref>, and USTB-SV1K <ref type="bibr" target="#b43">[45]</ref>. Note that we transferred polygonal annotations to the minimum area rectangle for training.</p><p>The ablation results are presented in <ref type="table">Table 5</ref>. If we only were to use the pretrained data without the split training data from the ReCTS, the result in the ReCTS validation set would be significantly worse than that of the baseline, even if the pretrained model were trained with more iterations. This is because the diversity and annotation granularity of the selected pretrained dataset is still very different from that of the ReCTS dataset. However, using the <ref type="table">Table 5</ref>: Ablation studies of different refinements based on our method. Each variation is evaluated on the ReCTS validation set. It is worth mentioning that we regard difficult samples as true negatives in the validation because they cannot be recognized and only loosely annotated in the competition dataset. However, in the final ranking, detection box in the difficult region are set to "do not care", which can result in a leap improvement. We evaluate variations of our baseline model (second row). Every row corresponds to one variation in different part. We train each variation with ResNet-101-FPN and fixed random seeds and equal 80,000 iterations (unless specifying) and report Hmean in the best confident threshold (grid search).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Best </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation study of pre-processing</head><p>Our baseline model used a pretrained model having only a flip strategy for data augmentation. We compared the baseline with various other data augmentation methods.</p><p>Cropping and rotation. Without introducing extra parameters or training/testing times, the results presented in <ref type="table">Table  5</ref> verify that both rotation and data cropping augmentation strategies improved the detection results. We further conducted a sensitivity analysis of how the ratios of using these two strategies influence the performance, as shown in <ref type="figure" target="#fig_14">Figure  11</ref>. Some useful findings can be derived from <ref type="figure" target="#fig_14">Figure 11</ref>(a), as summarized below.</p><p>-With appropriate ratios, three rotated degrees (30 • , 15 • , and 5 • ) outperformed the baseline method in most ratios, with 0.5, 0.6, and 0.4%, respectively. -Under a 0.1 rotated ratio, the performances with the three rotated degrees were all worse than the baseline. This may be because the pseudo samples changed the distribution of the original dataset, whereas very few pseudo samples were insufficient to improve the generalization ability. Conversely, the ratios to achieve the best results for various rotated degrees always lie between 0.3 and 0.8, which empirically suggests that using a medium ratio for the rotated data augmentation strategy might be a suitable choice. -We can also see that the performance using a rotated angle of 15 • was consistently better than that with 30 • and 5 • .</p><p>Compared with the rotated data augmentation strategy, the random cropping strategy significantly improved detection performance. The best performance, as shown in Table 5, achieved a 1.9% improvement in terms of Hmean, compared with the baseline method. Sensitivity analysis, as shown in <ref type="figure" target="#fig_14">Figure 11(b)</ref>, was also conducted, revealing that, as <ref type="bibr" target="#b76">78</ref>  the crop ratio improved, the performance also tended to improve. The result suggests that always using the crop strategy was conducive to improving the detection results. Note that a crop ratio of 0.1 only improved the Hmean by 0.5%, whereas other ratios improved it by more than 1%, which is similar to the phenomenon when using a rotated ratio 0.1.</p><p>Color jittering. We also conduct a simple ablation study to evaluate the performance of color jittering. Based on the same settings as of the baseline method, we empirically set the ratios of brightness, contrast, saturation, and hue to 0.5, 0.5, 0.5, and 0.1, respectively. The ratio represents the degree of disturbance of each specific transformation. The results in <ref type="table">Table 5</ref> indicate that using color jittering data augmentation slightly improved the result by 0.2% in terms of Hmean.</p><p>Training image scale. The training image scale/size is specifically important for a scene text detection. To evaluate how the training scale influences the results of our method, we used two parameters (i.e., scale and MaxSize) to control the training scale. The first item resized the minimum side of the image to a specific parameter. In our implementation, there are a set of values for random scaling. The second item restricts the maximum size of the image sides. The value of scale must be less than MaxSize, and the entire scaling process strictly retains the  <ref type="table" target="#tab_7">Table 6</ref>, which verify the following: 1) a larger training scale requires a larger testing scale for the best performance. 2) As the larger training scale increases, so does the performance. Note that, although a larger training scale can improve performance, it is costly and may require significantly more GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation study of the backbone</head><p>A well-known hypothesis is that a deeper and wider network architecture delivers better performance than does a shallower and thinner one. However, increasing the network depth naively will significantly increase the computational cost with only limited improvement. Therefore, we investigate different types of backbone architectures. The results are shown in <ref type="table">Table 5</ref> and are summarized as follows:</p><p>-By changing the backbone, ResNet-101-FPN of the baseline model into a ResNeXt-152-32x8d-FPN-IN5k, Hmean can be increased by 2.5%. Note that the pretrained model of ResNeXt-152-32x8d-FPN-IN5k was pretrained on ImageNet using the Facebook Detectron framework. -Atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b44">[46]</ref> is effective in semantic segmentation, which is known for its function in increasing the receptive field. However, in this scene text detection task, using ASPP in the KE head or backbone reduced performance by 1.1 and 2.1%, respectively. One possible reason is that the change in network architecture usually requires more iterations. However, the best confidence thresholds for the best performance using ASPP were 0.91 and 0.89, which are similar to the best threshold of the baseline model, suggesting that the network had already converged.</p><p>-Deformable convolution <ref type="bibr" target="#b45">[47]</ref> is an effective module used for many tasks. It adds 2D offsets to the regular sampling grid of the standard convolution, allowing free form deformation of the convolutional operation. This is suitable for scene text detection, owing to the mutable characteristics of the text. We experimented with three methods of deformable convolution by adding deformable convolutions from the C4-1, C4-2, and C3 of the backbone, and the results show that the performance could be significantly improved by 2.6, 2.5, and 2.5%, respectively, in terms of Hmean. -Motivated by the panoptic feature pyramid networks <ref type="bibr" target="#b46">[48]</ref>, we also tested whether a panoptic segmentation loss was useful for scene text detection. To this end, we used a dice loss in the output of the FPN for panoptic segmentation, which had two classes: background and text. The result in <ref type="table">Table 5</ref> indicates that Hmean was reduced by 0.1%. However, the best threshold was 0.67, which indicates that the background noise may have somehow reduced the confidence of the training procedure. -The pyramid attention network (PAN) <ref type="bibr" target="#b47">[49]</ref> is a novel structure that combines an attention mechanism and a spatial pyramid to extract precise dense features for semantic segmentation tasks. Because it can effectively suppress false alarms caused by text-like backgrounds, we integrated it into the backbone and tested its function. The results show that using PAN led to a 1.2% improvement in terms of Hmean, but it also increased the computational cost with an increase of 2.4 GB video memory. -The multi-scale network (MSN) <ref type="bibr" target="#b20">[22]</ref> is robust for scene text detection because it employs multiple network channels to extract and fuse features at different scales concurrently. In our experiment, integrating MSN into the backbone also increased the performance by 1.2% in terms of Hmean. Note that, compared with PAN, the recall of the MSN was much better under a higher best threshold, which suggests that different architectures may have had different functions related to the performance of the detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Ablation study on proposal generation</head><p>The proposed model is based on a two-stage framework, and the region proposal network (RPN) <ref type="bibr" target="#b48">[50]</ref> is used as the default proposal generation mechanism. Previous studies have modified the anchor generation mechanism, including DMPNet <ref type="bibr" target="#b7">[8]</ref>, DeRPN <ref type="bibr" target="#b49">[51]</ref>, Kmeans anchor <ref type="bibr" target="#b50">[52]</ref>, scale-adaptive anchor <ref type="bibr" target="#b51">[53]</ref>, and guided anchor <ref type="bibr" target="#b52">[54]</ref>, to improve the results. For simplicity, we retrain the default RPN structure with the statistical setting of the anchor box based on the training set. The other important part in this proposal generation stage is the sampling process, (e.g., RoI pooling <ref type="bibr" target="#b48">[50]</ref>, RoI align <ref type="bibr" target="#b18">[20]</ref> (our default setting), and PSRoI pooling <ref type="bibr" target="#b53">[55]</ref>. We choose to evaluate Deformable PSRoI Pooling <ref type="bibr" target="#b45">[47]</ref> for our method, because it has been effective for scene text detection <ref type="bibr" target="#b54">[56]</ref>, and the flexible process may be beneficial to the proposed OBD. The result is shown in <ref type="table">Table 5</ref>: using deformable PSRoI Pooling improved the baseline method by 0.9% in terms of Hmean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Ablation study on the prediction head</head><p>The final part of the two-stage detection framework is the prediction head. To clearly evaluate the effectiveness of the components, ablation experiments are separately conducted on different heads.</p><p>Box head. Empirically, online hard negative examples mining (OHEM) <ref type="bibr" target="#b55">[57]</ref> is not always effective with respect to different benchmarks. For example, using the same framework minus the training data can significantly improve the results with the ICDAR 2015 benchmark <ref type="bibr" target="#b24">[26]</ref> while reducing the results on the MLT benchmark <ref type="bibr" target="#b25">[27]</ref>. This finding may be related to the data distribution, which is difficult to trace.</p><p>Thus, we test three versions of the OHEM in the validation set. The first version, OHEMv1, is the same as the original implementation; the second version, OHEMv2, simply ignores the top 5 hard examples to avoid outliers. These two versions have the same ratio, which is set to 0.25. The third version, OHEMv3, simply uses a higher ratio (0.5) to guarantee more hard samples and less easy samples. The results in <ref type="table">Table 5</ref> show that three versions all reduce Hmean, by 0.7, 0.8, and 0.5, respectively. Note that using OHEM will also result in the reduction of the best confidence, which means that the forced learning of hard examples can reduce the confidence of normal examples. Conversely, we also evaluated the performance of the cascade R-CNN, and the results are shown in <ref type="table" target="#tab_8">Table 7</ref>. However, the results show that using a cascade does not result in further improvements.</p><p>Mask head. To improve the mask head, we evaluate two methods (i.e., mask scoring <ref type="bibr" target="#b47">[49]</ref>), as shown in <ref type="table">Table 5</ref>. The results show that modification of the mask head does not contribute to the detection performance. However, the mask prediction results are visually more compact and accurate compared with the baseline.</p><p>Character head. It is well known that stronger supervision can result in better performance. Because the competition also provides a character ground truth, we build and evaluate the performance of an auxiliary character head. The implementation of the character head is exactly the same as that for the box head, except for the ground truth. Unlike the box, mask, and KE head, the proposed character head is built on a different RPN. Thus, the character head does not share the same proposal with the other heads. The KE head directly produces a quadrilateral bounding box (word box) directly used for the final detection, and we test whether the auxiliary head could indirectly (shared backbone) improve the word-box detection performance. The ablation results in <ref type="table" target="#tab_9">Table 8</ref> demonstrate this idea, which shows that using a character head improved the Hmean by 0.7%. Additionally, if we add a mask prediction head to the character head (i.e., the mask character in <ref type="table" target="#tab_9">Table  8</ref>), the result would remain the same. Moreover, we employ a triplet loss to learn the connection between the characters. The ground truth includes whether the characters belong to the same text instances. However, the improvement is decreased to 0.5%. This may be because the instance connection introduced an inconsistent labeling issue. We further test the performance using only the character head with an instance connection and without the KE head. Hmean is reduced by 3.9% compared with the baseline method, suggesting that using character as an auxiliary head instead of the final prediction head is a good choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Ablation study of post-processing</head><p>The last step is to apply post-processing methods for final improvement. To this end, we compare the baseline with a series of standard and more effective post-processing methods. Polygonal non-maximize suppression (PNMS). Traditional non-maximum suppression (NMS) methods between horizontal rectangular bounding boxes can cause unnecessary suppression. Thus, we conduct ablation experiments to evaluate the performance of the PNMS. We use grid search to find the best threshold to find both NMS and PNMS for fair combination, which is 0.3 and 0.15, respectively. The result in <ref type="table">Table 5</ref> shows that using PNMS performs better than NMS by 0.8% in terms of Hmean. Additionally, PNMS is much more effective when using a test ensemble in practice.</p><p>Key edge RPP. The proposed key edge RPP proved effective on the ICDAR 2015 benchmark. Thus, we also test whether it applies to the competition dataset. The ablation result in <ref type="table">Table 5</ref> shows that it slightly improves the Hmean by 0.1% compared with the baseline. It is worth noticing that, although the best confidence threshold is 0.91, which is the same as that of the baseline, the recall is increased by 0.4% while only reducing the precision by 0.2%.</p><p>Large-scale testing. We also conduct experiments to evaluate how the testing scale influenced performance. The results are shown in <ref type="figure" target="#fig_4">Figure 12</ref>, which demonstrates that a proper setting of scale and MaxSize significantly improves the detection performance. Additionally, the results reveal that there is a limitation of the MaxSize. That is, if the value of MaxSize is higher than a certain value, the performance would be gradually reduced.</p><p>Test ensemble. To evaluate the performance of the test ensemble, we conduct ablation experiments with four different aspects: different backbone ensemble; multiple intermediate model ensemble; a multi-scale ensemble; and an independent model ensemble. Note that, to achieve the best performance, implementing ensemble or multi-scale testing requires some tricks. Otherwise, the results may be worse. We summarize the results as follows:</p><p>-Using a high confidence threshold. One weakness of multi-scale ensembling is that if a true-negative detection exists in one of the testing scales, it cannot be avoided unless we set a high confidence threshold to exclude it during the ensemble phase. Therefore, for each scale, we first test its best confidence threshold (cf) on the validation set. Then, we use a higher confidence for the model ensemble.</p><p>-Variant scale of multi-scale testing. The performance of small scale (600 (scale), 1200 (MaxSize)) is rated. For example, in the ReCTs competition, it is much worse than that of large-scale (1,600, 1,600). However, small scales are better for detecting large instances compared with large scales, and they can always be mutually promoted in practice. -Using a strict PNMS threshold. A normal case for the ensemble result is that the recall can be significantly improved, whereas the prediction is dramatically reduced. When observing the final integrated detection boxes, it is easy to find that the reduction was caused by boxes-inboxes and many stacked redundant boxes. Using a strict PNMS can effectively solve this issue.</p><p>Based on these principles, we conclude the results of the four ensemble aspects as follows.</p><p>-Different backbone ensembles. We train three models using the baseline setting with three types of deformable convolution, starting from C4-1, C4-2, and C3 of ResNet-101, respectively. The ensemble results of the three methods are shown in <ref type="table" target="#tab_10">Table 9</ref>. From the table, we can see that integrating the models with a series of simple backbone modifications improved the detection performance, even based on a relatively high baseline. Additionally, the results show that integrating more components resulted in better performance. -Multiple intermediate model ensembles. We also evaluate the performance of integrating intermediate models.  We use the trained model with the ResNext-152 backbone as a strong baseline and selected the last three intermediate iterating models with 10,000 iterations as intervals for the ensemble. The results shown in <ref type="table" target="#tab_10">Table 9</ref> also demonstrate that when using the model ensemble, the intermediate models could be mutually promoted. -Multi-scale ensemble. To evaluate the performance of the multi-scale ensemble, we use grid searching to find the best PNMS threshold for three specified settings (scale, MaxSize), representing large, medium, and small text instances, respectively. Each detection result was then integrated with a PNMS threshold 0.02 higher than the original best threshold, which resulted in approximate optimum integrating results with 0.6% improvement in terms of Hmean, as shown in <ref type="table" target="#tab_10">Table 9</ref>. -Independent model ensembles. Finally, we test the performance of integrating the two final models. The first model contains the baseline setting plus deformable convolution, and the second model contains the baseline setting with the ResNext-152 backbone. We independently integrate each model using an intermediate model ensemble and a multi-scale ensemble. Then, we assemble the final results of the two models. As shown in <ref type="table" target="#tab_10">Table 9</ref>, the detection result can still be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison with state-of-the-art methods</head><p>To further evaluate the effectiveness of the proposed method, we carry out experiments and compare our final model with other state-of-the-art methods on three scene text datasets: ICDAR 2015 <ref type="bibr" target="#b24">[26]</ref>, MLT <ref type="bibr" target="#b25">[27]</ref>, and ReCTS (See Section 4.3.1). We also conduct an experiment on one aerial dataset, HRSC2016 <ref type="bibr" target="#b75">[77]</ref>, to further demonstrate the generalization ability of our method.</p><p>Final model. The final model is designed by combine the effective modules evaluated in <ref type="table">Table 5</ref>. Specifically, based on the baseline setting, we refine our model in all six aspects. During the data arrangement stage, we use 60,000 pretrained data items to train a pretrained model for 200,000 <ref type="table" target="#tab_2">Table 12</ref>: Competition results on the ReCTS dataset. The results are from the competition website https://tinyurl.com/ReCTS2019. For the detection task, the ranking is based on Hmean. For End-to-End detection and recognition task, the ranking is based on 1-NED. NED: normalized edit distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Affiliation</head><p>Detection iterations, and we then use the original training data of each dataset for finetuning. In the pre-processing part, apart from the baseline setting, we also apply color jittering, random cropping, and random rotation with their best ratios as evaluated on the validation dataset for data augmentation. Additionally, the images are trained with a medium setting of the random scale training for maximizing the utilization of the video memory. For the backbone setting, we integrate the ResNext-152-32x8d-FPN-IN5k model, deformable convolution (C4-2), PAN, and MSN modules together to construct a powerful feature extractor. During the proposal generation stage, we adopt deformable PSROI pooling for feature alignment, whereas in the prediction head, we only add an auxiliary character head for mutual promotion using only the ReCTS dataset. Finally, in the post-processing stage, we utilize all effective settings, including polygonal nonmaximum suppression, key edge RPP, intermediate model ensemble, and multi-scale ensemble.</p><p>The ICDAR 2015 Incidental Scene Text <ref type="bibr" target="#b24">[26]</ref> is one of the most popular benchmarks for oriented scene text detection. The images are incidentally captured mostly from streets and shopping malls. Thus, the challenges of this dataset rely on oriented, small, and low-resolution text. This dataset contains 1,000 training samples and 500 testing samples with approximately 2,000 content-recognizable quadri-lateral word-level bounding boxes. The results of ICDAR 2015 are given in <ref type="table" target="#tab_11">Table 10</ref>. From this table, it is clear that our method outperformed all previous methods.</p><p>The ICDAR 2017 MLT <ref type="bibr" target="#b25">[27]</ref> is the largest multi-lingual (nine languages) oriented scene text dataset, including 7,200 training samples, 1,800 validation samples, and 9,000 testing samples. The challenges associated with this dataset are manifold. Different languages have different annotating styles. For example, most Chinese annotations are long, and there is no specific word interval for sentences. However, most English annotations are short. The annotations of Bangla or Arabic may be frequently entwined with each other, and there is more multi-orientation, perspective distorted text on various complex backgrounds. Furthermore, many images have more than 50 text instances. All instances are well annotated with compact quadrangles. As shown in <ref type="table" target="#tab_12">Table 11</ref>, the proposed approach achieved the best performance on the MLT dataset.</p><p>ReCTS is the recent ICDAR 2019 Robust Reading Challenge 1 described in Section 4.3.1. Competitors were restricted to submitting at most five results, and all results were evaluated after the deadline. The competition attracted numerous competitors from well-known universities and hightech companies. The results of the ReCTS are shown in Ta-ble 12. Our method won first place in the ReCTS detection competition. To clearly evaluate the performance of the final model, we also provide the results of our method on the ReCTS validation set without using a model ensemble. As shown in <ref type="table">Table 5</ref>, the final model significantly outperformed the baseline by 7.1% in terms of Hmean.</p><p>ReCTS End-to-End. One of the main goals of scene text detection is to recognize a text instance <ref type="bibr" target="#b68">[70]</ref> that is highly related to the performance of the detection system. To validate the effectiveness and robustness of our detection method, we build a recognition system that incorporate several state-of-the-art methods. Typically, the recognition performance is highly relevant to the quality of the detected boxes. To reveal the precision of our detection, we construct an end-to-end recognition system to demonstrate how our method benefits recognition models. We first crop the images using detected boxes and fed them into four popular recognition models, including decouple attention network <ref type="bibr" target="#b76">[78]</ref>, convolutional recurrent neural network <ref type="bibr" target="#b77">[79]</ref>, network of show, attend, read <ref type="bibr" target="#b78">[80]</ref>, and transformer-based networks <ref type="bibr" target="#b79">[81]</ref>. The four models are trained on real samples and 600,000 extra synthetic samples following their default settings for training. The real samples are provided by the official training set, whereas the synthetic samples are synthesized using a render engine <ref type="bibr" target="#b0">[1]</ref> and the corpus of the official training set. All images are resized to a specific required height for each recognition model while maintaining the aspect ratio of the original image. In a data batch, all images are padded with white to the maximum width of the images. During the inference stage, we choose the prediction having the highest confidence as the final ensemble result. Both quantitative and qualitative results are presented in <ref type="table" target="#tab_2">Table 12</ref> and <ref type="figure" target="#fig_7">Figure 14</ref>(b), respectively.</p><p>HRSC2016. To demonstrate the generalization ability of our method, we further evaluate its performance on the Level-1 task of the HRSC2016 dataset <ref type="bibr" target="#b75">[77]</ref> to demonstrate multi-directional object detection. The ship instances in this dataset are presented in various orientations, and the annotating bounding boxes are based on rotated rectangles. There were 436, 181, and 444 images for training, validating, and testing, respectively. Only the training and validation sets are used for training. The evaluation metric is the same as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">26]</ref>. The result is shown in <ref type="table" target="#tab_3">Table 13</ref>, showing a significant improvement over the TIoU-Hmean <ref type="bibr" target="#b80">[82]</ref>. It also demonstrates the robustness of our method. Qualitative examples of the detection results are shown in <ref type="figure" target="#fig_5">Figure 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have addressed multi-orientation scene text detection using an effective OBD method. Using discretization methodology, OBD, can solve the inconsistent labeling issue by discretizing the point-wise prediction into orderless  <ref type="table" target="#tab_3">Table 13</ref>: Experimental results for HRSC 2016 dataset. cf: confidence threshold, which is set to 0.01 in the last line. key edges. To decode accurate vertex positions, we have proposed a simple but effective MTL method to reconstruct the quadrilateral bounding box. Benefiting from OBD, we improve the reliability of the confidence of the bounding box and adopted more effective post-processing methods to improve performance. Additionally, based on our method, we have conducted thorough ablation studies on six training components, including data arrangement, pre-processing, backbone, proposal generation, prediction head, and post-processing, to explore the potential upper limit of our method. By combining effective modules, we have achieved state-of-the-art results on various benchmarks and won the first place in the recent ICDAR 2019 Robust Reading Challenge on Reading Chinese Text on Signboards. Moreover, using a recognition model, we perform the best in the end-to-end detection and recognition task, verifying that our method is conducive to current recognition methods. To test the generalization ability, we have conducted an experiment on an oriented general object dataset HRSC2016; the results verify that our method can significantly outperform recent state-of-the-art methods.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :</head><label>2</label><figDesc>Previous solutions can be negatively affected by the inconsistent labeling issue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>Overview of the proposed detection framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, the proposals processed by RoIAlign are fed into the OBD block with the pooling size of 14 × 14, where the feature maps are forwarded through four convolutional layers with 256 output channels. The output features are then upsampled by a 2× deconvolutional layer and a 2× bilinear upscaling layers. Thus, the output size of the feature maps F out is M × M, where M is 56 in our implementation. Furthermore, two convolution kernels shaped as 1 × M and M × 1 with six channels are employed to shrink the horizontal and vertical features for the x-KEs and y-KEs, respectively. Finally, the OBD model is trained by minimizing the cross-entropy loss L ke over an Mway softmax output, where the corresponding positions of the ground-truth KEs are assigned to each output channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 :</head><label>4</label><figDesc>PredictionMatch-Type (num:24) Ouput Illustration of the OBD and MTL blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 :</head><label>5</label><figDesc>The proposed framework can break the restrictions of the RoIs. The green solid quadrilateral and red dashed rectangular boxes represent the predictions and proposals, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>A 4 4 =</head><label>4</label><figDesc>24. For example, the predicted matching-type in Figure 6(a) is [(x min , y 2 ), (x 2 , y max ), (x 3 , y min ), (x max , y 3 )]. Based on this, a simple yet effective MTL module is proposed to learn the connections between x-KEs and y-KEs. Specifically, as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 :</head><label>6</label><figDesc>Illustration of different matching types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 :</head><label>7</label><figDesc>Different patterns of S OBD outputted by OBD block. (a) is the normal case while (b)(c)(d) are abnormal cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 :</head><label>9</label><figDesc>Ablation study on the ICDAR 2015 benchmark. X-axis represents confidence threshold and Y-axis represents Hmean result. Baseline represents Mask R-CNN. By integrating with proposed OBD, the detection results can be substantially better than the results of the Mask R-CNN baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 :</head><label>10</label><figDesc>Example images of the ReCTS. Small, stacked multiorientation, illumination, and annotation ambiguity are the main challenges for this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 11 :</head><label>11</label><figDesc>Ablation studies of data augmentation strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 12 :</head><label>12</label><figDesc>Ablation study of the testing scale. Note that the training scale is the default setting mentioned in Section 4.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 13 :</head><label>13</label><figDesc>Qualitative detection results on the HRSC2016 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>1 (</head><label>1</label><figDesc>a) Detection only results on MLT dataset. End-to-end results on ReCTS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 14 :</head><label>14</label><figDesc>Visualization of the qualitative results outputted by the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and y-KEs, y i ∈ [y min , y 2 , y 3 , y max ].</figDesc><table><row><cell>SBD Block</cell><cell></cell><cell></cell><cell>Y-KeyEdges Ymin Ymax Y2 Y3</cell></row><row><cell></cell><cell></cell><cell>Conv 56 x 1</cell><cell></cell></row><row><cell></cell><cell>1x1 Conv</cell><cell></cell><cell></cell></row><row><cell>5 6 x 5 6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv-deconv</cell><cell>1x1 Conv</cell><cell>Conv 1 x 56</cell><cell>xmi n x2 x3 xmax</cell></row><row><cell></cell><cell></cell><cell></cell><cell>X-KeyEdges</cell></row><row><cell></cell><cell>Concat</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1x1 Conv</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Conv 56 x 56</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies for comparing the mask branch and KE branch. The γ of RPP is set to 0.8 (best practice). Compared to Table 1, the results here are all tested in different branches of the same model without any data augmentation.</figDesc><table><row><cell>Datasets</cell><cell>Algorithms</cell><cell>Hmean</cell></row><row><cell></cell><cell>Mask branch</cell><cell>79.4%</cell></row><row><cell>ICDAR2015</cell><cell cols="2">KE branch without RPP 80.4% (↑ 1.0%)</cell></row><row><cell></cell><cell>KE branch with RPP</cell><cell>81.0% (↑ 1.6%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison on ICDAR 2015 dataset showing different methods' ability of resistant to the inconsistent labeling issue (by adding rotated pseudo samples). TB: Textboxes++. LD: using lower rotation degrees.</figDesc><table><row><cell></cell><cell>TB</cell><cell>East</cell><cell>CTD</cell><cell>APE</cell><cell>Ours</cell></row><row><cell>Hmean (baseline)</cell><cell>80.1%</cell><cell>78.3%</cell><cell>74.7%</cell><cell>79.4</cell><cell>80.4%</cell></row><row><cell>Hmean (rotation)</cell><cell>70.4%</cell><cell>64.6%</cell><cell>50.1%</cell><cell>77.4</cell><cell>80.7%</cell></row><row><cell>Variance</cell><cell cols="5">↓ 9.7% ↓ 13.7% ↓ 24.6% ↓ 2.0% ↑ 0.3%</cell></row><row><cell>Hmean (LD)</cell><cell>79.5%</cell><cell>76.0%</cell><cell>68.5%</cell><cell>80.1%</cell><cell>81.5%</cell></row><row><cell>Variance (LD)</cell><cell>↓ 0.6%</cell><cell>↓ 2.3%</cell><cell>↓ 6.2%</cell><cell cols="2">↑ 0.7% ↑ 1.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Hmean results under different rotation degrees on ICDAR 2015 dataset. The rotation angle represents the value used for the data augmentation during the training phase. Ours ↑0.9% ↑1.1% ↑1.3% ↑0.3%</figDesc><table><row><cell>5 •</cell><cell>30 •</cell><cell>60 •</cell><cell>90 •</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>model trained with pretrained data is better than using the ImageNet model. For example, when directly using the Im-ageNet ResNet-101 model instead of the MLT pretrained model from the baseline method, the Hmean is reduced by 0.5%. Using the model having 60,000 pretrained data, followed by finetuning the model on the split ReCTS training data improved the result by 1.2% in terms of Hmean. To evaluate the importance of the data quality, we mimicked the manual annotation error by removing 5% of the training annotation instances and did not correct some samples with annotation ambiguity from the original ReCTS training data. The results indicate that using defective training data significantly degrades the performance.</figDesc><table><row><cell></cell><cell cols="5">threshold Recall (%) Precision (%) Hmean (%) ∆ Hmean</cell></row><row><cell>Baseline model (based on OBD [12])</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with mlt pretrained model with flip (0.5)</cell><cell>0.91</cell><cell>78.1</cell><cell>80.1</cell><cell>79.1</cell><cell>-</cell></row><row><cell>test scale: min size: 1400; max size: 2000.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data arrangement</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>With data cleaning</cell><cell>0.93</cell><cell>77.7</cell><cell>80.3</cell><cell>79.0</cell><cell>↓ 0.1</cell></row><row><cell>With only mlt pretrained data (100k iters)</cell><cell>0.97</cell><cell>53.4</cell><cell>56.1</cell><cell>54.7</cell><cell>↓ 24.4</cell></row><row><cell>With only 60k pretrained data (200k iters)</cell><cell>0.81</cell><cell>50.8</cell><cell>61.0</cell><cell>55.5</cell><cell>↓ 23.6</cell></row><row><cell>With defect data</cell><cell>0.91</cell><cell>75.8</cell><cell>72.5</cell><cell>74.1</cell><cell>↓ 5.0</cell></row><row><cell>Without MLT data pretrain</cell><cell>0.85</cell><cell>75.5</cell><cell>81.9</cell><cell>78.6</cell><cell>↓ 0.5</cell></row><row><cell>With 60k pretrained model</cell><cell>0.91</cell><cell>78.8</cell><cell>81.9</cell><cell>80.3</cell><cell>↑ 1.2</cell></row><row><cell>Pre-processing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>With random crop (best ratio)</cell><cell>0.91</cell><cell>78.4</cell><cell>83.7</cell><cell>81.0</cell><cell>↑ 1.9</cell></row><row><cell>With random rotate (best ratio)</cell><cell>0.91</cell><cell>77.6</cell><cell>81.8</cell><cell>79.7</cell><cell>↑ 0.6</cell></row><row><cell>With color jittering</cell><cell>0.91</cell><cell>76.4</cell><cell>82.5</cell><cell>79.3</cell><cell>↑ 0.2</cell></row><row><cell>With medium random scale training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ori: (560,600,...,920,) max: 1300</cell><cell>0.89</cell><cell>80.3</cell><cell>82.2</cell><cell>81.3</cell><cell>↑ 2.2</cell></row><row><cell>to: (680,720,...,1120,) max: 1800</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>With large random scale training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ori: (560,600,...,920,) max: 1300</cell><cell>0.89</cell><cell>80.2</cell><cell>83.6</cell><cell>81.9</cell><cell>↑ 2.8</cell></row><row><cell>to: (800,840,...,1400,) max: 2560</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>With ResNext-152-32x8d-FPN-IN5k (using detectron pretrained model) v1</cell><cell>0.91</cell><cell>79.4</cell><cell>84.0</cell><cell>81.6</cell><cell>↑ 2.5</cell></row><row><cell>With ASPP in KE head</cell><cell>0.91</cell><cell>76.1</cell><cell>80.1</cell><cell>78.0</cell><cell>↓ 1.1</cell></row><row><cell>With ASPP in (backbone 1/16)</cell><cell>0.89</cell><cell>73.1</cell><cell>81.3</cell><cell>77.0</cell><cell>↓ 2.1</cell></row><row><cell>With deformable convolution (C4-1)</cell><cell>0.87</cell><cell>79.5</cell><cell>83.9</cell><cell>81.7</cell><cell>↑ 2.6</cell></row><row><cell>With deformable convolution (C4-2)</cell><cell>0.89</cell><cell>79.1</cell><cell>84.3</cell><cell>81.6</cell><cell>↑ 2.5</cell></row><row><cell>With deformable convolution (C3-)</cell><cell>0.83</cell><cell>81.2</cell><cell>81.9</cell><cell>81.6</cell><cell>↑ 2.5</cell></row><row><cell>With panoptic segmentation (dice loss)</cell><cell>0.67</cell><cell>77.7</cell><cell>80.3</cell><cell>79.0</cell><cell>↓ 0.1</cell></row><row><cell>With pyramid attention network (PAN)</cell><cell>0.85</cell><cell>77.6</cell><cell>83.1</cell><cell>80.3</cell><cell>↑ 1.2</cell></row><row><cell>With multi-scale network (MSN)</cell><cell>0.91</cell><cell>79.0</cell><cell>81.6</cell><cell>80.3</cell><cell>↑ 1.2</cell></row><row><cell>Proposal generation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>With deformable PSROI pooling</cell><cell>0.91</cell><cell>80.7</cell><cell>79.4</cell><cell>80.0</cell><cell>↑ 0.9</cell></row><row><cell>Prediction head</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>With character head</cell><cell>0.93</cell><cell>77.7</cell><cell>82.0</cell><cell>79.8</cell><cell>↑ 0.7</cell></row><row><cell>With OHEMv1</cell><cell>0.59</cell><cell>76.9</cell><cell>80.0</cell><cell>78.4</cell><cell>↓ 0.7</cell></row><row><cell>With OHEMv2</cell><cell>0.65</cell><cell>75.8</cell><cell>81.1</cell><cell>78.3</cell><cell>↓ 0.8</cell></row><row><cell>With OHEMv3</cell><cell>0.55</cell><cell>77.5</cell><cell>79.8</cell><cell>78.6</cell><cell>↓ 0.5</cell></row><row><cell>With mask scoring</cell><cell>0.93</cell><cell>75.7</cell><cell>81.8</cell><cell>78.6</cell><cell>↓ 0.5</cell></row><row><cell>With cascade r-cnn (ensemble)</cell><cell>-</cell><cell>77.7</cell><cell>80.3</cell><cell>79.0</cell><cell>↓ 0.1</cell></row><row><cell>Post-processing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>With polygonal non-maximum suppression</cell><cell>0.91</cell><cell>77.2</cell><cell>82.8</cell><cell>79.9</cell><cell>↑ 0.8</cell></row><row><cell>With Key Edge RPP</cell><cell>0.91</cell><cell>78.5</cell><cell>79.9</cell><cell>79.2</cell><cell>↑ 0.1</cell></row><row><cell>Final model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>accumulating effective modules</cell><cell>0.91</cell><cell>83.2</cell><cell>89.5</cell><cell>86.2</cell><cell>↑ 7.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablation experiments for large scale training. Hmean 1 , Hmean 2 , and Hmean 3 represent default training scale, medium training scale, and large training scale, respectively. The first row compares the performance based on the baseline setting. The other three rows are the best setting (using grid search to find the best scale and MaxSize) for each training scale.</figDesc><table><row><cell cols="4">(Scale, MaxSize) Hmean 1 (%) Hmean 2 (%) Hmean 3</cell></row><row><cell>(1400, 2000)</cell><cell>79.1</cell><cell>81.3</cell><cell>81.9</cell></row><row><cell>(800, 1300)</cell><cell>81.5</cell><cell>-</cell><cell>-</cell></row><row><cell>(1600, 1600)</cell><cell>-</cell><cell>82.2</cell><cell>-</cell></row><row><cell>(1600, 1700)</cell><cell>-</cell><cell>-</cell><cell>82.5</cell></row><row><cell cols="4">original aspect ratio. We primarily compare three differ-</cell></row><row><cell cols="4">ent settings: the default training scale (scale: 560 to 920</cell></row><row><cell cols="4">with intervals of 40, MaxSize was 1,300); medium training</cell></row><row><cell cols="4">scale (scale: 680 to 1,120 with intervals of 40, MaxSize was</cell></row><row><cell cols="4">1,800); and large training scale (scale 800 to 1,400 with in-</cell></row><row><cell cols="2">tervals of 40, MaxSize was 2,560).</cell><cell></cell><cell></cell></row><row><cell cols="2">The results are presented in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Ablation results of using cascade r-cnn. cf: best threshold. R: recall. P: precision. H: Hmean.</figDesc><table><row><cell>Method</cell><cell>cf</cell><cell cols="3">R (%) P (%) H (%)</cell><cell>∆ H</cell></row><row><cell cols="2">Baseline model 0.91</cell><cell>78.1</cell><cell>80.1</cell><cell>79.1</cell><cell>-</cell></row><row><cell>Stage 1</cell><cell>0.91</cell><cell>74.7</cell><cell>81.8</cell><cell>78.1</cell><cell>↓ 1.0</cell></row><row><cell>Stage 2</cell><cell>0.87</cell><cell>76.3</cell><cell>81.1</cell><cell>78.6</cell><cell>↓ 0.5</cell></row><row><cell>Stage 3</cell><cell>0.87</cell><cell>75.9</cell><cell>79.5</cell><cell>77.7</cell><cell>↓ 1.4</cell></row><row><cell>ensemble</cell><cell>-</cell><cell>77.7</cell><cell>80.3</cell><cell>79.0</cell><cell>↓ 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Ablation experiments for using character head. H: Hmean.</figDesc><table><row><cell>Method</cell><cell>H (%)</cell><cell>∆ H</cell></row><row><cell>Baseline</cell><cell>79.1</cell><cell>-</cell></row><row><cell>Baseline + character head</cell><cell>79.8</cell><cell>↑ 0.7</cell></row><row><cell>Baseline + character head + mask character</cell><cell>79.8</cell><cell>↑ 0.7</cell></row><row><cell>Baseline + character head + instance connection</cell><cell>79.6</cell><cell>↑ 0.5</cell></row><row><cell>Baseline + character head</cell><cell></cell><cell></cell></row><row><cell>+ instance connection</cell><cell>75.2</cell><cell>↓ 3.9</cell></row><row><cell>-KE head</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Ablation experiments for different approaches of model ensemble. 'def': deformable convolution.</figDesc><table><row><cell>Method</cell><cell cols="3">Backbone ensemble</cell><cell cols="3">Intermediate model ensemble</cell><cell cols="3">Multi-scale ensemble (scale, MaxSize)</cell><cell cols="2">Model ensemble</cell></row><row><cell>Components</cell><cell>def-C4-1</cell><cell>def-C4-2</cell><cell>def-C3</cell><cell cols="3">x152-60k x152-70k x152-80k</cell><cell cols="2">(600,1600) (1200,1600)</cell><cell>(1600,1600)</cell><cell>M1</cell><cell>M2</cell></row><row><cell>Hmean (%)</cell><cell>81.7</cell><cell>81.6</cell><cell>81.6</cell><cell>80.7</cell><cell>81.7</cell><cell>81.6</cell><cell>79.8</cell><cell>82.1</cell><cell>82.6</cell><cell>83.2</cell><cell>83.5</cell></row><row><cell>Ensemble</cell><cell>def-C4-1 &amp; def-C4-2</cell><cell>def-C4-1 &amp; defC3</cell><cell>def-C4-1 &amp; def-C3 &amp; def-C4-2</cell><cell cols="3">x152-60k &amp; x152-70k &amp; x152-80k</cell><cell cols="3">(600, 1600) &amp; (1200, 1600) &amp; (1600, 1600)</cell><cell cols="2">M1 &amp; M2</cell></row><row><cell>Hmean (%)</cell><cell>81.8</cell><cell>82.1</cell><cell>82.2</cell><cell></cell><cell>81.9</cell><cell></cell><cell></cell><cell>83.2</cell><cell></cell><cell></cell><cell>83.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Experimental results for the ICDAR 2015 dataset. R: recall.</figDesc><table><row><cell>: precision.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Algorithms R(%) P(%) Hmean(%)</cell></row><row><cell>Tian et al. [6]</cell><cell>52.0</cell><cell>74.0</cell><cell>61.0</cell></row><row><cell>Shi et al. [9]</cell><cell>76.8</cell><cell>73.1</cell><cell>75.0</cell></row><row><cell>Liu et al. [8]</cell><cell>68.2</cell><cell>73.2</cell><cell>70.6</cell></row><row><cell>Zhou et al. [25]</cell><cell>73.5</cell><cell>83.6</cell><cell>78.2</cell></row><row><cell>Ma et al. [23]</cell><cell>73.2</cell><cell>82.2</cell><cell>77.4</cell></row><row><cell>Hu et al. [58]</cell><cell>77.0</cell><cell>79.3</cell><cell>78.2</cell></row><row><cell>Liao et al. [15]</cell><cell>79.0</cell><cell>85.6</cell><cell>82.2</cell></row><row><cell>Deng et al. [34]</cell><cell>82.0</cell><cell>85.5</cell><cell>83.7</cell></row><row><cell>Ma et al. [23]</cell><cell>82.2</cell><cell>73.2</cell><cell>77.4</cell></row><row><cell>Lyu et al. [35]</cell><cell>79.7</cell><cell>89.5</cell><cell>84.3</cell></row><row><cell>He et al. [18]</cell><cell>80.0</cell><cell>82.0</cell><cell>81.0</cell></row><row><cell>Xu et al. [59]</cell><cell>80.5</cell><cell>84.3</cell><cell>82.4</cell></row><row><cell>Tang et al. [60]</cell><cell>80.3</cell><cell>83.7</cell><cell>82.0</cell></row><row><cell>Wang et al. [37]</cell><cell>84.5</cell><cell>86.9</cell><cell>85.7</cell></row><row><cell>Xie et al. [11]</cell><cell>85.8</cell><cell>88.7</cell><cell>87.2</cell></row><row><cell>Zhang et al. [61]</cell><cell>83.5</cell><cell>91.3</cell><cell>87.2</cell></row><row><cell>Liu et al. [16]</cell><cell>87.9</cell><cell>91.9</cell><cell>89.8</cell></row><row><cell>Baek et al. [62]</cell><cell>84.3</cell><cell>89.8</cell><cell>86.9</cell></row><row><cell>Huang et al. [63]</cell><cell>81.5</cell><cell>90.8</cell><cell>85.9</cell></row><row><cell>Zhong et al. [64]</cell><cell>80.1</cell><cell>87.8</cell><cell>83.8</cell></row><row><cell>He et al. [65]</cell><cell>86.0</cell><cell>87.0</cell><cell>87.0</cell></row><row><cell>Liu et al. [66]</cell><cell>87.6</cell><cell>86.6</cell><cell>87.1</cell></row><row><cell>Liao et al. [14]</cell><cell>78.5</cell><cell>87.8</cell><cell>82.9</cell></row><row><cell>Long et al. [67]</cell><cell>80.4</cell><cell>84.9</cell><cell>82.6</cell></row><row><cell>He et al. [68]</cell><cell>79.7</cell><cell>92.0</cell><cell>85.4</cell></row><row><cell>Lyu et al. [69]</cell><cell>81.0</cell><cell>91.6</cell><cell>86.0</cell></row><row><cell>He et al. [17]</cell><cell>73.0</cell><cell>80.0</cell><cell>77.0</cell></row><row><cell>Wang et al. [70]</cell><cell>79.6</cell><cell>83.2</cell><cell>81.4</cell></row><row><cell>Liao et al. [71]</cell><cell>87.3</cell><cell>86.6</cell><cell>87.0</cell></row><row><cell>Wang et al. [72]</cell><cell>81.9</cell><cell>84.0</cell><cell>82.9</cell></row><row><cell>Wang et al. [73]</cell><cell>86.0</cell><cell>89.2</cell><cell>87.6</cell></row><row><cell>Qin et al. [74]</cell><cell>88.0</cell><cell>91.7</cell><cell>89.8</cell></row><row><cell>Feng et al. [75]</cell><cell>83.8</cell><cell>92.5</cell><cell>87.9</cell></row><row><cell>Liu et al. [12]</cell><cell>83.8</cell><cell>89.4</cell><cell>86.5</cell></row><row><cell>Ours</cell><cell>88.2</cell><cell>92.1</cell><cell>90.1</cell></row></table><note>P</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Experimental results for the MLT dataset. SS represents a single scale. R: recall. P: precision. Note that we only used a single scale for all experiments.</figDesc><table><row><cell cols="4">Algorithms R(%) P(%) Hmean(%)</cell></row><row><cell cols="3">linkage-ER-Flow [27] 25.59 44.48</cell><cell>32.49</cell></row><row><cell cols="3">TH-DL [27] 34.78 67.75</cell><cell>45.97</cell></row><row><cell>SARI FDU RRPN v2 [23]</cell><cell>67.0</cell><cell>55.0</cell><cell>61.0</cell></row><row><cell>SARI FDU RRPN v1 [23]</cell><cell>55.5</cell><cell>71.17</cell><cell>62.37</cell></row><row><cell>Sensetime OCR [27]</cell><cell>69.0</cell><cell>67.75</cell><cell>45.97</cell></row><row><cell>SCUT DLVClab1 [8]</cell><cell>62.3</cell><cell>80.28</cell><cell>64.96</cell></row><row><cell>AF-RNN [76]</cell><cell>66.0</cell><cell>75.0</cell><cell>70.0</cell></row><row><cell>Lyu et al. [35]</cell><cell>70.6</cell><cell>74.3</cell><cell>72.4</cell></row><row><cell>FOTS [16]</cell><cell>62.3</cell><cell>81.86</cell><cell>70.75</cell></row><row><cell>CRAFT [62]</cell><cell>68.2</cell><cell>80.6</cell><cell>73.9</cell></row><row><cell>Liu et al. [12]</cell><cell>70.1</cell><cell>83.6</cell><cell>76.3</cell></row><row><cell cols="3">Ours 76.44 82.75</cell><cell>79.47</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://rrc.cvc.uab.es/?ch=12&amp;com=introduction</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time lexicon-free scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1872" to="1885" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient scene text localization and recognition with local character refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Doc. Anal. and Recognit</title>
		<meeting>Int. Conf. Doc. Anal. and Recognit</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="746" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Text flow: A unified text detection system in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeptext: A unified framework for text proposal generation and text detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07314</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multi-oriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate scene text detection through border semantics awareness and bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="355" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scene text detection with supervised pyramid context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Omnidirectional scene text detection with sequential-free box discretization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artificial Intell</title>
		<meeting>Int. Joint Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sliding line point regression for shape robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Patt. Recogn</title>
		<meeting>Int. Conf. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Msr: Multi-scale shape regression for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artificial Intell</title>
		<meeting>Int. Joint Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An end-to-end quadrilateral regression network for comic panel extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">East: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ICDAR 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Doc. Anal. and Recognit</title>
		<meeting>Int. Conf. Doc. Anal. and Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chazalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Doc. Anal. and Recognit</title>
		<meeting>Int. Conf. Doc. Anal. and Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Total-text: toward orientation robustness in scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Ch&amp;apos;ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Doc. Anal. Recognit</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Curved scene text detection via transverse and longitudinal sequence connection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="337" to="345" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<title level="m">ICDAR 2019 Competition on Large-scale Street View Text with Partial Labeling-RRC-LSVT</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Proc. Int. Conf. Doc. Anal. and Recognit</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Doc. Anal. and Recognit</title>
		<meeting>Int. Conf. Doc. Anal. and Recognit</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">IC-DAR2019 Robust Reading Challenge on Multi-lingual Scene Text Detection and Recognition-RRC-MLT-2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Busta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Khlif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Burie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Doc. Anal. and Recognit</title>
		<meeting>Int. Conf. Doc. Anal. and Recognit</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multioriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4159" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-organized text detection with minimal post-processing via border learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5000" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shape Robust Text Detection with Progressive Scale Expansion Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Text-attentional convolutional neural network for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2529" to="2541" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive period embedding for representing oriented objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geoscience &amp; Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Doc. Anal. and Recognit</title>
		<meeting>Int. Conf. Doc. Anal. and Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ICDAR2017 competition on reading chinese text in the wild (RCTW-17)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Doc. Anal. and Recognit</title>
		<meeting>Int. Conf. Doc. Anal. and Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1429" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G I</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazàn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P D L</forename><surname>Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Doc. Anal. and Recognit</title>
		<meeting>Int. Conf. Doc. Anal. and Recognit</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07140</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-orientation scene text detection with adaptive clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1930</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">DeRPN: Taking a further step toward more general object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Inceptext: A new inception-text module with deformable psroi pooling for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Joint Conf. Artificial Intell</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Textfield: Learning a deep direction field for irregular scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Detecting dense and arbitrary-shaped scene text by instance-aware component grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Pattern Recogn</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Look More Than Once: An Accurate Detector for Text of Arbitrary Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Character Region Awareness for Text Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mask r-cnn with pyramid attention network for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Winter Conf. Appl. of Comp. Vis</title>
		<meeting>Winter Conf. Appl. of Comp. Vis</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Improved localization accuracy by locnet for faster r-cnn based text detection in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An endto-end textspotter with explicit alignment and attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Arbitrarily shaped scene text detection with a mask tightness text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Realtime multi-scale scene text detection with scale-based region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Ogier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Convolutional attention networks for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Efficient and Accurate Arbitrary-Shaped Text Detection with Pixel Aggregation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Arbitrary shape scene text detection with adaptive text region representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Towards Unconstrained End-to-End Text Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">TextDragon: An End-to-End Framework for Arbitrary Shaped Text Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wenhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu-Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">An anchor-free region proposal network for faster r-cnn-based text detection approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Document Analysis &amp; Recogn</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Rotated region based cnn for ship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Decoupled attention network for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tianwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuanzhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lianwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">A Simple and Robust Convolutional-Attention Network for Irregular Text Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01375</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Tightnessaware evaluation protocol for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
