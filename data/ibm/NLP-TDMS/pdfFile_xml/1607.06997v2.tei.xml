<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Peak-Piloted Deep Network for Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
							<email>liuluoqi@360.cntenglwy@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">360 AI Institute</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugang</forename><surname>Han</surname></persName>
							<email>hanyugang@360.cnnvasconcelos@ucsd.edueleyans@nus.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="department">360 AI Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">360 AI Institute</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Peak-Piloted Deep Network for Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Facial Expression Recognition</term>
					<term>Peak-Piloted</term>
					<term>Deep Network</term>
					<term>Peak Gradient Suppression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Objective functions for training of deep networks for face-related recognition tasks, such as facial expression recognition (FER), usually consider each sample independently. In this work, we present a novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from non-peak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities. A specialpurpose back-propagation procedure, peak gradient suppression (PGS), is proposed for network training. It drives the intermediate-layer feature responses of non-peak expression samples towards those of the corresponding peak expression samples, while avoiding the inverse. This avoids degrading the recognition capability for samples of peak expression due to interference from their non-peak expression counterparts. Extensive comparisons on two popular FER datasets, Oulu-CASIA and CK+, demonstrate the superiority of the PPDN over state-ofthe-art FER methods, as well as the advantages of both the network structure and the optimization strategy. Moreover, it is shown that PPDN is a general architecture, extensible to other tasks by proper definition of peak and non-peak samples. This is validated by experiments that show state-of-the-art performance on pose-invariant face recognition, using the Multi-PIE dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Facial Expression Recognition (FER) aims to predict the basic facial expressions (e.g. happy, sad, surprise, angry, fear, disgust) from a human face image, as illustrated in <ref type="figure">Fig. 1</ref>. <ref type="bibr" target="#b0">1</ref> Recently, FER has attracted much research attention <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. It can facilitate other face-related tasks, such as face recognition <ref type="bibr" target="#b7">[8]</ref> and alignment <ref type="bibr" target="#b8">[9]</ref>. Despite <ref type="bibr" target="#b0">1</ref> This work was performed when Xiaoyun Zhao was an intern at 360 AI Institute. arXiv:1607.06997v2 [cs.CV] 3 Jan 2017 significant recent progress <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>, FER is still a challenging problem, due to the following difficulties. First, as illustrated in <ref type="figure">Fig. 1</ref>, different subjects often dsiplay the same expression with diverse intensities and visual appearances. In a videostream, an expression will first appear in a subtle form and then grow into a strong display of the underlying feelings. We refer to the former as a non-peak and to the latter as a peak expression. Second, peak and non-peak expressions by the same subject can have significant variation in terms of attributes such as mouth corner radian, facial wrinkles, etc. Third, non-peak expressions are more commonly displayed than peak expressions. It is usually difficult to capture critical and subtle expression details from non-peak expression images, which can be hard to distinguish across expressions. For example, the non-peak expressions for fear and sadness are quite similar in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Peak expression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surprise</head><p>Angry Happy Fear Sad Disgust</p><p>Non-peak expression <ref type="figure">Fig. 1</ref>. Examples of six facial expression samples, including surprise, angry, happy, fear, sad and disgust. For each subject, the peak and non-peak expressions are shown.</p><p>Recently, deep neural network architectures have shown excellent performance in face-related recognition tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. The has led to the introduction of FER network architectures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. There are, nevertheless, some important limitations. First, most methods consider each sample independently during learning, ignoring the intrinsic correlations between each pair of samples (e.g., easy and hard samples). This limits the discriminative capabilities of the learned models. Second, they focus on recognizing the clearly separable peak expressions and ignore the most common non-peak expression samples, whose discrimination can be extremely challenging.</p><p>In this paper, we propose a novel peak-piloted deep network (PPDN) architecture, which implicitly embeds the natural evolution of expressions from non-peak to peak expression in the learning process, so as to zoom in on the subtle differences between weak expressions and achieve invariance to expression intensity. Intuitively, as illustrated in <ref type="figure">Fig. 2</ref>, peak and non-peak expressions from the same subject often exhibit very strong visual correlations (e.g., similar face parts) and can mutually help the recognition of each other. The proposed PPDN uses the feature responses to samples of peak expression (easy samples) to supervise the responses to samples of non-peak expression (hard samples) of the same type and from the same subject. The resulting mapping of nonpeak expressions into their corresponding peak expressions magnifies their critical and subtle details, facilitating their recognition. <ref type="figure">Fig. 2</ref>. Expression evolving process from non-peak expression to peak expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surprise Happy</head><p>In principle, an explicit mapping from non-peak to peak expression could significantly improve recognition. However, such a mapping is challenging to generate, since the detailed changes of face features (e.g., mouth corner radian and wrinkles) can be quite difficult to predict. We avoid this problem by focusing on the high-level feature representation of the facial expressions, which is both more abstract and directly related to facial expression recognition. In particular, the proposed PPDN optimizes the tasks of 1) feature transformation from non-peak to peak expression and 2) recognition of facial expressions in a unified manner. It is, in fact, a general approach, applicable to many other recognition tasks (e.g. face recognition) by proper definition of peak and non-peak samples (e.g. frontal and profile faces). By implicitly learning the evolution from hard poses (e.g., profile faces) to easy poses (e.g., near-frontal faces), it can improve the recognition accuracy of prior solutions to these problems, making them more robust to pose variation.</p><p>During training, the PPDN takes an image pair with a peak and a non-peak expression of the same type and from the same subject. This image pair is passed through several intermediate layers to generate feature maps for each expression image. The L2-norm of the difference between the feature maps of non-peak and peak expression images is then minimized, to embed the evolution of expressions into the PPDN framework. In this way, the PPDN incorporates the peak-piloted feature transformation and facial expression recognition into a unified architecture. The PPDN is learned with a new back-propagation algorithm, denotes peak gradient suppression (PGS), which drives the feature responses to non-peak expression instances towards those of the corresponding peak expression images, but not the contrary. This is unlike the traditional optimization of Siamese networks <ref type="bibr" target="#b12">[13]</ref>, which encourages the feature pairs to be close to each other, treating the feature maps of the two images equally. Instead, the PPDN focuses on transforming the features of non-peak expressions towards those of peak expressions. This is implemented by, during each back-propagation iteration, ignoring the gradient information due to the peak expression image in the L2-norm minimization of feature differences, while keeping that due to the non-peak expression. The gradients of the recognition loss, for both peak and non-peak expression images, are the same as in traditional back-propagation. This avoids the degradation of the recognition capability of the network for samples of peak expression due to the influence of non-peak expression samples.</p><p>Overall, this work has four main contributions. 1) The PPDN architecture is proposed, using the responses to samples of peak expression (easy samples) to supervise the responses to samples of non-peak expression (hard samples) of the same type and from the same subject. The targets of peak-piloted feature transformation and facial expression recognition, for peak and non-peak expressions, are optimized simultaneously. 2) A tailored back-propagation procedure, PGS, is proposed to drive the responses to non-peak expressions towards those of the corresponding peak expressions, while avoiding the inverse. 3) The PPDN is shown to perform intensity-invariant facial expression recognition, by effectively recognizing the most common non-peak expressions. 4) Comprehensive evaluations on several FER datasets, namely CK+ <ref type="bibr" target="#b16">[17]</ref> and Oulu-CASIA <ref type="bibr" target="#b17">[18]</ref>, demonstrate the superiority of the PPDN over previous methods. Its generalization to other tasks is also demonstrated through state-of-the-art robust face recognition performance on the public Multi-PIE dataset <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There have been several recent attempts to solve the facial expression recognition problem. These methods can be grouped into two categories: sequence-based and still image approaches. In the first category, sequence-based approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref> exploit both the appearance and motion information from video sequences. In the second category, still image approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref> recognize expressions uniquely from image appearance patterns. Since still image methods are more generic, recognizing expressions in both still images and sequences, we focus on models for still image expression recognition. Among these, both hand-crafted pipelines and deep learning methods have been explored for FER. Hand-crafted approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11]</ref> perform three steps sequentially: feature extraction, feature selection and classification. This can lead to suboptimal recognition, due to the combination of different optimization targets.</p><p>Convolutional Neural Network (CNN) architectures <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> have recently shown excellent performance on face-related recognition tasks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. Methods that resort to the CNN architecture have also been proposed for FER. For example, Yu et al. <ref type="bibr" target="#b4">[5]</ref> used an ensemble of multiple deep CNNs. Mollahosseini et al. <ref type="bibr" target="#b15">[16]</ref> used three inception structures <ref type="bibr" target="#b23">[24]</ref> in convolution for FER. All these methods treat expression instances of different intensities of the same subject independently. Hence, the correlations between peak and non-peak expressions are overlooked during learning. In contrast, the proposed PPDN learns to embed the evolution from non-peak to peak expressions, so as to facilitate image-based FER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Peak-Piloted Deep Network (PPDN)</head><p>In this work we introduce the PPDN framework, which implicitly learns the evolution from non-peak to peak expressions, in the FER context. As illustrated in <ref type="figure">Fig. 3</ref>, during training the PPDN takes an image pair as input. This consists of a peak and a nonpeak expression of the same type and from the same subject. This image pair is passed through several convolutional and fully-connected layers, generating pairs of feature </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Peak Expression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Peak Expression</head><formula xml:id="formula_0">|| 1 − 2 ( )|| 2 || 1 − 2 ( )|| 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-entropy</head><p>Cross-entropy Convolutional Architecture <ref type="figure">Fig. 3</ref>. Illustration of the training stage of PPDN. During training, PPDN takes the pair of peak and non-peak expression images as input. After passing the pair through several convolutional and fully-connected layers, the intermediate feature maps can be obtained for peak and nonpeak expression images, respectively. The L2-norm loss between these feature maps is optimized for driving the features of the non-peak expression image towards those of the peak expression image. The network parameters can thus be updated by jointly optimizing the L2-norm losses and the losses of recognizing two expression images. During the back-propagation process, the Peak Gradient Suppression (PGS) is utilized.</p><p>maps for each expression image. To drive the feature responses to the non-peak expression image towards those of the peak expression image, the L2-norm of the feature differences is minimized. The learning algorithm optimizes a combination of this L2-norm loss and two recognition losses, one per expression image. Due to its excellent performance on several face-related recognition tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, the popular GoogLeNet <ref type="bibr" target="#b23">[24]</ref> is adopted as the basic network architecture. The incarnations of the inception architecture in GoogLeNet are restricted to filters sizes 1×1, 3×3 and 5×5. In total, the GoogLeNet implements nine inception structures after two convolutional layers and two max pooling layers. After that, the first fully-connected layer produces the intermediate features with 1024 dimensions, and the second fully-connected layer generates the label predictions for six expression labels. During testing, the PPDN takes one still image as input, outputting the predicted probabilities for all six expression labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Optimization</head><p>The goal of the PPDN is to learn the evolution from non-peak to peak expressions, as well as recognize the basic facial expressions. We denote the training set as S = {x p i , x n i , y p i , y n i , i = 1, ..., N }, where sample x n i denotes a face with non-peak expression, x p i a face with the corresponding peak expression, and y n i and y p i are the corre-sponding expression labels. To supervise the feature responses to the non-peak expression instance with those of the peak expression instance, the network is learned with a loss function that includes the L2-norm of the difference between the feature responses to peak and non-peak expression instances. Cross-entropy losses are also used to optimize the recognition of the two expression images. Overall, the loss of the PPDN is</p><formula xml:id="formula_1">J = 1 N (J 1 + J 2 + J 3 + λ N i=1 ||W || 2 ) = 1 N N i=1 j∈Ω f j (x p i , W ) − f j (x n i , W ) 2 + 1 N N i=1 L(y p i , f (x p i ; W )) + 1 N N i=1 L(y n i , f (x n i ; W )) + λ||W || 2 ,<label>(1)</label></formula><p>where J 1 , J 2 and J 3 indicate the L2-norm of the feature differences and the two crossentropy losses for recognition, respectively. Note that the peak-piloted feature transformation is quite generic and could be applied to the features produced by any layers. We denote Ω as the set of layers that employ the peak-piloted transformation, and f j , j ∈ Ω as the feature maps in the j-th layer. To reduce the effects caused by scale variability of the training data, the features f j are L2 normalized before the L2-norm of the difference is computed. More specifically, the feature maps f j are concatenated into one vector, which is L2 normalized. In the second and third terms, L represents the cross-entropy loss between the ground-truth labels and the predicted probabilities of all labels. The final regularization term is used to penalize the complexity of network parameters W . Since the evolution from non-peak to peak expression is embedded into the network, the latter learns a more robust expression recognizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Peak Gradient Suppression (PGS)</head><p>To train the PPDN, we propose a special-purpose back-propagation algorithm for the optimization of (1). Rather than the traditional straightforward application of stochastic gradient descent <ref type="bibr" target="#b12">[13]</ref> [29], the goal is to drive the intermediate-layer responses of non-peak expression instances towards those of the corresponding peak expression instances, while avoiding the reverse. Under traditional stochastic gradient decent (SGD) <ref type="bibr" target="#b30">[31]</ref>, the network parameters would be updated with</p><formula xml:id="formula_2">W + = W − γ∇ W J(W ; x p i , x p i , y n i , y p i ) = W − γ N ∂J 1 (W ; x n i , x p i ) ∂f j (W ; x n i ) × ∂f j (W ; x n i ) ∂W − γ N ∂J 1 (W ; x n i , x p i ) ∂f j (W ; x p i ) × ∂f j (W ; x p i ) ∂W − 1 N γ∇ W J 2 (W ; x p i , y p i ) − 1 N γ∇ W J 3 (W ; x n i , y n i ) − 2γW,<label>(2)</label></formula><p>where γ is the learning rate. The proposed peak gradient suppression (PGS) learning algorithm uses instead the updates</p><formula xml:id="formula_3">W + = W − γ N ∂J 1 (W ; x n i , x p i ) ∂f j (W ; x n i ) × ∂f j (W ; x n i ) ∂W − 1 N γ∇ W J 2 (W ; x p i , y p i ) − 1 N γ∇ W J 3 (W ; x n i , y n i ) − 2γW.<label>(3)</label></formula><p>The difference between <ref type="formula" target="#formula_3">(3)</ref> and <ref type="formula" target="#formula_2">(2)</ref> is that the gradients due to the feature responses of the peak expression image,</p><formula xml:id="formula_4">− γ N ∂J1(W ;x n i ,x p i ) ∂fj (W ;x p i ) × ∂fj (W ;x p i ) ∂W</formula><p>are suppressed in <ref type="bibr" target="#b2">(3)</ref>. In this way, PGS drives the feature responses of non-peak expressions towards those of peak expressions, but not the contrary. In the appendix, we show that this does not prevent learning, since the weight update direction of PGS is a descent direction of the overall loss, although not a steepest descent direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate the PPDN, we conduct extensive experiments on two popular FER datasets: CK+ <ref type="bibr" target="#b16">[17]</ref> and Oulu-CASIA <ref type="bibr" target="#b17">[18]</ref>. To further demonstrate that the PPDN generalizes to other recognition tasks, we also evaluate its performance on face recognition over the public Multi-PIE dataset <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Facial Expression Recognition</head><p>Training. The PPDN uses the GoogLeNet <ref type="bibr" target="#b23">[24]</ref> as basic network structure. The peakpiloted feature transformation is only employed in the last two fully-connected layers. Other configurations, using the peak-piloted feature transformation on various convolutional layers are also reported. Since it is not feasible to train the deep network on the small FER datasets available, we pre-trained GoogLeNet <ref type="bibr" target="#b23">[24]</ref> on a large-scale face recognition dataset, the CASIA Webface dataset <ref type="bibr" target="#b31">[32]</ref>. This network was then fine-tuned for FER. The CASIA Webface dataset contains 494,414 training images from 10,575 subjects, which were used to pre-train the network for 60 epochs with an initial learning rate of 0.01.For fine-tuning, the face region was first aligned with the detected eyes and mouth positions.The face regions were then resized to 128×128. The PPDN takes a pair of peak and non-peak expression images as input. The convolutional layer weights were initialized with those of the pre-trained model. The weights of the fully connected layer were initialized randomly using the "xaiver" procedure <ref type="bibr" target="#b32">[33]</ref>. The learning rate of the fully connected layers was set to 0.0001 and that of pre-trained convolutional layers to 0.000001. ALL models were trained using a batch size of 128 image pairs and a weight decay of 0.0002. The final trained model was obtained after 20,000 iterations. For fair comparison with previous methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4]</ref>, we did not use any data augmentation in our experiments.</p><p>Testing and Evaluation Metric. In the testing phase, the PPDN takes one testing image as the input and produces its predicted facial expression label. Following the standard setting of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, 10-fold subject-independent cross-validation was adopted for evaluation in all experiments. <ref type="table">Table 1</ref>. Performance comparisons on six facial expressions with four state-of-the-art methods and the baseline using GoogLeNet in terms of average classification accuracy by the 10-fold cross-validation evaluation on CK+ database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average Accuracy CSPL <ref type="bibr" target="#b9">[10]</ref> 89.9% AdaGabor <ref type="bibr" target="#b33">[34]</ref> 93.3% LBPSVM <ref type="bibr" target="#b10">[11]</ref> 95.1% BDBN <ref type="bibr" target="#b3">[4]</ref> 96.7% GoogLeNet(baseline) 95.0% PPDN 97.3% <ref type="table">Table 2</ref>. Performance comparisons on six facial expressions with UDCS method and the baseline using GoogLeNet in terms of average classification accuracy under same setting as UDCS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average Accuracy UDCS <ref type="bibr" target="#b34">[35]</ref> 49.5% GoogLeNet(baseline) 66.6% PPDN 72.4%</p><p>Datasets. FER datasets usually provide video sequences for training and testing the facial expression recognizers. We conducted all experiments on two popular datasets, CK+ <ref type="bibr" target="#b16">[17]</ref> and Oulu-CASIA dataset <ref type="bibr" target="#b17">[18]</ref>. For each sequence, the face often gradually evolves from a neutral to a peak facial expression. CK+ includes six basic facial expressions (angry, happy, surprise, sad, disgust, fear) and one non basic expression (contempt). It contains 593 sequences from 123 subjects, of which only 327 are annotated with expression labels. Oulu-CASIA contains 480 sequences of six facial expressions under normal illumination, including 80 subjects between 23 and 58 years old.</p><p>Comparisons with Still Image-based Approaches. <ref type="table">Table 1</ref> compares the PPDN to still image-based approaches on CK+, under the standard setting in which only the last one to three frames (i.e., nearly peak expressions) per sequence are considered for training and testing. Four state-of-the-art methods are considered: common and specific patches learning (CSPL) <ref type="bibr" target="#b9">[10]</ref>, which employs multi-task learning for feature selection, AdaGabor <ref type="bibr" target="#b33">[34]</ref> and LBPSVM <ref type="bibr" target="#b10">[11]</ref>, which are based on AdaBoost <ref type="bibr" target="#b35">[36]</ref>, and Boosted Deep Belief Network (BDBN) <ref type="bibr" target="#b3">[4]</ref>, which jointly optimizes feature extraction and feature selection. In addition, we also compare the PPDN to the baseline "GoogLeNet (baseline)," which optimizes the standard GoogLeNet with SGD. Similarly to previous methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4]</ref>, the PPDN is evaluated on the last three frames of each sequence. <ref type="table">Table 2</ref> compares the PPDN with UDCS <ref type="bibr" target="#b34">[35]</ref> on Oulu-CASIA, under a similar setting where the first 9 images of each sequence are ignored, the first 40 individuals are taken as training samples and the rest as testing. In all cases, the PPDN input is the pair of one of the non-peak frames (all frames other than the last one) and the corresponding peak frame (the last frame) in a sequence. The PPDN significantly outperforms all <ref type="table">Table 3</ref>. Performance comparison on CK+ database in terms of average classification accuracy of the 10-fold cross-validation when evaluating on three different test sets, including "weak expression", "peak expression" and "combined", respectively. Training and Testing with More Non-peak Expressions. The main advantage of the PPDN is its improved ability to recognize non-peak expressions. To test this, we compared how performance varies with the number of non-peak expressions. Note that for each video sequence, the face expression evolves from neutral to a peak expression. The first six frames within a sequence are usually neutral, with the peak expression appearing in the final frames. Empirically, we determined that the 7th to 9th frame often show non-peak expressions with very weak intensities, which we denote as "weak expressions." In addition to the training images used in the standard setting, we used all frames beyond the 7th for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method weak expression peak expression combined</head><p>Since the previous methods did not publish their codes, we only compare the PPDN to the baseline "GoogLeNet (baseline)". <ref type="table">Table 3</ref> reports results for CK+ and <ref type="table">Table 4</ref> for Oulu-CASIA. Three different test sets were considered: "weak expression" indicates that the test set only contains the non-peak expression images from the 7th to the 9th frames; "peak expression" only includes the last frame; and "combined" uses all frames from the 7th to the last. "PPDN (standard SGD)" is the version of PPDN trained with standard SGD optimization, and "GoogLeNet (baseline)" the basic GoogLeNet, taking each expression image as input and trained with SGD. The most substantial improvements are obtained on the "weak expression" test set, 83.36% and 67.95% of "PPDN" vs. 78.10% and 64.64% of "GoogLeNet (baseline)" on CK+ and Oulu-CASIA, respectively. This is evidence in support of the advantage of explicitly learning the evolution from non-peak to peak expressions. In addition, the PPDN outperforms "PPDN (standard SGD)" and "GoogLeNet (baseline)" on the combined sets, where both peak and non-peak expressions are evaluated.</p><p>Comparisons with Sequence-based Approaches. Unlike the still-image recognition setting, which evaluates the predictions of frames from a sequence, the sequence-based setting requires a prediction for the whole sequence. Previous sequence-based approaches take the whole sequence as input and use motion information during inference. Instead, the PPDN regards each pair of non-peak and peak frame as input, and only outputs the label of the peak frame as prediction for the whole sequence, in the testing phase. <ref type="table">Tables 5 and 6</ref> compare the PPDN to several sequence-based approaches plus <ref type="table">Table 4</ref>. Performance comparison on Oulu-CASIA database in terms of average classification accuracy of the 10-fold cross-validation when evaluating on three different test sets, including "weak expression", "peak expression" and "combined", respectively.  <ref type="table">Table 5</ref>. Performance comparisons with three sequence-based approaches and the baseline "GoogLeNet (baseline)" in terms of average classification accuracy of the 10-fold crossvalidation on CK+ database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Experimental Settings Average Accuracy 3DCNN-DAP <ref type="bibr" target="#b36">[37]</ref> sequence-based 92.4% STM-ExpLet <ref type="bibr" target="#b0">[1]</ref> sequence-based 94.2% DTAGN(Joint) <ref type="bibr" target="#b6">[7]</ref> sequence-based 97.3% GoogLeNet (baseline) image-based 99.0% PPDN (standard SGD) image-based 99.1% PPDN w/o peak image-based 99.2% PPDN image-based 99.3% <ref type="table">Table 6</ref>. Performance comparisons with five sequence-based approaches and the baseline "GoogLeNet (baseline)" in terms of average classification accuracy of the 10-fold crossvalidation on Oulu-CASIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Experimental Settings Average Accuracy HOG 3D <ref type="bibr" target="#b20">[21]</ref> sequence-based 70.63% AdaLBP <ref type="bibr" target="#b17">[18]</ref> sequence-based 73.54% Atlases <ref type="bibr" target="#b19">[20]</ref> sequence-based 75.52% STM-ExpLet <ref type="bibr" target="#b0">[1]</ref> sequence-based 74.59% DTAGN(Joint) <ref type="bibr" target="#b6">[7]</ref> sequence-based 81.46% GoogLeNet (baseline) image-based 79.21% PPDN (standard SGD) image-based 82.91% PPDN w/o peak image-based 83.67% PPDN image-based 84.59% "GoogLeNet(baseline)" on CK+ and Oulu-CASIA. Compared with <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b6">7]</ref>, which leverage motion information, the PPDN, which only relies on appearance information, achieves significantly better prediction performance. On CK+, it has gains of 5.1% and 2% over 'STM-ExpLet" <ref type="bibr" target="#b0">[1]</ref> and "DTAGN(Joint)" <ref type="bibr" target="#b6">[7]</ref>. On Oulu-CASIA it achieves 84.59% vs. the 75.52% of "Atlases" <ref type="bibr" target="#b19">[20]</ref> and the 81.46% of "DTAGN(Joint)" <ref type="bibr" target="#b6">[7]</ref>. In addition, we evaluate this experiment without peak information, i.e. selecting image with highest classification scores for all categories as peak frame in testing. PPDN achieves 99.2% on CK+ and 83.67% on Oulu-CASIA. <ref type="table">Table 7</ref>. Performance comparisons by adding the peak-piloted feature transformation on different convolutional layers when evaluated on Oulu-CASIA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method inception layersthe last FC layerthe first FC layerboth FC layers</head><formula xml:id="formula_5">Inception-3a ! # # # Inception-3b ! # # # Inception-4a ! # # # Inception-4b ! # # # Inception-4c ! # # # Inception-4d ! # # # Inception-4e ! # # # Inception-5a ! # # # Inception-5b ! # # # Fc1 ! # ! ! Fc2 ! ! # !</formula><p>Average Accuracy 74.49% 73.33% 73.48% 74.99% <ref type="table">Table 8</ref>. Comparisons of the version with and without using peak information on Oulu-CASIA database in terms of average classification accuracy of the 10-fold cross-validation. PGS vs. standard SGD. As discussed above, PGS suppresses gradients from peak expressions, so as to drive the features of non-peak expression samples towards those of peak expression samples, but not the contrary. Standard SGD uses all gradients, due to both non-peak and peak expression samples. We hypothesized that this will degrade recognition for samples of peak expressions, due to interference from non-peak expression samples. This hypothesis is confirmed by the results of <ref type="table">Tables 3 and 4</ref>. PGS outperforms standard SGD on all three test sets.</p><p>Ablative Studies on Peak-Piloted Feature Transformation. The peak-piloted feature transformation, which is the key innovation of the PPDN, can be used on all layers of the network. Employing the transformation on different convolutional and fully-connected layers can result in different levels of supervision of non-peak responses by peak responses. For example, early convolutional layers extract fine-grained details (e.g., local boundaries or illuminations) of faces, while later layers capture more semantic information, e.g., the appearance pattens of mouths and eyes. <ref type="table">Table 7</ref> presents an extensive comparison, by adding peak-piloted feature supervision on various layers. Note that we employ GoogLeNet <ref type="bibr" target="#b23">[24]</ref>, which includes 9 inception layers, as basic network. Four different settings are tested: "inception layers" indicates that the loss of the peak-piloted feature transformation is appended for all inception layers plus the two fully-connected layers; "the first FC layer,""the last FC layer" and "both FC layers" append the loss to the first, last, and and both fully-connected layers, respectively. It can be seen that using the peak-piloted feature transformation only on the two fully connected layers achieves the best performance. Using additional losses on all inception layers has roughly the same performance. Eliminating the loss of a fullyconnected layer decreases performance by more than 1%. These results show that the peak-piloted feature transformation is more useful for supervising the highly semantic feature representations (two fully-connected layers) than the early convolutional layers.</p><p>Absence of Peak Information. <ref type="table">Table 8</ref> demonstrates that the PPDN can also be used when the peak frame is not known a priori, which is usually the case for real-world videos. Given all video sequences, we trained the basic "GoogLeNet (baseline)" with 10-fold cross validation. The models were trained with 9-folds and then used to predict the ground-truth expression label in the remaining fold. The frame with the highest prediction score in each sequence was treated as the peak expression image. The PPDN was finally trained using the strategy of the previous experiments. This training procedure is more applicable to videos where the information of the peak expression is not available. The PPDN can still obtain results comparable to those of the model trained with the ground-truth peak frame information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generalization Ability of the PPDN</head><p>The learning of the evolution from a hard sample to an easy sample is applicable to other face-related recognition tasks. We demonstrate this by evaluating the PPDN on face recognition. One challenge to this task is learning robust features, invariant to pose and view. In this case, near-frontal faces can be treated easy examples, similar to peak expressions in FER, while profile faces can be viewed as hard samples, similar to non-peak expressions. The effectiveness of PPDN in learning pose-invariant features is demonstrated by comparing PPDN features to the "GoogLeNet(baseline)" features on the popular Multi-PIE dataset <ref type="bibr" target="#b18">[19]</ref>.</p><p>All the following experiments were conducted on the images of "session 1" on Multi-PIE, where the face images of 249 subjects are provided. Two experimental settings were evaluated to demonstrate the generalization ability of PPDN on face recognition. For the "setting 1" of <ref type="table">Table 9</ref>, only images under normal illumination were used for training and testing, where seven poses of the first 100 subjects (ID from 001 to 100) were used for training and the six poses (from −45 • to 45 • ) of the remaining individuals used for testing. One frontal face per subject was used as gallery image. Overall, 700 images were used for training and 894 images for testing. By treating the frontal face and one of the profile faces as input, the PPDN can embed the implicit transformation from profile faces to frontal faces into the network learning, for face recognition purposes. In the "setting 2" of <ref type="table">Table 10</ref>, 100 subjects (ID 001 to 100) with seven different poses under 20 different illumination conditions were used for training and the rest with six poses and 19 illumination conditions were used for testing. This led to 14,000 training images and 16,986 testing images. Similarly to the first setting, PPDN takes the pair of a frontal face with normal illumination and one of the profile faces with 20 illuminations from the same subject as the input. The PPDN can thus learn the evolution from both the profile to the frontal face and non-normal to normal illumination. In addition to "GoogLeNet (baseline)," we compared the PPDN to four state-of-the-art methods: controlled pose feature(CPF) <ref type="bibr" target="#b27">[28]</ref>, controlled pose image(CPI) <ref type="bibr" target="#b27">[28]</ref>, Zhu et al. <ref type="bibr" target="#b26">[27]</ref> and Li et al. <ref type="bibr" target="#b37">[38]</ref>. The pre-trained model, prepocessing steps, and learning rate used in the FER experiments were adopted here. Under "setting 1" the network was trained with 10,000 iterations and under "setting 2" with 30,000 iterations. Face recognition performance is measured by the accuracy of the predicted subject identity.</p><p>It can be seen that the PPDN achieves considerable improvements over "GoogLeNet (baseline)" for the testing images of hard poses (i.e., −45 • and 45 • ) in both "setting 1" and "setting 2". Significant improvements over "GoogLeNet (baseline)" are also observed for the average over all poses (97.98% vs 95.99% under "setting 1" and 83.22% vs 74.84% under "setting 2"). The PPDN also beats all baselines by 2.52% under "setting 2". This supports the conclusion that the PPDN can be effectively generalized to face recognition tasks, which benefit from embedding the evolution from hard to easy samples into the network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a novel peak-piloted deep network for facial expression recognition. The main novelty is the embedding of the expression evolution from non-peak to peak into the network parameters. PPDN jointly optimizes an L2-norm loss of peakpiloted feature transformation and the cross-entropy losses of expression recognition. By using a special-purpose back-propagation procedure (PGS) for network optimization, the PPDN can drive the intermediate-layer features of the non-peak expression sample towards those of the peak expression sample, while avoiding the inverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The loss</p><formula xml:id="formula_6">J 1 = N i=1 j∈Ω f j (x p i , W ) − f j (x n i , W ) 2 (A-1)</formula><p>has gradient</p><formula xml:id="formula_7">∇ W J 1 = 2 N i=1 j∈Ω (f j (x p i , W ) − f j (x n i , W ))∇ W f j (x n i , W ) + 2 N i=1 j∈Ω (f j (x p i , W ) − f j (x n i , W ))∇ W f j (x p i , W ). (A-2)</formula><p>The PGS is</p><formula xml:id="formula_8">∇ W J 1 = 2 N i=1 j∈Ω (f j (x p i , W ) − f j (x n i , W ))∇ W f j (x n i , W ) (A-3) Defining A = N i=1 j∈Ω (f j (x p i , W ) − f j (x n i , W ))∇ W f j (x n i , W ) (A-4) and B = N i=1 j∈Ω (f j (x p i , W ) − f j (x n i , W ))∇ W f j (x p i , W ) (A-5) it follows that &lt; ∇ W J 1 , ∇ W J 1 &gt; = −4 &lt; A, B &gt; +4 A 2 (A-6) or &lt; ∇ W J 1 , ∇ W J 1 &gt; = −4 A B cos θ + 4 A 2 (A-7)</formula><p>where θ is angle between A and B. Hence, the dot-product is greater than zero when B cos θ &lt; A .</p><p>(A-8)</p><p>This holds for sure as ∇ W f j (x n i , W ) converges to ∇ W f j (x p i , W ) which is the goal of optimization, but is generally true if the sizes of gradients ∇ W f j (x n i , W ) and ∇ W f j (x p i , W ) are similar on average. Since the dot-product is positive, ∇ W J 1 is a descent (although not a steepest descent) direction for the loss function J 1 . Hence, the PGS is a descent direction for the total loss. Note that, because there are also the gradients of J 2 and J 3 , this can hold even when (A-8) is violated, if the gradients of J2 and J3 are dominant. Hence, the PGS is likely to converge to a minimum of the loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 9 .Table 10 .</head><label>910</label><figDesc>Face recognition rates for various poses under "setting 1". Method −45 • −30 • −15 • +15 • +30 • +45 • Average GoogLeNet (baseline) 86.57% 99.3% 100% 100% 100% 90.06% 95.99% PPDN 93.96% 100% 100% 100% 100% 93.96% 97.98% Face recognition rates for various poses under "setting 2". Method −45 • −30 • −15 • +15 • +30 • +45</figDesc><table><row><cell cols="2">Method</cell><cell cols="3">weak expression peak expression combined</cell></row><row><cell cols="2">PPDN w/o peak</cell><cell>67.52%</cell><cell>83.79%</cell><cell>74.01%</cell></row><row><cell cols="2">PPDN</cell><cell>67.95%</cell><cell>84.59%</cell><cell>74.99%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>• Average</cell></row><row><cell>Li et al. [38]</cell><cell cols="4">56.62% 77.22% 89.12% 88.81% 79.12% 58.14% 74.84%</cell></row><row><cell>Zhu et al. [27]</cell><cell cols="4">67.10% 74.60% 86.10% 83.30% 75.30% 61.80% 74.70%</cell></row><row><cell>CPI [28]</cell><cell cols="4">66.60% 78.00% 87.30% 85.50% 75.80% 62.30% 75.90%</cell></row><row><cell>CPF [28]</cell><cell cols="4">73.00% 81.70% 89.40% 89.50% 80.50% 70.30% 80.70%</cell></row><row><cell cols="5">GoogLeNet (baseline) 56.62% 77.22% 89.12% 88.81% 79.12% 58.14% 74.84%</cell></row><row><cell>PPDN</cell><cell cols="4">72.06% 85.41% 92.44% 91.38% 87.07% 70.97% 83.22%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1749" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d model-based continuous emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1836" to="1845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pairwise conditional random forests for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubuisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3783" to="3791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Facial expression recognition via a boosted deep belief network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1805" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image based static facial expression recognition with multiple deep network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
		<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Au-aware deep networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference and Workshops on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Expression-invariant face recognition with expression classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd Canadian Conference on</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="77" to="77" />
		</imprint>
	</monogr>
	<note>Computer and Robot Vision</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning active facial patches for expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2562" to="2569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Facial expression analysis based on high dimensional binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Froumenty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="135" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep cascaded regression for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.09083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04110</idno>
		<title level="m">Going deeper in facial expression recognition using deep neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The extended cohnkanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Facial expression recognition from near-infrared videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="607" to="619" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic facial expression recognition using longitudinal facial expression atlases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="631" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2008-19th British Machine Vision Conference, British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="275" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring bag of words architectures in the facial expression domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012. Workshops and Demonstrations</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning identity-preserving face space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rotating your face using multi-task deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="676" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<title level="m">Deepid3: Face recognition with very deep neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognizing facial expression: machine learning and application to spontaneous behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lainscsek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="568" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The uncorrelated and discriminant colour space for facial expression recognition. Optimization and Control Techniques and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="167" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second European Conference on Computational Learning Theory</title>
		<meeting>the Second European Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeply learning deformable facial action parts model for dynamic expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Coupled bias-variance tradeoff for cross-pose face recognition. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="305" to="315" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
