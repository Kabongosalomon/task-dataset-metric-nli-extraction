<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frustratingly Simple Few-Shot Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
						</author>
						<title level="a" type="main">Frustratingly Simple Few-Shot Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting rare objects from a few examples is an emerging problem. Prior works show metalearning is a promising approach. But, finetuning techniques have drawn scant attention. We find that fine-tuning only the last layer of existing detectors on rare classes is crucial to the few-shot object detection task. Such a simple approach outperforms the meta-learning methods by roughly 2∼20 points on current benchmarks and sometimes even doubles the accuracy of the prior methods. However, the high variance in the few samples often leads to the unreliability of existing benchmarks. We revise the evaluation protocols by sampling multiple groups of training examples to obtain stable comparisons and build new benchmarks based on three datasets: PASCAL VOC, COCO and LVIS. Again, our fine-tuning approach establishes a new state of the art on the revised benchmarks. The code as well as the pretrained models are available at https://github.com/ucbdrive/ few-shot-object-detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine perception systems have witnessed significant progress in the past years. Yet, our ability to train models that generalize to novel concepts without abundant labeled data is still far from satisfactory when compared to human visual systems. Even a toddler can easily recognize a new concept with very little instruction <ref type="bibr" target="#b10">(Landau et al., 1988;</ref><ref type="bibr" target="#b17">Samuelson &amp; Smith, 2005;</ref><ref type="bibr" target="#b19">Smith et al., 2002)</ref>.</p><p>The ability to generalize from only a few examples (so called few-shot learning) has become a key area of interest in the machine learning community. Many <ref type="bibr" target="#b21">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b20">Snell et al., 2017;</ref><ref type="bibr" target="#b3">Finn et al., 2017;</ref><ref type="bibr" target="#b4">Gidaris &amp; Komodakis, 2018;</ref><ref type="bibr" target="#b22">Wang et al., 2019a)</ref> have explored techniques to transfer knowledge from the data-abundant base classes to the data-scarce novel classes through meta-learning. They use simulated few-shot tasks by sampling from base classes during training to learn to learn from the few examples in the novel classes.</p><p>However, much of this work has focused on basic image classification tasks. In contrast, few-shot object detection has received far less attention. Unlike image classification, object detection requires the model to not only recognize the object types but also localize the targets among millions of potential regions. This additional subtask substantially raises the overall complexity. Several <ref type="bibr" target="#b8">(Kang et al., 2019;</ref><ref type="bibr" target="#b24">Yan et al., 2019;</ref><ref type="bibr" target="#b23">Wang et al., 2019b)</ref> have attempted to tackle the under-explored few-shot object detection task, where only a few labeled bounding boxes are available for novel classes. These methods attach meta learners to existing object detection networks, following the meta-learning methods for classification. But, current evaluation protocols suffer from statistical unreliability, and the accuracy of baseline methods, especially simple fine-tuning, on few-object detection are not consistent in the literature.</p><p>In this work, we propose improved methods to evaluate few-shot object detection. We carefully examine finetuning based approaches, which are considered to be underperforming in the previous works <ref type="bibr" target="#b8">(Kang et al., 2019;</ref><ref type="bibr" target="#b24">Yan et al., 2019;</ref><ref type="bibr" target="#b23">Wang et al., 2019b)</ref>. We focus on the training schedule and the instance-level feature normalization of the object detectors in model design and training based on fine-tuning.</p><p>We adopt a two-stage training scheme for fine-tuning as shown in <ref type="figure">Figure 1</ref>. We first train the entire object detector, such as Faster R-CNN <ref type="bibr" target="#b15">(Ren et al., 2015)</ref>, on the dataabundant base classes, and then only fine-tune the last layers of the detector on a small balanced training set consisting of both base and novel classes while freezing the other parameters of the model. During the fine-tuning stage, we introduce instance-level feature normalization to the box classifier inspired by <ref type="bibr" target="#b4">Gidaris &amp; Komodakis (2018)</ref>; <ref type="bibr" target="#b14">Qi et al. (2018)</ref>; <ref type="bibr" target="#b0">Chen et al. (2019)</ref>.</p><p>We find that this two-stage fine-tuning approach (TFA) outperforms all previous state-of-the-art meta-learning based methods by 2∼20 points on the existing PASCAL VOC <ref type="bibr" target="#b2">(Everingham et al., 2007)</ref> and COCO <ref type="bibr" target="#b11">(Lin et al., 2014)</ref> benchmarks. When training on a single novel example (one-shot <ref type="figure">Figure 1</ref>. Illustration of our two-stage fine-tuning approach (TFA). In the base training stage, the entire object detector, including both the feature extractor F and the box predictor, are jointly trained on the base classes. In the few-shot fine-tuning stage, the feature extractor components are fixed and only the box predictor is fine-tuned on a balanced subset consisting of both the base and novel classes. learning), our method can achieve twice the accuracy of prior sophisticated state-of-the-art approaches.</p><p>Several issues with the existing evaluation protocols prevent consistent model comparisons. The accuracy measurements have high variance, making published comparisons unreliable. Also, the previous evaluations only report the detection accuracy on the novel classes, and fail to evaluate knowledge retention on the base classes.</p><p>To resolve these issues, we build new benchmarks on three datasets: PASCAL VOC, COCO and LVIS <ref type="bibr" target="#b5">(Gupta et al., 2019)</ref>. We sample different groups of few-shot training examples for multiple runs of the experiments to obtain a stable accuracy estimation and quantitatively analyze the variances of different evaluation metrics. The new evaluation reports the average precision (AP) on both the base classes and novel classes as well as the mean AP on all classes, referred to as the generalized few-shot learning setting in the few-shot classification literature <ref type="bibr" target="#b22">Wang et al., 2019a)</ref>.</p><p>Our fine-tuning approach establishes new states of the art on the benchmarks. On the challenging LVIS dataset, our two-stage training scheme improves the average detection precision of rare classes (&lt;10 images) by ∼4 points and common classes (10-100 images) by ∼2 points with negligible precision loss for the frequent classes (&gt;100 images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is related to the rich literature on few-shot image classification, which uses various meta-learning based or metric-learning based methods. We also draw connections between our work and the existing meta-learning based fewshot object detection methods. To the best of our knowledge, we are the first to conduct a systematic analysis of finetuning based approaches on few-shot object detection.</p><p>Meta-learning. The goal of meta-learning is to acquire task-level meta knowledge that can help the model quickly adapt to new tasks and environments with very few labeled examples. Some <ref type="bibr" target="#b3">(Finn et al., 2017;</ref><ref type="bibr" target="#b16">Rusu et al., 2018;</ref><ref type="bibr" target="#b13">Nichol et al., 2018)</ref> learn to fine-tune and aim to obtain a good parameter initialization that can adapt to new tasks with a few scholastic gradient updates. Another popular line of research on meta-learning is to use parameter generation during adaptation to novel tasks. <ref type="bibr" target="#b4">Gidaris &amp; Komodakis (2018)</ref> propose an attention-based weight generator to generate the classifier weights for the novel classes. <ref type="bibr" target="#b22">Wang et al. (2019a)</ref> construct task-aware feature embeddings by generating parameters for the feature layers. These approaches have only been used for few-shot image classification and not on more challenging tasks like object detection.</p><p>However, some  raise concerns about the reliability of the results given that a consistent comparison of different approaches is missing. Some simple fine-tuning based approaches, which draw little attention in the community, turn out to be more favorable than many prior works that use meta-learning on few-shot image classification <ref type="bibr" target="#b1">Dhillon et al., 2019)</ref>. As for the emerging few-shot object detection task, there is neither consensus on the evaluation benchmarks nor a consistent comparison of different approaches due to the increased network complexity, obscure implementation details, and variances in evaluation protocols.</p><p>Metric-learning. Another line of work <ref type="bibr" target="#b9">(Koch, 2015;</ref><ref type="bibr" target="#b20">Snell et al., 2017;</ref><ref type="bibr" target="#b21">Vinyals et al., 2016)</ref> focuses on learning to compare or metric-learning. Intuitively, if the model can construct distance metrics to estimate the similarity between two input images, it may generalize to novel categories with few labeled instances. More recently, several <ref type="bibr" target="#b4">Gidaris &amp; Komodakis, 2018;</ref><ref type="bibr" target="#b14">Qi et al., 2018)</ref> adopt a cosine similarity based classifier to reduce the intraclass variance on the few-shot classification task, which leads to favorable performance compared to many metalearning based approaches. Our method also adopts a cosine similarity classifier to classify the categories of the region proposals. However, we focus on the instance-level distance measurement rather than on the image level.</p><p>Few-shot object detection. There are several early attempts at few-shot object detection using meta-learning. <ref type="bibr" target="#b8">Kang et al. (2019)</ref> and <ref type="bibr" target="#b24">Yan et al. (2019)</ref> apply feature re-weighting schemes to a single-stage object detector (YOLOv2) and a two-stage object detector (Faster R-CNN), with the help of a meta learner that takes the support images (i.e., a small number of labeled images of the novel/base classes) as well as the bounding box annotations as inputs. <ref type="bibr" target="#b23">Wang et al. (2019b)</ref> propose a weight prediction meta-model to predict parameters of category-specific components from the few examples while learning the category-agnostic components from base class examples.</p><p>In all these works, fine-tuning based approaches are considered as baselines with worse performance than metalearning based approaches. They consider jointly finetuning, where base classes and novel classes are trained together, and fine-tuning the entire model, where the detector is first trained on the base classes only and then fine-tuned on a balanced set with both base and novel classes. In contrast, we find that fine-tuning only the last layer of the object detector on the balanced subset and keeping the rest of model fixed can substantially improve the detection accuracy, outperforming all the prior meta-learning based approaches. This indicates that feature representations learned from the base classes might be able to transfer to the novel classes and simple adjustments to the box predictor can provide strong performance gain <ref type="bibr" target="#b1">(Dhillon et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithms for Few-Shot Object Detection</head><p>In this section, we start with the preliminaries on the fewshot object detection setting. Then, we talk about our twostage fine-tuning approach in Section 3.1. Section 3.2 summarizes the previous meta-learning approaches.</p><p>We follow the few-shot object detection settings introduced in <ref type="bibr" target="#b8">Kang et al. (2019)</ref>. There are a set of base classes C b that have many instances and a set of novel classes C n that have only K (usually less than 10) instances per category. For an object detection dataset D = {(x, y), x ∈ X , y ∈ Y}, where x is the input image and y = {(c i , l i ), i = 1, ..., N } denotes the categories c ∈ C b ∪ C n and bounding box coordinates l of the N object instances in the image x. For synthetic few-shot datasets using PASCAL VOC and COCO, the novel set for training is balanced and each class has the same number of annotated objects (i.e., K-shot). The recent LVIS dataset has a natural long-tail distribution, which does not have the manual K-shot split. The classes in LVIS are divided into frequent classes (appearing in more than 100 images), common classes (10-100 images), and rare classes (less than 10 images). We consider both synthetic and natural datasets in our work and follow the naming convention of k-shot for simplicity.</p><p>The few-shot object detector is evaluated on a test set of both the base classes and the novel classes. The goal is to optimize the detection accuracy measured by average precision (AP) of the novel classes as well as the base classes. This setting is different from the N -way-K-shot setting <ref type="bibr" target="#b3">(Finn et al., 2017;</ref><ref type="bibr" target="#b21">Vinyals et al., 2016;</ref><ref type="bibr" target="#b20">Snell et al., 2017)</ref> commonly used in few-shot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Two-stage fine-tuning approach</head><p>We describe our two-stage fine-tuning approach (TFA) for few-shot object detection in this section. We adopt the widely used Faster R-CNN <ref type="bibr" target="#b15">(Ren et al., 2015)</ref>, a two-stage object detector, as our base detection model. As shown in <ref type="figure">Figure 1</ref>, the feature learning components, referred to as F, of a Faster R-CNN model include the backbone (e.g., ResNet <ref type="bibr" target="#b7">(He et al., 2016)</ref>, VGG16 <ref type="bibr" target="#b18">(Simonyan &amp; Zisserman, 2014)</ref>), the region proposal network (RPN), as well as a twolayer fully-connected (FC) sub-network as a proposal-level feature extractor. There is also a box predictor composed of a box classifier C to classify the object categories and a box regressor R to predict the bounding box coordinates. Intuitively, the backbone features as well as the RPN features are class-agnostic. Therefore, features learned from the base classes are likely to transfer to the novel classes without further parameter updates. The key component of our method is to separate the feature representation learning and the box predictor learning into two stages.</p><p>Base model training. In the first stage, we train the feature extractor and the box predictor only on the base classes C b , with the same loss function used in <ref type="bibr" target="#b15">Ren et al. (2015)</ref>. The joint loss is,</p><formula xml:id="formula_0">L = L rpn + L cls + L loc ,<label>(1)</label></formula><p>where L rpn is applied to the output of the RPN to distinguish foreground from backgrounds and refine the anchors, L cls is a cross-entropy loss for the box classifier C, and L loc is a smoothed L 1 loss for the box regressor R.</p><p>Few-shot fine-tuning. In the second stage, we create a small balanced training set with K shots per class, containing both base and novel classes. We assign randomly initialized weights to the box prediction networks for the novel classes and fine-tune only the box classification and regression networks, namely the last layers of the detection model, while keeping the entire feature extractor F fixed. We use the same loss function in Equation 1 and a smaller learning rate. The learning rate is reduced by 20 from the first stage in all our experiments.</p><p>Cosine similarity for box classifier. We consider using a classifier based on cosine similarity in the second fine-tuning stage, inspired by Gidaris &amp; Komodakis <ref type="formula" target="#formula_0">(2018)</ref>  <ref type="figure">Figure 2</ref>. Abstraction of the meta-learning based few-shot object detectors. A meta-learner is introduced to acquire task-level meta information and help the model generalize to novel classes through feature re-weighting (e.g., FSRW and Meta R-CNN) or weight generation (e.g., MetaDet). A two-stage training approach (meta-training and meta fine-tuning) with episodic learning is commonly adopted.</p><p>scaled similarity scores S of the input feature F(x) and the weight vectors of different classes. The entries in S are</p><formula xml:id="formula_1">s i,j = αF(x) i w j F(x) i w j ,<label>(2)</label></formula><p>where s i,j is the similarity score between the i-th object proposal of the input x and the weight vector of class j. α is the scaling factor. We use a fixed α of 20 in our experiments. We find empirically that the instance-level feature normalization used in the cosine similarity based classifier helps reduce the intra-class variance and improves the detection accuracy of novel classes with less decrease in detection accuracy of base classes when compared to a FC-based classifier, especially when the number of training examples is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Meta-learning based approaches</head><p>We describe the existing meta-learning based few-shot object detection networks, including FSRW <ref type="bibr" target="#b8">(Kang et al., 2019)</ref>, Meta R-CNN <ref type="bibr" target="#b24">(Yan et al., 2019)</ref> and MetaDet <ref type="bibr" target="#b23">(Wang et al., 2019b)</ref>, in this section to draw comparisons with our approach. <ref type="figure">Figure 2</ref> illustrates the structures of these networks.</p><p>In meta-learning approaches, in addition to the base object detection model that is either single-stage or two-stage, a meta-learner is introduced to acquire class-level meta knowledge and help the model generalize to novel classes through feature re-weighting, such as FSRW and Meta R-CNN, or class-specific weight generation, such as MetaDet. The input to the meta learner is a small set of support images with the bounding box annotations of the target objects.</p><p>The base object detector and the meta-learner are often jointly trained using episodic training <ref type="bibr" target="#b21">(Vinyals et al., 2016)</ref>. Each episode is composed of a supporting set of N objects and a set of query images. In FSRW and Meta R-CNN, the support images and the binary masks of the annotated objects are used as input to the meta-learner, which generates class reweighting vectors that modulate the feature representation of the query images. As shown in <ref type="figure">Figure 2</ref>, the training procedure is also split into a meta-training stage, where the model is only trained on the data of the base classes, and a meta fine-tuning stage, where the support set includes the few examples of the novel classes and a subset of examples from the base classes.</p><p>Both the meta-learning approaches and our approach have a two-stage training scheme. However, we find that the episodic learning used in meta-learning approaches can be very memory inefficient as the number of classes in the supporting set increases. Our fine-tuning method only finetunes the last layers of the network with a normal batch training scheme, which is much more memory efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive comparisons with previous methods on the existing few-shot object detection benchmarks using PASCAL VOC and COCO, where our approach can obtain about 2∼20 points improvement in all settings (Section 4.1). We then introduce a new benchmark on three datasets (PASCAL VOC, COCO and LVIS) with revised evaluation protocols to address the unreliability of previous benchmarks (Section 4.2). We also provide various ablation studies and visualizations in Section 4.3.</p><p>Implementation details. We use Faster R-CNN <ref type="bibr" target="#b15">(Ren et al., 2015)</ref> as our base detector and Resnet-101 <ref type="bibr" target="#b7">(He et al., 2016)</ref> with a Feature Pyramid Network <ref type="bibr" target="#b12">(Lin et al., 2017)</ref> as the backbone. All models are trained using SGD with a minibatch size of 16, momentum of 0.9, and weight decay of 0.0001. A learning rate of 0.02 is used during base training and 0.001 during few-shot fine-tuning. For more details, the code is available at https://github.com/ ucbdrive/few-shot-object-detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Existing few-shot object detection benchmark</head><p>Existing benchmarks. Following the previous work <ref type="bibr" target="#b8">(Kang et al., 2019;</ref><ref type="bibr" target="#b24">Yan et al., 2019;</ref><ref type="bibr" target="#b23">Wang et al., 2019b)</ref>, we first <ref type="table">Table 1</ref>. Few-shot detection performance (mAP50) on the PASCAL VOC dataset. We evaluate the performance on three different sets of novel classes. Our approach consistently outperforms baseline methods by a large margin (about 2∼20 points), especially when the number of shots is low. FRCN stands for Faster R-CNN. TFA w/ cos is our approach with a cosine similarity based box classifier. <ref type="table" target="#tab_2">Novel Set 1  Novel Set 2  Novel Set 3  1  2  3  5  10  1  2  3  5  10  1  2  3  5  10</ref> YOLO-joint <ref type="bibr" target="#b8">(Kang et al., 2019)</ref> YOLOv2 0.0 0.0 1.8 1.8 1.8 0.0 0.1 0.0 1.8 0.0 1.8 1.8 1.8 3.6 3.9 YOLO-ft <ref type="bibr" target="#b8">(Kang et al., 2019)</ref> 3.2 6.5 6.4 7.5 12.3 8.2 3.8 3.5 3.5 7.8 8.1 7.4 7.6 9.5 10.5 YOLO-ft-full <ref type="bibr" target="#b8">(Kang et al., 2019)</ref> 6.6 10.7 12.5 24.8 38.6 12.5 4.2 11.6 16.1 33.9 13.0 15.9 15.0 32.2 38.4 FSRW <ref type="bibr" target="#b8">(Kang et al., 2019)</ref> 14.8 15.5 26.7 33.9 47.2 15.7 15.3 22.7 30.1 40.5 21.3 25.6 28.4 42.8 45.9 MetaDet <ref type="bibr" target="#b23">(Wang et al., 2019b)</ref> 17.1 19.1 28.9 35.0 48.8 18.2 20.6 25.9 30.6 41.5 20.1 22.3 27.9 41.9 42.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method / Shot Backbone</head><p>FRCN+joint <ref type="bibr" target="#b23">(Wang et al., 2019b)</ref> FRCN w/VGG16 0.3 0.0 1.2 0.9 1.7 0.0 0.0 1.1 1.9 1.7 0.2 0.5 1.2 1.9 2.8 FRCN+joint-ft <ref type="bibr" target="#b23">(Wang et al., 2019b)</ref> 9.1 10.9 13.7 25.0 39.5 10.9 13.2 17.6 19.5 36.5 15.0 15.1 18.3 33.1 35.9 MetaDet <ref type="bibr" target="#b23">(Wang et al., 2019b)</ref> 18.9 20.6 30.  <ref type="bibr" target="#b8">Kang et al. (2019)</ref> and <ref type="bibr" target="#b24">Yan et al. (2019)</ref>.</p><p>Results on PASCAL VOC. We provide the average AP50 of the novel classes on PASCAL VOC with three random splits in <ref type="table">Table 1</ref>. Our approach uses ResNet-101 as the backbone, similar to Meta R-CNN. We implement FRCN+ft-full in our framework, which roughly matches the results reported in <ref type="bibr" target="#b24">Yan et al. (2019)</ref>. MetaDet uses VGG16 as the backbone, but the performance is similar and sometimes worse compared to Meta R-CNN.</p><p>In all different data splits and different numbers of training shots, our approach (the last row) is able to outperform the previous methods by a large margin. It even doubles the performance of the previous approaches in the one-shot cases. The improvements, up to 20 points, is much larger than the gap among the previous meta-learning based approaches, indicating the effectiveness of our approach. We also compare the cosine similarity based box classifier (TFA +w/cos) with a normal FC-based classifier (TFA +w/fc) and find that TFA +w/cos is better than TFA +w/fc on extremely low shots (e.g., 1-shot), but the two are roughly similar when there are more training shots, e.g., 10-shot.</p><p>For more detailed comparisons, we cite the numbers from <ref type="bibr" target="#b24">Yan et al. (2019)</ref> of their model performance on the base classes in <ref type="table" target="#tab_2">Table 2</ref>. We find that our model has a much higher average AP on the base classes than Meta R-CNN with a gap of about 10 to 15 points. To eliminate the differences in implementation details, we also report our re-implementation of FRCN+ft-full and training base only, where the base classes should have the highest accuracy as the model is only trained on the base classes examples. We find that our model has a small decrease in performance, less than 2 points, on the base classes. Results on COCO. Similarly, we report the average AP and AP75 of the 20 novel classes on COCO in <ref type="table" target="#tab_3">Table 3</ref>. AP75 means matching threshold is 0.75, a more strict metric than AP50. Again, we consistently outperform previous methods across all shots on both novel AP and novel AP75. We achieve around 1 point improvement in AP over the best performing baseline and around 2.5 points improvement in AP75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generalized few-shot object detection benchmark</head><p>Revised evaluation protocols. We find several issues with existing benchmarks. First, previous evaluation protocols focus only on the performance on novel classes. This ignores the potential performance drop in base classes and thus the overall performance of the network. Second, the sample variance is large due to the few samples that are used for training. This makes it difficult to draw conclusions from comparisons against other methods, as differences in performance could be insignificant.</p><p>To address these issues, we first revise the evaluation protocol to include evaluation on base classes. On our benchmark, we report AP on base classes (bAP) and the overall AP in addition to AP on the novel classes (nAP). This allows us to observe trends in performance on both base and novel classes, and the overall performance of the network.</p><p>Additionally, we train our models for multiple runs on different random samples of training shots to obtain averages and confidence intervals. In <ref type="figure" target="#fig_0">Figure 3</ref>, we show the cumulative mean and 95% confidence interval across 40 repeated runs with K = 1, 3, 5, 10 on the first split of PASCAL VOC.</p><p>Although the performance is high on the first random sample, the average decreases significantly as more samples are used. Additionally, the confidence intervals across the first few runs are large, especially in the low-shot scenario. When we use more repeated runs, the averages stabilizes and the confidence intervals become small, which allows for better comparisons.</p><p>Results on LVIS. We evaluate our approach on the recently introduced LVIS dataset ( <ref type="bibr" target="#b5">Gupta et al., 2019)</ref>. The number of images in each category in LVIS has a natural long-tail distribution. We treat the frequent and common classes as base classes, and the rare classes as novel classes. The base training is the same as before. During few-shot fine-tuning, we artificially create a balanced subset of the entire dataset by sampling up to 10 instances for each class and fine-tune on this subset.</p><p>We show evaluation results on LVIS in <ref type="table">Table 4</ref>. Compared to the methods in <ref type="bibr" target="#b5">Gupta et al. (2019)</ref>, our approach is able to achieve better performance of ∼1-1.5 points in overall AP and ∼2-4 points in AP for rare and common classes. We also demonstrate results without using repeated sampling, which is a weighted sampling scheme that is used in <ref type="bibr" target="#b5">Gupta et al. (2019)</ref> to address the data imbalance issue. In this setting, the baseline methods can only achieve ∼2-3 points in AP for rare classes. On the other hand, our approach is able to greatly outperform the baseline and increase the AP on rare classes by around 13 points and on common classes by around 1 point. Our two-stage fine-tuning scheme is able to address the severe data imbalance issue without needing repeated sampling.</p><p>Results on PASCAL VOC and COCO. We show evaluation results on generalized PASCAL VOC in <ref type="figure" target="#fig_1">Figure 4</ref> and <ref type="table">Table 4</ref>. Generalized object detection benchmarks on LVIS. We compare our approach to the baselines provided in LVIS <ref type="bibr" target="#b5">(Gupta et al., 2019)</ref>. Our approach outperforms the corresponding baseline across all metrics, backbones, and sampling schemes.  COCO in <ref type="figure">Figure 5</ref>. On both datasets, we evaluate on the base classes and the novel classes and report AP scores for each. On PASCAL VOC, we evaluate our models over 30 repeated runs and report the average and the 95% confidence interval. On COCO, we provide results on 1, 2, 3, and 5 shots in addition to the 10 and 30 shots used by the existing benchmark for a better picture of performance trends in the low-shot regime. For the full quantitative results of other metrics (e.g., AP50 and AP75), more details are available in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study and visualization</head><p>Weight initialization. We explore two different ways of initializing the weights of the novel classifier before few-shot fine-tuning: (1) random initialization and <ref type="formula" target="#formula_1">(2)</ref> fine-tuning a predictor on the novel set and using the classifier's weights as initialization. We compare both methods on K = 1, 3, 10 on split 3 of PASCAL VOC and COCO and show the results in <ref type="table" target="#tab_6">Table 5</ref>. On PASCAL VOC, simple random initialization can outperform initialization using fine-tuned novel weights. On COCO, using the novel weights can improve the performance over random initialization. This is probably due to the increased complexity and number of classes of COCO compared to PASCAL VOC. We use random initialization for all PASCAL VOC experiments and novel initialization for all COCO and LVIS experiments.</p><p>Scaling factor of cosine similarity. We explore the effect of different scaling factors for computing cosine similarity. We compare three different factors, α = 10, 20, 50. We use the same evaluation setting as the previous ablation study fork 92% <ref type="figure">Figure 6</ref>. Success (green boxes) and failure (red boxes) cases of our approach on novel classes from split 1 of PASCAL VOC (bird, bus, cow, sofa, and motorbike) and COCO (bird, cat, dog, train, and bottle). The black boxes are detected objects of irrelevant classes, which can be ignored. and report the results in <ref type="table" target="#tab_7">Table 6</ref>. On PASCAL VOC, α = 20 outperforms the other scale factors in both base AP and novel AP. On COCO, α = 20 achieves better novel AP at the cost of worse base AP. Since it has the best performance on novel classes across both datasets, we use α = 20 in all of our experiments with cosine similarity.</p><p>Detection results. We provide qualitative visualizations of the detected novel objects on PASCAL VOC and COCO in <ref type="figure">Figure 6</ref>. We show both success (green boxes) and failure cases (red boxes) when detecting novel objects for each dataset to help analyze the possible error types. On the first split of PASCAL VOC, we visualize the results of our 10-shot TFA w/ cos model. On COCO, we visualize the results of the 30-shot TFA w/cos model. The failure cases include misclassifying novel objects as similar base objects, e.g., row 2 columns 1, 2, 3, and 4, mislocalizing the objects, e.g., row 2 column 5, and missing detections, e.g., row 4 columns 1 and 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a simple two-stage fine-tuning approach for few-shot object detection. Our method outperformed the previous meta-learning methods by a large margin on the current benchmarks. In addition, we built more reliable benchmarks with revised evaluation protocols. On the new benchmarks, our models achieved new states of the arts, and on the LVIS dataset our models improved the AP of rare classes by 4 points with negligible reduction of the AP of frequent classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generalized Object Detection Benchmarks</head><p>We present the full benchmark results of PASCAL VOC <ref type="table" target="#tab_8">(Table 7)</ref> and COCO <ref type="table" target="#tab_10">(Table 8)</ref> on the revised benchmark used in this work. We report the average AP, AP50 and AP75 for all the classes, base classes only, and novel classes only in the tables. For each evaluation metric, we report the average value of n repeated runs with different groups of randomly sampled training shots (30 for PASCAL VOC and 10 for COCO) as well as the 95% confidence interval estimate of the mean values. The 95% confidence interval is calculated by</p><formula xml:id="formula_2">95% CI = 1.96 · s √ n ,<label>(3)</label></formula><p>where 1.96 is the Z-value, s is the standard deviation, and n is the number of repeated runs.</p><p>We compare two of our methods, one using a FC-based classifier (TFA w/fc) and one using a cosine similarity based classifier (TFA w/cos). We also compare against a finetuning baseline FRCN+ft-full and against FSRW <ref type="bibr" target="#b8">(Kang et al., 2019)</ref> using their released code on PASCAL VOC shown in <ref type="table" target="#tab_8">Table 7</ref>.</p><p>As shown in <ref type="table" target="#tab_8">Table 7</ref>, TFA w/cos is able to significantly outperform TFA w/fc in overall AP across most splits and shots. We observe that using a cosine similarity based classifier can achieve much higher accuracy on base classes, especially in higher shots. On split 1 and 3, TFA w/cos is able to outperform TFA w/fc by over 3 points on bAP on 5 and 10 shots. Across all shots in split 1, TFA w/cos consistently outperforms TFA w/fc on nAP75 by over 2 points in the novel classes.</p><p>Moreover, the AP of our models is usually over 10 points higher than that of FRCN+ft-full and FSRW on all settings. Note that FSRW uses YOLOv2 as the base object detector, while we are using Faster R-CNN. <ref type="bibr" target="#b23">Wang et al. (2019b)</ref> shows that there are only about 2 points of difference when using a one or two-stage detector. Therefore, our improvements should still be significant despite the difference in the base detector.</p><p>We evaluate on COCO over six different number of shots K = 1, 2, 3, 5, 10, 30 shown in <ref type="table" target="#tab_10">Table 8</ref>. Although the differences are less significant than on PASCAL VOC, similar observations can be made about accuracy on base classes and novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance over Multiple Runs</head><p>In our revised benchmark, we adopt n repeated runs with different randomly sampled training shots to increase the reliability of the benchmark. In our experiments, we adopt n = 30 for PASCAL VOC and n = 10 for COCO.</p><p>In this section, we provide plots of cumulative means with 95% confidence intervals of the repeated runs to show that the selected value of n is sufficient to provide statistically stable results.</p><p>We plot the model performance measured by AP, AP50 and AP75 of up to 40 random groups of training shots across all three splits in <ref type="figure">Figure 7</ref>. For COCO, we plot up to 10 random groups of training shots in <ref type="figure">Figure 8</ref>.</p><p>As we can observe from both <ref type="figure">Figure 7</ref> and <ref type="figure">Figure 8</ref>, after around 30 runs on PASCAL VOC and 8 runs on COCO, the means and variances stabilize and our selected values of n are sufficient to obtain stable estimates of the model performances and reliable comparisons across different methods.</p><p>We also observe that the average value across multiple runs is consistently lower than that on the first run, especially in the one-shot case. For example, the average AP50 across 40 runs is around 15 points lower than the AP50 on the first run in the 1-shot case on split 1 on PASCAL VOC. This indicates that the accuracies on the first run, adopted by the previous work <ref type="bibr" target="#b8">(Kang et al., 2019;</ref><ref type="bibr" target="#b24">Yan et al., 2019;</ref><ref type="bibr" target="#b23">Wang et al., 2019b)</ref>, often overestimate the actual performance and thus lead to unreliable comparison between different approaches.  <ref type="bibr" target="#b8">(Kang et al., 2019)</ref> 27.6 ± 0.5 50.8 ± 0.9 26.5 ± 0.6 34.1 ± 0.5 62.9 ± 0.9 32.6 ± 0.5 8.0 ± 1.0 14.2 ± 1.7 7.9 ± 1.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Cumulative means with 95% confidence intervals across 40 repeated runs, computed on the novel classes of the first split of PASCAL VOC. The means and variances become stable after around 30 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Generalized object detection benchmarks on PASCAL VOC. For each metric, we report the average and 95% confidence interval computed over 30 random samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>15.8±0.7 25.0±1.1 17.3±0.7 6.6±0.6 17.2±0.8 23.5±0.7 20.0±0.9 31.4±1.5 22.2±1.0 3.1±0.3 6.1±0.6 2.9±0.3 TFA w/fc 24.5±0.4 39.3±0.6 26.5±0.5 13.9±0.3 27.1±0.5 32.7±0.7 31.4±0.5 49.8±0.7 34.3±0.6 3.8±0.5 7.8±0.8 3.2±0.6 TFA w/cos 24.9±0.6 40.1±0.9 27.0±0.7 14.9±0.7 27.3±0.6 32.3±0.6 31.9±0.7 50.8±1.1 34.8±0.8 3.9±0.4 7.8±0.7 3.6±0.6 3 FRCN+ft-full 15.0±0.7 23.9±1.2 16.4±0.7 6.0±0.6 16.1±0.9 22.6±0.9 18.8±0.9 29.5±1.5 20.7±0.9 3.7±0.4 7.1±0.8 3.5±0.4 TFA w/fc 24.9±0.5 39.7±0.7 27.1±0.6 14.1±0.4 27.5±0.6 33.4±0.8 31.5±0.6 49.6±0.7 34.6±0.7 5.0±0.5 9.9±1.0 4.6±0.6 TFA w/cos 25.3±0.6 40.4±1.0 27.6±0.7 14.8±0.7 27.7±0.6 33.1±0.7 32.0±0.7 50.5±1.0 35.1±0.7 5.1±0.6 9.9±0.9 4.8±0.6 5 FRCN+ft-full 14.4±0.8 23.0±1.3 15.6±0.8 5.6±0.4 15.2±1.0 21.9±1.1 17.6±0.9 27.8±1.5 19.3±1.0 4.6±0.5 8.7±1.0 4.4±0.6 TFA w/fc 25.6±0.5 40.7±0.8 28.0±0.5 14.3±0.4 28.2±0.6 34.4±0.6 31.8±0.5 49.8±0.7 35.2±0.5 6.9±0.7 13.4±1.2 6.3±0.8 TFA w/cos 25.9±0.6 41.2±0.9 28.4±0.6 15.0±0.6 28.3±0.5 34.1±0.6 32.3±0.6 50.5±0.9 35.6±0.6 7.0±0.7 13.3±1.2 6.5±0.7 10 FRCN+ft-full 13.4±1.0 21.8±1.7 14.5±0.9 5.1±0.4 14.3±1.2 20.1±1.5 16.1±1.0 25.7±1.8 17.5±1.0 5.5±0.9 10.0±1.6 5.5±0.9 TFA w/fc 26.2±0.5 41.8±0.7 28.6±0.5 14.5±0.3 29.0±0.5 35.2±0.6 32.0±0.5 49.9±0.7 35.3±0.6 9.1±0.5 17.3±1.0 8.5±0.5 TFA w/cos 26.6±0.5 42.2±0.8 29.0±0.6 15.0±0.5 29.1±0.4 35.2±0.5 32.4±0.6 50.6±0.9 35.7±0.7 9.1±0.5 17.1±1.1 8.8±0.5 30 FRCN+ft-full 13.5±1.0 21.8±1.9 14.5±1.0 5.1±0.3 14.6±1.2 19.9±2.0 15.6±1.0 24.8±1.8 16.9±1.0 7.4±1.1 13.1±2.1 7.4±1.0 TFA w/fc 28.4±0.3 44.4±0.6 31.2±0.3 15.7±0.3 31.2±0.3 38.6±0.4 33.8±0.3 51.8±0.6 37.6±0.4 12.0±0.4 22.2±0.6 11.8±0.4 TFA w/cos 28.7±0.4 44.7±0.7 31.5±0.4 16.1±0.4 31.2±0.3 38.4±0.4 34.2±0.4 52.3±0.7 38.0±0.4 12.1±0.4 22.0±0.7 12.0±0.5Cumulative means with 95% confidence intervals across 40 repeated runs, computed on the novel classes of all three splits of PASCAL VOC. The means and variances become stable after around 30 runs. Cumulative means with 95% confidence intervals across 10 repeated runs, computed on the novel classes of COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>56.0 23.5 26.9 34.1 35.1 39.1 30.8 34.8 42.8 49.5 49.8 evaluate our approach on PASCAL VOC 2007+2012 and COCO, using the same data splits and training examples provided by<ref type="bibr" target="#b8">Kang et al. (2019)</ref>. For the few-shot PAS-CAL VOC dataset, the 20 classes are randomly divided into 15 base classes and 5 novel classes, where the novel classes have K = 1, 2, 3, 5, 10 objects per class sampled from the combination of the trainval sets of the 2007 and 2012 versions for training. Three random split groups are considered in this work. PASCAL VOC 2007 test set is used for evaluation. For COCO, the 60 categories disjoint with PASCAL VOC are used as base classes while the remaining 20 classes are used as novel classes with K = 10, 30. For evaluation metrics, AP50 (matching threshold is 0.5) of the novel classes is used on PASCAL VOC and the COCO-style AP of the novel classes is used on COCO.</figDesc><table><row><cell></cell><cell></cell><cell>2 36.8 49.6 21.8 23.1 27.8 31.7 43.0 20.6 23.9 29.4 43.9 44.1</cell></row><row><cell>FRCN+joint (Yan et al., 2019)</cell><cell></cell><cell>2.7 3.1 4.3 11.8 29.0 1.9 2.6 8.1 9.9 12.6 5.2 7.5 6.4 6.4 6.4</cell></row><row><cell>FRCN+ft (Yan et al., 2019) FRCN+ft-full (Yan et al., 2019)</cell><cell>FRCN w/R-101</cell><cell>11.9 16.4 29.0 36.9 36.9 5.9 8.5 23.4 29.1 28.8 5.0 9.6 18.1 30.8 43.4 13.8 19.6 32.8 41.5 45.6 7.9 15.3 26.2 31.6 39.1 9.8 11.3 19.1 35.0 45.1</cell></row><row><cell>Meta R-CNN (Yan et al., 2019)</cell><cell></cell><cell>19.9 25.5 35.0 45.7 51.5 10.4 19.4 29.6 34.8 45.4 14.3 18.2 27.5 41.2 48.1</cell></row><row><cell>FRCN+ft-full (Our Impl.)</cell><cell></cell><cell>15.2 20.3 29.0 40.1 45.5 13.4 20.6 28.6 32.4 38.8 19.6 20.8 28.7 42.2 42.1</cell></row><row><cell>TFA w/ fc (Ours)</cell><cell>FRCN w/R-101</cell><cell>36.8 29.1 43.6 55.7 57.0 18.2 29.0 33.4 35.5 39.0 27.7 33.6 42.5 48.7 50.2</cell></row><row><cell>TFA w/ cos (Ours)</cell><cell></cell><cell>39.8 36.1 44.7 55.7</cell></row></table><note>Baselines. We compare our approach with the meta- learning approaches FSRW, Meta-RCNN and MetaDet together with the fine-tuning based approaches: jointly training, denoted by FRCN/YOLO+joint, where the base and novel class examples are jointly trained in one stage, and fine-tuning the entire model, denoted by FRCN/YOLO+ft-full, where both the feature extractor F and the box predictor (C and R) are jointly fine-tuned until convergence in the second fine-tuning stage. FRCN is Faster R-CNN for short. Fine-tuning with less iterations, denoted by FRCN/YOLO+ft, are reported in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Few-shot detection performance for the base and novel classes on Novel Set 1 of the PASCAL VOC dataset. Our approach outperforms baselines on both base and novel classes and does not degrade the performance on the base classes greatly.</figDesc><table><row><cell># shots</cell><cell>Method</cell><cell cols="2">Base AP50 Novel AP50</cell></row><row><cell></cell><cell>FRCN+ft-full (Yan et al., 2019)</cell><cell>63.6</cell><cell>32.8</cell></row><row><cell></cell><cell>Meta R-CNN (Yan et al., 2019)</cell><cell>64.8</cell><cell>35.0</cell></row><row><cell>3</cell><cell>Train base only (Our Impl.)</cell><cell>80.8</cell><cell>9.0</cell></row><row><cell></cell><cell>FRCN+ft-full (Our Impl.)</cell><cell>66.1</cell><cell>29.0</cell></row><row><cell></cell><cell>TFA w/ cos (Ours)</cell><cell>79.1</cell><cell>44.7</cell></row><row><cell></cell><cell>FRCN+ft-full (Yan et al., 2019)</cell><cell>61.3</cell><cell>45.6</cell></row><row><cell></cell><cell>Meta R-CNN (Yan et al., 2019)</cell><cell>67.9</cell><cell>51.5</cell></row><row><cell>10</cell><cell>Train base only</cell><cell>80.8</cell><cell>9.0</cell></row><row><cell></cell><cell>FRCN+ft-full (Our Impl.)</cell><cell>66.0</cell><cell>45.5</cell></row><row><cell></cell><cell>TFA w/ cos (Ours)</cell><cell>78.4</cell><cell>56.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Few-shot detection performance for the novel categories on the COCO dataset. Our approach consistently outperforms baseline methods across all shots and metrics.</figDesc><table><row><cell>Model</cell><cell cols="2">novel AP 10 30</cell><cell cols="2">novel AP75 10 30</cell></row><row><cell>FSRW (Kang et al., 2019)</cell><cell>5.6</cell><cell>9.1</cell><cell>4.6</cell><cell>7.6</cell></row><row><cell>MetaDet (Wang et al., 2019b)</cell><cell>7.1</cell><cell cols="2">11.3 6.1</cell><cell>8.1</cell></row><row><cell>FRCN+ft+full (Yan et al., 2019)</cell><cell>6.5</cell><cell cols="2">11.1 5.9</cell><cell>10.3</cell></row><row><cell>Meta R-CNN (Yan et al., 2019)</cell><cell>8.7</cell><cell cols="2">12.4 6.6</cell><cell>10.8</cell></row><row><cell>FRCN+ft-full (Our Impl.)</cell><cell>9.2</cell><cell cols="2">12.5 9.2</cell><cell>12.0</cell></row><row><cell>TFA w/ fc (Ours)</cell><cell cols="3">10.0 13.4 9.2</cell><cell>13.2</cell></row><row><cell>TFA w/ cos (Ours)</cell><cell cols="3">10.0 13.7 9.3</cell><cell>13.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation of weight initialization of the novel classifier.</figDesc><table><row><cell>Dataset</cell><cell>Init.</cell><cell>1</cell><cell>Base AP 3</cell><cell>10</cell><cell>1</cell><cell>Novel AP 3</cell><cell>10</cell></row><row><cell cols="8">PASCAL VOC Random 51.2 52.6 52.8 15.6 25.0 29.4</cell></row><row><cell>(split 3)</cell><cell cols="7">Novel 50.9 53.1 52.5 13.4 24.9 28.9</cell></row><row><cell>COCO</cell><cell cols="7">Random 34.0 34.7 34.6 3.2 6.4 9.6 Novel 34.1 34.7 34.6 3.4 6.6 9.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Ablation of scaling factor of cosine similarity.</figDesc><table><row><cell></cell><cell></cell><cell>Base AP</cell><cell></cell><cell></cell><cell>Novel AP</cell><cell></cell></row><row><cell>Dataset</cell><cell>Scale 1</cell><cell>3</cell><cell>10</cell><cell>1</cell><cell>3</cell><cell>10</cell></row><row><cell></cell><cell cols="6">10 51.0 52.8 52.7 9.9 24.9 19.4</cell></row><row><cell cols="7">PASCAL VOC 20 51.2 52.6 52.8 15.6 25.0 29.4</cell></row><row><cell>(split 3)</cell><cell cols="6">50 47.4 48.7 50.4 13.2 22.5 27.6</cell></row><row><cell></cell><cell cols="6">10 34.3 34.9 35.0 2.8 3.4 4.7</cell></row><row><cell>COCO</cell><cell cols="6">20 34.1 34.7 33.9 3.4 6.6 10.0</cell></row><row><cell></cell><cell cols="6">50 30.1 30.7 34.3 2.4 5.4 9.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Generalized object detection benchmarks on PASCAL VOC. For each metric, we report the average and 95% confidence interval computed over 30 random samples.</figDesc><table><row><cell>Split # shots</cell><cell>Method</cell><cell>Overall</cell><cell></cell><cell></cell><cell>Base class</cell><cell></cell><cell></cell><cell>Novel class</cell></row><row><cell></cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>bAP</cell><cell>bAP50</cell><cell>bAP75</cell><cell>nAP</cell><cell>nAP50</cell><cell>nAP75</cell></row><row><cell>FSRW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Split 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc><ref type="bibr" target="#b8">Kang et al., 2019)</ref> 30.1±0.3 53.8±0.5 29.3±0.4 34.1±0.3 60.5±0.4 33.6±0.4 18.0±0.7 33.8±1.4 16.5±0.8 FRCN+ft-full 32.4±0.5 51.7±0.8 34.4±0.6 38.5±0.5 61.0±0.7 41.3±0.6 14.0±0.9 23.9±1.7 13.7±0.9 TFA w/fc 41.3±0.5 67.1±0.6 44.0±0.6 48.0±0.5 75.8±0.5 52.2±0.6 21.4±0.9 40.8±1.3 19.4±1.0 TFA w/cos 44.1±0.3 69.1±0.4 47.8±0.4 51.3±0.2 78.5±0.3 56.4±0.3 22.8±0.9 40.8±1.4 22.1±1.1 10 FRCN+ft-full 33.1±0.5 53.1±0.7 35.2±0.5 38.0±0.5 60.5±0.7 40.7±0.6 18.4±0.8 31.0±1.2 18.7±1.0 TFA w/fc 42.2±0.4 68.3±0.5 44.9±0.6 48.5±0.4 76.2±0.4 52.9±0.5 23.3±0.8 44.6±1.1 21.0±1.2 TFA w/cos 45.0±0.3 70.3±0.4 48.9±0.4 51.6±0.2 78.6±0.2 57.0±0.3 25.4±0.7 45.6±1.1 24.7±1.1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>FRCN+ft-full</cell><cell>30.2±0.6 49.4±0.7 32.2±0.9 38.2±0.8 62.6±1.0 40.8±1.1 6.0±0.7</cell><cell>9.9±1.2</cell><cell>6.3±0.8</cell></row><row><cell></cell><cell></cell><cell>TFA w/fc</cell><cell cols="2">39.6±0.5 63.5±0.7 43.2±0.7 48.7±0.7 77.1±0.7 53.7±1.0 12.2±1.6 22.9±2.5 11.6±1.9</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">40.6±0.5 64.5±0.6 44.7±0.6 49.4±0.4 77.6±0.2 54.8±0.5 14.2±1.4 25.3±2.2 14.2±1.8</cell></row><row><cell></cell><cell></cell><cell cols="3">FSRW (Kang et al., 2019) 28.7±0.4 52.2±0.6 27.7±0.5 33.9±0.4 61.8±0.5 32.7±0.5 13.2±1.0 23.6±1.7 12.7±1.1</cell></row><row><cell></cell><cell>2</cell><cell>FRCN+ft-full TFA w/fc</cell><cell cols="2">30.5±0.6 49.4±0.8 32.6±0.7 37.3±0.7 60.7±1.0 40.1±0.9 9.9±0.9 15.6±1.4 10.3±1.0 40.5±0.5 65.5±0.7 43.8±0.7 47.8±0.7 75.8±0.7 52.2±1.0 18.9±1.5 34.5±2.4 18.4±1.9</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">42.6±0.3 67.1±0.4 47.0±0.4 49.6±0.3 77.3±0.2 55.0±0.4 21.7±1.0 36.4±1.6 22.8±1.3</cell></row><row><cell></cell><cell></cell><cell cols="3">FSRW (Kang et al., 2019) 29.5±0.3 53.3±0.6 28.6±0.4 33.8±0.3 61.2±0.6 32.7±0.4 16.8±0.9 29.8±1.6 16.5±1.0</cell></row><row><cell></cell><cell>3</cell><cell>FRCN+ft-full TFA w/fc</cell><cell cols="2">31.8±0.5 51.4±0.8 34.2±0.6 37.9±0.5 61.3±0.7 40.7±0.6 13.7±1.0 21.6±1.6 14.8±1.1 41.8±0.9 67.1±0.9 45.4±1.2 48.2±0.9 76.0±0.9 53.1±1.2 22.6±1.2 40.4±1.7 22.4±1.7</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">43.7±0.3 68.5±0.4 48.3±0.4 49.8±0.3 77.3±0.2 55.4±0.4 25.4±0.9 42.1±1.5 27.0±1.2</cell></row><row><cell></cell><cell></cell><cell cols="3">FSRW (Kang et al., 2019) 30.4±0.3 54.6±0.5 29.6±0.4 33.7±0.3 60.7±0.4 32.8±0.4 20.6±0.8 36.5±1.4 20.0±0.9</cell></row><row><cell></cell><cell>5</cell><cell>FRCN+ft-full TFA w/fc</cell><cell cols="2">32.7±0.5 52.5±0.8 35.0±0.6 37.6±0.4 60.6±0.6 40.3±0.5 17.9±1.1 28.0±1.7 19.2±1.3 41.9±0.6 68.0±0.7 45.0±0.8 47.2±0.6 75.1±0.6 51.5±0.8 25.9±1.0 46.7±1.4 25.3±1.2</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">44.8±0.3 70.1±0.4 49.4±0.4 50.1±0.2 77.4±0.3 55.6±0.3 28.9±0.8 47.9±1.2 30.6±1.0</cell></row><row><cell></cell><cell></cell><cell>FRCN+ft-full</cell><cell cols="2">33.3±0.4 53.8±0.6 35.5±0.4 36.8±0.4 59.8±0.6 39.2±0.4 22.7±0.9 35.6±1.5 24.4±1.0</cell></row><row><cell></cell><cell>10</cell><cell>TFA w/fc</cell><cell cols="2">42.8±0.3 69.5±0.4 46.0±0.4 47.3±0.3 75.4±0.3 51.6±0.4 29.3±0.7 52.0±1.1 29.0±0.9</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">45.8±0.2 71.3±0.3 50.4±0.3 50.4±0.2 77.5±0.2 55.9±0.3 32.0±0.6 52.8±1.0 33.7±0.7</cell></row><row><cell></cell><cell></cell><cell cols="3">FSRW (Kang et al., 2019) 28.4±0.5 51.7±0.9 27.3±0.6 35.7±0.5 64.8±0.9 34.6±0.7 6.3±0.9 12.3±1.9 5.5±0.7</cell></row><row><cell></cell><cell>1</cell><cell>FRCN+ft-full TFA w/fc</cell><cell cols="2">30.3±0.5 49.7±0.5 32.3±0.7 38.8±0.6 63.2±0.7 41.6±0.9 5.0±0.6 36.2±0.8 59.6±0.9 38.7±1.0 45.6±0.9 73.8±0.9 49.4±1.2 8.1±1.2 16.9±2.3 6.6±1.1 9.4±1.2 4.5±0.7</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">36.7±0.6 59.9±0.8 39.3±0.8 45.9±0.7 73.8±0.8 49.8±1.1 9.0±1.2 18.3±2.4</cell><cell>7.8±1.2</cell></row><row><cell></cell><cell></cell><cell cols="3">FSRW (Kang et al., 2019) 29.4±0.3 53.1±0.6 28.5±0.4 35.8±0.4 64.2±0.6 35.1±0.5 9.9±0.7 19.6±1.3 8.8±0.6</cell></row><row><cell></cell><cell>2</cell><cell>FRCN+ft-full TFA w/fc</cell><cell cols="2">30.7±0.5 49.7±0.7 32.9±0.6 38.4±0.5 61.6±0.7 41.4±0.7 7.7±0.8 13.8±1.4 7.4±0.8 38.5±0.5 62.8±0.6 41.2±0.6 46.9±0.5 74.9±0.5 51.2±0.7 13.1±1.0 26.4±1.9 11.3±1.1</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">39.0±0.4 63.0±0.5 42.1±0.6 47.3±0.4 74.9±0.4 51.9±0.7 14.1±0.9 27.5±1.6 12.7±1.0</cell></row><row><cell></cell><cell></cell><cell cols="3">FSRW (Kang et al., 2019) 29.9±0.3 53.9±0.4 29.0±0.4 35.7±0.3 63.5±0.4 35.1±0.4 12.5±0.7 25.1±1.4 10.4±0.7</cell></row><row><cell>Split 2</cell><cell>3</cell><cell>FRCN+ft-full TFA w/fc</cell><cell cols="2">31.1±0.3 50.1±0.5 33.2±0.5 38.1±0.4 61.0±0.6 41.2±0.5 9.8±0.9 17.4±1.6 9.4±1.0 39.4±0.4 64.2±0.5 42.0±0.5 47.5±0.4 75.4±0.5 51.7±0.6 15.2±0.8 30.5±1.5 13.1±0.8</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">40.1±0.3 64.5±0.5 43.3±0.4 48.1±0.3 75.6±0.4 52.9±0.5 16.0±0.8 30.9±1.6 14.4±0.9</cell></row><row><cell></cell><cell></cell><cell cols="3">FSRW (Kang et al., 2019) 30.4±0.4 54.6±0.5 29.5±0.5 35.3±0.3 62.4±0.4 34.9±0.5 15.7±0.8 31.4±1.5 13.3±0.9</cell></row><row><cell></cell><cell>5</cell><cell>FRCN+ft-full TFA w/fc</cell><cell cols="2">31.5±0.3 50.8±0.7 33.6±0.4 37.9±0.4 60.4±0.6 40.8±0.5 12.4±0.9 21.9±1.5 12.1±0.9 40.0±0.4 65.1±0.5 42.6±0.5 47.5±0.4 75.3±0.5 51.6±0.5 17.5±0.7 34.6±1.1 15.5±0.9</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">40.9±0.4 65.7±0.5 44.1±0.5 48.6±0.4 76.2±0.4 53.3±0.5 17.8±0.8 34.1±1.4 16.2±1.0</cell></row><row><cell></cell><cell></cell><cell>FRCN+ft-full</cell><cell cols="2">32.2±0.3 52.3±0.4 34.1±0.4 37.2±0.3 59.8±0.4 39.9±0.4 17.0±0.8 29.8±1.4 16.7±0.9</cell></row><row><cell></cell><cell>10</cell><cell>TFA w/fc</cell><cell cols="2">41.3±0.2 67.0±0.3 44.0±0.3 48.3±0.2 76.1±0.3 52.7±0.4 20.2±0.5 39.7±0.9 18.0±0.7</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">42.3±0.3 67.6±0.4 45.7±0.3 49.4±0.2 76.9±0.3 54.5±0.3 20.8±0.6 39.5±1.1 19.2±0.6</cell></row><row><cell></cell><cell></cell><cell cols="3">FSRW (Kang et al., 2019) 27.5±0.6 50.0±1.0 26.8±0.7 34.5±0.7 62.5±1.2 33.5±0.7 6.7±1.0 12.5±1.6 6.4±1.0</cell></row><row><cell></cell><cell>1</cell><cell>FRCN+ft-full TFA w/fc</cell><cell cols="2">30.8±0.6 49.8±0.8 32.9±0.8 39.6±0.8 63.7±1.0 42.5±0.9 4.5±0.7 39.0±0.6 62.3±0.7 42.1±0.8 49.5±0.8 77.8±0.8 54.0±1.0 7.8±1.1 15.7±2.1 6.5±1.0 8.1±1.3 4.2±0.7</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">40.1±0.3 63.5±0.6 43.6±0.5 50.2±0.4 78.7±0.2 55.1±0.5 9.6±1.1 17.9±2.0</cell><cell>9.1±1.2</cell></row><row><cell></cell><cell></cell><cell cols="3">FSRW (Kang et al., 2019) 28.7±0.4 51.8±0.7 28.1±0.5 34.5±0.4 62.0±0.7 34.0±0.5 11.3±0.7 21.3±1.0 10.6±0.8</cell></row><row><cell></cell><cell>2</cell><cell>FRCN+ft-full TFA w/fc</cell><cell cols="2">31.3±0.5 50.2±0.9 33.5±0.6 39.1±0.5 62.4±0.9 42.0±0.7 8.0±0.8 13.9±1.4 7.9±0.9 41.1±0.6 65.1±0.7 44.3±0.7 50.1±0.7 77.7±0.7 54.8±0.9 14.2±1.2 27.2±2.0 12.6±1.3</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">41.8±0.4 65.6±0.6 45.3±0.4 50.7±0.3 78.4±0.2 55.6±0.4 15.1±1.3 27.2±2.1 14.4±1.5</cell></row><row><cell></cell><cell></cell><cell cols="3">FSRW (Kang et al., 2019) 29.2±0.4 52.7±0.6 28.5±0.4 34.2±0.3 61.3±0.6 33.6±0.4 14.2±0.7 26.8±1.4 13.1±0.7</cell></row><row><cell>Split 3</cell><cell>3</cell><cell>FRCN+ft-full TFA w/fc</cell><cell cols="2">32.1±0.5 51.3±0.8 34.3±0.6 39.1±0.5 62.1±0.7 42.1±0.6 11.1±0.9 19.0±1.5 11.2±1.0 40.4±0.5 65.4±0.7 43.1±0.7 47.8±0.5 75.6±0.5 52.1±0.7 18.1±1.0 34.7±1.6 16.2±1.3</cell></row><row><cell></cell><cell></cell><cell>TFA w/cos</cell><cell cols="2">43.1±0.4 67.5±0.5 46.7±0.5 51.1±0.3 78.6±0.2 56.3±0.4 18.9±1.1 34.3±1.7 18.1±1.4</cell></row><row><cell></cell><cell></cell><cell>FSRW (</cell><cell></cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Generalized object detection benchmarks on COCO. For each metric, we report the average and 95% confidence interval computed over 10 random samples.FRCN+ft-full 16.2±0.9 25.8±1.2 17.6±1.0 7.2±0.6 17.9±1.0 23.1±1.1 21.0±1.2 33.3±1.7 23.0±1.4 1.7±0.2 3.3±0.3 1.6±0.2 TFA w/fc 24.0±0.5 38.9±0.5 25.8±0.6 13.8±0.4 26.6±0.4 32.0±0.6 31.5±0.5 50.7±0.6 33.9±0.8 1.6±0.4 3.4±0.6 1.3±0.4 TFA w/cos 24.4±0.6 39.8±0.8 26.1±0.8 14.7±0.7 26.8±0.5 31.4±0.7 31.9±0.7 51.8±0.9 34.3±0.9 1.9±0.</figDesc><table><row><cell># shots</cell><cell>Method</cell><cell></cell><cell cols="2">Overall</cell><cell></cell><cell></cell><cell></cell><cell>Base class</cell><cell></cell><cell></cell><cell>Novel class</cell></row><row><cell></cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>APs</cell><cell>APm</cell><cell>APl</cell><cell>bAP</cell><cell>bAP50</cell><cell>bAP75</cell><cell>nAP</cell><cell>nAP50</cell><cell>nAP75</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02729</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32 nd International Conference on Machine Learning</title>
		<meeting>the 32 nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The importance of shape in early lexical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive development</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="321" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<title level="m">Meta-learning with latent embedding optimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">They call it like they see it: Spontaneous naming and attention to shape. Developmental</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Samuelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="198" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object name learning provides on-the-job training for attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gershkoff-Stowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="19" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tafe-net: Task-aware feature embeddings for low shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Meta-learning to detect rare objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9925" to="9934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta r-cnn: Towards general solver for instance-level lowshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9577" to="9586" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
