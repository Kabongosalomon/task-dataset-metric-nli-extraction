<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Confidence Propagation through CNNs for Guided Sparse Depth Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Confidence Propagation through CNNs for Guided Sparse Depth Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Sparse data</term>
					<term>CNNs</term>
					<term>Depth completion</term>
					<term>Normalized convolution</term>
					<term>Confidence propagation !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generally, convolutional neural networks (CNNs) process data on a regular grid, e.g. data generated by ordinary cameras. Designing CNNs for sparse and irregularly spaced input data is still an open research problem with numerous applications in autonomous driving, robotics, and surveillance. In this paper, we propose an algebraically-constrained normalized convolution layer for CNNs with highly sparse input that has a smaller number of network parameters compared to related work. We propose novel strategies for determining the confidence from the convolution operation and propagating it to consecutive layers. We also propose an objective function that simultaneously minimizes the data error while maximizing the output confidence. To integrate structural information, we also investigate fusion strategies to combine depth and RGB information in our normalized convolution network framework. In addition, we introduce the use of output confidence as an auxiliary information to improve the results. The capabilities of our normalized convolution network framework are demonstrated for the problem of scene depth completion. Comprehensive experiments are performed on the KITTI-Depth and the NYU-Depth-v2 datasets. The results clearly demonstrate that the proposed approach achieves superior performance while requiring only about 1-5% of the number of parameters compared to the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S ENSORS with dense output such as monochrome, color, and thermal cameras have been extensively exploited by machine learning methods in many computer vision applications. Images generated by these sensors are typically fully dense due to their passive nature and different image regions are initially equally relevant to the machine learning algorithms. However, other, mostly active, sensors such as ToF cameras, LiDAR, RGB-D, and event cameras produce sparse output. This sparsity is usually caused by their active sensing, which leaves many data regions empty. The sparse output from these sensors imposes fundamental challenges on the machine learning methods as data relevance is not uniform and further processing is required to either reconstruct or ignore these missing regions.</p><p>The degree of sparsity and data pattern differ from one sensor to another and machine learning methods should be able to handle different scenarios. Handling sparsity would open up for numerous applications in robotics, autonomous driving, and surveillance due to the depth information made available by active sensors. Therefore, a major task is scene depth completion, which aims to reconstruct a dense depth map from the sparse output produced by active depth sensors. Scene depth completion is crucial for tasks that require situation awareness for decision support. Besides, the availability of a reliability measure, i.e. confidence, is also desirable since it gives an indication about the trustworthiness of the output. A confidence measure would be highly beneficial for safety and decision making applications such as obstacle detection and avoidance in autonomous driving.</p><p>A key problem in scene depth completion is the identification of missing values and distinguishing them from regions with zero values. One way to identify missing data is using binary validity masks with zeros at regions with missing values and ones otherwise. Validity masks have been extensively used in the literature <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[2]</ref>, <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref> to inform the learning method about the missing regions in the data. However, validity masks suffer from saturation in multi-stage learning such as Convolutional Neural Networks (CNNs) as shown by <ref type="bibr" target="#b7">[7]</ref>. Instead, the saturation problem can be avoided by treating the binary validity masks as continuous confidence fields describing the reliability of the data. Additionally, this enables confidence propagation, which helps to keep track of the reliability of the data throughout the processing pipeline.</p><p>Most recent works for solving the scene depth completion task are based on CNNs, and show a great success <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b8">[8]</ref>. Typically, a deep CNN is trained to construct a dense depth map given either a sparse depth input only or sparse depth aside with an RGB image. The former case is denoted as unguided depth completion, while the latter is called guided depth completion. The role of the network in both cases is to learn the manifold where the data live in. Since the publicly available datasets for scene depth completion such as the KITTI-Depth dataset <ref type="bibr" target="#b0">[1]</ref> have a very high spatial resolution, the state-of-the-art methods <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b8">[8]</ref> demand huge CNNs with millions of parameters to solve the problem. Unfortunately, this hinders the deployment of such methods in autonomous driving and robotic systems with limited computational and memory resources. It is desirable to design compact CNN architectures, while arXiv:1811.01791v2 [cs.CV] 5 Aug 2019 <ref type="figure">Fig. 1</ref>. Our scene depth completion pipeline on an example image from the KITTI-Depth dataset <ref type="bibr" target="#b0">[1]</ref>. The input to the pipeline is a very sparse projected LiDAR point cloud, an input confidence map which has zeros at missing pixels and ones otherwise, and an RGB image. The sparse point cloud input and the input confidence are fed to a multi-scale unguided network that acts as a generic estimator for the data. Afterwards, the continuous output confidence map is concatenated with the RGB image and fed to a feature extraction network. The output from the unguided network and the RGB feature extraction networks are concatenated and fed to a fusion network which produces the final dense depth map. [*Images were dilated for visual clarity ]</p><p>propagating confidences, for real-world applications with limited resources.</p><p>In this paper, we introduce the normalized convolution layer, which allows performing unguided scene depth completion on highly sparse data with a smaller number of parameters than related methods. Our proposed method treats the validity masks as a continuous confidence field and we propose a new criteria to propagate confidences between CNN layers, thereby allowing us to produce a point-wise continuous confidence map for the output from the network. Furthermore, we algebraically constrain the network learned filters to be non-negative, acting as a weighting function for the neighborhood. This allows the network to converge faster and achieves remarkably better results. We also propose a loss function that aims to simultaneously minimize the data error and maximize the output confidence.</p><p>The proposed normalized convolution network generally performs well on smooth surfaces when using only the depth information. However, it performs less well across edges due to lack of structural information. This can be mitigated by using guidance from RGB images in order to integrate useful structural information into our network. Both RGB and depth information can be fused in our proposed framework in multiple ways. In this work, we investigate both early and late fusion schemes in two state-of-the-art architectures. The first is a multi-stream architecture inspired by <ref type="bibr" target="#b5">[5]</ref> which is highly relevant to our proposed method and the second is an encoder-decoder architecture with skipconnections inspired by <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b9">[9]</ref>. In addition, we introduce the use of output confidences as guidance information aside with the RGB images. On the KITTI-Depth <ref type="bibr" target="#b0">[1]</ref> and the NYU-Depth-v2 <ref type="bibr" target="#b10">[10]</ref> datasets, our proposed method achieves state-of-the-art results while requiring a significantly lower number of parameters (∼ 356k and ∼ 484k parameters) respectively, compared to all the existing state-of-the-art methods (with millions of parameters). This proves the efficiency of our proposed method. An illustration of the proposed pipeline is shown in <ref type="figure">Figure 1</ref>.</p><p>The rest of the paper is organized as follows. Section 2 gives an overview of the relevant work in the literature. In section 3, we describe the normalized convolution framework in details. Section 4 describes our early work on unguided depth completion in <ref type="bibr" target="#b11">[11]</ref>. Section 5 introduces our proposed approach to fuse sparse depth, RGB images, and the output confidences. Extensive experiments on both our prior work <ref type="bibr" target="#b11">[11]</ref> and the proposed fusion schemes are presented in section 6. Finally, we provide a thorough analysis for our proposed method in section 7. The conclusion is given in section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Scene depth completion has become a fundamental task in computer vision ever since the emergence of active sensors with depth capabilities. Generally, it was treated as a holefilling or inpainting problem using classical image processing methods <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>. Recently, with the advent of deep learning, specifically Convolutional Neural Networks (CNNs), scene depth completion has matured into a separate task than inpainting. Typically, scene depth completion is performed on depth maps with optional guidance from RGB images. Differently, inpainting is mostly performed on RGB or grayscale images. In addition, the objective of scene depth completion is to minimize some error measure such as the L1 or the L2 norm, while inpainting aims also to provide realistic output <ref type="bibr" target="#b4">[4]</ref>.</p><p>For the task of unguided scene depth completion, where the input is only the depth map, Chodosh et al. <ref type="bibr" target="#b3">[3]</ref> utilized compressed sensing to handle the sparsity, while using a binary mask to filter out the missing values. Ma et al. <ref type="bibr" target="#b8">[8]</ref> utilized an encoder-decoder architecture with selfsupervised framework to predict the dense output. Uhrig et al. <ref type="bibr" target="#b0">[1]</ref> proposed a sparsity-invariant convolution layer that utilizes binary validity masks to normalize the sparse input. The proposed layer was used to train a network with a sparse depth map aside with a binary validity mask as input and a dense depth map as output. Similarly, <ref type="bibr" target="#b6">[6]</ref> utilized the sparsity-invariant layer in more complex CNN architectures. Hua and Gong <ref type="bibr" target="#b5">[5]</ref> proposed a similar layer, which uses the trained convolution filter to normalize the sparse input. Contrarily, Jaritz et al. <ref type="bibr" target="#b7">[7]</ref> compared different architectures and argued that the use of validity masks degrades the performance due to the saturation of the masks at early layers within the CNN. This effect is avoided by the use of continuous confidences as proposed in our prior work on unguided depth completion <ref type="bibr" target="#b11">[11]</ref>.</p><p>Due to the sub-optimal performance of unguided methods across edges, several recent approaches urged to use guidance from auxiliary data such as RGB images or surface normals. Ma et al. <ref type="bibr" target="#b8">[8]</ref> used an early fusion scheme to combine sparse depth input with the corresponding RGB image, which was demonstrated to perform very well. On the other hand, Jaritz et al. <ref type="bibr" target="#b7">[7]</ref> argued that late-fusion performs better with their proposed architecture, which was also demonstrated in <ref type="bibr" target="#b5">[5]</ref>. Wirges et al. <ref type="bibr" target="#b15">[15]</ref> used a combination of RGB images and surface normals to guide the process of depth upsampling. Konno et al. <ref type="bibr" target="#b16">[16]</ref> utilized a residual interpolation method to combine a low-resolution depth map with a high resolution RGB image to produce a high resolution depth map.</p><p>Different to the aforementioned approaches, we proposed a normalized convolution layer in <ref type="bibr" target="#b11">[11]</ref>, which takes in a sparse input aside with a continuous confidence map to perform unguided scene depth completion. Different to <ref type="bibr" target="#b5">[5]</ref>, we impose algebraic constraints on the trained filters to be non-negative, which allow the network to converge faster while requiring significantly lower number of parameters. Further, we derive a confidence propagation criteria between layers, which enables producing a point-wise continuous confidence map aside with the dense output from the CNN. As an extention to our priod work <ref type="bibr" target="#b11">[11]</ref>, we use guidance from RGB images, and Different to <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b8">[8]</ref>, we also use guidance from the output confidence produced by the unguided network. Our results clearly show that using guidance from the output confidence leads to a significant improvement in performance. Our proposed multi-stream architecture with late fusion achieves remarkable results compared to published state-of-the-art methods while requiring significantly lower number of parameters than comparable methods. This demonstrates the efficiency of our proposed method, which eliminated the need for a huge number of parameters to achieve state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NORMALIZED CONVOLUTION</head><p>The concept of Normalized Convolution was first introduced by Knutsson and Westin <ref type="bibr" target="#b17">[17]</ref> based on the theory of confidence-equipped signals. Assume a discrete finite signal f that is periodically sampled. This signal f could, e.g., depict a sparse depth field. At each sample point, the neighborhood is finite and can be represented as a vector f ∈ C n . This signal is accompanied by a confidence field c, which describes the reliability of each sample value. Confidences are typically non-negative, and in the case of the sparse depth field, zero confidence indicates the absence of the corresponding depth sample value. The confidence field c is sampled in the same manner as f and the confidence of each neighborhood is represented as a finite vector c ∈ C n .</p><p>In <ref type="bibr" target="#b17">[17]</ref>, the signal is modeled locally by projecting each sample f onto a subspace spanned by some basis functions, e.g. polynomials, complex exponentials, or the naïve basis. The set of basis functions {b i ∈ C n } m 1 have the same dimensionality as the sample f and its corresponding confidence c.</p><p>The sample f is expressed with respect to the basis functions arranged into the columns of a n × m matrix B as:</p><formula xml:id="formula_0">f = Br ,<label>(1)</label></formula><p>where r holds the coordinates of the sample f with respect to the basis functions. Finding these coordinates is usually formulated as a weighted least-squares problem:</p><formula xml:id="formula_1">arg min r∈C n ||Br − f || W ,<label>(2)</label></formula><p>where W is the weight matrix for the least-squares problem. In our case, the confidence c is used to weight the sample f and an applicability function a ∈ C n , acts as a weight for the basis. The solutionr to this weighted least-squares problem reads:r</p><formula xml:id="formula_2">= (B * WB) −1 B * Wf , = (B * D a D c B) −1 B * D a D c f ,<label>(3)</label></formula><p>where D a and D c are diagonal matrices with a and c on the main diagonal, respectively. This formulation can be applied to the whole signal f and its corresponding confidence c in a convolution-like structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Naïve Basis</head><p>The most basic choice for the basis is a constant function, and it is denoted as the naïve basis. In this case, the applicability acts as a convolution filter. This choice of basis B = 1 simplifies (3) to:r</p><formula xml:id="formula_3">= (1 * D a D c 1) −1 1 * D a D c f = a · (c f ) a · c ,<label>(4)</label></formula><p>where is the Hadamard product and · is the scalar product. Note that the matrix multiplication has been replaced with point-wise operations since D a and D c are diagonal matrices. This can be formulated for the whole signal f as a convolution operation as follows:</p><formula xml:id="formula_4">r [k] = n i a[i] f [k − i] c[k − i] n i a[i] c[k − i] .<label>(5)</label></formula><p>This formulation allows for densifying a sparse depth field f given a point-wise confidence c for each sample point in the depth field, where zero indicates a missing sample. With a proper choice of the applicability function a, a dense depth field is obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Applicability Function</head><p>The applicability function a is required to be non-negative and it acts as a windowing function for the basis, e.g. giving more importance to the central part of the signal over the vicinity. The choice of the applicability depends on the application and the characteristics of the signal. For example, for orientation estimation, it is desired that the applicability is isotropic <ref type="bibr" target="#b18">[18]</ref>. On the other side, image inpainting requires the applicability to be anisotropic depending on the local structure of the image <ref type="bibr" target="#b19">[19]</ref>. The handcrafting of the applicability function is not within the scope of this paper as we aim to learn it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Propagating Confidences</head><p>A core advantage of normalized convolution is the separation between the signal and the confidence allowing confidence adaptive processing and determination of output confidence. The output confidence reflects the density of the input confidence as well as the coherence of the data under the chosen basis. Westelius <ref type="bibr" target="#b20">[20]</ref> proposed a measure for the output confidence defined as:</p><formula xml:id="formula_5">c out = det G det G 0 1 m ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">G = B * D a D c B and G 0 = B * D a B.</formula><p>This corresponds to a geometric ratio between the Grammians of the basis B in case of partial and full confidence. Similarly, Karlholm <ref type="bibr" target="#b21">[21]</ref> proposed another measure defined as:</p><formula xml:id="formula_7">c out = 1 G −1 2 G 0 2 .<label>(7)</label></formula><p>These two measures were shown to perform well in case of the polynomial and exponential basis <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UNGUIDED NORMALIZED CNNS</head><p>Based on our prior work <ref type="bibr" target="#b11">[11]</ref>, CNNs can be used to learn the optimal applicability function in case of the naïve basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training the Applicability</head><p>As explained in section 3.2, the applicability is a windowing function and it needs to be non-negative. This is enforced in CNN frameworks by applying a suitable differentiable function with non-negative co-domain acting on the convolution kernels prior to the forward pass. During backpropagation, the weights will be differentiated with respect to this function using the chain rule. Examples for differentiable functions with non-negative co-domain are shown in <ref type="figure" target="#fig_0">Figure 2a</ref>. <ref type="figure" target="#fig_0">Figure 2b</ref> shows how the SoftPlus function, Γ(z) = log(1 + exp(z)), translates the co-domain of a 2D surface to be non-negative while preserving the surface trend. Given a function Γ(·) with a non-negative co-domain, the gradients of the weight for the l th convolution layer are obtained as:</p><formula xml:id="formula_8">∂E ∂W l m,n = i,j ∂E ∂Z l i,j · ∂Z l i,j ∂ Γ(W l m,n ) · ∂ Γ(W l m,n ) ∂W l m,n , (8) Data input to layer Z l-1 Data confidence C l-1 ⊙ * * Γ(W l ) ⊙ ⊙ Σ 1/x + Bias b l Data Output Z l Confidence Output C l</formula><p>Normalized Convolution Layer <ref type="figure">Fig. 3</ref>. An illustration of the Normalized Convolution layer that takes in two inputs: data and confidence. The Normalized Convolution layer outputs a data term and a confidence term. Convolution is denoted as * , the Hadamard product (point-wise) as , summation as Σ, and point-</p><formula xml:id="formula_9">wise inverse as 1/x.</formula><p>where E is the loss between the network output and the ground truth, and Z l i,j is the output of the l th layer at locations i, j depending on the weight elements W l m,n . Accordingly, the forward pass for normalized convolution is defined as:</p><formula xml:id="formula_10">Z l i,j = m,n Z l−1 i+m,j+n C l−1 i+m,j+n Γ(W l m,n ) m,n C l−1 i+m,j+n Γ(W l m,n ) + + b l ,<label>(9)</label></formula><p>where C l−1 is the output confidence from the previous layer, W l m,n is the applicability in this context, b l is the bias and is a constant to prevent division by zero. Note that this is formally a correlation, as it is a common notation in CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Propagating the Confidence</head><p>The confidence output measures described in section 3.3 have been shown to give reasonable results in case of nonnaïve basis <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>. In our earlier work <ref type="bibr" target="#b11">[11]</ref>, we proposed a confidence output measure for the naïve basis case. The proposed measure is derived from (6) and can utilize the already computed terms in the forward path. The measure is defined as:</p><formula xml:id="formula_11">C l i,j = m,n C l−1 i+m,j+n Γ(W l m,n ) + m,n Γ(W l m,n ) .<label>(10)</label></formula><p>This measure allows propagating confidence between CNN layers without facing the problem of "validity masks saturation" as described in <ref type="bibr" target="#b7">[7]</ref>, which affects several methods in the literature <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b5">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Normalized CNN Layer</head><p>The standard convolution layer in CNN frameworks can be replaced by a normalized convolution layer with minor modifications. First, the layer takes in two inputs simultaneously, the data and its confidence. The forward pass is then modified according to <ref type="bibr" target="#b9">(9)</ref> and the back-propagation is modified to include a derivative term for the non-negativity enforcement function Γ(·) as described in <ref type="bibr" target="#b8">(8)</ref>. To propagate the confidence to consecutive layers, the already-calculated denominator term in <ref type="formula" target="#formula_10">(9)</ref> is normalized by the sum of the filter elements as shown in <ref type="bibr" target="#b10">(10)</ref>. An illustration of the Normalized CNN layer is shown in <ref type="figure">Figure 3</ref>.  <ref type="figure">Fig. 4</ref>. Our proposed multi-scale architecture for the task of unguided scene depth completion that utilizes normalized convolution layers. Downsampling is performed using max pooling on confidence maps and the indices of the pooled pixels are used to select the pixels with highest confidences from the feature maps. Different scales are fused by upsampling the coarser scale and concatenating it with the finer scale. A normalized convolution layer is then used to fuse the feature maps based on the confidence information. Finally, a 1 × 1 normalized convolution layer is used to merge different channels into one channel and produce a dense output and an output confidence map.</p><formula xml:id="formula_12">NCONV_1 Z 1 C 1 Z 0 C 0 Z 2 C 2 Z 3 C 3 idx C 4 [ ] Z 4 Z 5 C 5 Z 8 C 8 CONCAT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Loss Function</head><p>In networks that perform pixel-wise tasks such as inpainting, upsampling, or segmentation, it is very common to use the L1 or the L2 norm. However, the former ignores outliers and focuses on the global level, while the latter focuses on local regions that have outliers. A good compromise is the Huber norm <ref type="bibr" target="#b22">[22]</ref>, which is defined as:</p><formula xml:id="formula_13">z − t H = 1 2 (z − t) 2 , |z − t| &lt; δ δ|z − t| − 1 2 δ 2 , otherwise<label>(11)</label></formula><p>The Huber norm corresponds to the L2 norm if the error is less than δ and to the L1 norm otherwise. Usually, the value of δ is set to 1 within CNN frameworks and referred to as the Smooth L1 loss.</p><p>In networks with normalized convolution layers, it is desirable to minimize the data error and maximize the output confidence at the same time. Thus, a loss function that simultaneously achieves both objectives is desired. Assume a data error term using the Huber norm:</p><formula xml:id="formula_14">E i,j = Z L i,j − T i,j H ,<label>(12)</label></formula><p>where Z L i,j is the data output from the last layer L and T i,j is the data ground truth. This is complemented with a term to maximize the confidence and the total lossẼ becomes:</p><formula xml:id="formula_15">E i,j = E i,j − 1 p C L i,j − E i,j C L i,j ,<label>(13)</label></formula><p>where C L i,j is the output confidence and p is the epoch number. The confidence term is decaying by dividing it by the epoch number p to prevent it from dominating the loss when the data error term starts to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Unguided Normalized CNN Architecture</head><p>In <ref type="bibr" target="#b11">[11]</ref>, we proposed a hierarchical multi-scale architecture inspired by the U-Net <ref type="bibr" target="#b9">[9]</ref>, which shares weights between different scales. The architecture acts as a generic estimator for different scales and gives a good approximation for the dense output at a very low computation cost. An illustration of the architecture is shown in <ref type="figure">Figure 4</ref>. At the first scale, a normalized convolution layer takes in the sparse input as well as a confidence map. Afterwards, two normalized convolution layers are applied followed by downsampling. The downsamping process is performed by applying a max pooling operator on the output confidence from the last normalized layer while keeping the indices of the pooled values as in unpooling operations <ref type="bibr" target="#b23">[23]</ref>. These indices are then used to select the values from the features maps that have the highest confidences. This enables propagating the most confident data to the subsequent scale. To maintain the absolute levels of confidences after downsampling, we divide the downsampled confidences by the Jacobian of the scaling.</p><p>The aforementioned pipeline is repeated as required depending on the sparsity level of the data. In order to fuse different scales, the output and the corresponding confidence from the last normalized convolution layers are upsampled using nearest-neighbor interpolation and concatenated with the corresponding scale through a skip connection. After each concatenation, a new normalized convolution layer is utilized to fuse data from the two scales based on their confidences. Finally, a 1 × 1 normalized convolution layer is used to fuse different channels into one channel that corresponds to the dense output. In addition to the dense output, a confidence output is also available that holds information about the confidence distribution of the output. The output confidence can be useful for safety application or for subsequent stages in the pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GUIDED NORMALIZED CNNS</head><p>In this section, we extend the unguided normalized convolution architecture described in section 4 with RGB and output confidence guidance. The unguided architecture acts as a generic estimator for different scales that is learned from the data. However, this generic estimator shows weaknesses at local regions with discontinuities such as edges and rough surfaces. <ref type="figure" target="#fig_1">Figure 5</ref> shows an example of the spatial error distribution for the output from the unguided normalized convolution network on the task of depth completion. It is clear that regions with edges have larger errors than flat regions. Therefore, auxiliary data such as RGB images and surface normals can be used to alleviate this problem by providing contextual information to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">RGB Image Fusion</head><p>RGB images can be very useful in guiding the network since convolution layers typically act as feature extractors. These features are usually edges, corners, color, or texture. Providing this information to the network was demonstrated to improve the results <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b8">[8]</ref>, especially across edges and in rough surfaces. Therefore, we incorporate RGB information into our network to handle discontinuities at edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Output Confidence Fusion</head><p>The RGB data is fused with a new form of auxiliary data, which is the output confidence from the unguided normalized convolution network. This output confidence holds useful information about the reliability of each pixel in the image. For example, regions in the sparse input that have a high density of sample points should have a higher confidence in the output. <ref type="figure">Figure 6</ref> illustrates how the output confidence from our unguided network (the dashed orange curve) is correlated with the density of the sample points in the input (the red crosses). The figure also shows how our unguided network can find a good approximation (the blue curve) for the sparse input. Therefore, We use the output confidence as an input to our guided network to provide information about reliability of different pixels in the output from the unguided network. We will demonstrate in the experiments that the use of the output confidence improves the depth results by approximately 10%. Further, we give statistical evidence that the output confidence correlates with the error of the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Network Architecture</head><p>We aim to fuse the sparse depth, the RGB image, and the output confidence to produce a dense depth map. Therefore, we look into two of the commonly used architectures from the literature on the task of guided scene depth completion. The first is a simple multi-stream network inspired by  <ref type="figure">Fig. 6</ref>. An example of the output confidence from our unguided normalized convolution network on the task of depth completion. Images on the left are from top-to-bottom: sparse input, the dense output from the unguided normalized convolution network and the output confidence.</p><p>The plot on the right shows the corresponding values for row 217. The red crosses are the sample points from the sparse input, the blue curve is the dense prediction and the orange curve is the output confidence (smoothed). It is shown that regions with high density of sample points tend to have a higher confidence. Note that all values are normalized to [0;1].</p><p>[5] which shares similarities with our proposed work and the second is an encoder-decoder architecture with skipconnections inspired by <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, which was demonstrated to achieve state-of-the-art results. The former employs a late-fusion scheme for combining different streams, while the latter adopts an early-fusion scheme. <ref type="table">Table 1</ref> summarizes some methods in the literature under this categorization.</p><p>We investigate all cases from <ref type="table">Table 1</ref> with both architectures and both fusion schemes. First, we utilize a multistream network with late fusion as is shown in <ref type="figure" target="#fig_3">Figure  7a</ref>. One stream contains our unguided network described in section 4 followed by refinement layers and the other contains the image concatenated with the output confidence from the unguided network. Eventually, both streams are fused by concatenation and then fed to a fusion network that produces the final output. In addition, we train the same network in an early fusion manner as illustrated in <ref type="figure" target="#fig_3">Figure 7c</ref>. Note that the number of channels for the depth stream was added to the RGB stream, while the number of channels for the fusion network were kept unchanged.</p><p>Secondly, we adopt a multi-scale encoder-decoder network with late fusion as shown in <ref type="figure" target="#fig_3">Figure 7b</ref>. One stream contains our unguided network followed by an encoder, where all convolution layers apply a stride of 2 to perform downsampling and a ReLU activation. In the other stream, both RGB image and output confidence from the unguided network are concatenated and then fed to an encoder similar to the previous one, but with a larger number of channels per layer. At the decoder, feature maps from both streams are upsampled and then concatenated with the feature maps having the same scale from the encoder. Afterwards, con-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Early Fusion Late Fusion</head><p>Multi-stream • <ref type="bibr" target="#b5">[5]</ref> Encoder-decoder <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b8">[8]</ref> [6], <ref type="bibr" target="#b7">[7]</ref> TABLE 1 A categorization of the state-of-the-art methods depending on the architecture and the fusion scheme. volution is performed followed by Leaky ReLU activation with α = 0.2. The final layer produces the final dense output. Similarly, we also apply an early fusion scheme to this architecture by concatenating both the output and the output confidence from the unguided network with the RGB image. Then, they are fed into a similar encoder-decoder as illustrated in <ref type="figure" target="#fig_3">Figure 7d</ref>. In this way, we evaluate all options listed in <ref type="table">Table 1</ref>. For all networks described here, we aim to reduce the number of parameters for computational efficiency. Therefore, we use a fixed number of feature channels of 16 per each input channel. For example, sparse depth input has only one channel, so we use 16 features at all layers in the depth stream, while we use 16 × 4 = 64 for the RGB image and the output confidence. Our experiments will demonstrate how the use of the unguided normalized convolution sub-network allows achieving state-of-the-art results on the task of guided depth completion without requiring huge networks with millions of parameters as in <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b8">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>To demonstrate the capabilities of our proposed approach, we evaluate our proposed networks on the KITTI-Depth Benchmark <ref type="bibr" target="#b0">[1]</ref> and the NYU-Depth-v2 <ref type="bibr" target="#b10">[10]</ref> dataset for the task of depth completion. <ref type="bibr" target="#b0">[1]</ref> includes depth maps from projected LiDAR point clouds that were matched against the depth estimation from the stereo cameras. The depth images are highly sparse with only 5% of the pixels available and the rest is missing. The dataset has 86k training images, 7k validation images, and 1k test set images on the benchmark server with no access to the ground truth. The test set will be used for evaluation against other methods, while a subset of the validation set (1k images) will be used for the analysis of our own method. We also use the RGB images from the raw data of the original KITTI dataset <ref type="bibr" target="#b24">[24]</ref>. It is worth mentioning that the groundtruth of the KITTI-Depth dataset is incomplete since pixels that were not consistent with the groundtruth form the stereo disparity have been removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI-Depth dataset</head><p>The NYU-Depth-v2 dataset <ref type="bibr" target="#b10">[10]</ref> is an RGB-D dataset for indoor scenes, captured with a Microsoft Kinect. We train a model that produces a dense depth map using the RGB image and a uniformly sampled depth points. Similar to <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, we use the official split with roughly 48k RGB-D pairs for the training and 654 pairs for testing. To match the resolution of the RGB images and the depth maps, the RGB images of size 640 × 480 are downsampled and centercropped to 304 × 228 as described in <ref type="bibr" target="#b27">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Setup</head><p>All experiments were performed on a workstation with 6 CPU cores, 112 GB of RAM, and an NVIDIA Tesla V100 GPU with 16 GB of memory. All guided networks were trained until convergence on the full training set with a batch size of 4 and 8 for the KITTI-Depth dataset and the NYU-Depth-v2 datasets, respectively. We use the ADAM optimizer with an initial learning rate of 10 −4 and a decaying factor of 0.1 after 20 epochs for the KITTI-Depth dataset and 10 epochs for the NYU-Depth-v2 dataset. When only the unguided normalized convolution network is trained on the KITTI-Depth dataset, we train on 10k images of the training set for 5 epochs using the ADAM optimizer with a learning rate of 0.01. We have implemented our network using the PyTorch framework and the source code is available on Github. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation Metrics</head><p>For the KITTI-Depth dataset, we adopt the evaluation metrics used in the benchmark <ref type="bibr" target="#b0">[1]</ref>: the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE) computed on the depth values. The MAE is an unbiased error metric takes takes an average of the error across the whole image and it is defined as:</p><formula xml:id="formula_16">MAE(Z, T ) = 1 M N   N i=0 M j=0 |Z(i, j) − T (i, j)|   ,<label>(14)</label></formula><p>while the RMSE penalizes outliers and it is defined as:</p><formula xml:id="formula_17">RMSE(Z, T ) = 1 M N   N i=0 M j=0 |Z(i, j) − T (i, j)| 2   1/2 .<label>(15)</label></formula><p>Additionally, we also use iMAE and iRMSE, which are calculated on the disparity instead of the depth. The 'i' indicates that disparity is proportional to the inverse of depth.</p><p>For the NYU-Depth-v2 dataset, we compute the RMSE, the mean absolute relative error (REL), and the inliers ratio as descriped in <ref type="bibr" target="#b27">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Evaluating Guided Normalized CNNs</head><p>First, we evaluate the different architectures described in section 5.3 on the KITTI-Depth dataset <ref type="bibr" target="#b0">[1]</ref>. The first architecture is a multi-stream network with early fusion denoted as MS-Net[EF] and its variant with late fusion MS-Net[LF]. The second architecture is an encoder-decoder architecture with early-fusion denoted as EncDec-Net[EF] and its variant with late fusion EncDec-Net <ref type="bibr">[LF]</ref>. For a fair comparison and to neutralize any influence from inefficient gradients, we train the unguided network separately using our proposed loss in <ref type="bibr" target="#b13">(13)</ref> and then attach it to the guided networks in comparison while freezing its weights.</p><p>All networks are trained using the Huber norm loss described in <ref type="bibr" target="#b11">(11)</ref>. <ref type="table">Table 2</ref> shows that the multi-stream network with late fusion, MS-Net[LF], outperforms all the other networks with respect to all evaluation metrics. The multi-stream network with early fusion, MS-Net[EF], achieves similar results with respect to MAE and iMAE, but the RMSE and iRMSE are slightly higher. For the encoder-decoder networks, EncDec-Net[EF] with early fusion achieves better results than the network with late fusion contrarily to the multi-stream network.</p><p>Next, we compare our best performing architectures using multi-stream, MS-Net[LF], and encoder-decoder, EncDec-Net[EF], against state-of-the-art methods on the KITTI-Depth and NYU-Depth-v2 datasets.  2 A quantitative comparison between different fusion schemes (described in section 5) on the selected validation set of the KITTI-Depth dataset <ref type="bibr" target="#b0">[1]</ref>. The different fusion schemes are MS-Net <ref type="bibr">[LF]</ref>, which is the multi-stream architecture with late fusion <ref type="figure" target="#fig_3">(Figure 7a)</ref>, MS-Net[EF], which applies early fusion <ref type="figure" target="#fig_3">(Figure 7c)</ref>, EncDec-Net <ref type="bibr">[LF]</ref>, which is the encoder-decoder architecture with late fusion <ref type="figure" target="#fig_3">(Figure 7b)</ref>, and EncDec-Net [EF], which applies early fusion <ref type="figure" target="#fig_3">(Figure 7d</ref>). MS-Net <ref type="bibr">[LF]</ref> achieves the best results with respect to all evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">The KITTI-Depth Dataset Comparison</head><p>We compare MS-Net[LF] and EncDec-Net[EF] against all published methods that have been submitted to the KITTI-Depth benchmark <ref type="bibr" target="#b0">[1]</ref>. SparseConv <ref type="bibr" target="#b0">[1]</ref> proposed a sparsity invariant layer that normalizes the sparse input using a binary validity mask. They also created three baselines: CNN, which trains a simple network directly on the sparse input, CNN+mask, which concatenates the validity mask with the sparse input and trains the same network, and NN+CNN, which performs nearest neighbor interpolation on the sparse input and then trains a refinement network. ADNN <ref type="bibr" target="#b3">[3]</ref> employed compressed sensing within CNNs to handle the sparsity in data. IP-Basic <ref type="bibr" target="#b28">[28]</ref> applied an extensive search on variations of morphology and simple image processing techniques to interpolate the sparse input. Spade <ref type="bibr" target="#b7">[7]</ref> proposed an encoder-decoder architecture with late-fusion to reconstruct a dense output from the sparse input. Sparse-to-Dense <ref type="bibr" target="#b8">[8]</ref> proposed a self-supervised approach to alleviate the incomplete groundtruth in the KITTI-Depth dataset. Their self-supervised approach requires a sequence of sparse depth and RGB images to reconstruct a dense depth map. Finally, HMS-Net derived variations of the sparsity invariant layer that were used to deploy larger and more complex networks. Quantitative results on the test set of the KITTI-Depth dataset [1] using evaluation metrics described above are shown in <ref type="table" target="#tab_4">Table 3</ref>. Our method MS-Net[LF]-L1 (gd) outperforms all the other methods with respect to the MAE with a large margin. When trained using the L2-norm, it was able to perform the second best with respect to RMSE with a small margin compared to Sparse-to-Dense (gd). However, our method has a significantly lower number of parameters (∼ 355k) compared to Sparse-to-Dense (gd) which has ∼ 5.5M parameters. This demonstrates that our method achieves state-of-the-art results while requiring a very small number of parameters. On the other hand, our method EncDec-Net[EF]-L1 achieves moderate results. Spade (gd) on the other hand achieves the best results with respect to iRMSE since it was trained on disparity using the L2-norm. However, our method MS-Net[LF]-L1 (gd) still outperformed Spade (gd) with respect to iMAE despite being trained on depth. <ref type="figure" target="#fig_4">Figure 8</ref> shows some qualitative results for the top performing methods from the benchmark server.   <ref type="bibr" target="#b0">[1]</ref>. The best method is shown in bold and the second best is shown in italic. The performance is shown on the test set for which the results were submitted to the benchmark evaluation server with no access to the ground truth. For all methods, (d) denotes that only the sparse depth input was used, while (gd) denotes that both the sparse depth and the RGB images were used. Our method MS-Net[LF]-L1 (gd) outperforms all methods with respect to the MAE error with a large margin. Our method MS-Net[LF]-L2 (gd) trained using the L2-norm achieves second best results with respect to RMSE with a small margin to the top-performing method Sparse-to-Dense (gd).</p><p>was trained using the L2-norm loss, which achieved the lowest RMSE error. Generally, our method and the other two methods in comparison perform equally well with some minor differences. Our method performs better with tiny details (highlighted using the yellow boxes) such as the car edges in the first row and the poles far away in the second row. Our method was also able to remove outliers and highly sparse regions as shown on the third row. On the other hand, Sparse-to-Dense (gd) produces smoother edges than our method and HMS-Net (gd) due to the use of a smoothness loss that penalizes the second derivative of the prediction during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">The NYU-Depth-v2 Dataset Comparison</head><p>On the NYU-Depth-v2 dataset, we compare MS-Net[LF] and EncDec-Net[EF] against published state-of-the-art methods that utilize the RGB image and uniformly sampled pixels from the depth map as an input. Sparse-to-dense <ref type="bibr" target="#b25">[25]</ref> utilizes a single deep regression network to learn a dense depth map from the RGB-D raw data input. Liao et al. <ref type="bibr" target="#b29">[29]</ref> solve this problem by constructing a residual network which combines classification and regression losses to learn a dense depth map. Cheng et al. <ref type="bibr" target="#b26">[26]</ref> proposed a convolutional spatial propagation network (CSPN), which learns the affinity matrix needed to predict a dense depth map. <ref type="table">Table 4</ref> shows the quantitative results for the methods in comparison on the NYU-Depth-v2 dataset. For a very sparse input (200 samples), our EncDec-Net[EF] achieves the best results with a huge margin to other methods in comparison. In addition, MS-Net[LF] achieving the second best  <ref type="bibr" target="#b6">[6]</ref>. For each method, the top image is the prediction and the lower image is the error. Our method MS-Net[LF]-L2 (gd) performs slightly better in handling outliers as highlighted with the yellow boxes, while Sparse-to-Dense produces smoother edges due to the use of a smoothness loss. Note that this figure is best viewed on screens.</p><p>results. On the other hand, Sparse-to-Dense <ref type="bibr" target="#b25">[25]</ref> performs significantly worse than our proposed method despite have two orders of magnitude larger number of parameters. For a denser input (500 samples), EncDec-Net[EF] achieves the second best results with a small margin to UNet+CSPN <ref type="bibr" target="#b26">[26]</ref>. However, <ref type="bibr" target="#b26">[26]</ref> requires "Preserving Depth" values from the input in order to update the learned affinity between layers. This requirement is not always appropriate, e.g.in case of corrupted/incorrect input in the KITTI-Depth dataset <ref type="bibr" target="#b30">[30]</ref> due to occlusion. <ref type="figure" target="#fig_5">Figure 9</ref> shows some qualitative examples on the NYU-Depth-v2 dataset. Both our methods EncDec-Net[EF] and MS-Net[LF] produce remarkably better reconstructions of the dense map that Sparse-to-Dense <ref type="bibr" target="#b25">[25]</ref>, in particular with respect to edges sharpness and the level of details. The predictions from <ref type="bibr" target="#b25">[25]</ref> are very blurry and give a global  <ref type="bibr" target="#b10">[10]</ref>. #Samples states the number of depth pixels that were uniformly sampled and the sparsity levels are indicated in brackets. depth estimation for local regions. However, EncDec-Net[EF] yields smoother and more consistent reconstruction than MS-Net[LF], especially along edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ANALYSIS</head><p>In this section, we analyze different components of our proposed method thoroughly. Since the NYU-Depth-v2 allows changing the degree of sparsity, we use it to evaluate our method's performance with varying degrees of sparsity. Other analyses are performed on the KITTI-Depth dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Varying Degree of Sparsity</head><p>The NYU-Depth-v2 dataset allows changing the degree of sparsity by altering the number of depth samples provided at the input. <ref type="figure">Figure 10</ref> shows approaches EncDec-Net[EF] until they produce very similar results at lower degrees of sparsity. Sparse-to-dense <ref type="bibr" target="#b25">[25]</ref> performs very well at very sparse input. However, with the decreasing level of sparsity, it does not seem that the network is significantly benefiting from the additional depth samples, contrarily to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">The Choice of the Non-negative Function</head><p>The choice of the non-negative function mainly depends on the desired characteristics. The most obvious choice is to clamp non-negative values as in the ReLU function. However, the ReLU has problems with the discontinuous derivative. Therefore, we consider a good continuous approximation for the ReLU, the SoftPlus function Γ(x) = 1 β log(1 + exp(βx). With the right choice of the β, we can have a very good approximation of the ReLU function, e.g. when β = 10, as shown in <ref type="figure" target="#fig_6">Figure 11a</ref>. The derivative of the SoftPlus function is continuous which gives more flexibility to the network during training. <ref type="figure" target="#fig_6">Figure 11b</ref> shows how the choice of the non-negative function affects the convergence of the network. The ReLU function shows poor convergence and keeps fluctuating as the derivatives are not continuous and the network struggles to converge. SoftPlus on the other hand converges very fast due to the continuous derivative and with the right choice of β, the results are improved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">The Non-negativity Constraint Impact</head><p>To study the effect of enforcing non-negativity constraints on our trained filters, we compare the convergence of our proposed unguided normalized convolution module described in section 4 in the presence and the absence of the non-negativity enforcement. Since our proposed confidence measure cannot be used in the absence of the non-negativity constraints, we propagate confidences by applying a max pooling operations on the confidence map as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>. Besides, only the data error term in our proposed loss in <ref type="bibr" target="#b13">(13)</ref> is used because of the absence of output confidence. Both networks were trained on 10k training images for 5 epochs with a constant learning rate of 0.01. The networks were trained on the disparity instead of depth since both networks perform better when trained on disparity. <ref type="figure" target="#fig_6">Figure  11b</ref> shows the convergence curves for both networks. When enforcing the non-negativity constraints, the network converges after 1 epoch, while the other network not enforcing the non-negativity constraint starts to converge after 4 epochs and to a higher error value. This demonstrates that our proposed non-negativity constraint helps the network to converge faster and to achieve significantly better results.</p><p>The overall effect of the non-negativity constraints on the guided network is shown in <ref type="table">Table 5</ref>. Discarding the nonnegativity constraint significantly degrades the results with respect to all evaluation metrics. This is potentially caused by the lack of guidance provided by the output confidence or by the the poor estimation of depth produced by the unguided network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 5</head><p>The impact of enforcing non-negativity on normalized convolution layers. NConv-CNN-L1 (gd) with SoftPlus achieves significantly better results than the case without enforcing non-negativity. The results shown are for the selected validation set of the KITTI-Depth dataset <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure" target="#fig_0">Fig. 12</ref>. The impact of the proposed loss on confidence levels. The right axis represents the mean and standard deviation of maximum output confidence value over all images, while the left axis has the MAE in meters. When using a loss with only a data term (Huber norm Loss), output confidence levels are lower, while our proposed loss achieves monotonically increasing confidence levels as well as a lower MAE. Note that the shaded area represents the standard deviation. The results shown are for the selected validation set of the KITTI-Depth dataset <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">The Impact of the Proposed Loss</head><p>To study the impact of the confidence term in our proposed loss in (13), we train our unguided normalized convolution network described in section 4 twice: once using the proposed loss function with confidence term and once using only the Huber norm loss <ref type="bibr" target="#b12">(12)</ref>. <ref type="figure" target="#fig_0">Figure 12</ref> shows the mean and the standard deviation of the maximum output confidence over images in the selected validation set on the right axis and the MAE error on the left axis. The network trained with our proposed loss produces a monotonically increasing confidence map while improving the data error until convergence. On the other hand, the network trained with only the Huber norm loss has lower levels of output confidence in general and it also converges to a higher MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">The Learned Filters</head><p>The unguided normalized convolution network acts as a multi-scale generic estimator for the data. During training, this generic estimator is learned from the data using backpropagation. Some examples of the learned filters are shown in <ref type="figure" target="#fig_7">Figure 13</ref>. The first row of the figure shows some of the learned filters for layers NCONV <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b2">[2]</ref><ref type="bibr" target="#b3">[3]</ref>, which are asymmetric low-pass filters. Those filters attempt to construct the missing pixels from their neighborhood. On the other hand, the second row of the figure shows the learned filters for layers NCONV <ref type="bibr" target="#b4">[4]</ref><ref type="bibr" target="#b5">[5]</ref><ref type="bibr" target="#b6">[6]</ref>, which resemble linear ramps. Those filters try to scale the output from each scale for an efficient fusion with other scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Guided Normalized CNN Ablation Study</head><p>In this section, we study the effect of different components of our best performing guided network. Since the benchmarks is ranked based on the RMSE, we use MS-Net[LF]-L2 (gd) as a baseline. When the whole network was trained end-to-end, the results are slightly degraded as shown in <ref type="table" target="#tab_9">Table 6</ref>. This could be a result of vanishing gradients since the network becomes deeper. Removing either the depth refinement layers or the output confidence has almost the same influence on the performance of the network as shown in <ref type="table" target="#tab_9">Table 6</ref>. The reason might be that the depth refinement layers contribute to handling some outliers that violate the estimation from the unguided network. On the other hand, the use of the output confidence provides the RGB stream with information about regions with low confidence that are highly likely to contribute to the error. Therefore, discarding the output confidence increases the error by approximately 10%, which demonstrates its contribution.</p><p>To further validate our proposed architecture, we perform an experiment on the KITTI-Depth dataset by increasing the number of network layers of EncDec-Net[EF] by a factor of 2 and removing both our confidence propagation and normalization components. We denoted this experiment as EncDec-Net[EF]×2 w/o NConv and   <ref type="figure">Figure 1</ref> and OC is the use of the output confidence in the RGB feature extraction network. When the baseline is trained end-to-end, the performance is slightly degraded. The depth refinement layers also contribute to the results. Discarding the output confidence, degrades the results. On the NYU-Depth-v2, a network with standard convolution and double number of layers fails to achieve comparable results to EncDec-Net[EF].  <ref type="bibr" target="#b4">4</ref>.84 × 10 5 0.01</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 7</head><p>Number of parameters and runtime for some methods in comparison (lower is better). The upper section is for unguided networks, the middle section is for guided networks and the lower section is for the NYU-Depth-v2 experiments. Note that the exact number of Spade (gd) <ref type="bibr" target="#b7">[7]</ref> is not mentioned in the paper, so we give the number of parameters for the NASNet [31] that they utilize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">Number of Parameters and Runtime Comparison</head><p>In this section, we compare the number of parameters and the runtime for some of the methods in comparison.</p><p>For the KITTI-Depth dataset, the number of parameters is calculated from the network descriptions in the related papers, while the runtime is taken from the benchmark server <ref type="bibr" target="#b0">[1]</ref>. <ref type="table">Table 7</ref> shows that our unguided network NConv-CNN (d) has the lowest number of parameters and runtime compared to all other unguided methods, which makes it most suitable for embedded applications with limited computational resources. We maintain the low number of parameters in our guided network MS-Net-L2[LF] (gd) that has 356k parameters, which is at least one order of magnitude fewer than all other guided methods in the comparison. This leads to the lowest runtime of 0.02 seconds among the guided methods which satisfies real-time constraints and has a high potential to maintain the real-time performance if evaluated on embedded devices. The huge decrease in the number of parameters did not degrade the results as was shown in the quantitative results earlier, which demonstrates the efficiency of our proposed method. Note that the runtime between our unguided and guided network do not scale linearly as our unguided network includes many time consuming operations such as downsampling, slicing and upsamping, while the guided network adds only convolution operations. For the NYU-Depth-v2 dataset, our method has a drastically lower number of parameters compared to Sparse-to-Dense <ref type="bibr" target="#b25">[25]</ref> (∼ 1% the number of parameters). However, our method still managed to achieve better results at different levels of sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8">Output Confidence/Error Correlation</head><p>We have shown empirically that the output confidence is useful to improve the results. To gain further understanding, we perform a correlation analysis between the prediction absolute error and the negative logarithm of the output confidence (similar to the log likelihood). We employ Pearson's correlation measure defined as:</p><formula xml:id="formula_18">ρ X,Y = cov(X, Y ) σ X σ Y .<label>(16)</label></formula><p>The analysis is performed on the output from our unguided network NConv-CNN (d) both on the KITTI-Depth and the NYU-Depth-v2 datasets. Since the distributions for the error and the output confidence are unknown, we perform histogram equalization and transform the error values and confidences accordingly.</p><p>To form a baseline, we produce a naive output confidence by interpolating the input confidence mask using the normalized convolution with the naive basis and a Gaussian applicability. This produces high confidence at location where the input is valid, and increasingly attenuated confidences the further we move from input point locations as illustrated in <ref type="figure" target="#fig_8">Figure 14b</ref>. The baseline gives an average Pearson's correlation measure of 0.1 on the NYU-Depth-v2 and -0.25 on the KITTI-Depth validation sets. We attribute this negative correlation to the faulty input in the KITTI-Depth dataset that does not match the groundtruth <ref type="bibr" target="#b30">[30]</ref>.</p><p>Contrarily, our proposed output confidence achieves a significantly higher correlation of 0.3 and 0.4. This correlation is considerably high as the correlation upperbound for unknown distributions is low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we have proposed a normalized convolution layer for unguided scene depth completion on highly sparse data by treating the validity masks as a continuous confidence field. We proposed a method to propagate confidences between CNN layers. This enabled us to produce a point-wise continuous confidence map for the output from the deep network. For fast convergence, we algebraically constrained the learned filters to be non-negative, acting as a weighting function for the neighborhood. Furthermore, a loss function is proposed that simultaneously minimizes the data error and maximizes the output confidence. Finally, we proposed a fusion strategy to combine depth and RGB information in our normalized convolution network framework for incorporating structural information. We performed comprehensive experiments on the KITTI-Depth benchmark, the NYU-Depth-v2 dataset and achieved superior performance with significantly fewer network parameters compared to the state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Examples for differentiable functions with non-negative codomain, (b) Applying the SoftPlus function to a 2D surface preserves the surface trend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>An example for the spatial error distribution of the output from our unguided normalized convolution network on the task of depthcompletion. (d)shows that the error is distributed around edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Encoder-Decoder (Early Fusion) (a) A multi-stream architecture that contains a stream for depth and another stream for RGB+Output Confidence feature extraction. Afterwards, a fusion network combines both streams to produce the final dense output. (d) A multi-scale encoder-decoder architecture where depth is fed to the unguided network followed by an encoder and output confidence and RGB image are concatenated then fed to a similar encoder. Both streams have skip-connection to the decoder between the corresponding scales. (c) is similar to (a), but with early fusion and (d) is similar to (b) but with early fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Some qualitative examples for the top three performing methods from the KITTI-Depth dataset [1] on the task of scene depth completion. (a) RGB input, (b) Our method MS-Net[LF]-L2 (gd), (c) Sparse-to-Dense (gd) [8] and (d) HMS-Net (gd)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Some qualitative results on the NYU-Depth-v2 dataset with 200 randomly sampled depth samples as the input. (a) RGB input, (b) The groundtruth depth, (c) our EncDec-Net[EF] results, (d) our MS-Net[LF] results, and (e) Sparse-to-Dense [25] results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>(a) The SoftPlus function with different scaling factors. At β = 10, the SoftPlus function gives a good differentiable approximation of the ReLU function. (b) Convergence curves for our unguided network with the presence and absence of the non-negativity constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 .</head><label>13</label><figDesc>A visualization of the learned filters from our proposed unguided normalized convolution network on the KITTI-Depth dataset<ref type="bibr" target="#b0">[1]</ref>. Layer names match their correspondences inFigure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 14 .</head><label>14</label><figDesc>An illustration of the baseline for output confidence/error correlation on the NYU-Depth-v2 dataset. (a) The binary input confidence map. (b) Interpolated input confidence map using the normalized convolution with the naive basis and a Gaussian applicability. Confidences are maximal at input points location and are increasingly attenuated the further we move from the input points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Quantitative results for methods in comparison on the KITTI-Depth benchmark</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>This is due to the use of multiple scales, which allows exploiting depth information at different scales. On the other hand, MS-Net[LF] performs worse as it has only one scale level. However, when the sparsity degree decreases, i.e. the number of depth samples is increased, MS-Net[LF]</figDesc><table><row><cell>how our architectures MS-</cell></row><row><cell>Net[LF] and EncDec-Net[EF] perform with varying degrees</cell></row><row><cell>of sparsity. EncDec-Net[EF] performs very well with dif-</cell></row><row><cell>ferent degrees of sparsity even with a very sparse input</cell></row><row><cell>(∼ 0.01%).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>shows that the resulting large network provides an 1122.51 [mm] RMSE score which is inferior to 1007.71 [mm] achieved by our proposed light-weight architecture with confidence propagation and normalization.</figDesc><table><row><cell></cell><cell cols="2">DS MAE</cell><cell>RMSE</cell><cell>iMAE</cell><cell>iRMSE</cell></row><row><cell></cell><cell></cell><cell>[mm]</cell><cell>[mm]</cell><cell>[1/km]</cell><cell>[1/km]</cell></row><row><cell>Baseline</cell><cell>K</cell><cell>233.25</cell><cell>870.82</cell><cell>1.03</cell><cell>2.75</cell></row><row><cell>Baseline (end-to-end)</cell><cell></cell><cell>244.98</cell><cell>886.09</cell><cell>1.11</cell><cell>3.02</cell></row><row><cell>Baseline w/o DR</cell><cell></cell><cell>245.11</cell><cell>912.82</cell><cell>1.10</cell><cell>3.03</cell></row><row><cell>Baseline w/o OC</cell><cell></cell><cell>244.87</cell><cell>919.55</cell><cell>1.09</cell><cell>2.97</cell></row><row><cell>EncDec-Net[EF]</cell><cell>N</cell><cell>236.83</cell><cell>1007.71</cell><cell>0.99</cell><cell>2.75</cell></row><row><cell>EncDec-Net[EF]×2</cell><cell></cell><cell>274.73</cell><cell>1122.51</cell><cell>1.19</cell><cell>3.28</cell></row><row><cell>w/o NConv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 6</head><label>6</label><figDesc>DS refers to the used dataset, K is the KITTI-Depth, N is the NYU-Depth-v2. On the KITTI-Depth dataset, baseline refers to MS-Net[LF]-L2 (gd), DR refers to the Depth Refinement layers as indicated in</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is funded by Vinnova through grant CYCLA, the Swedish Research Council through project grant 2018-04673, and VR starting grant (2016-05543).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abdelrahman Eldesokey is a Ph.D. student at the Computer Vision Laboratory, Linköping University, Sweden. . He received his M.Sc. degree in Informatics from Nile University, Egypt in 2016. He is also affiliated with the Wallenberg AI, Autonomous Systems and Software Program (WASP). His research interests include deep learning for computer vision and autonomous driving with a focus on uncertain and sparse data.</p><p>Michael Felsberg received the Ph.D. degree in engineering from the University of Kiel, Kiel,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sparsity Invariant CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="http://arxiv.org/abs/1708.06500" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shepard convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep Convolutional Compressed Sensing for LiDAR Depth Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1803.08949" />
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Image Inpainting for Irregular Holes Using Partial Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1804.07723" />
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A normalized convolutional neural network for guided sparse depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2283" to="2290" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">HMS-Net: Hierarchical Multi-scale Sparsity-invariant Network for Sparse Depth Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00769</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venturelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karaman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/978-3-319-24574-4{_}28</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-319-24574-4{}28" />
		<imprint>
			<date type="published" when="2015-10" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="234" to="241" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Propagating confidences through cnns for sparse data regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<meeting><address><addrLine>Newcastle upon Tyne, England, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-06" />
		</imprint>
		<respStmt>
			<orgName>Northumbria University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth hole filling using the depth distribution of neighboring regions of depth holes in the kinect sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Signal Processing</title>
		<imprint>
			<publisher>Communication and Computing</publisher>
			<date type="published" when="2012-08" />
			<biblScope unit="page" from="658" to="661" />
		</imprint>
	</monogr>
	<note>ICSPCC 2012</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth map enhancement method based on joint bilateral filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 7th International Congress on Image and Signal Processing</title>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="153" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Super-resolution reconstruction for kinect 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="2712" to="2715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Guided depth upsampling for precise mapping of urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wirges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rehder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2017-06" />
			<biblScope unit="page" from="1140" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intensity guided depth upsampling by residual interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Konno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Monno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Abstracts of the international conference on advanced mechatronics: toward evolutionary fusion of IT and mechatronics: ICAM 2015.6. The</title>
		<imprint>
			<publisher>Japan Society of Mechanical Engineers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Normalized and differential convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Knutsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Westin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 1993. Proceedings CVPR&apos;93</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="515" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Polynomial expansion for orientation and motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farnebäck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Linköping University Electronic Press</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Normalized averaging using adaptive applicability functions with applications in image reconstruction from sparsely and randomly sampled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Van Vliet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conference on Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="485" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Focus of attention and gaze control for robot vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Westelius</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>Linköping University, Computer Vision, The Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Local signal models for image sequence analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karlholm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Linköping University, Computer Vision, The Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of mathematical statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07492</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00036</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parse geometry from a line: Monocular depth estimation with partial laser observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5059" to="5066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00488</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
