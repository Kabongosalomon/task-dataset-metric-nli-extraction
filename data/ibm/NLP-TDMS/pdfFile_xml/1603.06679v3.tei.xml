<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-09-19">19 Sep 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SAP Innovation Center</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Sinno</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
							<email>â€¡d.dahlmeier@sap.com</email>
							<affiliation key="aff1">
								<orgName type="department">SAP Innovation Center</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
							<email>xkxiao@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-09-19">19 Sep 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In aspect-based sentiment analysis, extracting aspect terms along with the opinions being expressed from user-generated content is one of the most important subtasks. Previous studies have shown that exploiting connections between aspect and opinion terms is promising for this task. In this paper, we propose a novel joint model that integrates recursive neural networks and conditional random fields into a unified framework for explicit aspect and opinion terms co-extraction. The proposed model learns high-level discriminative features and double propagates information between aspect and opinion terms, simultaneously. Moreover, it is flexible to incorporate hand-crafted features into the proposed model to further boost its information extraction performance. Experimental results on the dataset from SemEval Challenge 2014 task 4 show the superiority of our proposed model over several baseline methods as well as the winning systems of the challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect-based sentiment analysis <ref type="bibr" target="#b27">(Pang and Lee, 2008;</ref> aims to extract important information, e.g. opinion targets, opinion expressions, target categories, and opinion polarities, from user-generated content, such as microblogs, reviews, etc. This task was first studied by <ref type="bibr" target="#b5">Hu and Liu (2004a;</ref><ref type="bibr" target="#b6">2004b)</ref>, followed by <ref type="bibr">(Popescu and Etzioni, 2005;</ref><ref type="bibr" target="#b40">Zhuang et al., 2006;</ref><ref type="bibr" target="#b29">Qiu et al., 2011;</ref><ref type="bibr" target="#b15">Li et al., 2010)</ref>. In aspect-based sentiment analysis, one of the goals is to extract explicit aspects of an entity from text, along with the opinions being expressed. For example, in a restaurant review "I have to say they have one of the fastest delivery times in the city.", the aspect term is delivery times, and the opinion term is fastest.</p><p>Among previous work, one of the approaches is to accumulate aspect and opinion terms from a seed collection without label information, by utilizing syntactic rules or modification relations between them <ref type="bibr" target="#b29">(Qiu et al., 2011;</ref><ref type="bibr" target="#b18">Liu et al., 2013b</ref>). In the above example, if we know fastest is an opinion word, then delivery times is probably deduced as an aspect because fastest is its modifier. However, this approach largely relies on hand-coded rules, and is restricted to certain Part-of-Speech (POS) tags, e.g., opinion words are restricted to be adjectives. Another approach focuses on feature engineering based on predefined lexicons, syntactic analysis, etc <ref type="bibr" target="#b10">(Jin and Ho, 2009;</ref><ref type="bibr" target="#b15">Li et al., 2010)</ref>. A sequence labeling classifier is then built to extract aspect and opinion terms. This approach requires extensive efforts for designing hand-crafted features, and only combines features linearly for classification, which ignores higher order interactions.</p><p>To overcome the limitations of existing methods, we propose a novel model, namely Recursive Neural Conditional Random Fields (RNCRF). Specifically, RNCRF consists of two main components. The first component is to construct a recursive neural network (RNN) 1 <ref type="bibr" target="#b30">(Socher et al., 2010)</ref> based on a dependency tree of each sentence. The goal is to learn a highlevel feature representation for each word in the context of each sentence, and make the representation learning for aspect and opinion terms interactive through the underlying dependency structure among them. The output of the RNN is then fed into a Conditional Random Field (CRF) <ref type="bibr" target="#b13">(Lafferty et al., 2001)</ref> to learn a discriminative mapping from high-level features to labels, i.e., aspects, opinions, or others, so that context information can be well captured. Our main contributions are to use RNN for encoding aspect-opinion relations in high-level representation learning, and to present a joint optimization approach based on maximum likelihood and backpropagation to learn the RNN and CRF components, simultaneously. In this way, the label information of aspect and opinion terms can be dually propagated from parameter learning in CRF to representation learning in RNN. We conduct expensive experiments on the dataset from SemEval challenge 2014 task 4 (subtask 1) <ref type="bibr" target="#b28">(Pontiki et al., 2014)</ref> to verify the superiority of RNCRF over several baseline methods as well as the winning systems of the challenge. <ref type="bibr" target="#b5">Hu et al. (2004a)</ref> proposed to extract product aspects through association mining, and opinion terms by augmenting a seed opinion set using synonyms and antonyms in WordNet. In follow-up work, syntactic relations are further exploited for aspect/opinion extraction <ref type="bibr">(Popescu and Etzioni, 2005;</ref><ref type="bibr" target="#b37">Wu et al., 2009;</ref><ref type="bibr" target="#b29">Qiu et al., 2011)</ref>. For example, <ref type="bibr" target="#b29">Qiu et al. (2011)</ref> used syntactic relations to double propagate and augment the sets of aspects and opinions. Though the above models are unsupervised, they heavily depend on predefined rules for extraction, and are also restricted to specific types of POS tags for product aspects and opinions. <ref type="bibr" target="#b10">Jin et al. (2009</ref><ref type="bibr" target="#b15">), Li et al. (2010</ref>, <ref type="bibr" target="#b9">Jakob et al. (2010)</ref> and <ref type="bibr" target="#b21">Ma et al. (2010)</ref> modeled the extraction problem as a sequence tagging problem, and proposed to use HMMs or CRFs to solve it. These methods rely on richly handcrafted features, and do not consider interactions between aspect and opinion terms explicitly. Another direction is to use word alignment model to capture opinion relations among a sentence <ref type="bibr" target="#b16">(Liu et al., 2012;</ref><ref type="bibr" target="#b17">Liu et al., 2013a)</ref>. This method requires sufficient data for modeling desired relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Aspects and Opinions Co-Extraction</head><p>Besides explicit aspects and opinions extraction, there are also other lines of research related to aspect-based sentiment analysis, including aspect classification <ref type="bibr">(Lakkaraju et al., 2014;</ref><ref type="bibr" target="#b22">McAuley et al., 2012)</ref>, aspect rating <ref type="bibr">(Titov and McDonald, 2008;</ref><ref type="bibr" target="#b36">Wang et al., 2011;</ref><ref type="bibr" target="#b35">Wang and Ester, 2014)</ref>, domainspecific and target-dependent sentiment classification <ref type="bibr" target="#b36">(Lu et al., 2011;</ref><ref type="bibr">Ofek et al., 2016;</ref><ref type="bibr" target="#b0">Dong et al., 2014;</ref><ref type="bibr" target="#b34">Tang et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning for Sentiment Analysis</head><p>Recent studies have shown that deep learning models can automatically learn the inherent semantic and syntactic information from data and thus achieve better performance for sentiment analysis <ref type="bibr" target="#b31">(Socher et al., 2011b;</ref><ref type="bibr" target="#b32">Socher et al., 2012;</ref><ref type="bibr" target="#b32">Socher et al., 2013;</ref><ref type="bibr" target="#b2">Glorot et al., 2011;</ref><ref type="bibr">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b11">Kim, 2014;</ref><ref type="bibr" target="#b14">Le and Mikolov, 2014)</ref>. These methods generally belong to sentence-level or phrase/wordlevel sentiment polarity predictions. Regarding aspect-based sentiment analysis, <ref type="bibr">Irsoy et al. (2014)</ref> applied deep recurrent neural networks for opinion expression extraction. <ref type="bibr" target="#b0">Dong et al. (2014)</ref> proposed an adaptive recurrent neural network for target-dependent sentiment classification, where targets or aspects are given as input. <ref type="bibr" target="#b34">Tang et al. (2015)</ref> used Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) for the same task. Nevertheless, there is little work in aspects and opinions co-extraction using deep learning models.</p><p>To the best of our knowledge, the most related works to ours are <ref type="bibr" target="#b38">Yin et al., 2016)</ref>.  proposed to combine recurrent neural network and word embeddings to extract explicit aspects. However, the proposed model simply uses recurrent neural network on top of word embeddings, and thus its performance heavily depends on the quality of word embeddings. In addition, it fails to explicitly model dependency relations or compositionalities within certain syntactic structure in a sentence. Recently, <ref type="bibr" target="#b38">Yin et al. (2016)</ref> proposed an unsupervised learning method to improve word embeddings using dependency path embeddings. A CRF is then trained with the embeddings independently in the pipeline.</p><p>Different from <ref type="bibr" target="#b38">(Yin et al., 2016)</ref>, our model does not focus on developing a new unsupervised word embedding methods, but encoding the information of dependency paths into RNN for constructing syntactically meaningful and discriminative hidden rep-resentations with labels. Moreover, we integrate RNN and CRF into a unified framework, and develop a joint optimization approach, instead of training word embeddings and a CRF separately as in <ref type="bibr" target="#b38">(Yin et al., 2016)</ref>. Note that <ref type="bibr">Weiss et al. (2015)</ref> proposed to combine deep learning and structured learning for language parsing which can be learned by structured perceptron. However, they also separate neural network training with structured prediction.</p><p>Among deep learning methods, RNN has shown promising results on various NLP tasks, such as learning phrase representations <ref type="bibr" target="#b30">(Socher et al., 2010)</ref>, sentence-level sentiment analysis <ref type="bibr" target="#b32">(Socher et al., 2013)</ref>, language parsing <ref type="bibr" target="#b31">(Socher et al., 2011a)</ref>, and question answering <ref type="bibr" target="#b8">(Iyyer et al., 2014)</ref>. The tree structures used for RNNs include constituency tree and dependency tree. In a constituency tree, all the words lie at leaf nodes, each internal node represents a phrase or a constituent of a sentence, and the root node represents the entire sentence <ref type="bibr" target="#b30">(Socher et al., 2010;</ref><ref type="bibr" target="#b32">Socher et al., 2012;</ref><ref type="bibr" target="#b32">Socher et al., 2013)</ref>. In a dependency tree, each node including terminal and nonterminal nodes, represents a word, with dependency connections to other nodes <ref type="bibr" target="#b8">Iyyer et al., 2014)</ref>. The resultant model is known as dependency-tree RNN (DT-RNN). An advantage of using dependency tree over the other is the ability to extract word-level representations considering syntactic relations and semantic robustness. Therefore, we adopt DT-RNN in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Statement</head><p>Suppose that we are given a training set of customer reviews in a specific domain, denoted by S = {s 1 , ..., s N }, where N is the number of review sentences. For any s i âˆˆ S, there may exist a set of aspect terms A i = {a i1 , ..., a il }, where each a ij âˆˆ A i can be a single word or a sequence of words expressing explicitly some aspect of an entity, and a set of opinion terms O i = {o i1 , ..., o im }, where each o ir can be a single word or a sequence of words expressing the subjective sentiment of the comment holder. The task is to learn a classifier to extract the set of aspect terms A i and the set of opinion terms O i from each review sentence s i âˆˆ S.</p><p>This task can be formulated as a sequence tagging problem by using the BIO encoding scheme. Specifically, each review sentence s i is composed of a sequence of words s i = {w i1 , ..., w in i }. Each word w ip âˆˆ s i is labeled as one out of the following 5 classes: "BA" (beginning of aspect), "IA" (inside of aspect), "BO" (beginning of opinion), "IO" (inside of opinion), and "O" (others). Let L = {BA, IA, BO, IO, O}. We are also given a test set of review sentences denoted by</p><formula xml:id="formula_0">S â€² = {s â€² 1 , ..., s â€² N â€² }, where N â€² is the number of test reviews. For each test review s â€² i âˆˆ S â€² , our objective is to predict the class label y â€² iq âˆˆ L for each word w â€² iq .</formula><p>Note that a sequence of predictions with "BA" at the beginning followed by "IA" are indication of one aspect, which is similar for opinion terms. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Recursive Neural CRFs</head><p>As described in Section 1, RNCRF consists of two main components: 1) a DT-RNN to learn a highlevel representation for each word in a sentence, and 2) a CRF to take the learned representation as input to capture context around each word for explicit aspect and opinion terms extraction. Next, We present these two components in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dependency-Tree RNNs</head><p>We begin by associating each word w in our vocabulary with a feature vector x âˆˆ ÃŠ d , which corresponds to a column of a word embedding matrix W e âˆˆ ÃŠ dÃ—v , where v is the size of the vocabulary.</p><p>For each sentence, we build a DT-RNN based on the corresponding dependency parse tree with word embeddings as initialization. An example of the dependency parse tree is shown in <ref type="figure">Figure 1</ref>(a), where each edge starts from the parent and points to its dependent with a syntactic relation.</p><p>In a DT-RNN, each node n, including leaf nodes, internal nodes and the root node, in a specific sentence is associated with a word w, an input feature vector x w , and a hidden vector h n âˆˆ ÃŠ d of the same dimension as x w . Each dependency relation r is associated with a separate matrix W r âˆˆ ÃŠ dÃ—d . In addi-(a) Example of a dependency tree.</p><p>(b) Example of a DT-RNN tree structure.</p><p>(c) Example of a RNCRF structure. <ref type="figure">Figure 1</ref>: Examples of dependency tree, DT-RNN structure and RNCRF structure for a review sentence.</p><p>tion, a common transformation matrix W v âˆˆ ÃŠ dÃ—d is introduced to map the word embedding x w at node n to its corresponding hidden vector h n . Along with a particular dependency tree, a hidden vector h n is computed from its own word embedding x w at node n with the transformation matrix W v and its children's hidden vectors h child(n) with the corresponding relation matrices {W r }'s. For instance, given the parse tree shown in <ref type="figure">Figure 1</ref>(a), we first compute the leaf nodes associated with the words I and the using W v as follows,</p><formula xml:id="formula_1">h I = f (W v Â· x I + b), h the = f (W v Â· x the + b),</formula><p>where f is a nonlinear activation function and b is a bias term. In this paper, we adopt tanh(Â·) as the activation function. Once the hidden vectors of all the leaf nodes are generated, we can recursively generate hidden vectors for interior nodes using the corresponding relation matrix W r and the common transformation matrix W v as follows,</p><formula xml:id="formula_2">h food = f (W v Â· x food + W DET Â· h the + b), h like = f (W v Â· x like + W DOBJ Â· h food +W NSUBJ Â· h I + b).</formula><p>The resultant DT-RNN is shown in <ref type="figure">Figure 1(b)</ref>. In general, a hidden vector for any node n associated with a word vector x w can be computed as follows,</p><formula xml:id="formula_3">h n = f ï£« ï£­ W v Â· x w + b + kâˆˆKn W r nk Â· h k ï£¶ ï£¸ ,<label>(1)</label></formula><p>where K n denotes the set of children of node n, r nk denotes the dependency relation between node n and its child node k, and h k is the hidden vector of the child node k. The parameters of DT-RNN, Î˜ RNN = {W v , W r , W e , b}, are learned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Integration with CRFs</head><p>CRFs are a discriminant graphical model for structured prediction. In RNCRF, we feed the output of DT-RNN, i.e., the hidden representation of each word in a sentence, to a CRF. Updates of parameters for RNCRF are carried out successively from the top to bottom, by propagating errors through CRF to the hidden layers of RNN (including word embeddings) using backpropagation through structure (BPTS) <ref type="bibr" target="#b3">(Goller and KÃ¼chler, 1996)</ref>. Formally, for each sentence s i , we denote the input for CRF by h i , which is generated by DT-RNN.</p><p>Here h i is a matrix with columns of hidden vectors {h i1 , ..., h in i } to represent a sequence of words {w i1 , ..., w in i } in a sentence s i . The model computes a structured output y i = {y i1 , ..., y in i } âˆˆ Y, where Y is a set of possible combinations of labels in label set L. The entire structure can be represented by an undirected graph G = (V, E) with cliques c âˆˆ C. In this paper, we employed linear-chain CRF, which has two different cliques: unary clique (U) representing input-output connection, and pairwise clique (P) representing adjacent output connection, as shown in <ref type="figure">Figure 1(c)</ref>. During inference, the model aims to outputÅ· with the maximum conditional probability p(y|h). (We drop the subscript i here for simplicity.) The probability is computed from potential outputs of the cliques:</p><formula xml:id="formula_4">p(y|h) = 1 Z(h) câˆˆC Ïˆ c (h, y c ),<label>(2)</label></formula><p>where Z(h) is the normalization term, and Ïˆ c (h, y c ) is the potential of clique c, computed as Ïˆ c (h, y c ) = exp W c , F (h, y c ) , where the RHS is the exponential of a linear combination of feature vector F (h, y c ) for clique c, and the weight vector W c is tied for unary and pairwise cliques. We also incorporate a context window of size 2T + 1 when computing unary potentials. Thus, the potential of unary clique at node k can be written as</p><formula xml:id="formula_5">Ïˆ U (h, y k ) = exp (W 0 ) y k Â·h k + T t=1 (W âˆ’t ) y k Â·h kâˆ’t + T t=1 (W +t ) y k Â· h k+t ,<label>(3)</label></formula><p>where W 0 , W +t and W âˆ’t are weight matrices of the CRF for the current position, the t-th position to the right, and the t-th position to the left within context window, respectively. The subscript y k indicates the corresponding row in the weight matrix.</p><p>For instance, <ref type="figure" target="#fig_0">Figure 2</ref> shows an example of window size 3. At the second position, the input features for like are composed of the hidden vectors at position 1 (h I ), position 2 (h like ) and position 3 (h the ). Therefore, the conditional distribution for the entire sequence y in <ref type="figure">Figure 1</ref>(c) can be calculated as</p><formula xml:id="formula_6">p(y|h)= 1 Z(h) exp 4 k=1 (W 0 ) y k Â·h k + 4 k=2 (W âˆ’1 ) y k Â·h kâˆ’1 + 3 k=1 (W +1 ) y k Â·h k+1 + 3 k=1 V y k ,y k+1 ,</formula><p>where the first three terms in the exponential of the RHS consider unary clique while the last term considers the pairwise clique with matrix V representing pairwise state transition score. For simplicity in description on parameter updates, we denote the log-potential for clique c âˆˆ {U, P } by g c (h, y c ) = W c , F (h, y c ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint Training for RNCRF</head><p>Through the objective of maximum likelihood, updates for parameters of RNCRF are first conducted on the parameters of the CRF (unary weight matrices Î˜ U = {W 0 , W +t , W âˆ’t } and pairwise weight matrix V ) by applying chain rule to log-potential updates. Below is the gradient for Î˜ U (updates for V are similar through the log-potential of pairwise clique g P (y â€² k , y â€² k+1 )):</p><formula xml:id="formula_7">â–³Î˜ U = âˆ‚ âˆ’ log p(y|h) âˆ‚g U (h, y â€² k ) Â· âˆ‚g U (h, y â€² k ) âˆ‚Î˜ U ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">âˆ‚ âˆ’ log p(y|h) âˆ‚g U (h, y â€² k ) = âˆ’(1 y k =y â€² k âˆ’ p(y â€² k |h)),<label>(5)</label></formula><p>and y â€² k represents possible label configuration of node k. The hidden representations of each word and the parameters of DT-RNN are updated subsequently by applying chain rule with (5) through BPTS as follows,</p><formula xml:id="formula_9">â–³h root = âˆ‚ âˆ’ log p(y|h) âˆ‚g U (h, y â€² root ) Â· âˆ‚g U (h, y â€² root ) âˆ‚h root ,<label>(6)</label></formula><formula xml:id="formula_10">â–³h k =root = âˆ‚ âˆ’ log p(y|h) âˆ‚g U (h, y â€² k ) Â· âˆ‚g U (h, y â€² k ) âˆ‚h k +â–³h par(k) Â· âˆ‚h par(k) âˆ‚h k ,<label>(7)</label></formula><formula xml:id="formula_11">â–³Î˜ RNN = K k=1 âˆ‚ âˆ’ log p(y|h) âˆ‚h k Â· âˆ‚h k âˆ‚Î˜ RNN ,<label>(8)</label></formula><p>where h root represents the hidden vector of the word pointed by ROOT in the corresponding DT-RNN. Since this word is the topmost node in the tree, it only inherits error from the CRF output. In <ref type="formula" target="#formula_10">(7)</ref>, h par(k) denotes the hidden vector of the parent node of node k in DT-RNN. Hence the lower nodes receive error from both the CRF output and error propagation from parent node. The parameters within DT-RNN, Î˜ RNN , are updated by applying chain rule with respect to updates of hidden vectors, and aggregating among all associated nodes, as shown in <ref type="formula" target="#formula_11">(8)</ref>. The overall procedure of RNCRF is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The best performing system (Toh and Wang, 2014) for SemEval challenge 2014 task 4 (subtask 1) employed CRFs with extensive hand-crafted features including those induced from dependency trees. However, their experiments showed that the addition of the features induced from dependency relations does not improve the performance. This indicates the infeasibility or difficulty of incorporating dependency structure explicitly as input features, which motivates the design of our model to use DT-RNN to encode dependency between words for feature learning. The most important advantage of RN-CRF is the ability to learn the underlying dual propagation between aspect and opinion terms from the tree structure itself. Specifically as shown in <ref type="figure">Figure 1(c)</ref>, where the aspect is food and the opinion expression is like. In the dependency tree, food depends on like with the relation DOBJ. During training, RNCRF computes the hidden vector h like for like, which is also obtained from h food . As a result, the prediction for like is affected by h food . This is one-way propagation from food to like. During backpropagation, the error for like is propagated through a top-down manner to revise the representation h food . This is the other-way propagation from like to food. Therefore, the dependency structure together with the learning approach help to enforce the dual propagation of aspect-opinion pairs as long as the dependency relation exists, either directly or indirectly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Adding Linguistic/Lexicon Features</head><p>RNCRF is an end-to-end model, where feature engineering is not necessary. However, it is flexible to incorporate light hand-crafted features into RN-CRF to further boost its performance, such as features with POS tags, name-list, or sentiment lexicon. These features could be appended to the hidden vector of each word, but keep fixed during training, unlike learnable neural inputs and the CRF weights as described in Section 4.3. As will be shown in experiments, RNCRF without any hand-crafted features <ref type="table" target="#tab_0">Domain  Training  Test  Total  Restaurant  3,041  800  3,841  Laptop  3,045  800  3,845  Total  6,086</ref> 1,600 7,686 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dataset and Experimental Setup</head><p>We evaluate our model on the dataset from SemEval Challenge 2014 task 4 (subtask 1), which includes reviews from two domains: restaurant and laptop reviews 3 . The detailed description of the dataset is given in <ref type="table" target="#tab_0">Table 1</ref>. As the original dataset only includes manually annotate labels for aspect terms but not for opinion terms, we manually annotated opinion terms for each sentence by ourselves to facilitate our experiments. For word vector initialization, we train word embeddings with word2vec <ref type="bibr" target="#b24">(Mikolov et al., 2013)</ref> on the Yelp Challenge dataset 4 for the restaurant domain and on the Amazon reviews 5 <ref type="bibr" target="#b23">(McAuley et al., 2015)</ref> for the laptop domain. The Yelp dataset contains 2.2M restaurant reviews with 54K vocabulary size. For the Amazon reviews, we only extracted the electronic domain that contains 1M reviews with 590K vocabulary size.</p><p>We vary different dimensions for word embeddings and chose 300 for both domains. Empirical sensitivity studies on different dimensions of word embeddings are also conducted. Dependency trees are generated using Stanford Dependency Parser . Regarding CRFs, we implement a linear-chain CRF using CRFSuite <ref type="bibr" target="#b26">(Okazaki, 2007)</ref>. Because of the relatively small size of training data and a large number of parameters, we perform pretraining on the parameters of DT-RNN with cross-entropy error, which is a common strategy for deep learning <ref type="bibr" target="#b1">(Erhan et al., 2009)</ref>. We implement mini-batch stochastic gradient descent (SGD) with a batch size of 25, and an adaptive learning rate (AdaGrad) initialized at 0.02 for pretraining of DT-RNN, which runs 4 epochs for the restaurant domain and 5 epochs for the laptop domain. For parameter learning of the joint model RNCRF, we implement SGD with a decaying learning rate initialized at 0.02. We also try with varying context window size, and use 3 for the laptop domain and 5 for the restaurant domain, respectively. All parameters are chosen by cross validation.</p><p>As discussed in Section 5.1, hand-crafted features can be easily incorporated into RNCRF. We generate three types of simple features based on POS tags, name-list and sentiment lexicon to show further improvement by incorporating these features. Following (Toh and Wang, 2014), we extract two sets of name list from the training data for each domain, where one includes high-frequency aspect terms, and the other includes high-probability aspect words. These two sets are used to construct two lexicon features, i.e. we build a 2D binary vector: if a word is in a set, the corresponding value is 1, otherwise 0. For POS tags, we use Stanford POS tagger <ref type="bibr" target="#b34">(Toutanova et al., 2003)</ref>, and convert them to universal POS tags that have 15 different categories. We then generate 15 one-hot POS tag features. For sentiment lexicon, we use the collection of commonly used opinion words (around 6,800) <ref type="bibr" target="#b5">(Hu and Liu, 2004a)</ref>. Similar to name list, we create a binary feature to indicate whether the word belongs to opinion lexicon. We denote by RN-CRF+F the proposed model with the three types of features.</p><p>Compared to the winning systems of SemEval Challenge 2014 task 4 (subtask 1), RNCRF or RN-CRF+F uses additional labels of opinion terms for training. Therefore, to conduct fair comparison experiments with the winning systems, we implement RNCRF-O by omitting opinion labels to train our model (i.e., labels become "BA", "IA", "O"). Accordingly, we denote by RNCRF-O+F the RNCRF-O model with the three additional types of handcrafted features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Results</head><p>We compare our model with several baselines:</p><p>â€¢ CRF-1: a linear-chain CRF with standard linguistic features including word string, stylistics, POS tag, context string, and context POS tags.</p><p>â€¢ CRF-2: a linear-chain CRF with both standard linguistic features and dependency information including head word, dependency relations with parent token and child tokens.</p><p>â€¢ LSTM: an LSTM network built on top of word embeddings proposed by . We keep original settings in  but replace their word embeddings with ours (300 dimension). We try different hidden layer dimensions <ref type="bibr">(50,</ref><ref type="bibr">100,</ref><ref type="bibr">150,</ref><ref type="bibr">200)</ref> and reported the best result with size 50.</p><p>â€¢ LSTM+F: the above LSTM model with the three additional types of hand-crafted features as with RNCRF.</p><p>â€¢ SemEval-1, SemEval-2: the top two winning systems for SemEval challenge 2014 task 4 (subtask 1).</p><p>â€¢ WDEmb+B+CRF 6 : the model proposed by <ref type="bibr" target="#b38">(Yin et al., 2016)</ref> using word and dependency path embeddings combined with linear context embedding features, dependency context embedding features and hand-crafted features (i.e., feature engineering) as CRF input.</p><p>The comparison results are shown in <ref type="table" target="#tab_3">Table 2</ref> for both the restaurant domain and the laptop domain. Note that we provide the same annotated dataset (both aspect labels and opinion labels are included for training) for CRF-1, CRF-2 and LSTM for fair comparison. It is clear that our proposed model RNCRF achieves superior performance compared with most of the baseline models. The performance is even better by adding simple hand-crafted features, i.e., RN-CRF+F, with 0.92% and 3.87% absolute improvement over the best system in the challenge for aspect  show promising results in sequence tagging problems, they fail to achieve comparable performance when lacking of extensive features (e.g., CRF-1). By adding dependency information explicitly in CRF-2, the result only improves slightly for aspect extraction. Alternatively, by incorporating dependency information into a deep learning model (e.g., RN-CRF), the result shows more than 7% improvement for aspect extraction and 2% for opinion extraction. By removing the labels for opinion terms, RNCRF-O produces inferior results than RNCRF because the effect of dual propagation of aspect and opinion pairs disappears with the absence of opinion labels. This verifies our previous assumption that DT-RNN could learn the interactive effects within aspects and opinions. However, the performance of RNCRF-O is still comparable to the top systems and even better with the addition of simple linguistic features: 0.24% and 2.71% superior than the best system in the challenge for the restaurant domain and the laptop domain, respectively. This shows the robustness of our model even without additional opinion labels.</p><p>LSTM has shown comparable results for aspect extraction . However, in their work, they used well-pretrained word embeddings by training with large corpus or extensive external resources, e.g. chunking, and NER. To compare their model with RNCRF, we re-implement LSTM  To test the impact of each component of RNCRF and the three types of hand-crafted features, we conduct experiments on different model settings:</p><p>â€¢ DT-RNN+SoftMax: rather than using a CRF, a softmax classifier is used on top of DT-RNN.</p><p>â€¢ CRF+word2vec: a linear-chain CRF with word embeddings only without using DT-RNN.</p><p>â€¢ RNCRF+POS/NL/Lex: the RNCRF model with POS tag or name list or sentiment lexicon feature(s).</p><p>The comparison results are shown in <ref type="table" target="#tab_5">Table 3</ref>. Similarly, both aspect and opinion term labels are provided for training for each of the above models. Firstly, RNCRF achieves much better results compared to DT-RNN+SoftMax (+11.60% and +10.72% for the restaurant domain and the laptop domain in aspect extraction). This is because DT-RNN fails to fully exploit context information for sequence labeling, which, in contrast, can be  achieved by CRF. Secondly, RNCRF outperforms CRF+word2vec, which proves the importance of DT-RNN for modeling interactions between aspects and opinions. Hence, the combination of DT-RNN and CRF inherits the advantages from both models. Moreover, by separately adding hand-crafted features, we can observe that name-list based features and the sentiment lexicon feature are most effective for aspect extraction and opinion extraction, respectively. This may be explained by the fact that name-list based features usually contain informative evident for aspect terms and sentiment lexicon provides explicit indication about opinions.</p><p>Besides the comparison experiments, we also conduct sensitivity test for our proposed model in terms of word vector dimensions. We tested a set of different dimensions ranging from 25 to 400, with 25 increment. The sensitivity plot is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The performance for aspect extraction is smooth with different vector lengths for both domains. For restaurant domain, the result is stable when dimension is larger than or equal to 100, with the highest at 325. For the laptop domain, the best result is at dimension 300, but with relatively small variations. For opinion extraction, the performance reaches a good level when the dimension is larger than or equal to 75 for the restaurant domain and 125 for the laptop domain. This proves the stability and robustness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a joint model, RNCRF, that achieves the state-of-the-art performance for explicit aspect and opinion term extraction on a benchmark dataset. With the help of DT-RNN, high-level features can be learned by encoding the underlying dual propagation of aspect-opinion pairs. RNCRF combines the advantages of DT-RNNs and CRFs, and thus outperforms the traditional rule-based methods in terms of flexibility, because aspect terms and opinion terms are not only restricted to certain observed relations and POS tags. Compared to feature engineering methods with CRFs, the proposed model saves much effort in composing features, and it is able to extract higher-level features obtained from non-linear transformations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An example for computing input-ouput potential for the second position like.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Sensitivity studies on word embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1</head><label>1</label><figDesc>Recursive Neural CRFsInput: A set of customer review sequences: S = {s1, ..., sN }, and feature vectors of d dimensions for each word {xw}'s, window size T for CRFs Output: Parameters: Î˜ = Î˜ RNN , Î˜U , V Initialization: Initialize We using word2vec. Initialize Wv and {Wr}'s randomly with uniform distribution between</figDesc><table><row><cell>âˆ’</cell><cell>âˆš 2d+1 , 6 âˆš</cell><cell>âˆš</cell><cell>âˆš 2d+1 . Initialize W0, {W+t}'s, {Wâˆ’t}'s, V , 6</cell></row><row><cell cols="4">and b with all 0's</cell></row><row><cell cols="4">for each sentence si do</cell></row><row><cell cols="4">1: Use DT-RNN (1) to generate hi</cell></row><row><cell cols="4">2: Compute p(yi|hi) using (2)</cell></row><row><cell cols="4">3: Use the backpropagation algorithm to update parame-</cell></row><row><cell cols="4">ters Î˜ through (4)-(8)</cell></row><row><cell cols="2">end for</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: SemEval Challenge 2014 task 4 dataset</cell></row><row><cell>slightly outperforms the best performing systems</cell></row><row><cell>that involve heavy feature engineering efforts, and</cell></row><row><cell>RNCRF with light feature engineering can achieve</cell></row><row><cell>better performance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison results in terms of F1 scores.</figDesc><table><row><cell>extraction for the restaurant domain and the laptop</cell></row><row><cell>domain, respectively. This shows the advantage of</cell></row><row><cell>combining high-level continuous features and dis-</cell></row><row><cell>crete hand-crafted features. Though CRFs usually</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Impact of different components.</figDesc><table><row><cell>with the same word embedding strategy and label-</cell></row><row><cell>ing resources as ours. The results show that our</cell></row><row><cell>model outperforms LSTM in aspect extraction by</cell></row><row><cell>2.90% and 4.10% for the restaurant domain and</cell></row><row><cell>the laptop domain, respectively. We conclude that</cell></row><row><cell>a standard LSTM model fails to extract the rela-</cell></row><row><cell>tions between aspect and opinion terms. Even with</cell></row><row><cell>the addition of same linguistic features, LSTM is</cell></row><row><cell>still inferior than RNCRF itself in terms of as-</cell></row><row><cell>pect extraction. Moreover, our result is compara-</cell></row><row><cell>ble with WDEmb+B+CRF in the restaurant domain</cell></row><row><cell>and better in the laptop domain (+3.26%). Note that</cell></row><row><cell>WDEmb+B+CRF appended dependency context in-</cell></row><row><cell>formation into CRF while our model encode such</cell></row><row><cell>information into high-level representation learning.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that in this paper, RNN stands for recursive neural network instead of recurrent neural network.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In this work we focus on extraction of aspect and opinion terms, not polarity predictions on opinion terms. Polarity prediction can be done by either post-processing on the extracted opinion terms or redefining the BIO labels by encoding the polarity information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Experiments with more publicly available datasets, e.g. restaurant review dataset from SemEval Challenge 2015 task 12 will be conducted in our future work. 4 http://www.yelp.com/dataset challenge 5 http://jmcauley.ucsd.edu/data/amazon/links.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We report the best results from the original paper<ref type="bibr" target="#b38">(Yin et al., 2016)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is partially funded by the Economic Development Board and the National Research Foundation of Singapore. Sinno J. Pan thanks the support from Fuji Xerox Corporation through joint research on Multilingual Semantic Analysis and the NTU Singapore Nanyang Assistant Professorship (NAP) grant M4081532.020.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The difficulty of training deep architectures and the effect of unsupervised pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
	<note>Samy Bengio, and Pascal Vincent</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation for largescale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Glorot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>KÃ¼chler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICNN</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and JÃ¼rgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu2004a] Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining opinion features in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu2004b] Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="755" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Ozanirsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
	<note>Ä°rsoy and Cardie2014</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><forename type="middle">Max</forename><surname>Batista Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>DaumÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
	<note>Iyyer et al.2014</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extracting opinion targets in a single-and cross-domain setting with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1035" to="1045" />
		</imprint>
	</monogr>
	<note>Jakob and Gurevych2010</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A novel lexicalized hmm-based learning framework for web opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho2009] Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">Hay</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<editor>Kalchbrenner et al.2014] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2003] Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Representation Learning</title>
		<imprint>
			<publisher>Richard Socher, and Christopher D. Manning</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><surname>Mikolov2014</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structure-aware review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="653" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Opinion target extraction using word-based translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="1346" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Opinion target extraction using partially-supervised word alignment model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2134" to="2140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A logic programming approach to aspect extraction in opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="276" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fine-grained opinion mining with recurrent neural networks and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic construction of a context-aware sentiment lexicon: An optimization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data</title>
		<editor>Lu et al.2011] Yue Lu, Malu Castellanos, Umeshwar Dayal, and ChengXiang Zhai</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
	<note>WWW</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Opinion target extraction in chinese news comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan2010] Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="782" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning attitudes and attributes from multi-aspect reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1020" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mcauley</surname></persName>
		</author>
		<title level="m">Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-based recommendations on styles and substitutes. In SIGIR</title>
		<imprint>
			<publisher>Julian McAuley</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised commonsense knowledge enrichment for domain-specific sentiment analysis</title>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<editor>Nir Ofek, Soujanya Poria, Lior Rokach, Erik Cambria, Amir Hussain, and Asaf Shabtai</editor>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="467" to="477" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<ptr target="http://www.chokkan.org/software/crfsuite/" />
		<title level="m">Crfsuite: a fast implementation of conditional random fields (CRFs)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee2008] Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extracting product features and opinions from reviews</title>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<editor>Ana-Maria Popescu and Oren Etzioni</editor>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<editor>Socher et al.2011b] Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<editor>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Target-dependent sentiment classification with long short term memory. CoRR, abs/1512.01100</title>
	</analytic>
	<monogr>
		<title level="m">Toh and Wang2014] Zhiqiang Toh and Wenting Wang</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>NAACL</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A sentiment-aligned topic model for product aspect rating prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ester2014] Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Latent aspect rating analysis without aspect keyword supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<editor>Weiss et al.2015] David Weiss, Chris Alberti, Michael Collins, and Slav Petrov</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Phrase dependency parsing for opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1533" to="1541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised word and dependency path embeddings for aspect term extraction</title>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2979" to="2985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Extracting and ranking product features in opinion documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1462" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Movie review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
