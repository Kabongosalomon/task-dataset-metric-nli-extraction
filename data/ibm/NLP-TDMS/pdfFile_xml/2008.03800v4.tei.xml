<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatiotemporal Contrastive Video Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spatiotemporal Contrastive Video Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a self-supervised Contrastive Video Representation Learning (CVRL) method to learn spatiotemporal visual representations from unlabeled videos. Our representations are learned using a contrastive loss, where two augmented clips from the same short video are pulled together in the embedding space, while clips from different videos are pushed away. We study what makes for good data augmentations for video self-supervised learning and find that both spatial and temporal information are crucial. We carefully design data augmentations involving spatial and temporal cues. Concretely, we propose a temporally consistent spatial augmentation method to impose strong spatial augmentations on each frame of the video while maintaining the temporal consistency across frames. We also propose a sampling-based temporal augmentation method to avoid overly enforcing invariance on clips that are distant in time. On Kinetics-600, a linear classifier trained on the representations learned by CVRL achieves 70.4% top-1 accuracy with a 3D-ResNet-50 (R3D-50) backbone, outperforming ImageNet supervised pre-training by 15.7% and SimCLR unsupervised pre-training by 18.8% using the same inflated R3D-50. The performance of CVRL can be further improved to 72.9% with a larger R3D-152 (2× filters) backbone, significantly closing the gap between unsupervised and supervised video representation learning. Our code and models will be available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Representation learning is of crucial importance in computer vision tasks, and a number of highly promising recent developments in this area have carried over successfully from the static image domain to the video domain. Classic hand-crafted local invariant features (e.g., SIFT <ref type="bibr" target="#b43">[44]</ref>) * The first two authors contributed equally. This work was performed while Rui Qian worked at Google. <ref type="bibr" target="#b24">25</ref> 50  <ref type="bibr" target="#b32">[33]</ref> and SimCLR unsupervised <ref type="bibr" target="#b9">[10]</ref> pre-training using the same 3D inflated ResNets, closing the gap between unsupervised and supervised video representation learning.</p><p>for images have their counterparts (e.g., 3D SIFT <ref type="bibr" target="#b54">[55]</ref>) in videos, where the temporal dimension of videos gives rise to key differences between them. Similarly, state-of-the-art neural networks for video understanding <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16]</ref> often extend 2D convolutional neural networks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36]</ref> for images along the temporal dimension. More recently, unsupervised or self-supervised learning of representations from unlabeled visual data <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7]</ref> has gained momentum in the literature partially thanks to its ability to model the abundantly available unlabeled data. However, self-supervised learning gravitates to different dimensions in videos and images, respectively. It is natural to engineer self-supervised learning signals along the temporal dimension in videos. Examples abound, including models for predicting the future <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28]</ref>, changing temporal sampling rates <ref type="bibr" target="#b72">[73]</ref>, sorting video frames or clips <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b71">72]</ref> and combining a few tasks <ref type="bibr" target="#b4">[5]</ref>. Meanwhile, in the domain of static images, some recent work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7]</ref> that exploits spatial self-supervision has reported unprecedented performance on image representation learning.</p><p>The long-standing pursuit after temporal cues for self-supervised video representation learning has left selfsupervision signals in the spatial subspace under-exploited for videos. To promote the spatial self-supervision signals in videos, we build a Contrastive Video Representation Learning (CVRL) framework to learn spatiotemporal representations from unlabeled videos. <ref type="figure" target="#fig_9">Figure 2</ref> illustrates our framework, which contrasts the similarity between two positive video clips against those of negative pairs using the InfoNCE contrastive loss <ref type="bibr" target="#b47">[48]</ref>. Since there is no label in self-supervised learning, we construct positive pairs as two augmented video clips sampled from the same input video. We carefully design data augmentations to involve both spatial and temporal cues for CVRL. Simply applying spatial augmentation independently to video frames actually hurts the learning because it breaks the natural motion along the time dimension. Instead, we propose a temporally consistent spatial augmentation method by fixing the randomness across frames. It is simple and yet vital as demonstrated in our experiments. For temporal augmentation, we take visual content into account by a sampling strategy tailored for the CVRL framework. On the one hand, a pair of positive clips that are temporally distant may contain very different visual content, leading to a low similarity that could be indistinguishable from those of the negative pairs. On the other hand, completely discarding the clips that are far in time reduces the temporal augmentation effect. To this end, we propose a sampling strategy to ensure the time difference between two positive clips follows a monotonically decreasing distribution. Effectively, CVRL mainly learns from positive pairs of temporally close clips and secondarily sees some temporally distant clips during training. The efficacy of the proposed spatial and temporal augmentation methods is verified by extensive ablation studies.</p><p>We primarily evaluate the learned video representations on both Kinetics-400 <ref type="bibr" target="#b37">[38]</ref> and Kinetics-600 <ref type="bibr" target="#b7">[8]</ref> by training a linear classifier following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref> on top of frozen backbones. We also study semi-supervised learning, downstream action classification and detection to further assess CVRL. We next summarize our main findings.</p><p>Mixing spatial and temporal cues boosts the performance. Relying on spatial or temporal augmentation only yields relatively low performance, as shown in <ref type="table">Table 9</ref>. In contrast, we achieve an improvement of 22.9% top-1 accuracy by combining both augmentations in the manner we proposed above, i.e., temporally consistent spatial augmentation and the temporal sampling strategy.</p><p>Our representations outperform prior arts. The linear evaluation of CVRL achieves more than 15% gain over competing baselines, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="table" target="#tab_2">Table 2</ref>. On Kinetics-400, CVRL achieves 12.6% improvement over ImageNet pre-training, which were shown competitive in previous work <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b23">24]</ref>. For semi-supervised learning ( <ref type="table" target="#tab_3">Table 3</ref>), CVRL surpasses all other baselines especially when there is only 1% labeled data, indicating the advantage of our self-learned feature is more profound with limited labels. For downtream action classification on UCF-101 <ref type="bibr" target="#b56">[57]</ref> and HMDB-51 <ref type="bibr" target="#b40">[41]</ref>, CVRL has obvious advantages over other methods based on the vision modality and is competitive with state-of-the-art multimodal methods <ref type="table">(Table 4</ref>).</p><p>Our CVRL framework benefits from larger datasets and networks. We study the effect of more training data in CVRL. We design an evaluation protocol by first pre-training models on different amounts of data with same iterations, and then comparing the performance on the same validation set. As shown in <ref type="figure" target="#fig_8">Figure 4</ref>, a clear improvement is observed by using 50% more data, demonstrating the potential of CVRL to scale to larger unlabeled datasets. We also conduct experiments with wider &amp; deeper networks and observe consistent improvements <ref type="table" target="#tab_2">(Table 2)</ref>, demonstrating that CVRL is more effective with larger networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-supervised video representation learning. It is natural to exploit the temporal dimension in self-supervised video representation learning. Some early work predicts the future on top of frame-wise representations <ref type="bibr" target="#b57">[58]</ref>. More recent work learns from raw videos by predicting motion and appearance statistics <ref type="bibr" target="#b65">[66]</ref>, speed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b66">67]</ref> and encodings <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Aside from predicting the future, other common approaches include sorting frames or video clips <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b17">18]</ref> along the temporal dimension and learning from proxy tasks like rotation <ref type="bibr" target="#b36">[37]</ref>. Yang et al. <ref type="bibr" target="#b72">[73]</ref> learn by maintaining consistent representations of different sampling rates. Furthermore, videos can often supply multimodal signals for cross-modality self-supervision, such as geometric cues <ref type="bibr" target="#b18">[19]</ref>, speech or language <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b44">45]</ref>, audio <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b3">4]</ref>, optical flow <ref type="bibr" target="#b29">[30]</ref> or combinations of multiple modalities <ref type="bibr" target="#b1">[2]</ref> and tasks <ref type="bibr" target="#b51">[52]</ref>.</p><p>Self-supervised image representation learning. Some early work learns visual representations from unlabeled images via manually specified pretext tasks, for instance, the auto-encoding methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b77">78]</ref> that leverage contexts, channels, or colors. Other pretext tasks include but are not limited to relative patch location <ref type="bibr" target="#b13">[14]</ref>, jigsaw puzzles <ref type="bibr" target="#b46">[47]</ref>, and image rotations <ref type="bibr" target="#b21">[22]</ref>. Interestingly, most of the pretext tasks can be integrated into a contrastive learning framework <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b60">61]</ref>, which maintains relative consistency between the representations of an image and its augmented view. The augmentation could encompass various pretext tasks. Tian et al. <ref type="bibr" target="#b61">[62]</ref> study what makes a good view in this framework. Clustering can also provide an effective addition to the framework <ref type="bibr" target="#b6">[7]</ref>. It is worth noting that the recent wave of contrastive learning shares a similar loss objective as instance discrimination <ref type="bibr" target="#b69">[70]</ref>.  <ref type="figure" target="#fig_9">Figure 2</ref>. Overview of the proposed spatiotemporal Contrastive Video Representation Learning (CVRL) framework. From a raw video, we first sample a temporal interval from a monotonically decreasing distribution. The temporal interval represents the number of frames between the start points of two clips, and we sample two clips from a video according to this interval. Afterwards we apply a temporally consistent spatial augmentation to each of the clips and feed them into a 3D backbone with an MLP head. The contrastive loss is used to train the network to attract the clips from the same video and repel the clips from different videos in the embedding space.</p><p>Videos as supervision for images and beyond. Video can help supervise the learning of image representations <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b52">53]</ref>, correspondences <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b14">15]</ref>, and robotic behaviors <ref type="bibr" target="#b55">[56]</ref> thanks to its rich content about different views of objects and its motion and tracking cues. On the other hand, Girdhar et al. <ref type="bibr" target="#b22">[23]</ref> propose to learn video representations by distillation from image representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video Representation Learning Framework</head><p>We build our self-supervised contrastive video representation learning framework as illustrated in <ref type="figure" target="#fig_9">Figure 2</ref>. The core of this framework is an InfoNCE contrastive loss <ref type="bibr" target="#b47">[48]</ref> applied on features extracted from augmented videos. Suppose we sample N raw videos and augment them, resulting in 2N clips (the augmentation module is described in Section 3.3). Denote z i , z i as the encoded representations of the two augmented clips of the i-th input video. The In-foNCE contrastive loss is defined as</p><formula xml:id="formula_0">L = 1 N N i=1 L i and L i = − log exp(sim(z i , z i )/τ ) 2N k=1 1 [k =i] exp(sim(z i , z k )/τ ) ,<label>(1)</label></formula><p>where sim(u, v) = u v/ u 2 v 2 is the inner product between two 2 normalized vectors, 1 <ref type="bibr">[·]</ref> is an indicator excluding from the denominator the self-similarity of the encoded video z i , and τ &gt; 0 is a temperature parameter. The loss allows the positive pair (z i , z i ) to attract mutually while they repel the other items in the mini-batch. We discuss other components of the framework as follows: (1) an encoder network maps an input video clip to its representation z, (2) spatiotemporal augmentations to construct positive pairs (z i , z i ) and the properties they induce, and (3) methods to evaluate the learned representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Video Encoder</head><p>We encode a video sequence using 3D-ResNets <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b30">31]</ref> as backbones. We expand the original 2D convolution kernels to 3D to capture spatiotemporal information in videos. The design of our 3D-ResNets mainly follows the "slow" pathway of the SlowFast network <ref type="bibr" target="#b16">[17]</ref> with two minor modifications: (1) the temporal stride of 2 in the data layer, and (2) the temporal kernel size of 5 and stride of 2 in the first convolution layer. We also take as input a higher temporal resolution. <ref type="table" target="#tab_1">Table 1</ref> and Section 4.1 provide more details of the network. The video representation is a 2048dimensional feature vector. As suggested by SimCLR <ref type="bibr" target="#b9">[10]</ref>, we add a multi-layer projection head onto the backbone to obtain the encoded 128-dimensional feature vector z used in Equation 1. During evaluation, we discard the MLP and use the 2048-dimensional representation directly from the backbone to make the video encoder compatible with other supervised learning methods. We also experiment with 2× and 4× backbones, which multiply the number of filters in the network, including the backbone's output feature dimension and all layers in MLP, by 2× and 4× accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data Augmentation</head><p>The flexibility of CVRL allows us to study a variety of desired properties, which are incorporated in the form of data augmentations. We focus on the augmentations in both temporal and spatial dimensions.</p><formula xml:id="formula_1">Stage Network Output size T × S 2 raw clip - 32 × 224 2 data stride 2, 1 2 16 × 224 2 conv 1 5 × 7 2 , 64 8 × 112 2 stride 2, 2 2 pool 1 1 × 3 2 max 8 × 56 2 stride 1, 2 2 conv 2   1×1 2 , 64 1×3 2 , 64 1×1 2 , 256   ×3 8 × 56 2 conv 3   1×1 2 , 128 1×3 2 , 128 1×1 2 , 512   ×4 8 × 28 2 conv 4   3×1 2 , 256 1×3 2 , 256 1×1 2 , 1024   ×6 8 × 14 2 conv 5   3×1 2 , 512 1×3 2 , 512 1×1 2 , 2048   ×3 8 × 7 2</formula><p>global average pooling 1 × 1 2 <ref type="table" target="#tab_1">Table 1</ref>. Our video encoder: a 3D-ResNet-50 (R3D-50). The input video has 16 frames (stride 2) in self-supervised pre-training and 32 frames (stride 2) in linear evaluation, semi-supervised learning, supervised learning and downstream tasks.</p><p>Temporal Augmentation: a sampling perspective. It is straightforward to take two clips from an input video as a positive pair, but how to sample the two clips matters. Previous work provides temporal augmentation techniques like sorting video frames or clips <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b71">72]</ref>, altering playback rates <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b66">67]</ref>, etc. However, directly incorporating them into CVRL would result in learning temporally invariant features, which opposes the temporally evolving nature of videos. We instead account for the temporal changes using a sampling strategy. The main motivation is that two clips from the same video would be more distinct when their temporal interval is larger. If we sample temporally distant clips with smaller probabilities, the contrastive loss (Equation 1) would focus more on the temporally close clips, pulling their features closer and imposing less penalty over the clips that are far away in time. Given an input video of length T , our sampling strategy takes two steps. We first draw a time interval t from a distribution P (t) over [0, T ]. We then uniformly sample a clip from [0, T −t], followed by the second clip which is delayed by t after the first. More details on the sampling procedure can be found in Appendix A. We experiment with monotonically increasing, decreasing, and uniform distributions, as illustrated in <ref type="figure">Figure 3</ref>. We find that decreasing distributions (a-c) generally perform better than the uniform (d) or increasing ones (e-f), aligning well with our motivation above of assigning lower sampling probability on larger temporal intervals.       <ref type="figure">Figure 3</ref>. Performance of different sampling distributions. The x-axis is the temporal interval t between two clips in a video, and the y-axis is the sampling probability P (t). We report linear evaluation accuracy upon 200 epochs of pre-training on Kinetics-400.</p><p>Spatial Augmentation: a temporally consistent design. Spatial augmentation is widely used in both supervised learning and unsupervised learning for images. Although the question of how to apply strong spatial augmentations to videos remains open, a natural strategy is to utilize existing image-based spatial augmentation methods to the video frames one by one. However, this method could break the motion cues across frames. Spatial augmentation methods often contain some randomness such as random cropping, color jittering and blurring as important ways to strengthen their effectiveness. In videos, however, such randomness between consecutive frames, could negatively affect the representation learning along the temporal dimension. Therefore, we design a simple yet effective approach to address this issue, by making the spatial augmentations consistent along the temporal dimension. With fixed randomness across frames, the 3D video encoder is able to better utilize spatiotemporal cues. This approach is validated by experimental results in <ref type="table">Table 9</ref>. Algorithm 1 demonstrates the detailed procedure of our temporally consistent spatial augmentations, where the hyper-parameters are only generated once for each video and applied to all frames. An illustration can be found in Appendix C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation</head><p>As a common practice in self-supervised representation learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>, we mainly evaluate the learned video representations by fixing the weights in the pre-trained video encoder and training a linear classifier on top of it. We also  </p><formula xml:id="formula_2">k ∈ {1, . . . , M } do f k = Resize Crop(f k , size = S, aspect = A) f k = Flip(f k ) if F f = 1 f k = Color jitter(f k ) if Fj = 1 f k = Greyscale(f k ) if Fg = 1 f k = Gaussian blur(f k ) end for Output: Augmented video clip V = {f 1 , f 2 , · · · , f M }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We mainly conduct experiments on the Kinetics-400 (K400) <ref type="bibr" target="#b37">[38]</ref> and Kinetics-600 <ref type="bibr" target="#b7">[8]</ref> (K600) datasets. K400 consists of about 240k training videos and 20k validation videos belonging to 400 action classes. K600 is a superset of K400 by revising ambiguous classes and adding 200 more classes, containing about 360k training and 28k validation videos from 600 classes. We note that K400 has been extensively used in the literature and hope our additional results on K600 would further demonstrate the effectiveness of CVRL and offer a reference to the field. The videos in Kinetics have a duration of around 10 seconds, with 25 frames per second (i.e., around 250 frames per video). We adopt the standard protocol <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref> of self-supervised pretraining and linear evaluation as the primary metric for evaluating the learned representations. We also evaluate the learned representations via semi-supervised learning and downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We use SGD as our optimizer with the momentum of 0.9. All models are trained with the mini-batch size of 1024 except for downstream tasks. We linearly warm-up the learning rate in the first 5 epochs <ref type="bibr" target="#b24">[25]</ref> followed by the scheduling strategy of half-period cosine learning rate decay <ref type="bibr" target="#b33">[34]</ref>. We apply the proposed temporal and spatial augmentations for the self-supervised pre-training. For other tasks, we only use standard data augmentations of cropping, resizing, and flipping. During testing, we densely sample 10 clips from each video and apply a 3-crop evaluation following <ref type="bibr" target="#b16">[17]</ref>.</p><p>Self-supervised pre-training. We sample two 16-frame clips with the temporal stride of 2 from each video for the self-supervised pre-training of video representations. The duration of a clip is 1.28 seconds out of around 10 seconds of a video. We use synchronized batch normalization to avoid information leakage or overfitting <ref type="bibr" target="#b9">[10]</ref>. The temperature τ is set to 0.1 in the InfoNCE loss for all experiments. The initial learning rate is set to 0.32. Linear evaluation. We evaluate video representations using a linear classifier by fixing all the weights in the backbone. During training, we sample a 32-frame clip with the temporal stride of 2 from each video to train the linear classifier for 100 epochs with an initial learning rate of 32. We 2 normalize the feature before feeding it to the classifier. Semi-supervised learning. We conduct semi-supervised learning, namely, by fine-tuning the pre-trained network on small subsets of Kinetics. We sample 1% and 10% videos from each class in the training set, forming two balanced subsets, respectively. The evaluation set remains the same. We use pre-trained backbones to initialize network parameters and fine-tune all layers using an initial learning rate of 0.2 without warm-up. We train the model for 100 epochs on the 1% subset and 50 epochs on the 10% subset. Downstream action classification. On UCF-101 <ref type="bibr" target="#b56">[57]</ref> and HMDB-51 <ref type="bibr" target="#b40">[41]</ref>, we use the pre-trained backbone on Kinetics to initialize the network parameters. We report results for both fine-tuning (i.e., fine-tune all layers) and linear evaluation (i.e., train a linear classifier by fixing all backcbone weights). All the models are trained with a minibatch size of 128 for 50 epochs. We use an initial learning rate of 0.16 for fine-tuning on both datasets, 0.8 for linear evaluation on UCF-101 and 0.2 for HMDB-51. Downstream action detection. For action detection, we work on AVA <ref type="bibr" target="#b26">[27]</ref> containing 211k training and 57k validation videos. AVA provides spatiotemporal labels of each action in long videos of 15 to 30 minutes. Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b72">73]</ref>, we adopt a Faster-RCNN <ref type="bibr" target="#b53">[54]</ref> baseline with modifications to enable it to process videos. We use pre-trained backbones on Kinetics-400 to initialize the detector, and train with the standard 1× schedule (12 epochs, decay learning rate by 10× at 8-th and 11-th epoch). We use an initial learning rate of 0.2 with 32 videos per batch. Supervised learning. To understand where CVRL stands, we also report supervised learning results. The setting for supervised learning is the same as linear evaluation except that we train the entire encoder network from scratch for 200 epochs without feature normalization. We use an initial learning rate of 0.8 and a dropout rate of 0.5 following <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>Comparison baselines. We compare our CVRL method with two baselines: (1) ImageNet inflated <ref type="bibr" target="#b32">[33]</ref>: inflating the 2D ResNets pre-trained on ImageNet to our 3D ResNets by duplicating it along the temporal dimension, and (2) SimCLR inflated <ref type="bibr" target="#b9">[10]</ref>: inflating the 2D ResNets pre-trained with SimCLR on the frame images of Kinetics 1 . SimCLR inflated serves as an important frame-based baseline for our method by directly applying the state-ofthe-art image self-supervised learning algorithm to Kinetics frames, where no temporal information is learned. In addition, we present the results of supervised learning as an upper bound of our method.</p><p>Notations. We aim at providing an extensive comparison with prior work, but video self-supervised learning methods could be diverse in pre-training datasets and input modalities. For pre-training datasets, we use K400 in short for Kinetics-400 <ref type="bibr" target="#b37">[38]</ref>, K600 for Kinetics-600 <ref type="bibr" target="#b7">[8]</ref>, HT for HowTo100M <ref type="bibr" target="#b45">[46]</ref>, AS for AudioSet <ref type="bibr" target="#b19">[20]</ref>, IG65M for In-stagram65M <ref type="bibr" target="#b20">[21]</ref>, and YT8M for YouTube8M <ref type="bibr" target="#b0">[1]</ref>. We also calculate the total length of the videos in one dataset to indicate its scale, namely duration in the table, by using years or days. Following <ref type="bibr" target="#b1">[2]</ref>, we divide modalities into four types: Vision, Flow, Audio and Text.</p><p>Linear evaluation. Linear evaluation is the most straightforward way to quantify the quality of the learned representation. As shown in <ref type="table" target="#tab_2">Table 2</ref>, while some previous stateof-the-art methods <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b23">24]</ref> are worse than ImageNet inflated, our CVRL outperforms the ImageNet inflated by 12.6% in top-1 accuracy on K400. Compared with the frame-based SimCLR inflated encoder, CVRL has 19.7% improvement, demonstrating the advantage of the learned <ref type="bibr" target="#b0">1</ref> We find a SimCLR model pre-trained on Kinetics frames slightly outperforms the same model pre-trained on ImageNet released by <ref type="bibr" target="#b9">[10]</ref>. This is probably due to the domain difference between ImageNet and Kinetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Semi-supervised learning. For semi-supervised learning on K400, as presented in <ref type="table" target="#tab_3">Table 3</ref>, CVRL surpasses all other baselines across different architectures and label fractions, especially when there is only 1% labeled data for finetuning, indicating that the advantage of our self-supervised CVRL is more profound when the labeled data is limited. Results on K600 can be found in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downstream action classification.</head><p>Pre-training the network encoder on a large dataset and fine-tuning all layers or conducting linear evaluation on UCF-101 <ref type="bibr" target="#b56">[57]</ref> and HMDB-51 <ref type="bibr" target="#b40">[41]</ref> is the most common evaluation protocol in the video self-supervised learning literature. We organize previous methods mainly by <ref type="bibr" target="#b0">(1)</ref> what input modality is used and <ref type="formula" target="#formula_3">(2)</ref> which dataset is pre-trained on. We provide a comprehensive comparison in <ref type="table">Table 4</ref>. We first divide all entries by the input modality they used. Inside each modality, we arrange the entries w.r.t. the performance on UCF-101 by ascending order. We notice there is inconsistency in previous work on reporting results with different splits of UCF-101 and HMDB-51, so we report on both split-1 and 3 splits average. For fine-tuning, CVRL significantly outperforms methods using Vision modality only. Compared with multimodal methods using Vision and Flow <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, Vision and Text <ref type="bibr" target="#b44">[45]</ref>, CVRL still ranks top. Multimodal methods using Vision and Audio are able to achieve better performance starting from GDT <ref type="bibr" target="#b50">[51]</ref> on AS, while it is worth to point out that the pre-trained dataset is 9× larger than K400 which we pre-train CVRL on with single Vision modality.</p><p>For CVRL pre-trained on K600, it is only worse than the best model of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b1">2]</ref> on UCF-101 and outperforms the best model of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b51">52]</ref> on HMDB-51, where their pretraining datasets are 108× to 174× larger than K600. For linear evaluation, CVRL is better than all single and multi modal methods with the only exception of MMV [2] on UCF-101. On HMDB-51, CVRL demonstrates very competitive performance, outperforming all methods using vision modality and multimodal methods of <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>In conclusion, CVRL shows competitive performance on downstream action classification, compared with single and mutilmodal video self-supervised learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downstream action detection.</head><p>We conduct experiments on AVA <ref type="bibr" target="#b26">[27]</ref> dataset which benchmarks methods for detecting when an action happens in the temporal domain and where it happens in the spatial domain. Each video in AVA is annotated for 15 to 30 minutes and we consider this as an important experiment to demonstrate the transferability of CVRL learned features. We adopt Faster-RCNN <ref type="bibr" target="#b53">[54]</ref> and replace the 2D ResNet backbone with our video encoder in <ref type="table" target="#tab_1">Table 1</ref>. Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b72">73]</ref>, we compute region-of-interest (RoI) features by using a 3D RoIAlign on the features from the last conv block. We then perform a temporal average pooling followed by a spatial max pooling, and feed the feature into a sigmoid-based classifier for mutli-label prediction. We use pre-trained weights to initialize the video encoder, and fine-tune all layers for 12 epochs. We report mean Average-Precision (mAP) in <ref type="table">Table 5</ref>, where CVRL shows better performance than baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We conduct extensive ablation studies for CVRL based on 200 epochs pre-training on Kinetics-400 and report top-1 linear evaluation accuracy on Kinetics-400.   <ref type="table">Table 9</ref>. Ablation study on data augmentation. Temporal interval sampling distribution. As shown in <ref type="figure">Figure 3</ref>, we experiment with monotonically decreasing distributions (a-c), uniform distribution (d) and monotonically increasing distributions (e-f). We find decreasing distributions are better. We also compare different power functions for decreasing distribution and choose a simple exponent of 1 (i.e., linear) due to its simplicity and best performance.</p><p>Spatial and temporal augmentation. We conduct an ablation study on the proposed temporally consistent spatial augmentation. From results in <ref type="table">Table 9</ref>, we have three major observations. First, both temporal and spatial augmentations are indispensable. Specifically, using both temporal and spatial augmentations yields 52.3% top-1 accuracy, significantly outperforming the same model pre-trained with temporal augmentation only (33.0%) or spatial augmentation only (40.9%). Second, the proposed temporally consistent module plays a critical role in achieving good performance. Adding temporal consistency further improves the top-1 accuracy to 63.8% by a large margin of 11.5% over 52.3%. Third, spatial augmentations, which are ignored to some degree in existing self-supervised video representation learning literature, not only matter, but also contribute more than the temporal augmentations.</p><p>More training data. We study whether using more data would improve the performance of CVRL. We design an evaluation protocol by first pre-training models on different amount of data (K600 and K400) with same iterations to remove the advantage brought by longer training, and then comparing the performance on same validation set (K400 val). We verify that the training data of K600 has no overlapping video ids with the validation set of K400. We present results of 46k (200 K400 epochs), 184k (800 K400 epochs) and 284k (800 K600 epochs) pre-training iterations in <ref type="figure" target="#fig_8">Figure 4</ref>. We find more training data in K600 is beneficial, demonstrating the potential of CVRL's scalability on larger unlabeled datasets.  Layers of projection head. We experiment with different number of hidden layers. Unlike <ref type="bibr" target="#b10">[11]</ref>, we only use different layers in pre-training and perform the linear evaluation on top of the same backbone by removing the entire projection head. In <ref type="table">Table 6</ref>, we can see using 3 hidden layers yields the best performance and we choose this as our default setting.</p><p>Batch size. The batch size determines how many negative pairs we use for each positive pair during training. Our experimental results show that a batch size of 1024 already achieves high performance. Larger batch sizes could negatively impact the performance as shown in <ref type="table">Table 7</ref>.</p><p>Pre-training epoch. As presented in <ref type="table">Table 8</ref>, we experiment with pre-training epochs varying from 100 to 800 and find consistent improvement with longer pre-training epochs. We choose 800 epochs as our default setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work presents a Contrastive Video Representation Learning (CVRL) framework leveraging spatial and temporal cues to learn spatiotemporal representations from unlabeled videos. Extensive studies on linear evaluation, semisupervised learning and various downstream tasks demonstrate promising results of CVRL. In the future, we plan to apply CVRL to a large set of unlabeled videos and incorporate additional modalities into our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details on Temporal Interval Sampling</head><p>Here we describe how to sample the temporal interval t ∈ [0, T ] from a given distribution P (t). Suppose P (t) is a power function:</p><formula xml:id="formula_3">P (t) = at b + c,<label>(2)</label></formula><p>where a, b and c are constants. We adopt the technique of inverse transform sampling <ref type="bibr" target="#b63">[64]</ref> by first calculating the cumulative distribution function (CDF) F (t) of P (t) as:</p><formula xml:id="formula_4">F (t) = t −∞ P (x) dx = a b + 1 t b+1 + ct,<label>(3)</label></formula><p>where t ∈ [0, T ]. To sample a temporal interval t, we then generate a random variable v ∼ U (0, 1) from a standard uniform distribution and calculate t = F −1 (v). Notice that it is difficult to directly compute the closed-form solution of the inverse function of F (·). Considering the facts that the temporal interval t is an integer representing the number of frames between the start frames of two clips and F (·) is monotonically increasing, we use a simple binary search method in Algorithm 2 to find t. The algorithm is demonstrated below and the complexity is O(log T ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Semi-Supervised Learning on Kinetics-600</head><p>We also conduct semi-supervised learning on K600. Similar to K400, we sample 1% and 10% videos from each class in the training set, forming two balanced subsets, respectively. The evaluation set remains the same. As in Table 10, CVRL shows strong performance especially when there is only 1% labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Comparison with RandAugment</head><p>We are interested in the performance of strong spatial augmentations that are widely used in supervised learning. We experiment with RandAugment <ref type="bibr" target="#b11">[12]</ref> to randomly select 2 operators from a pool of 14. We conduct experiments with 200 epochs pre-training on Kinetics-400 <ref type="bibr" target="#b37">[38]</ref>. For linear evaluation, RandAugment with temporal consistency achieves 54.2% top-1 accuracy as shown in <ref type="table" target="#tab_1">Table 11</ref>, which is lower than our temporally consistent spatial augmentation presented in Algorithm 1, implying that strong augmentations optimized for supervised image recognition do not necessarily perform as well in the self-supervised video representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Illustrations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Pre-Training and Linear Evaluation</head><p>More detailed pre-training statistics on Kinetics-400 <ref type="bibr" target="#b37">[38]</ref> are illustrated in <ref type="figure">Figure 5</ref>. We display four metrics: (1) contrastive loss, (2) regularization loss, (3) entropy and (4) pretraining accuracy. The total loss is the sum of contrastive loss and regularization loss. We also provide linear evaluation statistics in <ref type="figure">Figure 6</ref>, where all models are pre-trained on Kinetics-400 for 800 epochs corresponding to <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Temporally Consistent Spatial Augmentation</head><p>We illustrate the proposed temporally consistent spatial augmentation method in <ref type="figure" target="#fig_11">Figure 7</ref>. Given an original video clip (top row), simply applying spatial augmentations to each frame independently would break the motion cues across frames (middle row). The proposed temporally consistent spatial augmentation (bottom row) would augment the spatial domain of the video clip while maintaining their natural temporal motion changes.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Kinetics-600 top-1 linear classification accuracy of different spatiotemporal representations. CVRL outperforms Ima-geNet supervised</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b) P (t) ∝ −t 0.5 + c (63.1% acc.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(c) P (t) ∝ −t 2 + c (62.9% acc.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(d) P (t) ∝ c (62.7% acc.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(e) P (t) ∝ t + c (62.4% acc.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(f) P (t) ∝ t 2 + c (61.9% acc.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>assess the learned representations by fine-tuning the entire video encoder network in a semi-supervised learning setting as well as in downstream action classification and detection tasks. More details to come in Section 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>More data is beneficial for CVRL. All models are evaluated on the validation set of K400 to provide a fair comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 2 :</head><label>2</label><figDesc>Temporal Interval Sampling Input: random variable v ∼ U (0, 1), CDF function F (·) upper bound = T lower bound = 0 while upper bound − lower bound &gt; 1 do t = int((upper bound + lower bound)/2) if F (t) &gt; v do upper bound = t else do lower bound = t end while Output: temporal interval t ≈ F −1 (v)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Model pre-training statistics: contrastive loss, regularization loss, entropy and pre-training accuracy on Kinetics-400. Linear evaluation training (dashed-line) and evaluation (solid-line) top-1 accuracy on Kinetics-400.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Illustration of temporally consistent spatial augmentation. The middle row indicates frame-level spatial augmentations without temporal consistency which would be detrimental to the video representation learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Temporally consistent spatial augmentation Input: Video clip V = {f1, f2, · · · , fM } with M frames Crop: Randomly crop a spatial region with size ratio S in range of [0.3, 1] and aspect ratio A in [0.5, 2] Resize: Resize the cropped region to size of 224 × 224 Flip: Draw a flag F f from {0, 1} with 50% on 1 Jitter: Draw a flag Fj from {0, 1} with 80% on 1 Grey: Draw a flag Fg from {0, 1} with 20% on 1 for</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="5">Backbone (#params) Pre-train data (duration) Mod. Linear eval. Top-1 Acc. (%)</cell></row><row><cell>VTHCL [73]</cell><cell>R3D-50 (31.7M)</cell><cell>K400 (28d)</cell><cell>V</cell><cell>K400</cell><cell>37.8</cell></row><row><cell>SimCLR inflated</cell><cell>R3D-50 (31.7M)</cell><cell>K400 (28d)</cell><cell>V</cell><cell>K400</cell><cell>46.8</cell></row><row><cell>VINCE [24]</cell><cell>R-50 (23.5M)</cell><cell>K400 (28d)</cell><cell>V</cell><cell>K400</cell><cell>49.1</cell></row><row><cell>ImageNet inflated</cell><cell>R3D-50 (31.7M)</cell><cell>ImageNet (N/A)</cell><cell>V</cell><cell>K400</cell><cell>53.5</cell></row><row><cell>SeCo [74]</cell><cell>R-50 (23.5M)</cell><cell>K400 (28d)</cell><cell>V</cell><cell>K400</cell><cell>61.9</cell></row><row><cell>CVRL</cell><cell>R3D-50 (31.7M)</cell><cell>K400 (28d)</cell><cell>V</cell><cell>K400</cell><cell>66.1</cell></row><row><cell>CVRL</cell><cell>R3D-101 (59.7M)</cell><cell>K400 (28d)</cell><cell>V</cell><cell>K400</cell><cell>67.6</cell></row><row><cell>CVRL</cell><cell>R3D-152 (2×) (328.0M)</cell><cell>K600 (44d)</cell><cell>V</cell><cell>K400</cell><cell>71.6</cell></row><row><cell>Supervised (K400)</cell><cell>R3D-50 (31.7M)</cell><cell>N/A</cell><cell>V</cell><cell>N/A</cell><cell>76.0</cell></row><row><cell>SimCLR inflated</cell><cell>R3D-50 (31.7M)</cell><cell>K600 (44d)</cell><cell>V</cell><cell>K600</cell><cell>51.6</cell></row><row><cell>ImageNet inflated</cell><cell>R3D-50 (31.7M)</cell><cell>ImageNet (N/A)</cell><cell>V</cell><cell>K600</cell><cell>54.7</cell></row><row><cell>MMV-VA [2]</cell><cell>S3D-G (9.1M)</cell><cell>AS + HT (16y)</cell><cell>VA</cell><cell>K600</cell><cell>59.8</cell></row><row><cell>MMV [2]</cell><cell>TSM-50×2 (93.9M)</cell><cell cols="2">AS + HT (16y) VAT</cell><cell>K600</cell><cell>70.5</cell></row><row><cell>CVRL</cell><cell>R3D-50 (31.7M)</cell><cell>K600 (44d)</cell><cell>V</cell><cell>K600</cell><cell>70.4</cell></row><row><cell>CVRL</cell><cell>R3D-101 (59.7M)</cell><cell>K600 (44d)</cell><cell>V</cell><cell>K600</cell><cell>71.6</cell></row><row><cell>CVRL</cell><cell>R3D-152 (2×) (328.0M)</cell><cell>K600 (44d)</cell><cell>V</cell><cell>K600</cell><cell>72.9</cell></row><row><cell>Supervised (K600)</cell><cell>R3D-50 (31.7M)</cell><cell>N/A</cell><cell>V</cell><cell>N/A</cell><cell>79.4</cell></row></table><note>. Linear evaluation results. CVRL shows superior performance compared to state-of-the-art methods and baselines, significantly closes the gap with supervised learning. R-50 in the network column represents the standard 2D ResNet-50.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Semi-supervised learning on Kinetics-400. ] on K600. CVRL achieves performance that is on par with MMV (70.4% vs. 70.5%), with 133× less pre-training data (44 days vs. 16 years), 3× fewer parameters (31.7M vs. 93.9M) and only a single vision modality (V vs. VAT). With a deeper R3D-101, CVRL is able to show better performance (71.6% vs. 70.5%) with only 60% parameters (59.7M vs. 93.9M). Pre-training and linear evaluation curves can be found in Appendix C.1.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">K400 Top-1 Acc. (∆ vs. Sup.)</cell></row><row><cell></cell><cell></cell><cell>1% label</cell><cell>10% label</cell></row><row><cell>Supervised</cell><cell>R3D-50</cell><cell>3.2</cell><cell>39.6</cell></row><row><cell>SimCLR infla.</cell><cell>R3D-50</cell><cell>11.8 (8.6↑)</cell><cell>46.1 (6.5↑)</cell></row><row><cell cols="2">ImageNet infla. R3D-50</cell><cell cols="2">16.0 (12.8↑) 49.1 (9.5↑)</cell></row><row><cell>CVRL</cell><cell>R3D-50</cell><cell cols="2">35.1 (31.9↑) 58.1 (18.5↑)</cell></row><row><cell cols="4">spatiotemporal representation over spatial only ones. Fi-</cell></row><row><cell cols="4">nally, compared with the supervised upper bound, CVRL</cell></row><row><cell cols="4">greatly closes the gap between self-supervised and super-</cell></row><row><cell cols="4">vised learning. We also compare CVRL with the very recent</cell></row><row><cell cols="4">state-of-the-art multimodal video self-supervised learning</cell></row><row><cell>method MMV [2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .Table 6 .Table 7 .Table 8 .</head><label>4678</label><figDesc>Downstream action classification results on UCF-101 and HMDB-51. CVRL shows competitive performance compared with single and muitl-modal methods, by using only the Vision modality on K400 and K600. † indicates split-1 accuracy, ‡ indicates averaged accuracy on 3 splits, § indicates evaluation split(s) not mentioned in paper. Ablation on hidden layers. Ablation on batch size. Ablation on pre-training epochs.</figDesc><table><row><cell cols="2">Method Rand.</cell><cell cols="4">ImageNet SimCLR CVRL Sup. infla. infla.</cell></row><row><cell>mAP</cell><cell>6.9</cell><cell>14.0</cell><cell>14.2</cell><cell>16.3</cell><cell>19.1</cell></row><row><cell cols="6">Table 5. Downstream action detection results on AVA. We report</cell></row><row><cell cols="6">mean Average-Precision (mAP) to assess the performance. CVRL</cell></row><row><cell cols="6">outperforms ImageNet inflated and SimCLR inflated. All methods</cell></row><row><cell cols="3">use R3D-50 as the backbone.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 .Table 11 .</head><label>1011</label><figDesc>(12.6↑) 51.4 (6.1↑) ImageNet infla. R3D-50 19.7 (15.4↑) 48.3 (3.0↑) CVRL R3D-50 36.7 (32.4↑) 56.1 (10.8↑) Semi-supervised learning results on Kinetics-600. Performance of different spatial augmentations in pre-training (200 epochs). Our proposed augmentation method outperforms RandAugment with temporal consistency.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">K600 Top-1 Acc. (∆ vs. Sup.) 1% label 10% label</cell></row><row><cell>Supervised</cell><cell>R3D-50</cell><cell>4.3</cell><cell>45.3</cell></row><row><cell cols="3">SimCLR infla. 16.9 Augmentation method R3D-50</cell><cell cols="2">Accuracy (%) top-1 top-5</cell></row><row><cell cols="3">RandAugment w/ temporal consistency</cell><cell>54.2</cell><cell>77.9</cell></row><row><cell></cell><cell>Proposed</cell><cell></cell><cell>63.8</cell><cell>85.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We would like to thank Yeqing Li and the TensorFlow TPU team for their infrastructure support; Tsung-Yi Lin, Ting Chen and Yonglong Tian for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8m: A largescale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Selfsupervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrià</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Labelling unlabelled videos from scratch with multi-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Yuki M Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Can temporal information help with contrastive self-supervised learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13046,2020.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speednet: Learning the speediness in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">A short note about kinetics-600. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamonet: Dynamic action and motion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal cycleconsistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Audio set: An ontology and humanlabeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Largescale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning video representations without a single labeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><forename type="middle">Ramanan</forename><surname>Distinit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Watching the world go by: Representation learning from unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiana</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07990</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Memoryaugmented dense predictive coding for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Selfsupervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Self-supervised spatiotemporal feature learning by video geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11387</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Distinctive image features from scaleinvariant keypoints. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Multimodal self-supervision from generalized data transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04298</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Evolving losses for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Demystifying contrastive self-supervised learning: Invariances, augmentations and dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Learning video representations using contrastive bidirectional transformer</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Computational methods for inverse problems. SIAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vogel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning by pace prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Video representation learning with visual tempo consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15489</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Seco: Exploring sequence supervision for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00975</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Video playback rate perception for self-supervised spatio-temporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
