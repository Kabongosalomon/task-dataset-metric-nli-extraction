<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BETTER FINE-TUNING BY REDUCING REPRESENTA- TIONAL COLLAPSE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
							<email>armenag@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
							<email>akshats@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
							<email>anchit@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><forename type="middle">Goyal</forename><surname>Facebook</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gupta</forename><surname>Sonal</surname></persName>
							<email>sonalgupta@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Facebook</surname></persName>
						</author>
						<title level="a" type="main">BETTER FINE-TUNING BY REDUCING REPRESENTA- TIONAL COLLAPSE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although widely adopted, existing approaches for fine-tuning pre-trained language models have been shown to be unstable across hyper-parameter settings, motivating recent work on trust region methods. In this paper, we present a simplified and efficient method rooted in trust region theory that replaces previously used adversarial objectives with parametric noise (sampling from either a normal or uniform distribution), thereby discouraging representation change during fine-tuning when possible without hurting performance. We also introduce a new analysis to motivate the use of trust region methods more generally, by studying representational collapse; the degradation of generalizable representations from pre-trained models as they are fine-tuned for a specific end task. Extensive experiments show that our fine-tuning method matches or exceeds the performance of previous trust region methods on a range of understanding and generation tasks (including DailyMail/CNN, Gigaword, Reddit TIFU, and the GLUE benchmark), while also being much faster. We also show that it is less prone to representation collapse; the pre-trained models maintain more generalizable representations every time they are fine-tuned.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Pre-trained langauge models <ref type="bibr" target="#b20">(Radford et al., 2019;</ref><ref type="bibr" target="#b6">Devlin et al., 2018;</ref> have been shown to capture a wide array of semantic, syntactic and world knowledge <ref type="bibr" target="#b3">(Clark et al., 2019)</ref>, and provide the defacto initialization for modeling most existing NLP tasks. However, fine-tuning them for each task has been shown to be a highly unstable process, with many hyperparmeter settings producing failed fine-tuning runs, unstable results (large variation between random seeds), over-fitting and other unwanted consequences <ref type="bibr" target="#b7">Dodge et al., 2020)</ref>.</p><p>Recently, trust region or adversarial based based approaches, including SMART <ref type="bibr" target="#b11">(Jiang et al., 2019)</ref> and FreeLB <ref type="bibr" target="#b32">(Zhu et al., 2019)</ref>, have been shown to increase the stability and accuracy of finetuning, by adding extra constraints limiting how much the fine-tuning changes the initial parameters. However, these methods are significantly more computationally and memory intensive than the more commonly adopted simple-gradient-based approaches.</p><p>In this paper, we present a lightweight fine-tuning strategy which matches or improves performance relative to SMART and FreeLB, while needing just a fraction of the computational and memory overhead and no additional backwards passes. Our approach is motivated by trust region theory while also reducing to simply regularizing the model relative to parametric noise applied to the original pre-trained representations. We show uniformly better performance, setting a new state of the art for RoBERTa fine-tuning on GLUE and reaching state of the art on XNLI using no novel pretraining approaches <ref type="bibr" target="#b26">Wang et al., 2018;</ref><ref type="bibr" target="#b4">Conneau et al., 2018)</ref>. Furthermore, the low overhead of our family of fine-tuning methods allows our method to be applied to generation tasks where we consistently outperform standard fine-tuning, setting state of the art on summarization tasks.</p><p>We also introduce a new analysis to motivate the use of trust-region-style methods more generally, by defining a new notion of representational collapse and introducing new methodology for measuring it during fine-tuning. Representational collapse is the degradation of generalizable representations of pre-trained models during the fine-tuning stage. We empirically show that standard finetuning degrades generalizable representations through a series of probing experiments on GLUE tasks. Furthermore, we attribute this phenomena to using standard gradient descent algorithms for the fine-tuning stage. We also find that (1) recently proposed fine-tuning methods rooted in trust region, i.e. SMART, are capable of alleviating representation collapse, and (2) our methods alleviate representational collapse to an even great degree, manifesting in better performance across almost all datasets and models.</p><p>Our contributions in this paper are the following.</p><p>• We propose a novel approach to fine-tuning rooted in trust-region theory which we show directly alleviates representational collapse at a fraction of the cost of other recently proposed fine-tuning methods.</p><p>• Through extensive experimentation, we show that our method outperforms standard finetuning methodology following recently proposed best practices from . We improve various SOTA models from sentence prediction to summarization, from monolingual to cross-lingual.</p><p>• We further define and explore the phenomena of representational collapse in fine-tuning and directly correlate it with generalization in tasks of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LEARNING ROBUST REPRESENTATIONS THROUGH REGULARIZED FINE-TUNING</head><p>We are interested in deriving methods for fine-tuning representations which provide guarantees on movement of representations, in the sense that they do not forget the original pre-trained representations when they are fine-tuned for a new tasks (see Section 4 for more details). We introduce a new finetuning method rooted in an approximation to trust region, which provide guarantees for stochastic gradient descent algorithms by bounding some divergence between model at update t and t + 1 <ref type="bibr" target="#b19">(Pascanu &amp; Bengio, 2013;</ref><ref type="bibr" target="#b23">Schulman et al., 2015;</ref><ref type="bibr" target="#b11">Jiang et al., 2019</ref>).</p><p>Let f : R m×n → R p be a function which returns some pre-trained representation parameterized by θ f from m tokens embedded into a fixed vector of size n. Let the learned classification head g : R p → R q be a function which takes an input from f and outputs a valid probability distribution parameterized by θ g in q dimensions. In the case of generation, we can assume the classification head is simply an identity function or softmax depending on the loss function. Let L(θ) denote a loss function given by θ = [θ f , θ g ]. We are interested in minimizing L with respect to θ such that each update step is constrained by movement in the representational density space p(f ). More formally given an arbitrary</p><formula xml:id="formula_0">arg min ∆θ L(θ + ∆θ) s.t. KL(p(f (· ; θ f ))||p(f (· ; θ f + ∆θ f ))) =<label>(1)</label></formula><p>This constrained optimization problem is equivalent to doing natural gradient descent directly over the representations <ref type="bibr" target="#b19">(Pascanu &amp; Bengio, 2013)</ref>. Unfortunately, we do not have direct access to the density of representations therefore it is not trivial to directly bound this quantity. Instead we propose to do natural gradient over g·f with an additional constraint that g is at most 1-Lipschitz (which naturally constrains change of representations, see Section A.1 in the Appendix). Traditional computation of natural gradient is computationally prohibitive due to the need of inverting the Hessian. An alternative formulation of natural gradient can be stated through mirror descent, using Bregmann divergences <ref type="bibr" target="#b22">(Raskutti &amp; Mukherjee, 2015;</ref><ref type="bibr" target="#b11">Jiang et al., 2019)</ref>.</p><formula xml:id="formula_1">L SM ART (θ, f, g) = L(θ) + λE x∼X sup x ∼ :|x ∼ −x|≤ KL S (g · f (x) g · f (x ∼ ))<label>(2)</label></formula><p>However, the supremum is computationally intractable. An approximation is possible by doing gradient ascent steps, similar to finding adversarial examples. This was first proposed by SMART with a symmetrical KL S (X, Y ) = KL(X||Y ) + KL(Y ||X) term <ref type="bibr" target="#b11">(Jiang et al., 2019)</ref>.</p><p>We propose an even simpler approximation which does not require extra backward computations and empirically works as well as or better than SMART. We completely remove the adversarial nature from SMART and instead optimize for a smoothness parameterized by KL S . Furthermore, we optionally also add a constraint on the smoothness of g by making it at most 1-Lipschitz, the intuition being if we can bound the volume of change in g we can more effectively bound f .</p><formula xml:id="formula_2">L R3 (f, g, θ) = L(θ) + λKL S (g · f (x) g · f (x + z)) R3F Method (3) s.t. z ∼ N (0, σ 2 I) or z ∼ U(−σ, σ) (4) s.t. Lip{g} ≤ 1 Optional R4F Method (5)</formula><p>where KL S is the symmetric KL divergence and z is a sample from a parametric distribution. In our work we test against two distributions, normal and uniform centered around 0. We denote this as the Robust Representations through Regularized Finetuning (R3F) method.</p><p>Additionally we propose an extension to R3F (R4F; Robust Representations through Regularized and Reparameterized Finetuning, which reparameterizes g to be at most 1-Lipschitz via Spectral Normalization <ref type="bibr" target="#b17">(Miyato et al., 2018)</ref>. By constraining g to be at most 1-Lipschitz, we can more directly bound the change in representation (Appendix Section A.1). Specifically we scale all the weight matrices of g by the inverse of their largest singular values W SN := W/σ(W ). Given that spectral radius σ(W SN ) = 1 we can bound Lip{g} ≤ 1. In the case of generation, g does not have any weights therefore we can only apply the R3F method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RELATIONSHIP TO SMART AND FREELB</head><p>Our method is most closely related to the SMART algorithm which utilizes an auxiliary smoothness inducing regularization term which directly optimizes the Bregmann divergence mentioned above in Equation 2 <ref type="bibr" target="#b11">(Jiang et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FP BP xFP</head><p>FreeLB SMART solves the supremum by using an adversarial methodology to ascent to the largest KL divergence with an −ball. We instead propose to remove the ascent step completely, optionally fixing the smoothness of the classification head g. This completely removes the adversarial nature of SMART and is more akin to optimizing the smoothness of g · f directly. Another recently proposed adversarial method for fine-tuning, FreeLB optimizes a direct adversarial loss L F reeLB (θ) = sup ∆θ:|∆θ|≤ L(θ + ∆θ) through iterative gradient ascent steps. Unfortunately the need for extra forward-backward passes can be prohibitively expensive when finetuning large pre-trained models <ref type="bibr" target="#b32">(Zhu et al., 2019)</ref>.</p><formula xml:id="formula_3">1 + S 1 + S 3 + 3S SMART 1 + S 1 + S 3 + 3S R3F/R4F 2 1 4 Standard 1 1 3</formula><p>Our method is significantly more computationally efficient than adversarial based fine-tuning methods, as seen in <ref type="table" target="#tab_0">Table 1</ref>. We show that this efficency does not hurt performance, we are able to match or exceed FreeLB and SMART on a large amount of tasks. In addition, the relatively low costs of our methods allows us to improve over fine-tuning on an array of generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We will first measure performance by fine-tuning on a range of tasks and languages. The next sections report analysis as to why methods rooted in trust region, including ours, outperform standard fine-tuning. Throughout all of our experiments, we aimed for fair comparisons, by using fixed budget hyper-parameters searches across all methods. Furthermore for computationally tractable tasks we report median/max numbers as well as show distributions across a large number of runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SENTENCE PREDICTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLUE</head><p>We will first test R3F and R4F on sentence classification tasks from the GLUE benchmark <ref type="bibr" target="#b26">(Wang et al., 2018)</ref>. We select the same subset of GLUE tasks that have been reported by prior work in this space <ref type="bibr" target="#b11">(Jiang et al., 2019)</ref>: MNLI , QQP <ref type="bibr" target="#b10">(Iyer et al., 2017)</ref>, RTE <ref type="bibr" target="#b0">(Bentivogli et al., 2009</ref>), QNLI <ref type="bibr" target="#b21">(Rajpurkar et al., 2016)</ref>, <ref type="bibr">MRPC (Dolan &amp; Brockett, 2005)</ref>, CoLA <ref type="bibr" target="#b27">(Warstadt et al., 2018)</ref>, SST-2 <ref type="bibr" target="#b24">(Socher et al., 2013)</ref>. 1</p><p>Consistent with prior work <ref type="bibr" target="#b11">(Jiang et al., 2019;</ref><ref type="bibr" target="#b32">Zhu et al., 2019)</ref>, we focus on improving the performance of RoBERTa-Large based models in the single task setting . We report performance of all models on the GLUE development set.</p><p>Method 5000 6000 7000 8000 9000 Seconds</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SST-2 Walltime Analysis</head><p>Finetuning Method Standard++ SMART R3F R3F R4F R4F <ref type="figure">Figure 1</ref>: Empirical evidence towards the computational benefits of our method we present training wall time analysis on the SST-2 dataset. Each method includes a violin plot for 10 random runs.</p><p>We define wall-time as the training time in seconds to best checkpoint.</p><p>We fine-tune each of the GLUE tasks with 4 methods: Standard (STD), the traditional finetuning scheme as done by RoBERTa ; Standard++ (STD++), a variant of standard fine-tuning that incorporates recently proposed best practices for fine-tuning, specifically longer fine-tuning and using bias correction in Adam ; and our proposed methods R3F and R4F. We compare against the numbers reported by SMART, FreeLB and RoBERTa on the validation set. For each method we applied a hyper-parameter search with equivalent fixed budgets per method. Fine-tuning each task has task specific hyperparameters described in the Appendix (Section A.2). After finding the best hyperparameters we replicated experiments with optimal parameters across 10 different random seeds. Our numbers reported are the maximum of 10 seeds to be comparable with other benchmarks in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>In addition to showing best performance, we also show the distribution of various methods across 10 seeds to demonstrate the stability properties of individual methods in <ref type="figure">Figure 2</ref>.</p><p>R3F and R4F unanimously improve over Standard and Standard++ fine-tuning. Furthermore our methods match or exceed adversarial methods such as SMART/FreeLB at a fraction of the computational cost when comparing median runs. We show computational cost in <ref type="figure">Figure 1</ref> for a single task, but the relative behavior of wall times are consistent across all other tasks in GLUE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XNLI</head><p>We hypothesize that staying closer to the original representations is especially important for crosslingual tasks, especially in the zero-shot fashion where drifting away from pre-trained representations for a single language might manifest in loss of cross-lingual capabilities. In particular we take  <ref type="figure">Figure 2</ref>: We show the results of our method against Standard++ fine-tuning and SMART across 3 tasks. Across 10 random seeds both max and median of our runs were higher using our method than both SMART and Standard++.   We present our result in <ref type="table" target="#tab_4">Table 3</ref>. R3F and R4F dominate standard pre-training on 14 out of the 15 languages in the XNLI task. R4F improves over the best known XLM-R XNLI results reaching SOTA with an average language score of 81.4 across 5 runs. The current state of the art required a novel pre-training method to reach the same numbers as <ref type="bibr" target="#b2">(Chi et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SUMMARIZATION</head><p>While prior work in non-standard finetuning methods tends to focus on sentence prediction and GLUE tasks <ref type="bibr" target="#b11">(Jiang et al., 2019;</ref><ref type="bibr" target="#b32">Zhu et al., 2019;</ref>, we look to improve abstractive summarization, due to its additional complexity and computational cost, specifically we look at three datasets: CNN/Dailymail <ref type="bibr" target="#b9">(Hermann et al., 2015)</ref>, Gigaword <ref type="bibr" target="#b18">(Napoles et al., 2012)</ref> and Reddit TIFU <ref type="bibr" target="#b12">(Kim et al., 2018)</ref>.  Like most other NLP tasks, summarization recently has been dominated by fine-tuning of large pretrained models. For example PEGASUS explicitly defines a pre-training objective to facilitate the learning of representations tailored to summarization tasks manifesting in state-of the art performance on various summarization benchmarks <ref type="bibr" target="#b30">(Zhang et al., 2019)</ref>. ProphetNet improved over these numbers by introducing their own novel self-supervised task <ref type="bibr" target="#b29">(Yan et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN/DailyMail</head><p>Independently of the pre-training task, standard fine-tuning on downstream tasks follows a simple formula of using a label smoothing loss while directly fine-tuning the whole model, without addition of any new parameters. We propose the addition of the R3F term directly to the label smoothing loss.</p><p>We present our results in <ref type="table" target="#tab_6">Table 4</ref>. Our method (R3F) outperforms standard fine-tuning across the board for three tasks across all of the variants of the ROUGE metric. Notably we improve Gigaword and Reddit TIFU ROUGE-1 scores by a point and 4 points respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">REPRESENTATIONAL COLLAPSE</head><p>Catastrophic forgetting, originally proposed as catastrophic interference, is a phenomena that occurs during sequential training where new updates interfere catastrophically with previous updates manifesting in forgetting of certain examples with respect to a fixed task <ref type="bibr" target="#b16">(McCloskey &amp; Cohen, 1989)</ref>. Inspired by this work, we explore the related problem of representational collapse; the degradation of generalizable representations of pre-trained models during the fine-tuning stage. This definition is independent of a specific fine-tuning task, but is rather over the internal representations generalizabality over a large union of tasks. Another view of this phenomena is that fine-tuning collapses the wide range of information available in the representations into a smaller set needed only for the immediate task and particular training set.</p><p>Measuring such degradations is non-trivial. Simple metrics such as the distance between pre-trained representations and fine-tuned representations is not sufficient (e.g. adding a constant to the pretrained representations will not change representation power, but will change distances). One approach would be to estimate mutual information of representations across tasks before and after fine-tuning, but estimation of mutual information is notoriously hard, especially in high-dimensions <ref type="bibr" target="#b25">(Tschannen et al., 2019)</ref>. We instead propose a series of probing experiments meant to provide us with empirical evidence of the existence of representation collapse on the GLUE benchmark <ref type="bibr" target="#b26">(Wang et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PROBING EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROBING GENERALIZATION OF FINE-TUNED REPRESENTATIONS</head><p>To measure the generalization properties of various fine-tuning methodologies, we follow probing methodology by first freezing the representations from the model trained on one task and then finetuning a linear layer on top of the model for another task. By doing this form of probing we can directly measure the quality of representations learned by various fine-tuning methods, as well as how much they collapse when fine-tuned on a sequence of tasks. In particular, we finetune a RoBERTa model on SST-2 and train a linear layer for 6 other GLUE tasks respectively. Our results are shown in <ref type="figure" target="#fig_0">Figure 3</ref>. Appendix A.2 presents the hyperparameters. Across all tasks, one of the two variants of our method performed best across various fine-tuning methods. Conversely standard fine-tuning produced representations which were worse than other fine-tuning methods across the board, hinting at the sub-optimality of standard fine-tuning. Furthermore R3F/R4F consistently outperforms the adversarial fine-tuning method SMART.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROBING REPRESENTATION DEGRADATION</head><p>In order to show the effect of representation collapse, we propose an experiment to measure how the fine-tuning process degrades representations by sequentially training on a series of GLUE tasks. We arbitrarily select 3 GLUE tasks (QNLI, QQP, and RTE) and a source task (SST-2). We begin by training a model on our source task, and then train on QNLI, QQP, and RTE  As we can see with the standard fine-tuning process our model diverges from the source task resulting in lower performance probes, however with our method the probes vary much less with sequential probing resulting in better probing and end performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROBING REPRESENTATION RETENTION</head><p>To further understand the impact of representational collapse, we extend our probing experiments to train a cyclic chain of tasks. In our prior experiments we showed that traditional fine-tuning degrades representations during the fine-tuning process, meaning standard fine-tuning learns poorer representation compared to alternative fine-tuning methods. The dual to looking at degradation is to look at the retainment of learned representations, to do this we take a look at cyclic sequential probing. Sequential probing involves training a model on task A, probing B, then training model fine-tuned on B and probing task C, and so forth. We then create a cyclic chain</p><formula xml:id="formula_4">A → B → C Cycle 1 → A → B → C Cycle 2</formula><p>from where we compare tasks via their probe performance at each cycle.</p><p>We expect probing performance to increase at every cycle, since every cycle the task we are probing on will undergo a full fine-tuning. What we are interested in is the level of retention in representations after the fine-tuning. Specifically we hypothesize that our method, specifically R4F will retain representations significantly better than the Standard++ fine-tuning method.</p><p>In our experiments we consider the following sequence of GLUE tasks: SST-2 → QNLI → QQP → RTE. We defer hyperparameter values to Appendix (Section A.2).  <ref type="figure">Figure 5</ref>: We present the results of cyclical sequential probing for 3 cycles.</p><p>Looking at <ref type="figure">Figure 5</ref>, we see that R4F retains quality of representations significantly better that standard fine-tuning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose a family of new fine-tuning approaches for pre-trained representations based on trustregion theory: R3F and R4F. Our methods are more computationally efficient and out perform prior work in fine-tuning via adversarial learning <ref type="bibr" target="#b11">(Jiang et al., 2019;</ref><ref type="bibr" target="#b32">Zhu et al., 2019)</ref>. We show that this is due to a new phenomena that occurs during fine-tuning: representational collapse, where representations learned during fine-tuning degrade leading to worse generalization. Our analysis show standard fine-tuning is sub-optimal when it comes to learning generalizable representations, and instead our methods retain representation generalizability and improve end task performance.</p><p>With our method we improve upon monolingual and multilingual sentence prediction tasks as well as generation tasks compared to standard and adversarial fine-tuning methods. Notably we set state of the art on DailyMail/CNN, Gigaword, Reddit TIFU, improve the best known results on finetuning RoBERTa on GLUE, and reach state of the art on zero-shot XNLI without the need for any new pre-training method.</p><p>Direct application of change of variable gives us</p><formula xml:id="formula_5">KL(p(f (x; θ f ))||p(f (x; θ f + ∆θ f ))) = (7) p(f (x; θ f )) log p(f (x; θ f )) p(f (x; θ f + ∆θ f )) = (8) p(g(f (x; θ f ))) det dg(f (x; θ f )) df (x; θ f ) [ (9) log p(g(f (x; θ f ))) + log det dg(f (x; θ f )) df (x; θ f ) (10) − log p(g(f (x; ∆θ f ))) − log det dg(f (x; ∆θ f )) df (x; ∆θ f ) (11) ]<label>(12)</label></formula><p>Let us make some more assumptions. Let g(y) = W y where the spectral norm of W, ρ(W ) = 1. We can then trivially bound det W ≤ 1. Then we have = p(g(f (x; θ f ))) det dg(f (x; θ f )) df (x; θ f ) [log p(g(f (x; θ f ))) − log p(g(f (x; ∆θ f )))]</p><p>= p(g(f (x; θ f ))) det dg(f (x; θ f )) df (x; θ f ) log p(g(f (x; θ f ))) p(g(f (x; ∆θ f ))) (14)</p><p>≤ p(g(f (x; θ f ))) log p(g(f (x; θ f ))) p(g(f (x; ∆θ f ))) (15) = KL(p(g(f (x; θ f )))||p(g(f (x; ∆θ f ))))</p><p>We also see that tightness is controlled by | det W | which is bounded by the singular value giving us intuition to the importance of using spectral normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 EXPERIMENT HYPER-PARAMETERS</head><p>For our GLUE related experiments both full fine-tuning and probing, the following parameters are used. For probing experiments the difference is our RoBERTa encoder is frozen and encoder dropout is removed.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Results from our probing experiments comparing our proposed algorithms R3F, R4F to standard fine-tuning. Variants of our method consistently outperform past work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>We show the results of the chained probing experiments. We do not show the distributional properties of the runs because there was very little variance in the results. in a sequential order using the best checkpoint from the prior iteration. At each point in the chain we probe the source task and measure performance. Our results are depicted inFigure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Computational cost of recently proposed fine-tuning algorithms. We show Forward Passes (FP), Backward Passes (BP) as well as computation cost as a factor of forward passes (xFP).</figDesc><table /><note>S is the number of gradient ascent steps, with a minimum of S ≥ 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Model</cell><cell>en fr</cell><cell>es</cell><cell>de el</cell><cell>bg ru</cell><cell>tr</cell><cell>ar</cell><cell>vi</cell><cell>th</cell><cell>zh hi</cell><cell>sw ur</cell><cell>Avg</cell></row><row><cell cols="12">XLM-R Base 85.8 79.7 80.7 78.7 77.5 79.6 78.1 74.2 73.8 76.5 74.6 76.7 72.4 66.5 68.3 76.2</cell></row><row><cell cols="12">XLM-R Large 89.1 84.1 85.1 83.9 82.9 84.0 81.2 79.6 79.8 80.8 78.1 80.2 76.9 73.9 73.8 80.9</cell></row><row><cell>+ R3F</cell><cell cols="11">89.4 84.2 85.1 83.7 83.6 84.6 82.3 80.7 80.6 81.1 79.4 80.1 77.3 72.6 74.2 81.2</cell></row><row><cell>+ R4F</cell><cell cols="11">89.6 84.7 85.2 84.2 83.6 84.6 82.5 80.3 80.5 80.9 79.2 80.6 78.2 72.7 73.9 81.4</cell></row><row><cell>InfoXLM</cell><cell cols="11">89.7 84.5 85.5 84.1 83.4 84.2 81.3 80.9 80.4 80.8 78.9 80.9 77.9 74.8 73.7 81.4</cell></row></table><note>We present our best results on the GLUE development set for various fine-tuning methods applied to the RoBERTa Large model. On the left side table we present our best numbers and numbers published in other papers. On the right side we present median numbers from 10 runs for mentioned methods.a look at the popular XNLI benchmark, containing 15 languages (Conneau et al., 2018). We com- pare our method against the standard trained XLM-R model in the zero-shot setting (Conneau et al., 2019).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Average of 5 runs of zero-shots results on the XNLI test set for our method applied to XLM-R Large. Variouns of our method win over the majority of languages. The bottom row shows the current SOTA on XNLI which requires the pre-training of novel model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>/15.03/35.48 35.70/16.75/32.83 15.89/1.94/12.22  BART  44.16/21.28/40.90 39.29/20.09/35.65 24.19/8.12/21.31  PEGASUS  44.17/21.47/41.11 39.12/19.86/36.24 26.63/9.01/21.60    </figDesc><table><row><cell>Gigaword</cell><cell>Reddit TIFU (Long)</cell></row><row><cell cols="2">Random Transformer 38.27ProphetNet (Old SOTA) 44.20/21.17/41.30 39.51/20.42/36.69 -</cell></row><row><cell cols="2">BART+R3F (New SOTA) 44.38/21.53/41.17 40.45/20.69/36.56 30.31/10.98/24.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Our results on various summarization data-sets. We report Rouge-1, Rouge-2 and Rouge-L per element in table. Following PEGASUS, we bold the best number and numbers within 0.15 of the best.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Task specific hyper parameters for GLUE experiments</figDesc><table><row><cell cols="2">Hyper parameter Value</cell><cell></cell><cell></cell></row><row><cell>Optimizer Adam-betas</cell><cell>Adam (0.9, 0.98)</cell><cell cols="2">Hyper parameter Value</cell></row><row><cell>Adam-eps</cell><cell>1e-6</cell><cell>λ</cell><cell>[0.1, 0.5, 1.0, 5.0]</cell></row><row><cell>LR Scheduler</cell><cell>polynomial decay</cell><cell>Noise Types</cell><cell>[U, N ]</cell></row><row><cell>Dropout</cell><cell>0.1</cell><cell>σ</cell><cell>1e − 5</cell></row><row><cell>Weight Decay</cell><cell>0.01</cell><cell></cell><cell></cell></row><row><cell cols="2">Warmup Updates 0.06 * max updates</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Hyper parameters for R3F and R4F experiments on GLUE</figDesc><table><row><cell cols="4">Hyper Parameter CNN/Dailymail Gigaword Reddit TIFU</cell></row><row><cell>Max Tokens</cell><cell>1024</cell><cell>2048</cell><cell>2048</cell></row><row><cell>Total updates</cell><cell>80000</cell><cell>200000</cell><cell>200000</cell></row><row><cell cols="2">Warmup Updates 1000</cell><cell>5000</cell><cell>5000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Task specific hyper parameters for Summarization experiments.</figDesc><table><row><cell cols="2">Hyper parameter Value</cell><cell cols="2">Hyper parameter Value</cell></row><row><cell>Optimizer Adam-betas Adam-eps LR Scheduler Learning Rate</cell><cell>Adam (0.9, 0.98) 1e-8 polynomial decay 3e-05</cell><cell>λ Noise Types σ Dropout Weight Decay Clip Norm</cell><cell>[0.001, 0.01, 0.1] [U, N ] 1e − 5 0.1 0.01 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Hyper parameters for R3F and R4F experiments on Summarization experiments.</figDesc><table><row><cell cols="2">Hyper parameter Value</cell><cell cols="2">Hyper parameter Value</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>λ</cell><cell>[0.5, 1, 3, 5]</cell></row><row><cell>Adam-betas</cell><cell>(0.9, 0.98)</cell><cell>Noise Types</cell><cell>[U, N ]</cell></row><row><cell>Adam-eps</cell><cell>1e-8</cell><cell>σ</cell><cell>1e − 5</cell></row><row><cell>LR Scheduler</cell><cell>polynomial decay</cell><cell>Total Updates</cell><cell>450000</cell></row><row><cell>Learning Rate</cell><cell>3e-05</cell><cell>Max Positions</cell><cell>512</cell></row><row><cell>Dropout</cell><cell>0.1</cell><cell>Max Tokens</cell><cell>4400</cell></row><row><cell>Weight Decay</cell><cell>0.01</cell><cell>Max Sentences</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Hyper parameters for R3F and R4F experiments on XNLI.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We do not test against STS-B because it is a regression task where our KL divergence is not defined<ref type="bibr" target="#b1">(Cer et al., 2017)</ref>.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 CONTROLLING CHANGE OF REPRESENTATION VIA CHANGE OF VARIABLE Let us say we have random variables in some type of markovian chain x, y, z; y = f (x; θ f ), z = g(y; θ g )</p><p>The change of variable formulation for probability densities is</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<editor>TAC</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Infoxlm: An information-theoretic framework for crosslingual language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04341</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xnli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05053</idno>
		<title level="m">Evaluating cross-lingual sentence representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06305</idno>
		<title level="m">Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
		<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">First quora dataset release: Question pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kornel</forename><surname>Csernai</surname></persName>
		</author>
		<ptr target="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03437</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Abstractive summarization of reddit posts with multi-level memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00783</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Pre-training via paraphrasing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Annotated gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Matthew R Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX)</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Revisiting natural gradient for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3584</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The information geometry of mirror descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garvesh</forename><surname>Raskutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayan</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1451" to="1457" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13625</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-5446" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N18-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04063</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08777</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Revisiting fewsample bert fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05987</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Freelb: Enhanced adversarial training for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
