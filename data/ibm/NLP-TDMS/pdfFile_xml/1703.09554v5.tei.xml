<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lucid Data Dreaming for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
							<email>khoreva@mpi-inf.mpg.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
							<email>benenson@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">Eddy</forename><surname>Ilg</surname></persName>
							<email>ilg@cs.uni-freiburg.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<email>brox@cs.uni-freiburg.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<email>schiele@mpi-inf.mpg.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Benenson</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lucid Data Dreaming for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional networks reach top quality in pixellevel video object segmentation but require a large amount of training data (1k ∼100k) to deliver such results. We propose a new training strategy which achieves state-of-the-art results across three evaluation datasets while using 20× ∼ 1000× less annotated data than competing methods. Our approach is suitable for both single and multiple object segmentation.</p><p>Instead of using large training sets hoping to generalize across domains, we generate in-domain training data using the provided annotation on the first frame of each video to synthesize ("lucid dream" 1 ) plausible future video frames. In-domain per-video training data allows us to train high quality appearance-and motion-based models, as well as tune the post-processing stage. This approach allows to reach competitive results even when training from only a single annotated frame, without ImageNet pre-training. Our results indicate that using a larger training set is not automatically better, and that for the video object segmentation task a 1 In a lucid dream the sleeper is aware that he or she is dreaming and is sometimes able to control the course of the dream. <ref type="figure">Figure 1</ref>: Starting from scarce annotations we synthesize indomain data to train a specialized pixel-level video object segmenter for each dataset or even each video sequence.</p><p>smaller training set that is closer to the target domain is more effective. This changes the mindset regarding how many training samples and general "objectness" knowledge are required for the video object segmentation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last years the field of localizing objects in videos has transitioned from bounding box tracking <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b34">34]</ref> to pixel-level segmentation <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b75">75]</ref>. Given a first frame labelled with the foreground object masks, one aims to find the corresponding object pixels in future frames. Segmenting objects at the pixel level enables a finer understanding of videos and is helpful for tasks such as video editing, rotoscoping, and summarisation.</p><p>Top performing results are currently obtained using convolutional networks (convnets) <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b44">44]</ref>. Like most deep learning techniques, convnets for video object segment-ation benefit from large amounts of training data. Current state-of-the-art methods rely, for instance, on pixel accurate foreground/background annotations of ∼ 2k video frames <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b6">6]</ref> , ∼10k images <ref type="bibr" target="#b31">[31]</ref>, or even more than 100k annotated samples for training <ref type="bibr" target="#b74">[74]</ref>. Labelling images and videos at the pixel level is a laborious task (compared e.g. to drawing bounding boxes for detection), and creating a large training set requires significant annotation effort.</p><p>In this work we aim to reduce the necessity for such large volumes of training data. It is traditionally assumed that convnets require large training sets to perform best. We show that for video object segmentation having a larger training set is not automatically better and that improved results can be obtained by using 20× ∼ 1000× less training data than previous approaches <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b74">74]</ref>. The main insight of our work is that for video object segmentation using few training frames (1 ∼ 100) in the target domain is more useful than using large training volumes across domains (1k∼100k).</p><p>To ensure a sufficient amount of training data close to the target domain, we develop a new technique for synthesizing training data particularly tailored for the pixel-level video object segmentation scenario. We call this data generation strategy "lucid dreaming", where the first frame and its annotation mask are used to generate plausible future frames of the videos. The goal is to produce a large training set of reasonably realistic images which capture the expected appearance variations in future video frames, and thus is, by design, close to the target domain.</p><p>Our approach is suitable for both single and multiple object segmentation in videos. Enabled by the proposed data generation strategy and the efficient use of optical flow, we are able to achieve high quality results while using only ∼ 100 individual annotated training frames. Moreover, in the extreme case with only a single annotated frame and zero pre-training (i.e. without ImageNet pre-training), we still obtain competitive video object segmentation results.</p><p>In summary, our contributions are the following: 1. We propose "lucid data dreaming", an automated approach to synthesize training data for the convnet-based pixel-level video object segmentation that leads to top results for both single and multiple object segmentation 2 . 2. We conduct an extensive analysis to explore the factors contributing to our good results. <ref type="bibr" target="#b2">3</ref>. We show that training a convnet for video object segmentation can be done with only few annotated frames. We hope these results will affect the trend towards even larger training sets, and popularize the design of video segmentation convnets with lighter training needs.</p><p>With the results for multiple object segmentation we took the second place in the 2017 DAVIS Challenge on Video Object Segmentation <ref type="bibr" target="#b54">[54]</ref>. A summary of the proposed ap-proach was provided online <ref type="bibr">[30]</ref>. This paper significantly extends <ref type="bibr">[30]</ref> with in-depth discussions on the method, more details of the formulation, its implementation, and its variants for single and multiple object segmentation in videos. It also offers a detailed ablation study and an error analysis as well as explores the impact of varying number of annotated training samples on the video segmentation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Box tracking. Classic work on video object tracking focused on bounding box tracking. Many of the insights from these works have been re-used for video object segmentation. Traditional box tracking smoothly updates across time a linear model over hand-crafted features <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b35">35]</ref>. Since then, convnets have been used as improved features <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b76">76]</ref>, and eventually to drive the tracking itself <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref>. Contrary to traditional box trackers (e.g. <ref type="bibr" target="#b23">[23]</ref>), convnetbased approaches need additional data for pre-training and learning the task.</p><p>Video object segmentation. In this paper we focus on generating a foreground versus background pixel-wise object labelling for each video frame starting from a first manually annotated frame. Multiple strategies have been proposed to solve this task.</p><p>Box-to-segment: First a box-level track is built, and a space-time grabcut-like approach is used to generate per frame segments <ref type="bibr" target="#b81">[81]</ref>.</p><p>Video saliency: this group of methods extracts the main foreground object pixel-level space-time tube. Both handcrafted models <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b46">46]</ref> or trained convnets <ref type="bibr" target="#b70">[70,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b62">62]</ref> have been considered. Because these methods ignore the first frame annotation, they fail in videos where multiple salient objects move (e.g. flock of penguins).</p><p>Space-time proposals: these methods partition the spacetime volume, and then the tube overlapping most with the first frame mask annotation is selected as tracking output <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b8">8]</ref>.</p><p>Mask propagation: Appearance similarity and motion smoothness across time is used to propagate the first frame annotation across the video <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b78">78,</ref><ref type="bibr" target="#b71">71]</ref>. These methods usually leverage optical flow and long term trajectories.</p><p>Convnets: following the trend in box tracking, recently convnets have been proposed for video object segmentation. <ref type="bibr" target="#b6">[6]</ref> trains a generic object saliency network, and fine-tunes it per-video (using the first frame annotation) to make the output sensitive to the specific object of interest. <ref type="bibr" target="#b31">[31]</ref> uses a similar strategy, but also feeds the mask from the previous frame as guidance for the saliency network. <ref type="bibr" target="#b74">[74]</ref> incorporates online adaptation of the network using the predictions from previous frames. <ref type="bibr" target="#b7">[7]</ref> extends the Gaussian-CRF approach to videos by exploiting spatio-temporal connections for pairwise terms and relying on unary terms from <ref type="bibr" target="#b74">[74]</ref>. Finally <ref type="bibr" target="#b29">[29]</ref> mixes convnets with ideas of bilateral filtering. Our approach also builds upon convnets. What makes convnets particularly suitable for the task, is that they can learn what are the common statistics of appearance and motion patterns of objects, as well as what makes them distinctive from the background, and exploit this knowledge when segmenting a particular object. This aspect gives convnets an edge over traditional techniques based on low-level hand-crafted features.</p><p>Our network architecture is similar to <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b31">31]</ref>. Other than implementation details, there are three differentiating factors. One, we use a different strategy for training: <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b74">74]</ref> rely on consecutive video training frames and <ref type="bibr" target="#b31">[31]</ref> uses an external saliency dataset, while our approach focuses on using the first frame annotations provided with each targeted video benchmark without relying on external annotations. Two, our approach exploits optical flow better than these previous methods. Three, we describe an extension to seamlessly handle segmentation of multiple objects. Interactive video segmentation. Interactive segmentation <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b77">77]</ref> considers more diverse user inputs (e.g. strokes), and requires interactive processing speed rather than providing maximal quality. Albeit our technique can be adapted for varied inputs, we focus on maximizing quality for the noninteractive case with no-additional hints along the video. Semantic labelling. Like other convnets in this space <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b31">31]</ref>, our architecture builds upon the insights from the semantic labelling networks <ref type="bibr" target="#b84">[84,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b80">80,</ref><ref type="bibr" target="#b1">2]</ref>. Because of this, the flurry of recent developments should directly translate into better video object segmentation results. For the sake of comparison with previous work, we build upon the well established VGG DeepLab architecture <ref type="bibr" target="#b10">[10]</ref>. Synthetic data. Like our approach, previous works have also explored synthesizing training data. Synthetic renderings <ref type="bibr" target="#b45">[45]</ref>, video game environment <ref type="bibr" target="#b57">[57]</ref>, mix-synthetic and real images <ref type="bibr" target="#b72">[72,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b15">15]</ref> have shown promise, but require taskappropriate 3d models. Compositing real world images provides more realistic results, and has shown promise for object detection <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b67">67]</ref>, text localization <ref type="bibr" target="#b21">[21]</ref> and pose estimation <ref type="bibr" target="#b51">[51]</ref>.</p><p>The closest work to ours is <ref type="bibr" target="#b47">[47]</ref>, which also generates video-specific training data using the first frame annotations. They use human skeleton annotations to improve pose estimation, while we employ pixel-level mask annotations to improve video object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LucidTracker</head><p>Section 3.1 describes the network architecture used, and how RGB and optical flow information are fused to predict the next frame segmentation mask. Section 3.2 discusses different training modalities employed with the proposed video object segmentation system. In Section 4 we discuss the training data generation, and sections 5/6 report results for single/multiple object segmentation in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>Approach. We model video object segmentation as a mask refinement task (mask: binary foreground/ background labelling of the image) based on appearance and motion cues. From frame t − 1 to frame t the estimated mask M t−1 is propagated to frame t, and the new mask M t is computed as a function of the previous mask, the new image I t , and the optical flow F t , i.e. M t = f (I t , F t , M t−1 ). Since objects have a tendency to move smoothly through space in time, there are little changes from frame to frame and mask M t−1 can be seen as a rough estimate of M t . Thus we require our trained convnet to learn to refine rough masks into accurate masks. Fusing the complementary image I t and motion flow F t enables to exploits the information inherent to video and enables the model to segment well both static and moving objects.</p><p>Note that this approach is incremental, does a single forward pass over the video, and keeps no explicit model of the object appearance at frame t. In some experiments we adapt the model f per video, using the annotated first frame I 0 , M 0 . However, in contrast to traditional techniques <ref type="bibr" target="#b23">[23]</ref>, this model is not updated while we process the video frames, thus the only state evolving along the video is the mask M t−1 itself.</p><p>First frame. In the video object segmentation task of our interest the mask for the first frame M 0 is given. This is the standard protocol of the benchmarks considered in sections 5 &amp; 6. If only a bounding box is available on the first frame, then the mask could be estimated using grabcut-like techniques <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b66">66]</ref>. RGB image I. Typically a semantic labeller generates pixelwise labels based on the input image (e.g. M = g (I)). We use an augmented semantic labeller with an input layer modified to accept 4 channels (RGB + previous mask) so as to generate outputs based on the previous mask estimate, e.g. M t = f I (I t , M t−1 ). Our approach is general and can leverage any existing semantic labelling architecture. We select the DeepLabv2 architecture with VGG base network <ref type="bibr" target="#b10">[10]</ref>, which is comparable to <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b31">31]</ref>; FusionSeg <ref type="bibr" target="#b28">[28]</ref> uses ResNet. Optical flow F. We use flow in two complementary ways. First, to obtain a better initial estimate of M t we warp M t−1 using the flow F t : M t = f I (I t , w(M t−1 , F t )); we call this "mask warping". Second, we use flow as a direct source of information about the mask M t . As can be seen in <ref type="figure">Figure  I</ref>   2, when the object is moving relative to background, the flow magnitude F t provides a very reasonable estimate of the mask M t . We thus consider using a convnet specifically for mask estimation from flow:</p><formula xml:id="formula_0">M t = f F ( F t , w(M t−1 , F t ))</formula><p>, and merge it with the image-only version by naive averaging</p><formula xml:id="formula_1">M t = 0.5 · f I (I t , . . .) + 0.5 · f F ( F t , . . .) .<label>(1)</label></formula><p>We use the state-of-the-art optical flow estimation method FlowNet2.0 <ref type="bibr" target="#b24">[24]</ref>, which itself is a convnet that computes F t = h (I t−1 , I t ) and is trained on synthetic renderings of flying objects <ref type="bibr" target="#b45">[45]</ref>. For the optical flow magnitude computation we subtract the median motion for each frame, average the magnitude of the forward and backward flow and scale the values per-frame to [0; 255], bringing it to the same range as RGB channels.</p><p>The loss function is the sum of cross-entropy terms over each pixel in the output map (all pixels are equally weighted).</p><p>In our experiments f I and f F are trained independently, via some of the modalities listed in Section 3.2. Our two streams architecture is illustrated in <ref type="figure" target="#fig_3">Figure 3a</ref>.</p><p>We also explored expanding our network to accept 5 input channels (RGB + previous mask + flow magnitude) in one stream:</p><formula xml:id="formula_2">M t = f I+F (I t , F t , w(M t−1 , F t ))</formula><p>, but did not observe much difference in the performance compared to naive averaging, see experiments in Section 5.4.3. Our one stream architecture is illustrated in <ref type="figure" target="#fig_3">Figure 3b</ref>. One stream network is more affordable to train and allows to easily add extra input channels, e.g. providing additionally semantic information about objects.</p><p>Multiple objects. The proposed framework can easily be extended to segmenting multiple objects simultaneously. Instead of having one additional input channel for the previ-  ous frame mask we provide the mask for each object instance in a separate channel, expanding the network to accept 3 + N input channels (RGB + N object masks):</p><formula xml:id="formula_3">M t = f I I t , w(M 1 t−1 , F t ), ..., w(M N t−1 , F t ) ,</formula><p>where N is the number of objects annotated on the first frame.</p><p>For multiple object segmentation we employ a one-stream architecture for the experiments, using optical flow F and semantic segmentation S as additional input channels:</p><formula xml:id="formula_4">M t =f I+F +S I t , F t , S t , w(M 1 t−1 , F t ), ..., w(M N t−1 , F t )</formula><p>. This allows to leverage the appearance model with semantic priors and motion information. See <ref type="figure">Figure 4</ref> for an illustration.</p><p>The one-stream network is trained with multi-class cross entropy loss and is able to segment multiple objects simultaneously, sharing the feature computation for different instances. This allows to avoid a linear increase of the cost with the number of objects. In our preliminary results using a single architecture also pro-vides better results than segmenting multiple objects separately, one at a time; and avoids the need to design a merging strategy amongst overlapping tracks.</p><p>Semantic labels S. To compute the pixel-level semantic labelling S t = h (I t ) we use the state-of-the-art convnet PSPNet <ref type="bibr" target="#b84">[84]</ref>, trained on Pascal VOC12 <ref type="bibr" target="#b16">[16]</ref>. Pascal VOC12 annotates 20 categories, yet we want to track any type of objects. S t can also provide information about unknown category instances by describing them as a spatial mixture of known ones (e.g. a sea lion might looks like a dog torso, and the head of cat). As long as the predictions are consistent through time, S t will provide a useful cue for segmentation. Note that we only use S t for the multi-object segmentation challenge, discussed in Section 6. In the same way as for the optical flow we scale S t to bring all the channels to the same range.</p><p>We additionally experiment with ensembles of different variants, that allows to make the system more robust to the challenges inherent in videos. For our main results on the multiple object segmentation task we consider an ensemble of four models:</p><formula xml:id="formula_5">M t =0.25 · (f I+F +S + f I+F + f I+S + f I ),</formula><p>where we merge the outputs of the models by naive averaging. See Section 6 for more details.</p><p>Temporal coherency. To improve the temporal coherency of the proposed video object segmentation framework we introduce an additional step into the system. Before providing as input the previous frame mask warped with the optical flow w(M t−1 , F t ), we look at frame t − 2 to remove inconsistencies between the predicted masks M t−1 and M t−2 . In particular, we split the mask M t−1 into connected components and remove all components from M t−1 which do not overlap with M t−2 . This way we remove possibly spurious blobs generated by our model in M t−1 . Afterwards we warp the "pruned" mask M t−1 with the optical flow and use w( M t−1 , F t ) as an input to the network. This step is applied only during inference, it mitigates error propagation issues, as well as help generating more temporally coherent results.</p><p>Post-processing. As a final stage of our pipeline, we refine per-frame t the generated mask M t using DenseCRF <ref type="bibr" target="#b32">[32]</ref>. This adjusts small image details that the network might not be able to handle. It is known by practitioners that Den-seCRF is quite sensitive to its parameters and can easily worsen results. We will use our lucid dreams to handle perdataset CRF-tuning too, see Section 3.2.</p><p>We refer to our full f I+F system as LucidTracker, and as LucidTracker − when no temporal coherency or postprocessing steps are used. The usage of S t or model ensemble will be explicitly stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training modalities</head><p>Multiple modalities are available to train a tracker. Trainingfree approaches (e.g. BVS <ref type="bibr" target="#b41">[41]</ref>, SVT <ref type="bibr" target="#b78">[78]</ref>) are fully handcrafted systems with hand-tuned parameters, and thus do not require training data. They can be used as-is over different datasets. Supervised methods can also be trained to generate a dataset-agnostic model that can be applied over different datasets. Instead of using a fixed model for all cases, it is also possible to obtain specialized per-dataset models, either via self-supervision <ref type="bibr" target="#b79">[79,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr">82,</ref><ref type="bibr" target="#b85">85]</ref> or by using the <ref type="figure">Figure 4</ref>: Extension of LucidTracker to multiple objects. The previous frame mask for each object is provided in a separate channel. We additionally explore using optical flow F and semantic segmentation S as additional inputs. See Section 3.1.</p><p>first frame annotation of each video in the dataset as training/tuning set. Finally, inspired by traditional box tracking techniques, we also consider adapting the model weights to the specific video at hand, thus obtaining per-video models. Section 5 reports new results over these four training modalities (training-free, dataset-agnostic, per-dataset, and per-video).</p><p>Our LucidTracker obtains best results when first pretrained on ImageNet, then trained per-dataset using all data from first frame annotations together, and finally fine-tuned per-video for each evaluated sequence. The post-processing DenseCRF stage is automatically tuned per-dataset. The experimental section 5 details the effect of these training stages. Surprisingly, we can obtain reasonable performance even when training from only a single annotated frame (without ImageNet pre-training, i.e. zero pre-training); this results goes against the intuition that convnets require large training data to provide good results.</p><p>Unless otherwise stated, we fine-tune per-video models relying solely on the first frame I 0 and its annotation M 0 . This is in contrast to traditional techniques <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b35">35]</ref> which would update the appearance model at each frame I t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Lucid data dreaming</head><p>To train the function f one would think of using ground truth data for M t−1 and M t (like <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b22">22]</ref>), however such data is expensive to annotate and rare. <ref type="bibr" target="#b6">[6]</ref> thus trains on a set of 30 videos (∼ 2k frames) and requires the model to transfer across multiple tests sets. <ref type="bibr" target="#b31">[31]</ref> side-steps the need for consecutive frames by generating synthetic masks M t−1 from a saliency dataset of ∼ 10k images with their corresponding mask M t . We propose a new data generation strategy to reach better results using only ∼ 100 individual training frames.</p><p>Ideally training data should be as similar as possible to the test data, even subtle differences may affect quality (e.g. training on static images for testing on videos underperforms <ref type="bibr" target="#b65">[65]</ref>). To ensure our training data is in-domain, we propose to generate it by synthesizing samples from the provided annotated frame (first frame) in each target video. This is akin to "lucid dreaming" as we intentionally "dream" the desired data by creating sample images that are plausible hypothetical future frames of the video. The outcome of this process is a large set of frame pairs in the target domain (2.5k pairs per annotation) with known optical flow and mask annotations, see <ref type="figure">Figure 5</ref>.</p><p>Synthesis process. The target domain for a tracker is the set of future frames of the given video. Traditional data augmentation via small image perturbation is insufficient to cover the expect variations across time, thus a task specific strategy is needed. Across the video the tracked object might change in illumination, deform, translate, be occluded, show different point of views, and evolve on top of a dynamic background. All of these aspects should be captured when synthesizing future frames. We achieve this by cutting-out the foreground object, in-painting the background, perturbing both foreground and background, and finally recomposing the scene. This process is applied twice with randomly sampled transformation parameters, resulting in a pair of frames (I τ −1 , I τ ) with known pixel-level ground-truth mask annotations (M τ −1 , M τ ), optical flow F τ , and occlusion regions. The object position in I τ is uniformly sampled, but the changes between I τ −1 , I τ are kept small to mimic the usual evolution between consecutive frames. In more details, starting from an annotated image: 1. Illumination changes: we globally modify the image by randomly altering saturation S and value V (from HSV colour space) via x = a·x b +c, where a ∈ 1±0.05, b ∈ 1±0.3, and c ∈ ±0.07. 2. Fg/Bg split: the foreground object is removed from the image I 0 and a background image is created by inpainting the cut-out area <ref type="bibr" target="#b13">[13]</ref>. 3. Object motion: we simulate motion and shape deformations by applying global translation as well as affine and non-rigid deformations to the foreground object. For I τ −1 the object is placed at any location within the image with a uniform distribution, and in I τ with a translation of ±10% of the object size relative to τ − 1. In both frames we apply random rotation ±30 • , scaling ±15% and thin-plate splines deformations <ref type="bibr" target="#b4">[4]</ref> of ±10% of the object size. 4. Camera motion: We additionally transform the background using affine deformations to simulate camera view changes. We apply here random translation, rotation, and scaling within the same ranges as for the foreground object. 5. Fg/Bg merge: finally (I τ −1 , I τ ) are composed by blending the perturbed foreground with the perturbed background using Poisson matting <ref type="bibr" target="#b64">[64]</ref>. Using the known transforma-tion parameters we also synthesize ground-truth pixel-level mask annotations (M τ −1 , M τ ) and optical flow F τ . <ref type="figure">Figure 5</ref> shows example results. Albeit our approach does not capture appearance changes due to point of view, occlusions, nor shadows, we see that already this rough modelling is effective to train our segmentation models.</p><p>The number of synthesized images can be arbitrarily large. We generate 2.5k pairs per annotated video frame. This training data is, by design, in-domain with regard of the target video. The experimental section 5 shows that this strategy is more effective than using thousands of manually annotated images from close-by domains.</p><p>The same strategy for data synthesis can be employed for multiple object segmentation task. Instead of manipulating a single object we handle multiple ones at the same time, applying independent transformations to each of them. We model occlusion between objects by adding a random depth ordering obtaining both partial and full occlusions in the training set. Including occlusions in the lucid dreams allows to better handle plausible interactions of objects in the future frames. See <ref type="figure">Figure 6</ref> for examples of the generated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Single object segmentation results</head><p>We present here a detailed empirical evaluation on three different datasets for the single object segmentation task: given a first frame labelled with the foreground object mask, the goal is to find the corresponding object pixels in future frames. (Section 6 will discuss the multiple objects case.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>Datasets. We evaluate our method on three video object segmentation datasets: DAVIS <ref type="bibr" target="#b16">16</ref>  <ref type="bibr" target="#b49">[49]</ref>, YouTubeObjects <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b26">26]</ref>, and SegTrack v2 <ref type="bibr" target="#b37">[37]</ref>. The goal is to track an object through all video frames given an object mask in the first frame. These three datasets provide diverse challenges with a mix of high and low resolution web videos, single or multiple salient objects per video, videos with flocks of similar looking instances, longer (∼ 400 frames) and shorter (∼ 10 frames) sequences, as well as the usual video segmentation challenges such as occlusion, fast motion, illumination, view point changes, elastic deformation, etc.</p><p>The DAVIS 16 <ref type="bibr" target="#b49">[49]</ref> video segmentation benchmark consists of 50 full-HD videos of diverse object categories with all frames annotated with pixel-level accuracy, where one single or two connected moving objects are separated from the background. The number of frames in each video varies from 25 to 104.</p><p>YouTubeObjects <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b26">26]</ref> includes web videos from 10 object categories. We use the subset of 126 video sequences with mask annotations provided by <ref type="bibr" target="#b26">[26]</ref> for evaluation, where one single object or a group of objects of the same category are separated from the background. In contrast to DAVIS <ref type="bibr" target="#b16">16</ref> these videos have a mix of static and moving objects. The number of frames in each video ranges from 2 to 401.</p><p>SegTrack v2 <ref type="bibr" target="#b37">[37]</ref> consists of 14 videos with multiple object annotations for each frame. For videos with multiple objects each object is treated as a separate problem, resulting in 24 sequences. The length of each video varies from 21 to 279 frames. The images in this dataset have low resolution and some compression artefacts, making it hard to track the object based on its appearance.</p><p>The main experimental work is done on DAVIS <ref type="bibr" target="#b16">16</ref> , since it is the largest densely annotated dataset out of the three, and provides high quality/high resolution data. The videos for this dataset were chosen to represent diverse challenges, making it a good experimental playground.</p><p>We additionally report on the two other datasets as complementary test set results. Evaluation metric. To measure the accuracy of video object segmentation we use the mean intersection-over-union overlap (mIoU) between the per-frame ground truth object mask and the predicted segmentation, averaged across all video sequences. We have noticed disparate evaluation procedures used in previous work, and we report here a unified evaluation across datasets. When possible, we re-evaluated certain methods using results provided by their authors. For all three datasets we follow the DAVIS 16 evaluation protocol, excluding the first frame from evaluation and using all other frames from the video sequences, independent of object presence in the frame. Training details. For training all the models we use SGD with mini-batches of 10 images and a fixed learning policy with initial learning rate of 10 −3 . The momentum and weight decay are set to 0.9 and 5 · 10 −4 , respectively.</p><p>Models using pre-training are initialized with weights trained for image classification on ImageNet <ref type="bibr" target="#b61">[61]</ref>. We then train per-dataset for 40k iterations with the RGB+Mask branch f I and for 20k iterations for the Flow+Mask f F branch. When using a single stream architecture (Section 5.4.3), we use 40k iterations.</p><p>Models without ImageNet pre-training are initialized using the Xavier (also known as Glorot) random weight initialization strategy <ref type="bibr" target="#b19">[19]</ref>. (The weights are initialized as random draws from a truncated normal distribution with zero mean and standard deviation calculated based on the number of input and output units in the weight tensor, see <ref type="bibr" target="#b19">[19]</ref> for details). The per-dataset training needs to be longer, using 100k iterations for the f I branch and 40k iterations for the f F branch.</p><p>For per-video fine-tuning 2k iterations are used for f I . To keep computing cost lower, the f F branch is kept fix across videos.  <ref type="table" target="#tab_0">Table 1</ref> presents our main result and compares it to previous work. Our full system, LucidTracker, provides the best video segmentation quality across three datasets while being trained on each dataset using only one frame per video (50 frames for DAVIS <ref type="bibr" target="#b16">16</ref> , 126 for YouTubeObjects, 24 for SegTrack v2 ), which is 20× ∼ 1000× less than the top competing methods. Ours is the first method to reach &gt; 75 mIoU on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Key results</head><p>Oracles and baselines. Grabcut oracle computes grabcut <ref type="bibr" target="#b58">[58]</ref> using the ground truth bounding boxes (box oracle). This oracle indicates that on the considered datasets separating foreground from background is not easy, even if a perfect box-level tracker was available. We provide three additional baselines. "Saliency" corres- ponds to using the generic (training-free) saliency method EQCut <ref type="bibr" target="#b0">[1]</ref> over the RGB image I t . "Flow saliency" does the same, but over the optical flow magnitude F t . Results indicate that the objects being tracked are not particularly salient in the image. On DAVIS 16 motion saliency is a strong signal but not on the other two datasets. Saliency methods ignore the first frame annotation provided for the task. We also consider the "Mask warping" baseline which uses optical flow to propagate the mask estimate from t to t + 1 via simple warping M t = w(M t−1 , F t ). The bad results of this baseline indicate that the high quality flow [24] that we use is by itself insufficient to solve the video object segmentation task, and that indeed our proposed convnet does the heavy lifting.</p><p>The large fluctuation of the relative baseline results across the three datasets empirically confirms that each of them presents unique challenges.</p><p>Comparison. Compared to flow propagation methods such as BVS, N15, ObjFlow, and STV, we obtain better results because we build per-video a stronger appearance model of the tracked object (embodied in the fine-tuned model). Compared to convnet learning methods such as VPN, OSVOS, DAVIS <ref type="bibr" target="#b16">16</ref> YouTubeObjects SegTrack v2 1st frame, GT segment 20% 40% 60% 80% 100% <ref type="figure">Figure 7</ref>: LucidTracker single object segmentation qualitative results. Frames sampled along the video duration (e.g. 50%: video middle point). Our model is robust to various challenges, such as view changes, fast motion, shape deformations, and out-of-view scenarios.</p><p>MaskTrack, OnAVOS, we require significantly less training data, yet obtain better results. <ref type="figure">Figure 7</ref> provides qualitative results of LucidTracker across three different datasets. Our system is robust to various challenges present in videos. It handles well camera view changes, fast motion, object shape deformation, outof-view scenarios, multiple similar looking objects and even low quality video. We provide a detailed error analysis in section 5.5.</p><p>Conclusion. We show that top results can be obtained while using less training data. This shows that our lucid dreams leverage the available training data better. We report top results for this task while using only 24∼126 training frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation studies</head><p>In this section we explore in more details how the different ingredients contribute to our results. <ref type="table" target="#tab_2">Table 2</ref> compares the effect of different ingredients in the LucidTracker − training. Results are obtained using RGB and flow, with warping, no CRF, and no temporal coherency;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Effect of training modalities</head><formula xml:id="formula_6">M t =f (I t , w(M t−1 , F t )).</formula><p>Training from a single frame. In the bottom row ("only per-video tuning"), the model is trained per-video without ImageNet pre-training nor per-dataset training, i.e. using a single annotated training frame. Our network is based on VGG16 <ref type="bibr" target="#b10">[10]</ref> and contains ∼ 20M parameters, all effectively learnt from a single annotated image that is augmented to become 2.5k training samples (see <ref type="bibr">Section 4)</ref>. Even with such minimal amount of training data, we still obtain a surprisingly good performance (compare 80.5 on DAVIS <ref type="bibr" target="#b16">16</ref> to others in <ref type="table" target="#tab_0">Table 1</ref>). This shows how effective is, by itself, the proposed training strategy based on lucid dreaming of the data.</p><p>Pre-training &amp; fine-tuning. We see that ImageNet pretraining does provide 2∼5 percent point improvement (depending on the dataset of interest; e.g. 82.0 → 83.7 mIoU on DAVIS <ref type="bibr" target="#b16">16</ref> ). Per-video fine-tuning (after doing per-dataset training) provides an additional 1 ∼ 2 percent point gain (e.g. 82.7 → 83.7 mIoU on DAVIS <ref type="bibr" target="#b16">16</ref> ). Both ingredients clearly contribute to the segmentation results.</p><p>Note that training a model using only per-video tuning takes about one full GPU day per video sequence; making these results insightful but not decidedly practical.</p><p>Preliminary experiments evaluating on DAVIS <ref type="bibr" target="#b16">16</ref> the impact of the different ingredients of our lucid dreaming data generation showed, depending on the exact setup, 3 ∼ 10 percent mIoU points fluctuations between a basic version  (e.g. without non-rigid deformations nor scene re-composition) and the full synthesis process described in Section 4.</p><p>Having a sophisticated data generation process directly impacts the segmentation quality.</p><p>Conclusion. Surprisingly, we discovered that per-video training from a single annotated frame provides already much of the information needed for the video object segmentation task. Additionally using ImageNet pre-training, and per-dataset training, provide complementary gains. <ref type="table" target="#tab_4">Table 3</ref> shows the effect of optical flow on LucidTracker results. Comparing our full system to the "No OF" row, we see that the effect of optical flow varies across datasets, from minor improvement in YouTubeObjects, to major difference in SegTrack v2 . In this last dataset, using mask warping is particularly useful too. We additionally explored tuning the optical flow stream per-video, which resulted in a minor improvement (83.7→83.9 mIoU on DAVIS 16 ). Our "No OF" results can be compared to OSVOS <ref type="bibr" target="#b6">[6]</ref> which does not use optical flow. However OSVOS uses a per-frame mask post-processing based on a boundary detector (trained on further external data), which provides ∼2 percent point gain. Accounting for this, our "No OF" (and no CRF, no temporal coherency) result matches theirs on DAVIS <ref type="bibr" target="#b16">16</ref> and YouTubeObjects despite using significantly less training data (see <ref type="table" target="#tab_0">Table 1</ref>, e.g. 79.8−2 ≈ 78.0 on DAVIS <ref type="bibr" target="#b16">16</ref> ). <ref type="table" target="#tab_5">Table 4</ref> shows the effect of using different optical flow estimation methods. For LucidTracker results, FlowNet2.0 <ref type="bibr" target="#b24">[24]</ref> was employed. We also explored using EpicFlow <ref type="bibr" target="#b56">[56]</ref>, as in <ref type="bibr" target="#b31">[31]</ref>. <ref type="table" target="#tab_5">Table 4</ref> indicates that employing a robust optical flow estimation across datasets is crucial to the performance (FlowNet2.0 provides ∼ 1.5 − 15 points gain on each dataset). We found EpicFlow to be brittle when going across different datasets, providing improvement for DAVIS <ref type="bibr" target="#b16">16</ref>    Conclusion. The results show that flow provides a complementary signal to RGB image only and having a robust optical flow estimation across datasets is crucial. Despite its simplicity our fusion strategy (f I + f F ) provides gains on all datasets, and leads to competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Effect of optical flow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Effect of CRF tuning</head><p>As a final stage of our pipeline, we refine the generated mask using DenseCRF <ref type="bibr" target="#b32">[32]</ref> per frame. This captures small image details that the network might have missed. It is known by practitioners that DenseCRF is quite sensitive to its parameters and can easily worsen results. We use our lucid dreams to enable automatic per-dataset CRF-tuning. Following <ref type="bibr" target="#b10">[10]</ref> we employ grid search scheme for tuning CRF parameters. Once the per-dataset model is trained, we apply it over a subset of its training set (5 random images from the lucid dreams per video sequence), apply Den-seCRF with the given parameters over this output, and then compare to the lucid dream ground truth.</p><p>The impact of the tuned parameter of DenseCRF postprocessing is shown in <ref type="table" target="#tab_7">Table 5</ref> and <ref type="figure">Figure 8</ref>. <ref type="table" target="#tab_7">Table 5</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Additional experiments</head><p>Other than adding or removing ingredients, as in Section 5.3, we also want to understand how the training data itself affects the obtained results.  Conclusion. These results show that, when using RGB information (I t ), increasing the number of training videos does not improve the resulting quality of our system. Even within a dataset, properly using the training sample(s) from within each video matters more than collecting more videos to build a larger training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Generalization across videos</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Generalization across datasets</head><p>Section 5.4.1 has explored the effect of changing the volume of training data within one dataset,  <ref type="table" target="#tab_10">Table 7</ref>: Generalization across datasets. Results with underline are the best per dataset, and in italic are the second best per dataset (ignoring all-in-one setup). We observe a significant quality gap between training from the target videos, versus training from other datasets; see Section 5.4.2.</p><p>The best performance is obtained when training on the first frames of the target set. There is a noticeable ∼10 percent points drop when moving to the second best choice (e.g. 80.9 → 67.0 for DAVIS <ref type="bibr" target="#b16">16</ref> ). Interestingly, when putting all the datasets together for training ("all-in-one" row, a datasetagnostic model) the results degrade, reinforcing the idea that "just adding more data" does not automatically make the performance better.</p><p>Conclusion. Best results are obtained when using training data that focuses on the test video sequences, using similar datasets or combining multiple datasets degrades the performance for our system.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Experimenting with the convnet architecture</head><p>Section 3.1 and <ref type="figure" target="#fig_3">Figure 3</ref> described two possible architectures to handle I t and F t . Previous experiments are all based on the two streams architecture. <ref type="table" target="#tab_12">Table 8</ref> compares two streams versus one stream, where the network to accepts 5 input channels (RGB + previous mask + flow magnitude) in one stream:</p><formula xml:id="formula_7">M t = f I+F (I t , F t , w(M t−1 , F t ))</formula><p>. Results are obtained using a base model with RGB and optical flow (no warping, no CRF, no temporal coherency), ImageNet pre-training, per-dataset training, and no per-video tuning.</p><p>We observe that both one stream and two stream architecture with naive averaging perform on par. Using a one stream network makes the training more affordable and allows more easily to expand the architecture with additional input channels.</p><p>Conclusion. The lighter one stream network performs as well as a network with two streams. We will thus use the one stream architecture in Section 6. <ref type="table" target="#tab_14">Table 9</ref> presents an expanded evaluation on DAVIS <ref type="bibr" target="#b16">16</ref> using evaluation metrics proposed in <ref type="bibr" target="#b49">[49]</ref>. Three measures are used: region similarity in terms of intersection over union (J), contour accuracy (F, higher is better), and temporal instability of the masks (T, lower is better). We outperform the competitive methods of <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b6">6]</ref> on all three measures. <ref type="table" target="#tab_0">Table 10</ref> reports the per-attribute based evaluation as defined in DAVIS <ref type="bibr" target="#b16">16</ref> . LucidTracker is best on all 15 video attribute categories. This shows that our LucidTracker can handle the various video challenges present in DAVIS <ref type="bibr" target="#b16">16</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Error analysis</head><p>We present the per-sequence and per-frame results of LucidTracker over DAVIS <ref type="bibr" target="#b16">16</ref> in <ref type="figure" target="#fig_5">Figure 10</ref>. On the whole we observe that the proposed approach is quite robust, most video sequences reach an average performance above 80 mIoU.</p><p>However, by looking at per-frame results for each video (blue dots in <ref type="figure" target="#fig_5">Figure 10</ref>) one can see several frames where our approach has failed (IoU less than 50) to correctly track the object. Investigating closely those cases we notice conditions where LucidTracker is more likely to fail. The same behaviour was observed across all three datasets. A few representatives of failure cases are visualized in <ref type="figure">Figure 9</ref>.</p><p>Since we are using only the mask annotation of the first frame for training the tracker, a clear failure case is caused by dramatic view point changes of the object from its first frame appearance, as in row 5 of <ref type="figure">Figure 9</ref>. Performing online adaptation every certain time step while exploiting the previous frame segments for data synthesis and marking unsure regions as ignore for training, similarly to <ref type="bibr" target="#b74">[74]</ref>, might resolve the potential problems caused by relying only on the first frame mask. The proposed approach also underperforms when recovering from occlusions: it might takes several frames for the full object mask to re-appear (rows 1-3 in <ref type="figure">Figure 9</ref>). This is mainly due to the convnet having learnt to follow-up the previous frame mask. Augmenting the lucid dreams with plausible occlusions might help mitigate this case. Another failure case occurs when two similar looking objects cross each other, as in row 6 in <ref type="figure">Figure 9</ref>. Here both cues: the previous frame guidance and learnt via per-video tuning appearance, are no longer discriminative to correctly continue propagating the mask.</p><p>We also observe that the LucidTracker struggles to track the fine structures or details of the object, e.g. wheels of the bicycle or motorcycle in rows 1-2 in <ref type="figure">Figure 9</ref>. This is the issue of the underlying choice of the convnet architecture, due to the several pooling layers the spatial resolution is lost and hence the fine details of the object are missing. This issue can be mitigated by switching to more recent semantic labelling architectures (e.g. <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b9">9]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>LucidTracker shows robust performance across different videos. However, a few failure cases were observed due to the underlying convnet architecture, its training, or limited visibility of the object in the first frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Multiple object segmentation results</head><p>We present here an empirical evaluation of LucidTracker for multiple object segmentation task: given a first frame labelled with the masks of several object instances, one aims to find the corresponding masks of objects in future frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental setup</head><p>Dataset. For the multiple object segmentation task we use the 2017 DAVIS Challenge on Video Object Segmentation 3 <ref type="bibr" target="#b54">[54]</ref> (DAVIS 17 ). Compared to DAVIS <ref type="bibr" target="#b16">16</ref> this is a larger, more challenging dataset, where the video sequences have multiple objects in the scene. Videos that have more than one visible object in DAVIS <ref type="bibr" target="#b16">16</ref> have been re-annotated (the objects were divided by semantics) and the train and val sets   Evaluation metric. The accuracy of multiple object segmentation is evaluated using the region (J) and boundary (F) measures proposed by the organisers of the challenge. The average of J and F measures is used as overall performance score (denoted as global mean in the tables). Please refer to <ref type="bibr" target="#b54">[54]</ref> for more details about the evaluation protocol.</p><p>Training details. All experiments in this section are done using the single stream architecture discussed in sections 3.1 and 5.4.3. For training the models we use SGD with minibatches of 10 images and a fixed learning policy with initial learning rate of 10 −3 . The momentum and weight decay are set to 0.9 and 5·10 −4 , respectively. All models are initialized DAVIS <ref type="bibr" target="#b16">16</ref> YouTubeObjects SegTrack v2 1st frame, GT segment 20% 40% 60% 80% 100% <ref type="figure">Figure 9</ref>: Failure cases. Frames sampled along the video duration (e.g. 50%: video middle point). For each dataset we show 2 out of 5 worst results (based on mIoU over the video). with weights trained for image classification on ImageNet <ref type="bibr" target="#b61">[61]</ref>. We then train per-video for 40k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Key results</head><p>Tables 11 and 12 presents the results of the 2017 DAVIS Challenge on test-dev and test-challenge sets <ref type="bibr" target="#b53">[53]</ref>.</p><p>Our main results for the multi-object segmentation challenge are obtained via an ensemble of four different models (f I , f I+F , f I+S , f I+F +S ), see Section 3.1.</p><p>The proposed system, LucidTracker, provides the best segmentation quality on the test-dev set and shows competitive performance on the test-challenge set, holding the second place in the competition. The full system is trained using the standard ImageNet pre-training initialization, Pascal VOC12 semantic annotations for the S t input (∼ 10k annotated images), and one annotated frame per test video, 30 frames total on each test set. As discussed in Section 6.3, even without S t LucidTracker obtains competitive results (less than 1 percent point difference, see <ref type="table" target="#tab_0">Table 13</ref> for details).</p><p>The top entry lixx <ref type="bibr" target="#b38">[38]</ref> uses a deeper convnet model (ImageNet pre-trained ResNet), a similar segmentation architecture, trains it over external segmentation data (using ∼ 120k pixel-level annotated images from MS-COCO and Pascal VOC for pre-training, and akin to <ref type="bibr" target="#b6">[6]</ref> fine-tuning on the DAVIS 17 train and val sets, ∼ 10k annotated frames), 1st frame, GT segment 20% 40% 60% 80% 100% <ref type="figure">Figure 11</ref>: LucidTracker qualitative results on DAVIS 17 , test-dev set. Frames sampled along the video duration (e.g. 50%: video middle point). The videos are chosen with the highest mIoU measure.</p><p>and extends it with a box-level object detector (trained over MS-COCO and Pascal VOC, ∼ 500k bounding boxes) and a box-level object re-identification model trained over ∼ 60k box annotations (on both images and videos). We argue that our system reaches comparable results with a significantly lower amount of training data. <ref type="figure">Figure 11</ref> provides qualitative results of LucidTracker on the test-dev set. The video results include successful handling of multiple objects, full and partial occlusions, distractors, small objects, and out-of-view scenarios. Conclusion. We show that top results for multiple object segmentation can be achieved via our approach that focuses on exploiting as much as possible the available annotation on the first video frame, rather than relying heavily on large external training data. We see that adding extra information (channels) to the system, either optical flow magnitude or semantic segmentation, or both, does provide 1 ∼ 2 percent point improvement. The results show that leveraging semantic priors and motion information provides a complementary signal to RGB image and both ingredients contribute to the segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation study</head><p>Combining in ensemble four different models (f I+F +S + f I+F + f I+S + f I ) allows to enhance the results even further, bringing 2.7 percent point gain (62.0 vs. 64.7 global mean). Excluding the models which use semantic information (f I+F +S and f I+S ) from the ensemble results only in a minor drop in the performance (64.2 vs. 64.7 global mean). This shows that the competitive results can be achieved even with the system trained only with one pixel-level mask annotation per video, without employing extra annotations from Pascal VOC12.</p><p>Our lucid dreams enable automatic CRF-tuning (see Section 5.3.3) which allows to further improve the results (64.7 → 65.2 global mean). Employing the proposed temporal coherency step (see Section 3.1) during inference brings an additional performance gain (65.2→66.6 global mean). Conclusion. The results show that both flow and semantic priors provide a complementary signal to RGB image only. Despite its simplicity our ensemble strategy provides additional gain and leads to competitive results. Notice that even without the semantic segmentation signal S t our ensemble result is competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Error analysis</head><p>We present the per-sequence results of LucidTracker on DAVIS <ref type="bibr" target="#b17">17</ref> in <ref type="figure" target="#fig_1">Figure 12</ref> (per frame results not available from evaluation server). We observe that this dataset is significantly more challenging than DAVIS <ref type="bibr" target="#b16">16</ref> (compare to <ref type="figure" target="#fig_5">Figure  10</ref>), with only 1 /3 of the test videos above 80 mIoU. This shows that multiple object segmentation is a much more challenging task than segmenting a single object.</p><p>The failure cases discussed in Section 5.5 still apply to the multiple objects case. Additionally, on DAVIS 17 we ob-    <ref type="figure" target="#fig_3">Figure 13</ref>: LucidTracker failure cases on DAVIS 17 , test-dev set. Frames sampled along the video duration (e.g. 50%: video middle point). We show 2 results mIoU over the video below 50.</p><p>serve a clear failure case when segmenting similar looking object instances, where the object appearance is not discriminative to correctly track the object, resulting in label switches or bleeding of the label to other look-alike objects. <ref type="figure" target="#fig_3">Figure 13</ref> illustrates this case. This issue could be mitigated by using object level instance identification modules, like <ref type="bibr" target="#b38">[38]</ref>, or by changing the training loss of the model to more severely penalize identity switches. Conclusion. In the multiple object case the LucidTracker results remain robust across different videos. The overall results being lower than for the single object segmentation case, there is more room for future improvement in the multiple object pixel-level segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have described a new convnet-based approach for pixellevel object segmentation in videos. In contrast to previous work, we show that top results for single and multiple object segmentation can be achieved without requiring external training datasets (neither annotated images nor videos). Even more, our experiments indicate that it is not always beneficial to use additional training data, synthesizing training samples close to the test domain is more effective than adding more training samples from related domains. Our extensive analysis decomposed the ingredients that contribute to our improved results, indicating that our new training strategy and the way we leverage additional cues such as semantic and motion priors are key.</p><p>Showing that training a convnet for video object segmentation can be done with only few (∼ 100) training samples changes the mindset regarding how much general knowledge about objects is required to approach this problem <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b28">28]</ref>, and more broadly how much training data is required to train large convnets depending on the task at hand.</p><p>We hope these new results will fuel the ongoing evolution of convnet techniques for single and multiple object segmentation in videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t Inputs M t−1 (shown over I t )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Data flow examples. I t , F t , M t−1 are the inputs, M t is the resulting output. Green boundaries outline the ground truth segments. Red overlay indicates M t−1 , M t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Two streams architecture, where image I t and optical flow information F t are used to update mask M t−1 into M t . See equation 1. (b) One stream architecture, where 5 input channels: image I t , optical flow information F t and mask M t−1 are used to estimate mask M t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Overview of the proposed one and two streams architectures. See Section 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Original image I 0 and mask annotation M 0 Generated image I τ −1 Generated image I τ Generated flow magnitude F τ Lucid data dreaming examples. From one annotated frame we generate pairs of images (I τ −1 , I τ ) that are plausible future video frames, with known optical flow (F τ ) and masks (green boundaries). Note the inpainted background and foreground/background deformations.(a) Original image I 0 and mask annotation M 0 (b) Generated image I τ and mask M τ (c) Generated flow magnitude F τ Lucid data dreaming examples with multiple objects. From one annotated frame we generate a plausible future video frame (I τ ), with known optical flow (F τ ) and mask (M τ ). All training parameters are chosen based on DAVIS 16 results. We use identical parameters on YouTubeObjects and SegTrack v2 , showing the generalization of our approach. It takes~3.5h to obtain each per-video model, including data generation, per-dataset training, per-video fine-tuning and per-dataset grid search of CRF parameters (averaged over DAVIS 16 , amortising the per-dataset training time over all videos). At test time our LucidTracker runs at~5s per frame, including the optical flow estimation with FlowNet2.0 [24] (~0.5s) and CRF post-processing [32] (~2s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Per-sequence results on DAVIS<ref type="bibr" target="#b16">16</ref> .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of video object segmentation results across three datasets. Numbers in italic are reported on subsets of DAVIS<ref type="bibr" target="#b16">16</ref> . Our LucidTracker consistently improves over previous results, see Section 5.2.</figDesc><table><row><cell>Ignores 1st frame Uses 1st frame</cell><cell>annotation annotation</cell><cell># training Flow images F DAVIS 16 YoutbObjs SegTrck v2 Dataset, mIoU 0 % 45.1 55.3 56.1 0 74.2 % 67.3 67.6 0 % 32.7 40.7 22.2 0 ! 64.1 --0 ! --69.1 MP-Net [69]~22.5k ! 69.7 Method Box oracle [31] Grabcut oracle [31] Saliency NLC [17] TRS [81] --Flow saliency 0 ! 70.7 36.3 35.9 FusionSeg [28]~95k ! 71.5 67.9 -LVO [70]~35k ! 75.9 -57.3 PDB [62]~18k --% 77.2 Mask warping 0 ! 32.1 43.2 42.0 FCP [50] 0 ! 63.1 --BVS [41] 0 % 66.5 59.7 58.4 N15 [42] 0 ! --69.6 ObjFlow [71] 0 ! 71.1 70.1 67.5 STV [78] 0 ! 73.6 --VPN [29]~2.3k % 75.0 --OSVOS [6]~2.3k % 79.8 72.5 65.4 MaskTrack [31]~11k ! 80.3 72.6 70.3 PReMVOS [25]~145k ! 84.9 --OnAVOS [74]~120k % 86.1 --VideoGCRF [7]~120k % 86.5 --LucidTracker 24~126 ! 86.6 77.3 78.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Ablation study of training modalities. ImageNet</cell></row><row><cell>pre-training and per-video tuning provide additional im-</cell></row><row><cell>provement over per-dataset training. Even with one frame</cell></row><row><cell>annotation for only per-video tuning we obtain good per-</cell></row><row><cell>formance. See Section 5.3.1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and SegTrack v2 (∼ 2 − 5 points gain), but underperforming for YouTubeObjects (74.7→71.3 mIoU).</figDesc><table><row><cell>Variant</cell><cell>I F</cell><cell>warp. w</cell><cell cols="2">Dataset, mIoU DAVIS 16 YoutbObjs SegTrck v2</cell></row><row><cell cols="3">LucidTracker ! ! ! LucidTracker − ! ! ! No warping ! ! % No OF ! % % OF only % ! !</cell><cell>86.6 83.7 82.0 78.0 74.5</cell><cell>77.3 76.2 74.6 74.7 43.1</cell><cell>78.0 76.8 70.5 61.8 55.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of flow ingredients. Flow complements image only results, with large fluctuations across datasets. See Section 5.3.2.</figDesc><table><row><cell>Variant</cell><cell>Optical</cell><cell></cell><cell>Dataset, mIoU</cell><cell></cell></row><row><cell></cell><cell>flow</cell><cell cols="3">DAVIS 16 YoutbObjs SegTrck v2</cell></row><row><cell></cell><cell>FlowNet2.0</cell><cell>83.7</cell><cell>76.2</cell><cell>76.8</cell></row><row><cell>LucidTracker −</cell><cell>EpicFlow</cell><cell>80.2</cell><cell>71.3</cell><cell>67.0</cell></row><row><cell></cell><cell>No flow</cell><cell>78.0</cell><cell>74.7</cell><cell>61.8</cell></row><row><cell>No ImageNet pre-training</cell><cell>FlowNet2.0 EpicFlow No flow</cell><cell>82.0 80.0 76.7</cell><cell>74.3 72.3 71.4</cell><cell>71.2 68.8 63.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Effect of optical flow estimation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>indicates that without per-dataset tuning DenseCRF is underperforming. Our automated tuning procedure allows to obtain consistent gains without the need for case-by-case manual tuning. Conclusion. Using default DenseCRF parameters would degrade performance. Our lucid dreams enable automatic per-dataset CRF-tuning which allows to further improve the results.</figDesc><table><row><cell>Method</cell><cell>CRF parameters</cell><cell cols="3">Dataset, mIoU DAVIS 16 YoutbObjs SegTrck v2</cell></row><row><cell>LucidTracker −</cell><cell>-</cell><cell>83.7</cell><cell>76.2</cell><cell>76.8</cell></row><row><cell>LucidTracker</cell><cell>default</cell><cell>84.2</cell><cell>75.5</cell><cell>72.2</cell></row><row><cell cols="2">LucidTracker tuned per-dataset</cell><cell>84.8</cell><cell>76.2</cell><cell>77.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Effect of CRF tuning (LucidTracker without temporal coherency). Without the automated per-dataset tuning DenseCRF will under-perform.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 explores</head><label>6</label><figDesc>The reported numbers are thus comparable withinTable 6, but not across to the other tables in the paper.Table 6 reports results with varying number of training videos and with/without including the first frames of each test video for per-dataset training. When excluding the test set first frames, the image frames used for training are separate from the test videos; and we are thus operating across (related) domains. When including the test set first frames, we operate in the usual LucidTracker mode, where the first frame from each test video is used to build the per-dataset training set. Comparing the top and bottom parts of the table, we see that when the annotated images from the test set video sequences are not included, segmentation quality drops drastically (e.g. 68.7 → 36.4 mIoU). Conversely, on subset of videos for which the first frame annotation is used for training, the quality is much higher and improves as the training samples become more and more specific (in-domain) to the target video (65.4 → 78.3 mIoU). Adding extra videos for training does not improve the performance. It is better (68.7 → 78.3 mIoU) to have 15 models each trained and evaluated on a single video (row top-1-1) than having one model trained over 15 test videos (row top-15-1). Training with an additional frame from each video (we added the last frame of each train video) significantly boosts the resulting within-video quality (e.g. row top-30-2 65.4 → 74.3 mIoU), because the training samples cover better the test domain.</figDesc><table><row><cell>no CRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with CRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>no CRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with CRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1st frame, GT segment</cell><cell>20%</cell><cell></cell><cell>40%</cell><cell>60%</cell><cell>80%</cell><cell>100%</cell></row><row><cell cols="7">Figure 8: Effect of CRF tuning. The shown DAVIS 16 videos have the highest margin between with and without CRF post-</cell></row><row><cell cols="3">processing (based on mIoU over the video).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Training set</cell><cell cols="2"># training # frames videos per video</cell><cell>mIoU</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>1</cell><cell>78.3</cell><cell></cell><cell></cell></row><row><cell>Includes 1st frames from test set</cell><cell>2 15 30</cell><cell>1 1 1</cell><cell>75.4 68.7 65.4</cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell>2</cell><cell>74.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>1</cell><cell>11.6</cell><cell></cell><cell></cell></row><row><cell>Excludes 1st frames</cell><cell>15</cell><cell>1</cell><cell>36.4</cell><cell></cell><cell></cell></row><row><cell>from test set</cell><cell>30</cell><cell>1</cell><cell>41.7</cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell>2</cell><cell>48.4</cell><cell></cell><cell></cell></row></table><note>the effect of segmentation quality as a func- tion of the number of training samples. To see more directly the training data effects we use a base model with RGB im- age I t only (no flow F, no CRF, no temporal coherency), and per-dataset training (no ImageNet pre-training, no per- video fine-tuning). We evaluate on two disjoint subsets of 15 DAVIS 16 videos each, where the first frames for per-dataset training are taken from only one subset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Varying the number of training videos. A smaller training set closer to the target domain is better than a larger one. See Section 5.4.1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>compares res-</cell></row><row><cell>ults when using different datasets for training. Results are</cell></row><row><cell>obtained using a base model with RGB and flow (M t =</cell></row><row><cell>f (I</cell></row></table><note>t , M t−1 ), no warping, no CRF, no temporal coherency), ImageNet pre-training, per-dataset training, and no per-video tuning to accentuate the effect of the training dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Experimenting with the convnet architecture. See Section 5.4.3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Mean ↑ Recall ↑ Decay ↓ Mean ↑ Recall ↑ Decay ↓ Mean ↓</figDesc><table><row><cell></cell><cell>Method</cell><cell># training images</cell><cell>Flow F</cell><cell></cell><cell>Region, J</cell><cell></cell><cell></cell><cell>DAVIS 16 Boundary, F</cell><cell></cell><cell>Temporal stability, T</cell></row><row><cell>Ignores 1st frame annotation Uses 1st frame annotation</cell><cell cols="2">Box oracle [31] Grabcut oracle [31] Saliency NLC [17] MP-Net [69]~22.5k 0 0 0 0 Flow saliency 0 FusionSeg [28]~95k LVO [70]~35k PDB [62]~18k Mask warping 0 FCP [50] 0 BVS [41] 0 ObjFlow [71] 0 STV [78] 0 VPN [29]~2.3k OSVOS [6]~2.3k MaskTrack [31]~11k PReMVOS [25]~145k OnAVOS [74]~120k VideoGCRF [7]~120k LucidTracker 50</cell><cell>% % % ! ! ! ! ! % ! ! % ! ! % % ! ! % % !</cell><cell>45.1 67.3 32.7 64.1 69.7 70.7 71.5 75.9 77.2 32.1 63.1 66.5 71.1 73.6 75.0 79.8 80.3 84.9 86.1 86.5 86.6</cell><cell>39.7 76.9 22.6 73.1 82.9 83.2 -89.1 90.1 25.5 77.8 76.4 80.0 --93.6 93.5 96.1 96.1 -97.3</cell><cell>-0.7 1.5 -0.2 8.6 5.6 6.7 -0.0 0.9 31.7 3.1 26.0 22.7 --14.9 8.9 8.8 5.2 -5.3</cell><cell>21.4 65.8 26.9 59.3 66.3 69.7 -72.1 74.5 36.3 54.6 65.6 67.9 72.0 72.4 80.6 75.8 88.6 84.9 -84.8</cell><cell>6.7 77.2 10.3 65.8 78.3 82.9 -83.4 84.4 23.0 60.4 77.4 78.0 --92.6 88.2 94.7 89.7 -93.1</cell><cell>1.8 2.9 0.9 8.6 6.7 7.9 -1.3 -0.2 32.8 3.9 23.6 24.0 --15.0 9.5 9.8 5.8 -7.5</cell><cell>1.0 34.0 32.8 35.8 68.6 48.2 -26.5 29.1 8.4 28.5 31.6 22.1 -29.5 37.6 18.3 19.7 19.0 -15.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Comparison of video object segmentation results on DAVIS 16 benchmark. Numbers in italic are computed based on subsets of DAVIS 16 . Our LucidTracker improves over previous results.</figDesc><table><row><cell>Attribute</cell><cell cols="5">Method BVS [41] ObjFlow [71] OSVOS [6] MaskTrack [31] LucidTracker</cell></row><row><cell cols="2">Appearance change 0.46</cell><cell>0.54</cell><cell>0.81</cell><cell>0.76</cell><cell>0.84</cell></row><row><cell>Background clutter</cell><cell>0.63</cell><cell>0.68</cell><cell>0.83</cell><cell>0.79</cell><cell>0.86</cell></row><row><cell>Camera-shake</cell><cell>0.62</cell><cell>0.72</cell><cell>0.78</cell><cell>0.78</cell><cell>0.88</cell></row><row><cell>Deformation</cell><cell>0.7</cell><cell>0.77</cell><cell>0.79</cell><cell>0.78</cell><cell>0.87</cell></row><row><cell cols="2">Dynamic background 0.6</cell><cell>0.67</cell><cell>0.74</cell><cell>0.76</cell><cell>0.82</cell></row><row><cell>Edge ambiguity</cell><cell>0.58</cell><cell>0.65</cell><cell>0.77</cell><cell>0.74</cell><cell>0.82</cell></row><row><cell>Fast-motion</cell><cell>0.53</cell><cell>0.55</cell><cell>0.76</cell><cell>0.75</cell><cell>0.85</cell></row><row><cell cols="2">Heterogeneous object 0.63</cell><cell>0.66</cell><cell>0.75</cell><cell>0.79</cell><cell>0.85</cell></row><row><cell>Interacting objects</cell><cell>0.63</cell><cell>0.68</cell><cell>0.75</cell><cell>0.77</cell><cell>0.85</cell></row><row><cell>Low resolution</cell><cell>0.59</cell><cell>0.58</cell><cell>0.77</cell><cell>0.77</cell><cell>0.84</cell></row><row><cell>Motion blur</cell><cell>0.58</cell><cell>0.6</cell><cell>0.74</cell><cell>0.74</cell><cell>0.83</cell></row><row><cell>Occlusion</cell><cell>0.68</cell><cell>0.66</cell><cell>0.77</cell><cell>0.77</cell><cell>0.84</cell></row><row><cell>Out-of-view</cell><cell>0.43</cell><cell>0.53</cell><cell>0.72</cell><cell>0.71</cell><cell>0.84</cell></row><row><cell>Scale variation</cell><cell>0.49</cell><cell>0.56</cell><cell>0.74</cell><cell>0.73</cell><cell>0.81</cell></row><row><cell>Shape complexity</cell><cell>0.67</cell><cell>0.69</cell><cell>0.71</cell><cell>0.75</cell><cell>0.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>DAVIS 16 per-attribute evaluation. LucidTracker improves across all video object segmentation challenges.were extended with more sequences. In addition, two other test sets (test-dev and test-challenge) were introduced. The complexity of the videos has increased with more distractors, occlusions, fast motion, smaller objects, and fine struc-tures. Overall, DAVIS 17 consists of 150 sequences, totalling 10 474 annotated frames and 384 objects.We evaluate our method on two test sets, the test-dev and test-challenge sets, each consists of 30 video sequences, on average ∼ 3 objects per sequence, the length of the sequences is ∼ 70 frames. For both test sets only the masks on the first frames are made public, the evaluation is done via an evaluation server. Our experiments and ablation studies are done on the test-dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13</head><label>13</label><figDesc>explores in more details how the different ingredients contribute to our results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Mean ↑ Recall ↑ Decay ↓ Mean ↑ Recall ↑ Decay ↓</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DAVIS 17 , test-dev set</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Rank</cell><cell>Global mean</cell><cell>↑</cell><cell>Region, J</cell><cell></cell><cell></cell><cell>Boundary, F</cell><cell></cell></row><row><cell>sidc</cell><cell>10</cell><cell>45.8</cell><cell>43.9</cell><cell>51.5</cell><cell>34.3</cell><cell>47.8</cell><cell>53.6</cell><cell>36.9</cell></row><row><cell>YXLKJ</cell><cell>9</cell><cell>49.6</cell><cell>46.1</cell><cell>49.1</cell><cell>22.7</cell><cell>53.0</cell><cell>56.5</cell><cell>22.3</cell></row><row><cell>haamooon [59]</cell><cell>8</cell><cell>51.3</cell><cell>48.8</cell><cell>56.9</cell><cell>12.2</cell><cell>53.8</cell><cell>61.3</cell><cell>11.8</cell></row><row><cell>Fromandtozh [83]</cell><cell>7</cell><cell>55.2</cell><cell>52.4</cell><cell>58.4</cell><cell>18.1</cell><cell>57.9</cell><cell>66.1</cell><cell>20.0</cell></row><row><cell>ilanv [60]</cell><cell>6</cell><cell>55.8</cell><cell>51.9</cell><cell>55.7</cell><cell>17.6</cell><cell>59.8</cell><cell>65.8</cell><cell>18.9</cell></row><row><cell>voigtlaender [73]</cell><cell>5</cell><cell>56.5</cell><cell>53.4</cell><cell>57.8</cell><cell>19.9</cell><cell>59.6</cell><cell>65.4</cell><cell>19.0</cell></row><row><cell>lalalafine123</cell><cell>4</cell><cell>57.4</cell><cell>54.5</cell><cell>61.3</cell><cell>24.4</cell><cell>60.2</cell><cell>68.8</cell><cell>24.6</cell></row><row><cell>wangzhe</cell><cell>3</cell><cell>57.7</cell><cell>55.6</cell><cell>63.2</cell><cell>31.7</cell><cell>59.8</cell><cell>66.7</cell><cell>37.1</cell></row><row><cell>lixx [38]</cell><cell>2</cell><cell>66.1</cell><cell>64.4</cell><cell>73.5</cell><cell>24.5</cell><cell>67.8</cell><cell>75.6</cell><cell>27.1</cell></row><row><cell>LucidTracker</cell><cell>1</cell><cell>66.6</cell><cell>63.4</cell><cell>73.9</cell><cell>19.5</cell><cell>69.9</cell><cell>80.1</cell><cell>19.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Comparison of video object segmentation results on DAVIS 17 , test-dev set. Our LucidTracker shows top performance.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DAVIS 17 , test-challenge set</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Rank</cell><cell>Global mean</cell><cell>↑</cell><cell>Region, J</cell><cell></cell><cell></cell><cell>Boundary, F</cell><cell></cell></row><row><cell>zwrq0</cell><cell>10</cell><cell>53.6</cell><cell>50.5</cell><cell>54.9</cell><cell>28.0</cell><cell>56.7</cell><cell>63.5</cell><cell>30.4</cell></row><row><cell>Fromandtozh [83]</cell><cell>9</cell><cell>53.9</cell><cell>50.7</cell><cell>54.9</cell><cell>32.5</cell><cell>57.1</cell><cell>63.2</cell><cell>33.7</cell></row><row><cell>wasidennis</cell><cell>8</cell><cell>54.8</cell><cell>51.6</cell><cell>56.3</cell><cell>26.8</cell><cell>57.9</cell><cell>64.8</cell><cell>28.8</cell></row><row><cell>YXLKJ</cell><cell>7</cell><cell>55.8</cell><cell>53.8</cell><cell>60.1</cell><cell>37.7</cell><cell>57.8</cell><cell>62.1</cell><cell>42.9</cell></row><row><cell>cjc [12]</cell><cell>6</cell><cell>56.9</cell><cell>53.6</cell><cell>59.5</cell><cell>25.3</cell><cell>60.2</cell><cell>67.9</cell><cell>27.6</cell></row><row><cell>lalalafine123</cell><cell>6</cell><cell>56.9</cell><cell>54.8</cell><cell>60.7</cell><cell>34.4</cell><cell>59.1</cell><cell>66.7</cell><cell>36.1</cell></row><row><cell>voigtlaender [73]</cell><cell>5</cell><cell>57.7</cell><cell>54.8</cell><cell>60.8</cell><cell>31.0</cell><cell>60.5</cell><cell>67.2</cell><cell>34.7</cell></row><row><cell>haamooon [59]</cell><cell>4</cell><cell>61.5</cell><cell>59.8</cell><cell>71.0</cell><cell>21.9</cell><cell>63.2</cell><cell>74.6</cell><cell>23.7</cell></row><row><cell>vantam299 [36]</cell><cell>3</cell><cell>63.8</cell><cell>61.5</cell><cell>68.6</cell><cell>17.1</cell><cell>66.2</cell><cell>79.0</cell><cell>17.6</cell></row><row><cell>LucidTracker</cell><cell>2</cell><cell>67.8</cell><cell>65.1</cell><cell>72.5</cell><cell>27.7</cell><cell>70.6</cell><cell>79.8</cell><cell>30.2</cell></row><row><cell>lixx [38]</cell><cell>1</cell><cell>69.9</cell><cell>67.9</cell><cell>74.6</cell><cell>22.5</cell><cell>71.9</cell><cell>79.1</cell><cell>24.1</cell></row></table><note>Mean ↑ Recall ↑ Decay ↓ Mean ↑ Recall ↑ Decay ↓</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>Comparison of video object segmentation results on DAVIS 17 , test-challenge set. Our LucidTracker shows competitive performance, holding the second place in the competition.</figDesc><table><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mIoU</cell><cell>20 40 60 80</cell><cell>golf</cell><cell>planes-crossing</cell><cell>skate-jump</cell><cell>man-bike</cell><cell>horsejump-stick</cell><cell>deer</cell><cell>rollercoaster</cell><cell>salsa</cell><cell>giant-slalom</cell><cell>subway</cell><cell>cats-car</cell><cell>carousel</cell><cell>mtb-race</cell><cell>lock</cell><cell>monkeys-trees</cell><cell>tractor</cell><cell>gym</cell><cell>people-sunset</cell><cell>tandem</cell><cell>girl-dog</cell><cell>aerobatics</cell><cell>tennis-vest</cell><cell>car-race</cell><cell>hoverboard</cell><cell>helicopter</cell><cell>seasnake</cell><cell>orchid</cell><cell>guitar-violin</cell><cell>chamaleon</cell><cell>slackline</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">video sequence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="16">Figure 12: Per-sequence results on DAVIS 17 , test-dev set.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">1st frame, GT segment</cell><cell></cell><cell cols="2">20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">80%</cell><cell></cell><cell></cell><cell></cell><cell cols="2">100%</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Lucid data dreaming synthesis implementation is available at https://www.mpi-inf.mpg.de/lucid-data-dreaming.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://davischallenge.org/challenge2017</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Eddy Ilg and Thomas Brox acknowledge funding by the DFG Grant BR 3815/7-1.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual saliency by extended quantum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Aytekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Ozan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiranyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06506</idno>
		<title level="m">Pixelnet: Representation of the pixels, by the pixels, and for the pixels</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09549</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Principal warps: Thin-plate splines and the decomposition of deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust tracking-by-detection using a detector confidence particle filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reichlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal random fields for efficient video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A video representation using temporal superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>arxiv: 1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to segment instances in videos with spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Synthesizing training data for object detection in indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07836</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting the circulant structure of tracking-by-detection with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PReMVOS: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Click carving: Segmenting objects in video with point clicks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HCOMP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05384</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05478</idno>
		<title level="m">Video propagation networks</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02646</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. 2011. 5</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2016 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2014 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Instance reidentification flow for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Nguyen-Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Ton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">H</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Video object segmentation with re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06612</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maerki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Modeling and propagating cnns in a tree structure for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with tiny synthetic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06370</idno>
		<title level="m">Learning features by watching objects move</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<ptr target="http://davischallenge.org/challenge2017.16" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Multipleinstance video segmentation with sequence-specific object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Firl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dhanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR Workshops</publisher>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Video object segmentation using tracked object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Smolyansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Fomtrace: Interactive video segmentation by image graphs and fuzzy object models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcão</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03369</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Poisson matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Shifting weights: Adapting object detectors from image to video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Normalized cut meets mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning people detectors for tracking in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05863</idno>
		<title level="m">Siamese instance search for tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07217</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01370.3</idno>
		<title level="m">Learning from synthetic humans</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Online adaptation of convolutional neural networks for the 2017 davis challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR Workshops</publisher>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Pixel-wise object segmentations for the VOT 2016 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Research report</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Touchcut: Fast image and video segmentation using single-touch interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Super-trajectory for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08634</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">J J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05842</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Some promising ideas about multi-instance video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR Workshops</publisher>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02295</idno>
		<title level="m">Guided optical flow learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
