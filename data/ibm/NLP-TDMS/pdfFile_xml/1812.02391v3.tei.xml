<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-Transfer Learning for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
							<email>qsun@mpi-inf.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
							<email>liuyaoyao@tju.edu.cndcssq</email>
							<affiliation key="aff1">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<email>schiele@mpi-inf.mpg.de</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-Transfer Learning for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Meta-learning has been proposed as a framework to address the challenging few-shot learning setting. The key idea is to leverage a large number of similar few-shot tasks in order to learn how to adapt a base-learner to a new task for which only a few labeled samples are available. As deep neural networks (DNNs) tend to overfit using a few samples only, meta-learning typically uses shallow neural networks (SNNs), thus limiting its effectiveness. In this paper we propose a novel few-shot learning method called meta-transfer learning (MTL) which learns to adapt a deep NN for few shot learning tasks. Specifically, meta refers to training multiple tasks, and transfer is achieved by learning scaling and shifting functions of DNN weights for each task. In addition, we introduce the hard task (HT) meta-batch scheme as an effective learning curriculum for MTL. We conduct experiments using (5-class, 1-shot) and (5-class, 5shot) recognition tasks on two challenging few-shot learning benchmarks: miniImageNet and Fewshot-CIFAR100. Extensive comparisons to related works validate that our meta-transfer learning approach trained with the proposed HT meta-batch scheme achieves top performance. An ablation study also shows that both components contribute to fast convergence and high accuracy 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While deep learning systems have achieved great performance when sufficient amounts of labeled data are available <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b45">46]</ref>, there has been growing interest in reducing the required amount of data. Few-shot learning tasks have been defined for this purpose. The aim is to learn new concepts from few labeled examples, e.g. 1-shot learning <ref type="bibr" target="#b24">[25]</ref>. While humans tend to be highly effective in this * Yaoyao Liu did this work during his internship at NUS. <ref type="bibr" target="#b0">1</ref>   <ref type="bibr" target="#b34">[35]</ref> Meta-Learning <ref type="bibr" target="#b8">[9]</ref> task 2 model 1 + FT ...</p><formula xml:id="formula_0">task 1 model 1 task N model N task N+1 model N+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Transfer Learning (ours)</head><p>...  <ref type="figure">Figure 1</ref>. Meta-transfer learning (MTL) is our meta-learning paradigm and hard task (HT) meta-batch is our training strategy. The upper three rows show the differences between MTL and related methods, transfer-learning <ref type="bibr" target="#b34">[35]</ref> and meta-learning <ref type="bibr" target="#b8">[9]</ref>. The bottom rows compare HT meta-batch with the conventional metabatch <ref type="bibr" target="#b8">[9]</ref>. FT stands for fine-tuning a classifier. SS represents the Scaling and Shifting operations in our MTL method. context, often grasping the essential connection between new concepts and their own knowledge and experience, it remains challenging for machine learning approaches. E.g., on the CIFAR-100 dataset, a state-of-the-art method <ref type="bibr" target="#b33">[34]</ref> achieves only 40.1% accuracy for 1-shot learning, compared to 75.7% for the all-class fully supervised case <ref type="bibr" target="#b5">[6]</ref>.</p><p>Few-shot learning methods can be roughly categorized into two classes: data augmentation and task-based metalearning. Data augmentation is a classic technique to increase the amount of available data and thus also useful for few-shot learning <ref type="bibr" target="#b20">[21]</ref>. Several methods propose to learn a data generator e.g. conditioned on Gaussian noise <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b53">54]</ref>. However, the generation models often underperform when trained on few-shot data <ref type="bibr" target="#b0">[1]</ref>. An alter-native is to merge data from multiple tasks which, however, is not effective due to variances of the data across tasks <ref type="bibr" target="#b53">[54]</ref>.</p><p>In contrast to data-augmentation methods, meta-learning is a task-level learning method <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b51">52]</ref>. Meta-learning aims to accumulate experience from learning multiple tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b12">13]</ref>, while base-learning focuses on modeling the data distribution of a single task. A state-of-theart representative of this, namely Model-Agnostic Meta-Learning (MAML), learns to search for the optimal initialization state to fast adapt a base-learner to a new task <ref type="bibr" target="#b8">[9]</ref>. Its task-agnostic property makes it possible to generalize to few-shot supervised learning as well as unsupervised reinforcement learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10]</ref>. However, in our view, there are two main limitations of this type of approaches limiting their effectiveness: i) these methods usually require a large number of similar tasks for meta-training which is costly; and ii) each task is typically modeled by a lowcomplexity base learner (such as a shallow neural network) to avoid model overfitting, thus being unable to use deeper and more powerful architectures. For example, for the mini-ImageNet dataset <ref type="bibr" target="#b52">[53]</ref>, MAML uses a shallow CNN with only 4 CONV layers and its optimal performance was obtained learning on 240k tasks.</p><p>In this paper, we propose a novel meta-learning method called meta-transfer learning (MTL) leveraging the advantages of both transfer and meta learning (see conceptual comparison of related methods in <ref type="figure">Figure 1</ref>). In a nutshell, MTL is a novel learning method that helps deep neural nets converge faster while reducing the probability to overfit when using few labeled training data only. In particular, "transfer" means that DNN weights trained on largescale data can be used in other tasks by two light-weight neuron operations: Scaling and Shifting (SS), i.e. αX + β. "Meta" means that the parameters of these operations can be viewed as hyper-parameters trained on few-shot learning tasks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b25">26]</ref>. Large-scale trained DNN weights offer a good initialization, enabling fast convergence of metatransfer learning with fewer tasks, e.g. only 8k tasks for miniImageNet <ref type="bibr" target="#b52">[53]</ref>, 30 times fewer than MAML <ref type="bibr" target="#b8">[9]</ref>. Lightweight operations on DNN neurons have less parameters to learn, e.g. less than <ref type="bibr">2 49</ref> if considering neurons of size 7 × 7 ( 1 49 for α and &lt; 1 49 for β), reducing the chance of overfitting. In addition, these operations keep those trained DNN weights unchanged, and thus avoid the problem of "catastrophic forgetting" which means forgetting general patterns when adapting to a specific task <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>The second main contribution of this paper is an effective meta-training curriculum. Curriculum learning <ref type="bibr" target="#b2">[3]</ref> and hard negative mining <ref type="bibr" target="#b46">[47]</ref> both suggest that faster convergence and stronger performance can be achieved by a better arrangement of training data. Inspired by these ideas, we design our hard task (HT) meta-batch strategy to offer a challenging but effective learning curriculum. As shown in the bottom rows of <ref type="figure">Figure 1</ref>, a conventional meta-batch contains a number of random tasks <ref type="bibr" target="#b8">[9]</ref>, but our HT meta-batch online re-samples harder ones according to past failure tasks with lowest validation accuracy.</p><p>Our overall contribution is thus three-fold: i) we propose a novel MTL method that learns to transfer largescale pre-trained DNN weights for solving few-shot learning tasks; ii) we propose a novel HT meta-batch learning strategy that forces meta-transfer to "grow faster and stronger through hardship"; and iii) we conduct extensive experiments on two few-shot learning benchmarks, namely miniImageNet <ref type="bibr" target="#b52">[53]</ref> and Fewshot-CIFAR100 (FC100) <ref type="bibr" target="#b33">[34]</ref>, and achieve the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Few-shot learning Research literature on few-shot learning exhibits great diversity. In this section, we focus on methods using the supervised meta-learning paradigm <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b8">9]</ref> most relevant to ours and compared to in the experiments. We can divide these methods into three categories. 1) Metric learning methods <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51]</ref> learn a similarity space in which learning is efficient for few-shot examples. 2) Memory network methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30]</ref> learn to store "experience" when learning seen tasks and then generalize that to unseen tasks. 3) Gradient descent based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b59">60]</ref> have a specific meta-learner that learns to adapt a specific base-learner (to few-shot examples) through different tasks. E.g. MAML <ref type="bibr" target="#b8">[9]</ref> uses a meta-learner that learns to effectively initialize a base-learner for a new learning task. Meta-learner optimization is done by gradient descent using the validation loss of the base-learner. Our method is closely related. An important difference is that our MTL approach leverages transfer learning and benefits from referencing neuron knowledge in pre-trained deep nets. Although MAML can start from a pre-trained network, its element-wise fine-tuning makes it hard to learn deep nets without overfitting (validated in our experiments). Transfer learning What and how to transfer are key issues to be addressed in transfer learning, as different methods are applied to different source-target domains and bridge different transfer knowledge <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59]</ref>. For deep models, a powerful transfer method is adapting a pre-trained model for a new task, often called fine-tuning (FT). Models pretrained on large-scale datasets have proven to generalize better than randomly initialized ones <ref type="bibr" target="#b7">[8]</ref>. Another popular transfer method is taking pre-trained networks as backbone and adding high-level functions, e.g. for object detection and recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b48">49]</ref> and image segmentation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5]</ref>. Our meta-transfer learning leverages the idea of transferring pre-trained weights and aims to meta-learn how to effectively transfer.  erations have been used to modulating the per-feature-map distribution of activations for visual reasoning <ref type="bibr" target="#b36">[37]</ref>. Some few-shot learning methods have been proposed to use pre-trained weights as initialization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b40">41]</ref>. Typically, weights are fine-tuned for each task, while we learn a meta-transfer learner through all tasks, which is different in terms of the underlying learning paradigm. Curriculum learning &amp; Hard sample mining Curriculum learning was proposed by Bengio et al. <ref type="bibr" target="#b2">[3]</ref> and is popular for multi-task learning <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b13">14]</ref>. They showed that instead of observing samples at random it is better to organize samples in a meaningful way so that fast convergence, effective learning and better generalization can be achieved. Pentina et al. <ref type="bibr" target="#b35">[36]</ref> use adaptive SVM classifiers to evaluate task difficulty for later organization. Differently, our MTL method does task evaluation online at the phase of episode test, without needing any auxiliary model.</p><p>Hard sample mining was proposed by Shrivastava et al. <ref type="bibr" target="#b46">[47]</ref> for object detection. It treats image proposals overlapped with ground truth as hard negative samples. Training on more confusing data enables the model to achieve higher robustness and better performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7]</ref>. Inspired by this, we sample harder tasks online and make our MTL learner "grow faster and stronger through more hardness". In our experiments, we show that this can be generalized to enhance other meta-learning methods, e.g. MAML <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary</head><p>We introduce the problem setup and notations of metalearning, following related work <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>. Meta-learning consists of two phases: meta-train and meta-test. A meta-training example is a classification task T sampled from a distribution p(T ). T is called episode, including a training split T (tr) to optimize the base-learner, and a test split T (te) to optimize the meta-learner. In particular, meta-training aims to learn from a number of episodes {T } sampled from p(T ). An unseen task T unseen in metatest will start from that experience of the meta-learner and adapt the base-learner. The final evaluation is done by test-ing a set of unseen datapoints T (te) unseen . Meta-training phase. This phase aims to learn a metalearner from multiple episodes. In each episode, metatraining has a two-stage optimization. Stage-1 is called base-learning, where the cross-entropy loss is used to optimize the parameters of the base-learner. Stage-2 contains a feed-forward test on episode test datapoints. The test loss is used to optimize the parameters of the meta-learner. Specifically, given an episode T ∈ p(T ), the base-learner θ T is learned from episode training data T (tr) and its corresponding loss L T (θ T , T (tr) ). After optimizing this loss, the baselearner has parametersθ T . Then, the meta-learner is updated using test loss L T (θ T , T (te) ). After meta-training on all episodes, the meta-learner is optimized by test losses {L T (θ T , T (te) )} T ∈p(T ) . Therefore, the number of metalearner updates equals to the number of episodes.</p><p>Meta-test phase. This phase aims to test the performance of the trained meta-learner for fast adaptation to unseen task. Given T unseen , the meta-learnerθ T teaches the baselearner θ Tunseen to adapt to the objective of T unseen by some means, e.g. through initialization <ref type="bibr" target="#b8">[9]</ref>. Then, the test result on T (te) unseen is used to evaluate the meta-learning approach. If there are multiple unseen tasks {T unseen }, the average result on {T (te) unseen } will be the final evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, our method consists of three phases. First, we train a DNN on large-scale data, e.g. on miniImageNet (64-class, 600-shot) <ref type="bibr" target="#b52">[53]</ref>, and then fix the low-level layers as Feature Extractor (Section 4.1). Second, in the meta-transfer learning phase, MTL learns the Scaling and Shifting (SS) parameters for the Feature Extractor neurons, enabling fast adaptation to few-shot tasks (Section 4.2). For improved overall learning, we use our HT meta-batch strategy (Section 4.3). The training steps are detailed in Algorithm 1 in Section 4.4. Finally, the typical meta-test phase is performed, as introduced in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">DNN training on large-scale data</head><p>This phase is similar to the classic pre-training stage as, e.g., pre-training on Imagenet for object recognition <ref type="bibr" target="#b39">[40]</ref>. Here, we do not consider data/domain adaptation from other datasets, and pre-train on readily available data of few-shot learning benchmarks, allowing for fair comparison with other few-shot learning methods. Specifically, for a particular few-shot dataset, we merge all-class data D for pretraining. For instance, for miniImageNet <ref type="bibr" target="#b52">[53]</ref>, there are totally 64 classes in the training split of D and each class contains 600 samples used to pre-train a 64-class classifier.</p><p>We first randomly initialize a feature extractor Θ (e.g. CONV layers in ResNets <ref type="bibr" target="#b16">[17]</ref>) and a classifier θ (e.g. the last FC layer in ResNets <ref type="bibr" target="#b16">[17]</ref>), and then optimize them by gradient descent as follows,</p><formula xml:id="formula_1">[Θ; θ] =: [Θ; θ] − α∇L D [Θ; θ] ,<label>(1)</label></formula><p>where L denotes the following empirical loss,</p><formula xml:id="formula_2">L D [Θ; θ] = 1 |D| (x,y)∈D l f [Θ;θ] (x), y ,<label>(2)</label></formula><p>e.g. cross-entropy loss, and α denotes the learning rate. In this phase, the feature extractor Θ is learned. It will be frozen in the following meta-training and meta-test phases, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The learned classifier θ will be discarded, because subsequent few-shot tasks contain different classification objectives, e.g. 5-class instead of 64-class classification for miniImageNet <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Meta-transfer learning (MTL)</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b), our proposed meta-transfer learning (MTL) method optimizes the meta operations Scaling and Shifting (SS) through HT meta-batch training (Section 4.3). <ref type="figure" target="#fig_2">Figure 3</ref> visualizes the difference of updating through SS and FT. SS operations, denoted as Φ S1 and Φ S2 , do not change the frozen neuron weights of Θ during learning, while FT updates the complete Θ.</p><p>In the following, we detail the SS operations. Given a task T , the loss of T (tr) is used to optimize the current base-learner (classifier) θ by gradient descent:</p><formula xml:id="formula_3">θ ← θ − β∇ θ L T (tr) [Θ; θ], Φ S {1,2} ,<label>(3)</label></formula><p>which is different to Eq. 1, as we do not update Θ. Note that here θ is different to the one from the previous phase, the large-scale classifier θ in Eq. 1. This θ concerns only a few of classes, e.g. 5 classes, to classify each time in a novel few-shot setting. θ corresponds to a temporal classifier only working in the current task, initialized by the θ optimized for the previous task (see <ref type="bibr">Eq. 5)</ref>. Φ S1 is initialized by ones and Φ S1 by zeros. Then, they are optimized by the test loss of T (te) as follows,  In this step, θ is updated with the same learning rate γ as in Eq. 4,</p><formula xml:id="formula_4">Φ Si =: Φ Si − γ∇ Φ S i L T (te) [Θ; θ ], Φ S {1,2} . (4)</formula><formula xml:id="formula_5">. . + + (a) Parameter-level Fine-Tuning (FT) C x C x C x C x : Cx4x3x3 C x : 1x4x1x1 C x : Cx4x3x3 W b W b Φ S1 Φ S2 Φ S1 Φ S2 T W b W b update T update</formula><formula xml:id="formula_6">θ =: θ − γ∇ θ L T (te) [Θ; θ ], Φ S {1,2} .<label>(5)</label></formula><p>Re-linking to Eq. 3, we note that the above θ comes from the last epoch of base-learning on T (tr) . Next, we describe how we apply Φ S {1,2} to the frozen neurons as shown in <ref type="figure" target="#fig_2">Figure 3</ref>(b). Given the trained Θ, for its l-th layer containing K neurons, we have K pairs of parameters, respectively as weight and bias, denoted as {(W i,k , b i,k )}. Note that the neuron location l, k will be omitted for readability. Based on MTL, we learn K pairs of scalars {Φ S {1,2} }. Assuming X is input, we apply</p><formula xml:id="formula_7">{Φ S {1,2} } to (W, b) as SS(X; W, b; Φ S {1,2} ) = (W Φ S1 )X + (b + Φ S2 ),<label>(6)</label></formula><p>where denotes the element-wise multiplication.</p><p>Taking <ref type="figure" target="#fig_2">Figure 3</ref>(b) as an example of a single 3 × 3 filter, after SS operations, this filter is scaled by Φ S1 then the feature maps after convolutions are shifted by Φ S2 in addition to the original bias b. Detailed steps of SS are given in Algorithm 2 in Section 4.4. <ref type="figure" target="#fig_2">Figure 3</ref>(a) shows a typical parameter-level Fine-Tuning (FT) operation, which is in the meta optimization phase of our related work MAML <ref type="bibr" target="#b8">[9]</ref>. It is obvious that FT updates the complete values of W and b, and has a large number of parameters, and our SS reduces this number to below <ref type="bibr">2 9</ref> in the example of the figure.</p><p>In summary, SS can benefit MTL in three aspects. 1) It starts from a strong initialization based on a large-scale trained DNN, yielding fast convergence for MTL. 2) It does not change DNN weights, thereby avoiding the problem of "catastrophic forgetting" <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> when learning specific tasks in MTL. 3) It is light-weight, reducing the chance of overfitting of MTL in few-shot scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hard task (HT) meta-batch</head><p>In this section, we introduce a method to schedule hard tasks in meta-training batches. The conventional metabatch is composed of randomly sampled tasks, where the randomness implies random difficulties <ref type="bibr" target="#b8">[9]</ref>. In our metatraining pipeline, we intentionally pick up failure cases in each task and re-compose their data to be harder tasks for adverse re-training. We aim to force our meta-learner to "grow up through hardness". Pipeline. Each task T has two splits, T (tr) and T (te) , for base-learning and test, respectively. As shown in Algorithm 2 line 2-5, base-learner is optimized by the loss of T (tr) (in multiple epochs). SS parameters are then optimized by the loss of T (te) once. We can also get the recognition accuracy of T (te) for M classes. Then, we choose the lowest accuracy Acc m to determine the most difficult class-m (also called failure class) in the current task.</p><p>After obtaining all failure classes (indexed by {m}) from k tasks in current meta-batch {T 1∼k }, we re-sample tasks from their data. Specifically, we assume p(T |{m}) is the task distribution, we sample a "harder" task T hard ∈ p(T |{m}). Two important details are given below. Choosing hard class-m. We choose the failure class-m from each task by ranking the class-level accuracies instead of fixing a threshold. In a dynamic online setting as ours, it is more sensible to choose the hardest cases based on ranking rather than fixing a threshold ahead of time. Two methods of hard tasking using {m}. Chosen {m}, we can re-sample tasks T hard by (1) directly using the samples of class-m in the current task T , or (2) indirectly using the label of class-m to sample new samples of that class. In fact, setting (2) considers to include more data variance of class-m and it works better than setting (1) in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Algorithm</head><p>Algorithm 1 summarizes the training process of two main stages: large-scale DNN training (line 1-5) and metatransfer learning (line <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. HT meta-batch re-sampling and continuous training phases are shown in lines 16-20, for which the failure classes are returned by Algorithm 2, see line 14. Algorithm 2 presents the learning process on a single task that includes episode training (lines 2-5) and episode test, i.e. meta-level update (lines 6). In lines 7-11, the recognition rates of all test classes are computed and returned to Algorithm 1 (line 14) for hard task sampling. Optimize Θ and θ by Eq. 1; 5 end 6 Initialize Φ S1 by ones, initialize Φ S2 by zeros; <ref type="bibr" target="#b6">7</ref> Reset and re-initialize θ for few-shot tasks; 8 for meta-batches do <ref type="bibr" target="#b8">9</ref> Randomly sample tasks {T } from p(T ); <ref type="bibr" target="#b9">10</ref> while not done do <ref type="bibr" target="#b10">11</ref> Sample task T i ∈ {T }; <ref type="bibr" target="#b11">12</ref> Optimize Φ S {1,2} and θ with T i by Algorithm 2; Compute Acc k for T (te) ; 10 end 11 Return class-m with the lowest accuracy Acc m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate the proposed MTL and HT meta-batch in terms of few-shot recognition accuracy and model convergence speed. Below we describe the datasets and detailed settings, followed by an ablation study and a comparison to state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and implementation details</head><p>We conduct few-shot learning experiments on two benchmarks, miniImageNet <ref type="bibr" target="#b52">[53]</ref> and Fewshot-CIFAR100 (FC100) <ref type="bibr" target="#b33">[34]</ref>. miniImageNet is widely used in related works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32]</ref>. FC100 is newly proposed in <ref type="bibr" target="#b33">[34]</ref> and is more challenging in terms of lower image resolution and stricter training-test splits than miniImageNet. miniImageNet was proposed by Vinyals et al. <ref type="bibr" target="#b52">[53]</ref> for fewshot learning evaluation. Its complexity is high due to the use of ImageNet images, but requires less resource and infrastructure than running on the full ImageNet dataset <ref type="bibr" target="#b39">[40]</ref>. In total, there are 100 classes with 600 samples of 84 × 84 color images per class. These 100 classes are divided into 64, 16, and 20 classes respectively for sampling tasks for meta-training, meta-validation and meta-test, following related works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32]</ref>. Fewshot-CIFAR100 (FC100) is based on the popular object classification dataset CIFAR100 <ref type="bibr" target="#b22">[23]</ref>. The splits were proposed by <ref type="bibr" target="#b33">[34]</ref> (Please check details in the supplementary). It offers a more challenging scenario with lower image resolution and more challenging meta-training/test splits that are separated according to object super-classes. It contains 100 object classes and each class has 600 samples of 32 × 32 color images. The 100 classes belong to 20 super-classes. Meta-training data are from 60 classes belonging to 12 super-classes. Meta-validation and meta-test sets contain 20 classes belonging to 4 super-classes, respectively. These splits accord to super-classes, thus minimize the information overlap between training and val/test tasks.</p><p>The following settings are used on both datasets. We train a large-scale DNN with all training datapoints (Section 4.1) and stop this training after 10k iterations. We use the same task sampling method as related works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39]</ref>. Specifically, 1) we consider the 5-class classification and 2) we sample 5-class, 1-shot (5-shot or 10-shot) episodes to contain 1 (5 or 10) samples for train episode, and 15 (uniform) samples for episode test. Note that in the state-ofthe-art work <ref type="bibr" target="#b33">[34]</ref>, 32 and 64 samples are respectively used in 5-shot and 10-shot settings for episode test. In total, we sample 8k tasks for meta-training (same for w/ and w/o HT meta-batch), and respectively sample 600 random tasks for meta-validation and meta-test. Please check the supplementary document (or GitHub repository) for other implementation details, e.g. learning rate and dropout rate. Network architecture. We present the details for the Fea-ture Extractor Θ, MTL meta-learner with Scaling Φ S1 and Shifting Φ S2 , and MTL base-learner (classifier) θ. The architecture of Θ have two options, ResNet-12 and 4CONV, commonly used in related works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref>. 4CONV consists of 4 layers with 3 × 3 convolutions and 32 filters, followed by batch normalization (BN) <ref type="bibr" target="#b18">[19]</ref>, a ReLU nonlinearity, and 2 × 2 max-pooling. ResNet-12 is more popular in recent works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32]</ref>. It contains 4 residual blocks and each block has 3 CONV layers with 3 × 3 kernels. At the end of each residual block, a 2 × 2 max-pooling layer is applied. The number of filters starts from 64 and is doubled every next block. Following 4 blocks, there is a mean-pooling layer to compress the output feature maps to a feature embedding. The difference between using 4CONV and using ResNet-12 in our methods is that ResNet-12 MTL sees the large-scale data training, but 4CONV MTL is learned from scratch because of its poor performance for large-scale data training (see results in the supplementary). Therefore, we emphasize the experiments of using ResNet-12 MTL for its superior performance. The architectures of Φ S1 and Φ S2 are generated according to the architecture of Θ, as introduced in Section 4.2. That is when using ResNet-12 in MTL, Φ S1 and Φ S2 also have 12 layers, respectively. The architecture of θ is an FC layer. We empirically find that a single FC layer is faster to train and more effective for classification than multiple layers. (see comparisons in the supplementary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study setting</head><p>In order to show the effectiveness of our approach, we design some ablative settings: two baselines without metalearning but more classic learning, three baselines of Fine-Tuning (FT) on smaller number of parameters <ref type="table" target="#tab_3">(Table 1)</ref>, and two MAML variants using our deeper pre-trained model and HT meta-batch ( <ref type="table">Table 2</ref> and <ref type="table">Table 3</ref>). Note that the alternative meta-learning operation to SS is the FT used in MAML. Some bullet names are explained as follows. update [Θ; θ] (or θ). There is no meta-training phase. During test phase, each task has its whole model [Θ; θ] (or the classifier θ) updated on T (tr) , and then tested on T (te) . FT [Θ4; θ] (or θ). These are straight-forward ways to define a smaller set of meta-learner parameters than MAML. We can freeze low-level pre-trained layers and meta-learn the classifier layer θ with (or without) high-level CONV layer Θ4 that is the 4th residual block of ResNet-12. <ref type="table" target="#tab_3">Table 1</ref>, <ref type="table">Table 2</ref> and <ref type="table">Table 3</ref> present the overall results on miniImageNet and FC100 datasets. Extensive comparisons are done with ablative methods and the state-of-the-arts. Note that tables present the highest accuracies for which the iterations were chosen by validation. For the miniIma-  geNet, iterations for 1-shot and 5-shot are at 17k and 14k, respectively. For the FC100, iterations are all at 1k. <ref type="figure" target="#fig_4">Figure 4</ref> shows the performance gap between with and without HT meta-batch in terms of accuracy and converging speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results and analysis</head><p>Result overview on miniImageNet. In <ref type="table">Table 2</ref>, we can see that the proposed MTL with SS [Θ; θ], HT meta-batch and ResNet-12(pre) achieves the best few-shot classification performance with 61.2% for (5-class, 1-shot). Besides, it tackles the (5-class, 5-shot) tasks with an accuracy of 75.5% that is comparable to the state-of-the-art results, i.e. 76.7%, reported by TADAM <ref type="bibr" target="#b33">[34]</ref> whose model used 72 additional FC layers in the ResNet-12 arch. In terms of the network arch, it is obvious that models using ResNet-12 (pre) outperforms those using 4CONV by large margins, e.g. 4CONV models have the best 1-shot result with 50.44% <ref type="bibr" target="#b50">[51]</ref> which is 10.8% lower than our best.</p><p>Result overview on FC100. In <ref type="table">Table 3</ref>, we give the results of TADAM using their reported numbers in the paper <ref type="bibr" target="#b33">[34]</ref>. We used the public code of MAML <ref type="bibr" target="#b8">[9]</ref> to get its results for this new dataset. Comparing these methods, we can see that MTL consistently outperforms MAML by large margins, i.e. around 7% in all tasks; and surpasses TADAM by a relatively larger number of 5% for 1-shot, and with 1.5% and 1.8% respectively for 5-shot and 10-shot tasks. MTL vs. No meta-learning. <ref type="table" target="#tab_3">Table 1</ref> shows the results of No meta-learning on the top block. Compared to these, our approach achieves significantly better performance even without HT meta-batch, e.g. the largest margins are 10.2% for 1-shot and 8.6% for 5-shot on miniImageNet. This validates the effectiveness of our meta-learning method for tackling few-shot learning problems. Between two No miniImageNet FC100 meta-learning methods, we can see that updating both feature extractor Θ and classifier θ is inferior to updating θ only, e.g. around 5% reduction on miniImageNet 1-shot. One reason is that in few-shot settings, there are too many parameters to optimize with little data. This supports our motivation to learn only θ during base-learning.</p><p>Performance effects of MTL components. MTL with full components, SS [Θ; θ], HT meta-batch and ResNet-12(pre), achieves the best performances for all few-shot settings on both datasets, see <ref type="table">Table 2</ref> and <ref type="table">Table 3</ref>. We can conclude that our large-scale network training on deep CNN significantly boost the few-shot learning performance. This is an important gain brought by the transfer learning idea in our MTL approach. It is interesting to note that this gain on FC100 is not as large as for miniImageNet: only 1.7%, 1.0% and 4.0%. The possible reason is that FC100 tasks for meta-train and meta-test are clearly split according to super-classes. The data domain gap is larger than that for miniImageNet, which makes transfer more difficult.</p><p>HT meta-batch and ResNet-12(pre) in our approach can be generalized to other meta-learning models. MAML 4CONV with HT meta-batch gains averagely 1% on two datasets. When changing 4CONV by deep ResNet-12 (pre) it achieves significant improvements, e.g. 10% and 9% on miniImageNet. Compared to MAML variants, our MTL results are consistently higher, e.g. 2.5% ∼ 3.3% on FC100. People may argue that MAML fine-tuning(FT) all network parameters is likely to overfit to few-shot data. In the middle block of <ref type="table" target="#tab_3">Table 1</ref>, we show the ablation study of freezing low-level pre-trained layers and meta-learn only the highlevel layers (e.g. the 4-th residual block of ResNet-12) by the FT operations of MAML. These all yield inferior performances than using our SS. An additional observation is that SS* performs consistently better than FT*.</p><p>Speed of convergence of MTL. MAML <ref type="bibr" target="#b8">[9]</ref> used 240k tasks to achieve the best performance on miniImageNet. Impressively, our MTL methods used only 8k tasks, see <ref type="figure" target="#fig_4">Figure 4</ref>(a)(b) (note that each iteration contains 2 tasks). This advantage is more obvious for FC100 on which MTL methods need at most 2k tasks, <ref type="figure" target="#fig_4">Figure 4</ref>(c)(d)(e). We attest this to two reasons. First, MTL starts from the pre-trained ResNet-12. And second, SS (in MTL) needs to learn only &lt; 2 9 parameters of the number of FT (in MAML) when using ResNet-12.</p><p>Speed of convergence of HT meta-batch. <ref type="figure" target="#fig_4">Figure 4</ref> shows 1) MTL with HT meta-batch consistently achieves higher performances than MTL with the conventional metabatch <ref type="bibr" target="#b8">[9]</ref>, in terms of the recognition accuracy in all settings; and 2) it is impressive that MTL with HT meta-batch achieves top performances early, after e.g. about 2k iterations for 1-shot, 1k for 5-shot and 1k for 10-shot, on the more challenging dataset -FC100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we show that our novel MTL trained with HT meta-batch learning curriculum achieves the top performance for tackling few-shot learning problems. The key operations of MTL on pre-trained DNN neurons proved highly efficient for adapting learning experience to the unseen task. The superiority was particularly achieved in the extreme 1-shot cases on two challenging benchmarks -miniImageNet and FC100. In terms of learning scheme, HT meta-batch showed consistently good performance for all baselines and ablative models. On the more challenging FC100 benchmark, it showed to be particularly helpful for boosting convergence speed. This design is independent from any specific model and could be generalized well whenever the hardness of task is easy to evaluate in online iterations.</p><p>In <ref type="figure">Figure S1</ref>, we present the 4CONV architecture for feature extractor Θ, as illustrated in Section 5.1 "Network architecture" of the main paper.</p><p>In <ref type="figure" target="#fig_1">Figure S2</ref>, we present the other architecture -ResNet-12. <ref type="figure" target="#fig_1">Figure S2(a)</ref> shows the details of a single residual block and <ref type="figure" target="#fig_1">Figure S2(b)</ref> shows the whole network consisting of four residual blocks and a mean-pooling layer.</p><p>The input of Θ is the 3-channel RGB image, and the output is the 512-dimensional feature vector. a = 0.1 is set for all leakyReLU activation functions in ResNet-12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>For the phase of DNN training on large-scale data, the model is trained by Adam optimizer <ref type="bibr" target="#b21">[22]</ref>. Its learning rate is initialized as 0.001, and decays to its half every 5k iterations until it is lower that 0.0001. We set the keep probability of the dropout as 0.9 and batch-size as 64. The pre-training stops after 10k iterations. Note that for the hyperparameter selection, we randomly choose 550 samples each class as the training set, and the rest as validation. After the grid search of hyperparameters, we fix them and mix up all samples (64 classes, 600 samples each class), in order to do the final pre-training. Besides, these pre-training samples are augmented with horizontal flip.</p><p>For the meta-train phase, we sample 5-class, 1-shot (5shot or 10-shot) episodes to contain 1 (5 or 10) sample(s) for episode training, and 15 samples for episode test uniformly, following the setting of MAML <ref type="bibr" target="#b8">[9]</ref>. The base-learner θ is optimized by batch gradient descent with the learning rate of 0.01. It gets updated with 20 and 60 epochs respectively for 1-shot and 5-shot tasks on the miniImageNet dataset, and 20 epochs for all tasks on the FC100 dataset. The metalearner, i.e., the parameters of the SS operations, is optimized by Adam optimizer <ref type="bibr" target="#b21">[22]</ref>. Its learning rate is initialized as 0.001, and decays to the half every 1k iterations until 0.0001. The size of meta-batch is set to 2 (tasks) due to the memory limit.</p><p>Using our HT meta-batch strategy, hard tasks are sampled every time after running 10 meta-batches, i.e., the failure classes used for sampling hard tasks are from 20 tasks. The number of hard task is selected for different settings by validation: 10 and 4 hard tasks respectively for the 1-shot 2 https://github.com/y2l/meta-transfer-learning-tensorflow and 5-shot experiments on the miniImageNet dataset; and respectively 20, 10 and 4 hard tasks for the 1-shot, 5-shot and 10-shot experiments on the FC100 dataset.</p><p>For the meta-test phase, we sample 5-class, 1-shot (5shot or 10-shot) episodes and each episode contains 1 (5 or 10) sample(s) for both episode train and episode test. On each dataset, we sample 600 meta-test tasks. All these settings are exactly the same as MAML <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Super-class splits on FC100</head><p>In this section, we show the details of the FC100 splits according to the super-class labels, same with TADAM <ref type="bibr" target="#b33">[34]</ref>. Training split super-class indexes: 1, 2, 3, 4, 5, <ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref>; and corresponding labels: fish, flowers, food containers, fruit and vegetables, household electrical devices, household furniture, large manmade outdoor things, large natural outdoor scenes, reptiles, trees, vehicles 1, vehicles 2. Validation split super-class indexes: <ref type="bibr">8, 11, 13, 16;</ref> and corresponding labels: large carnivores, large omnivores and herbivores, non-insect invertebrates, small mammals. Test split super-class indexes: 0, 7, 12, 14; and corresponding labels: aquatic mammals, insects, medium mammals, people.</p><p>An episode (task) is independently sampled from a corresponding split, e.g. a meta-train episode contains 5 classes that can only be belonging to the 12 super-classes in the training split. Therefore, there is no fine-grained information overlap between meta-train and meta-test tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Standard variance analysis</head><p>The final accuracy results reported in our main paper are the mean values and standard variances of the results of 600 meta-test tasks. The standard variance is affected by the number of episode test samples. As introduced in §B, we use the same setting as MAML <ref type="bibr" target="#b8">[9]</ref> which used a smaller number of samples for episode test (1 sample for 1-shot episode test and 5 samples for 5-shot), making the result variance higher. Other works that used more samples for episode test got lower variances, e.g., TADAM <ref type="bibr" target="#b33">[34]</ref> used 100 samples and its variances are about 1 6 and 1 3 of MAML's respectively for miniImageNet 1-shot and 5-shot.</p><p>In order to have a fair comparison with TADAM in terms of this issue, we supplement the experiments using 100 episode test samples at the meta-test. We get the new confidence intervals (using our method: MTL w/o HT metabatch) as 0.71% (0.3% for TADAM) and 0.54% (0.3% for TADAM) respectively for 1-shot and 5-shot on the mini-ImageNet dataset, and 0.70% (0.4% for TADAM), 0.63% (0.4% for TADAM) and 0.58% (0.5% for TADAM) respectively for 1-shot, 5-shot and 10-shot on the FC100 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional ablation study</head><p>We supplement the results in <ref type="table" target="#tab_3">Table S1</ref>, for the comparisons mentioned in Section 5.1 of main paper. Red numbers on the bottom row are copied from the main paper (corresponding to the MTL setting: SS Θ, meta-batch) and shown here for the convenience of comparison.</p><p>To get the first row, we train 4CONV net by large-scale data (same to the pre-training of ResNet-12) and get inferior results, as we declared in the main paper. Results on the second and third rows show the performance drop when changing the single FC layer θ to multiple layers, e.g. 2 FC layers and 3 FC layers. Results on the fourth row show the performance drop when updating both Θ and θ for the baselearning. The reason is that Θ has too many parameters to update with too little data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Interpretation of meta-learned SS</head><p>In <ref type="figure" target="#fig_2">Figure S3</ref>, we show the statistic histograms of learned SS parameters, taking miniImageNet 1-shot as an example setting. Scaling parameters Φ S1 are initialized as 1 and shifting parameters Φ S1 as 0. After meta-train, we observe that these statistics are close to Gaussian distributions respectively with (0.9962, 0.0084) and (0.0003, 0.0002) as (mean, variance) values, which shows that the uniform initialization has been changed to Gaussian distribution through few-shot learning. Possible interpretations are in three-fold: 1) majority patterns trained by a large number of few-shot tasks are close to the ones trained by large-scale data; 2) tail patterns with clear scale and shift values are the ones really contributing to adapting the model to few-shot tasks; 3) tail patterns are of small quantity, enabling the fast learning convergence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>SS 1 + FT 1 task N model + SS N + FT N task N+1 model + SS N + FT N+1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The pipeline of our proposed few-shot learning method, including three phases: (a) DNN training on large-scale data, i.e. using all training datapoints (Section 4.1); (b) Meta-transfer learning (MTL) that learns the parameters of Scaling and Shifting (SS), based on the pre-trained feature extractor (Section 4.2). Learning is scheduled by the proposed HT meta-batch (Section 4.3); and (c) meta-test is done for an unseen task which consists of a base-learner (classifier) Fine-Tuning (FT) stage and a final evaluation stage, described in the last paragraph in Section 3. Input data are along with arrows. Modules with names in bold get updated at corresponding phases. Specifically, SS parameters are learned by meta-training but fixed during meta-test. Base-learner parameters are optimized for every task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(a) Parameter-level Fine-Tuning (FT) is a conventional meta-training operation, e.g. in MAML<ref type="bibr" target="#b8">[9]</ref>. Its update works for all neuron parameters, W and b. (b) Our neuron-level Scaling and Shifting (SS) operations in MTL. They reduce the number of learning parameters and avoid overfitting problems. In addition, they keep large-scale trained parameters (in yellow) frozen, preventing "catastrophic fogetting"<ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>SS [Θ; θ], HT meta-batch SS [Θ; θ], meta-batch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>(a)(b) show the results of 1-shot and 5-shot on miniImageNet; (c)(d)(e) show the results of 1-shot, 5-shot and 10-shot on FC100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Meta-learner SSN Base-learner FTN classifier fine-tuningSSN Base-learner FTN+1 final evaluation</head><label></label><figDesc>In this paper, large-scale trained DNN weights are what to transfer, and the operations of Scaling and Shifting indicate how to transfer. Similar op-</figDesc><table><row><cell></cell><cell cols="2">whole training phase</cell><cell></cell><cell></cell><cell></cell></row><row><cell>all-class train samples D</cell><cell>Feature Extractor Base-learner</cell><cell>N HT meta-batches {T1∼k}1∼N</cell><cell cols="2">Feature Extractor</cell><cell>unseen task (train samples) T (tr unseen )</cell><cell>unseen task (test samples) Meta-learner Acc. Feature Extractor Feature Extractor Meta-learner SSN Base-learner FTN+1 T (te unseen )</cell></row><row><cell cols="2">(a) large-scale DNN training</cell><cell cols="3">(b) meta-transfer learning</cell><cell></cell><cell>(c) meta-test</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(only in the last epi-training epoch)</cell><cell>Difficulty</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>predictor</cell></row><row><cell>Training phase Test phase</cell><cell cols="3">Meta transferring of neuron weights</cell><cell></cell><cell></cell><cell>θ'</cell></row><row><cell></cell><cell cols="2">Feature extractor Θ (pre-trained &amp; frozen)</cell><cell></cell><cell></cell><cell></cell><cell>softmax loss (epi-training)</cell><cell>L2 loss</cell></row><row><cell>epi-training</cell><cell></cell><cell>element-wise product (neuron-level)</cell><cell>feature</cell><cell>Classifier θ</cell><cell></cell></row><row><cell>epi-test</cell><cell cols="2">Scaling &amp; Shifting Param Φ (meta-learner)</cell><cell></cell><cell></cell><cell></cell><cell>softmax loss (epi-test)</cell><cell>accuracy (epi-test)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">meta gradient back-prop. (once)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Regularization, useful???</cell><cell>meta gradient back-prop. (once)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Meta-transfer learning (MTL) Input: Task distribution p(T ) and corresponding dataset D, learning rates α, β and γ Output: Feature extractor Θ, base learner θ, SS parameters Φ S {1,2}</figDesc><table /><note>1 Randomly initialize Θ and θ;2 for samples in D do3 Evaluate L D ([Θ; θ]) by Eq. 2; 4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Detail learning steps within a task T Input: T , learning rates β and γ, feature extractor Θ, base learner θ, SS parameters Φ S {1,2} Output: Updated θ and Φ S {1,2} , the worst classified class-m in T 1 Sample T (tr) and T (te) from T ; 2 for samples in T (tr) do Evaluate L T (tr) ; Optimize Φ S {1,2} and θ by Eq. 4 and Eq. 5; 7 while not done do Sample class-k in T (te) ;</figDesc><table><row><cell></cell><cell>j</cell><cell>by</cell></row><row><cell></cell><cell>Algorithm 2 ;</cell></row><row><cell>19</cell><cell>end</cell></row><row><cell cols="2">20 21 end Empty {m}.</cell></row><row><cell cols="2">Algorithm 2: 4 Optimize θ by Eq. 3;</cell></row><row><cell cols="2">5 end</cell></row><row><cell>6 9</cell><cell></cell></row></table><note>13 Get the returned class-m then add it to {m};14 end 15 Sample hard tasks {T hard } from ⊆ p(T |{m});16 while not done do17 Sample task T hardj ∈ {T hard } ;18 Optimize Φ S {1,2} and θ with T hard38</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is part of NExT research which is supported by the National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative. It is also partially supported by German Research Foundation (DFG CRC 1223), and National Natural Science Foundation of China (61772359).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-shot learning method</head><p>Feature extractor 1-shot 5-shot</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data augmentation</head><p>Adv. ResNet, <ref type="bibr" target="#b28">[29]</ref> WRN-40 (pre) 55.2 69.6 Delta-encoder, <ref type="bibr" target="#b43">[44]</ref> VGG-16 (pre) 58.7 73.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric learning</head><p>Matching Nets, <ref type="bibr">[</ref>  <ref type="table">Table 3</ref>. The 5-way with 1-shot, 5-shot and 10-shot classification accuracy (%) on Fewshot-CIFAR100 (FC100) dataset. "pre" means pre-trained for a single classification task using all training datapoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary materials</head><p>These materials include the details of network architecture ( §A), implementation ( §B), FC100 dataset splits ( §C), standard variance analysis ( §D), additional ablation results ( §E), and some interpretation of our meta-learned model ( §F). In addition, our open-source code is on GitHub 2 . <ref type="bibr" target="#b4">5</ref> ResNet-12 (pre) 60.2 ± 1.8 74.3 ± 0.9 43.6 ± 1.8 55.4 ± 0.9 62.4 ± 0.8  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Few-shot generative modelling with generative matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the optimization of a synaptic learning rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cloutier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gecsei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimality in Artificial and Biological Neural Networks</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="6" to="8" />
		</imprint>
		<respStmt>
			<orgName>Univ. of Texas</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale hard sample mining with monte carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Canévet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bilevel programming for hyperparameter optimization and meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using fast weights to deblur old memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CogSci</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Griffiths. Recasting gradient-based meta-learning as hierarchical bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated curriculum learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning structure and strength of CNN filters for small sample size training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Keshari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>1703.09554</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. arXiv, 1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gradient-based meta-learning with learned layerwise metric and subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta-sgd: Learning to learn quickly for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generative adversarial residual pairwise networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dukkipati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1703" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Snail: A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Meta-neural networks that learn by learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mammone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN, 1992</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TADAM: task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Curriculum learning of multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Kakadiaris. Curriculum learning for multi-task classification of visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nikou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deltaencoder: an effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adapted deep embeddings: A synthesis of methods for k-shot inductive transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ridgeway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Natural and effective obfuscation by head inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A domain based approach to social relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to learn: Introduction and overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Lowshot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Transfer learning via learning to transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Curriculum learning by transfer learning: Theory and experiments with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adapting SVM classifiers to data with shifted distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM Workshops</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Geoffrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Metagan: An adversarial approach to few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Grahahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
