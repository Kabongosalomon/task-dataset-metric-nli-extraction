<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A New Local Transformation Module for Few-shot Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanman</forename><surname>Meng</surname></persName>
							<email>fmmeng@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A New Local Transformation Module for Few-shot Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Few-shot Segmentation · Transformation Module · Atten- tion · Matrix Transformation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot segmentation segments object regions of new classes with a few of manual annotations. Its key step is to establish the transformation module between support images (annotated images) and query images (unlabeled images), so that the segmentation cues of support images can guide the segmentation of query images. The existing methods form transformation model based on global cues, which however ignores the local cues that are verified in this paper to be very important for the transformation. This paper proposes a new transformation module based on local cues, where the relationship of the local features is used for transformation. To enhance the generalization performance of the network, the relationship matrix is calculated in a high-dimensional metric embedding space based on cosine distance. In addition, to handle the challenging mapping problem from the low-level local relationships to high-level semantic cues, we propose to apply generalized inverse matrix of the annotation matrix of support images to transform the relationship matrix linearly, which is non-parametric and class-agnostic. The result by the matrix transformation can be regarded as an attention map with high-level semantic cues, based on which a transformation module can be built simply. The proposed transformation module is a general module that can be used to replace the transformation module in the existing few-shot segmentation frameworks. We verify the effectiveness of the proposed method on Pascal VOC 2012 dataset. The value of mIoU achieves at 57.0% in 1-shot and 60.6% in 5-shot, which outperforms the state-ofthe-art method by 1.6% and 3.5%, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image segmentation is a basic computer vision task <ref type="bibr" target="#b0">[1]</ref>. In recent years, with the rapid development of deep learning method, several convolution neural network based segmentation methods have improved the performance of image segmentation greatly, such as FCN <ref type="bibr" target="#b0">[1]</ref> and DeepLab v3 <ref type="bibr" target="#b1">[2]</ref>. However, these methods rely heavily on a large amount of annotations. In order to overcome this shortcoming, few-shot segmentation task <ref type="bibr" target="#b2">[3]</ref> is proposed to achieve segmentation of new class with a few of manual annotations, such as one annotation (1-shot segmentation) and five annotations (5-shot segmentation). Few-shot segmentation is a challenging task due to the asymmetry of training data and testing data.</p><p>In few-shot segmentation task, the annotated and unlabeled images are called support images and query images respectively <ref type="bibr" target="#b2">[3]</ref>. The existing models <ref type="bibr" target="#b2">[3]</ref> [4] <ref type="bibr" target="#b4">[5]</ref> [6] <ref type="bibr" target="#b6">[7]</ref> usually consist of three terms. 1) The support branch that extracts feature from support images. 2) The query branch that extracts feature from query images.</p><p>3) The transformation module that transfers the features between support branch and query branch to facilitate the segmentation of query branch. The essential term is the transformation module and the most challenging obstacle is how to design a transformation module that is class-agnostic, so that the transformation module can be generalized to new classes efficiently. The existing methods <ref type="bibr" target="#b4">[5]</ref>  <ref type="bibr" target="#b6">[7]</ref> use the global cues of the support image to model the transformation process, which however ignores the geometry relationships of the local features. This paper demonstrates that the geometry relationships of the local features are very useful to the transformation module.</p><p>This paper proposes a new transformation module based on local cues, where the relationship of the local features is used to accomplish the transformation. Our idea is to use linear transformation of the relationship matrix in a highdimensional metric embedding space to accomplish the transformation. To this end, we firstly map the local features into an embedding space, where cosine distance is used to obtain the relationship matrix of local features. Then, the relationship matrix is transformed linearly by the generalized inverse matrix of the annotated matrix of support image. After linear transformation, the result is regarded as an attention map containing high-level semantic information, by which we establish a new attention transformation module. We verify the effectiveness of our transformation module on Pascal VOC 2012 dataset <ref type="bibr" target="#b8">[9]</ref>. The value of mIoU achieves at 57.0% in 1-shot and 60.6% in 5-shot, which outperform the state-of-the-art method by 1.6% and 3.5%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>Few-shot segmentation is a task that uses a few of annotations to segment unknown images for new classes.</p><formula xml:id="formula_0">Let S = {(I i s , Y i s )} k i=1</formula><p>be a set of support images and the corresponding manual annotations. Let Q = {I q } be query images set that needs to be segmented. The images in S and Q belong to the same new class l ∈ {L test }. Let {L train } be the training dataset of known classes that already exist, and {L train } ∩ {L test } = ∅. The goal of few-shot segmentation is to build a model f (I q , S) by {L train } that outputs binary mask Y q for query image I q based on S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Overview</head><p>Similar to the existing few-shot segmentation network, the proposed framework includes a support branch, a query branch and a transformation module, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. In order to make the network more generalized to unseen classes, our feature extraction backbone adopts relatively shallow layers, such as the first three layers of Resnet50 <ref type="bibr" target="#b17">[18]</ref>. In addition, the support branch and the query branch share the feature extraction backbone. After obtaining deep features F s and F q from support image and query image, F s is weighted spatially by the annotated mask G s to get the features F s , which guarantees the features F s only containing corresponding foreground regions. Such process can be represented as</p><formula xml:id="formula_1">F s (i, j) = F s (i, j) × G s (i, j)<label>(1)</label></formula><p>where i, j is spatial location of feature map F s or annotated mask G s . Then, the learned features F q and F s are mapped into a high-dimensional embedding space by further convolution operations T e to get corresponding embedding features E q and E s respectively, so that cosine distance can be used to calculate the relationship between the feature pixels in this space. Simultaneously, the feature F q is learned by convolution operation T f to getF q .</p><p>Based on the embedding E s , E q and the groundtruth mask of support images G s , the proposed T ransf ormer applies linear transformation of matrix to obtain the attention map A with high-level semantic cues. The detailed description refers to section 2.3. The attention map A finally filters the deep featuresF q tô F q byF</p><formula xml:id="formula_2">q (i, j) =F q (i, j) × A(i, j)<label>(2)</label></formula><p>where i, j is spatial location of feature mapsF q or attention map A. It is seen that the attention map A indicates a rough object area to be segmented. This coarse object area provides high-level semantic information for the subsequent U psam sub-network. We next use U psam sub-network to generate segmentation mask fromF q . The U psam sub-network is shown in <ref type="figure">Fig. 2</ref>. Specifically, in order to handle the scale changes of the object better, we introduce multi-scale feature fusion module ASPP <ref type="bibr" target="#b14">[15]</ref> and residual connection <ref type="bibr" target="#b17">[18]</ref> in U psam. The output of U psam is the probability map M with the same size to the query image, and the cross entropy loss in Eq. (3) is used to supervise the training of the model, i.e.,</p><formula xml:id="formula_3">L m = i j −(Y (i, j)log(M (i, j)) + (1 − Y (i, j))log(1 − M (i, j))) (3)</formula><p>where Y is the groundtruth mask of the query image, M is the predicted probability map of our few-shot segmentation model, and i, j is the spatial location of Y or M .</p><formula xml:id="formula_4">+ + + + Upsam Result Convolution Layers Deconvolution Layers + Element-wise Addition ′ Fig. 2.</formula><p>The structure of U psam module. The residual connection <ref type="bibr" target="#b17">[18]</ref> and multi-scale strategy ASPP <ref type="bibr" target="#b14">[15]</ref> are implemented to handle the scale variations of object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transformation module</head><p>Existing methods often establish the transformation between support image and query image by the global features of the support image, thus lose local geometric information, which however is also important to the transformation. The proposed transformation module is designed based on the relationship between local pixels (represented by the relationship matrix in NON-Local model <ref type="bibr" target="#b7">[8]</ref>), and more accurate segmentation can be realized by propagating local relationships.</p><p>The detailed steps are illustrated in <ref type="figure">Fig. 3</ref>. The transformation is achieved by the linear transformation of the relationship matrix. Specifically, the relationship matrix between E s (for support image) and E q (for query image) is linearly transformed by the generalized inverse matrix of G s (groundtruth mask matrix of the support image), which overcomes the difficulty of transforming local relationship matrix to high-level semantic information. The result of linear transformation is the attention map A of the query image.</p><p>Relationship Matrix For few-shot segmentation task, it is very important to model the relationship between each pair of deep local features of support image and query image. Due to the local computational nature of the convolution operation, the relationships between long-distance pixels cannot be established directly. Therefore, NON-Local <ref type="bibr" target="#b7">[8]</ref> structure was proposed to conquer it, where the feature tensor is reshaped into a matrix, and a relationship matrix is established by matrix product. This relationship matrix contains the relationships between each pair of local deep features. We imitate the relationship matrix in NON-Local <ref type="bibr" target="#b7">[8]</ref> to establish the relationship in few-shot segmentation.</p><p>In NON-Local <ref type="bibr" target="#b7">[8]</ref>, it is only desirable to establish long-distance constraints, and the matrix product is just used to describe the relationship between two local features. For few-shot segmentation task, this is a rough description of feature similarity. Therefore, in our proposed transformation module, the feature is firstly mapped into an embedding space, in which the cosine distance can be used to calculate the relationship between local features. Such process is represented as</p><formula xml:id="formula_5">R ij = E si , E qj E si 2 E qj 2<label>(4)</label></formula><p>where E si represents the ith local information in the embedding E s , and E qj represents the jth local information in the embedding E q . So we have established a relationship matrix R between query image and support image.</p><p>Linear transformation based on generalized inverse matrix With the above relationship matrix R, how to convert this relationship matrix R to highlevel semantic information of query image becomes another key point. Let G s and G q be the binary groundtruth mask of support image and query image respectively. Based on Eq. (4), the true relationship matrix R truth between query image and support image can be simplified by matrix product between G s and G q , i.e.,</p><formula xml:id="formula_6">R truth = G q · G s<label>(5)</label></formula><p>where · demonstrates matrix product. The original size of G s and G q are H ×W . The reshaped size of G s and G q are 1 × HW and HW × 1 respectively. The size of R truth is HW × HW , which contains the relationship information of each pair of local feature pixels of E q and E s . Our target is to obtain G q based on Eq. (5). We suppose the matrix R is approximately equal to the true relationship matrix R truth between query image and support image. Furthermore, we relax the binary groundtruth mask G q to the soft attention map A, which provides high-level semantic information. Since G s is known for few-shot segmentation task, the problem is transformed to get the attention map A based on,</p><formula xml:id="formula_7">R = A · G s<label>(6)</label></formula><p>Moreover, since the G s is not square matrix, its inverse matrix does not exist. But it can be regarded as matrix with row full rank. According to the generalized inverse matrix theory <ref type="bibr" target="#b18">[19]</ref>, the transformation problem can be represented by:</p><formula xml:id="formula_8">A = R · [(G s ) T (G s (G s ) T ) −1 ]<label>(7)</label></formula><p>where · demonstrates matrix product. (G s ) T (G s (G s ) T ) −1 is the right inverse matrix (one type of generalized inverse matrix) of G s . This is just a process that applies the generalized inverse matrix of G s to linearly transform the relationship matrix R. Finally, the attention map can be obtained by Eq. <ref type="formula" target="#formula_8">(7)</ref> directly.</p><p>In order to ensure that the learned relation matrix R is consistent with R truth during training, the mean square error loss is used to supervise it, i.e.,</p><formula xml:id="formula_9">L r = R − R truth 2 2 (8)</formula><p>Attention Map By linearly transforming the relationship matrix R by Eq. <ref type="bibr" target="#b6">(7)</ref>, the attention map of the query image A is obtained, with reshaped size to H × W . Moreover, we normalize it to 0 ∼ 1 bŷ</p><formula xml:id="formula_10">A = A − min(A) max(A) − min(A)<label>(9)</label></formula><p>whereÂ is normalized counterpart of A, for the convenience of expression, we do not distinguish them.</p><p>The deep feature of the query imageF q is filtered by the normalized attention mapÂ to getF q by Eq. (2). ThenF q is proceeded by U psam (as shown in <ref type="figure">Fig.  2</ref>) to obtain the segmentation result.</p><p>In order to ensure the accuracy of the attention map A, we regard it as the foreground probability map of the segmentation result, and 1 − A as the background probability map. The two maps are concatenated and resized to the same size of original query image (by bilinear interpolation). The combined map M a can be regarded as a segmentation result and supervised by the cross entropy loss.</p><formula xml:id="formula_11">L a = i j −(Y (i, j)log(M a (i, j)) + (1 − Y (i, j))log(1 − M a (i, j)))<label>(10)</label></formula><p>where Y is the groundtruth mask of the query image, M a is the probability map derived from attention map A. i and j are the spatial location of Y and M a . For 5-shot, there are five support images. In order to combine the attention maps provided by the five different images, we simply average the attention maps by</p><formula xml:id="formula_12">A 5−shot = 1 5 i A i<label>(11)</label></formula><p>In the training stage, we combine the three losses in Eq. <ref type="formula">(3)</ref>, <ref type="formula" target="#formula_1">(10)</ref> and <ref type="formula">(8)</ref> to supervise the learning of our model, i.e.,</p><formula xml:id="formula_13">L = λ m L m + λ a L a + λ r L r<label>(12)</label></formula><p>where λ m , λ a , λ r are the weights of corresponding loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>The project of our method is built based on the Pytorch library, Adam <ref type="bibr" target="#b15">[16]</ref> optimizer is adopted to update the parameters, and all experimental code is executed on a machine equipped with a Titan XP GPU. We set the initial learning rate to 1e-4. The backbone network of our feature extraction is pretrained on ImageNet <ref type="bibr" target="#b16">[17]</ref> dataset, and the parameters of the previous layers of backbone are frozen, and we apply the first three layers of Resnet50 <ref type="bibr" target="#b17">[18]</ref> as our backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Detail of Implementation</head><p>We validate the proposed method on the Pascal VOC 2012 <ref type="bibr" target="#b8">[9]</ref> dataset and its enhanced dataset SDS <ref type="bibr" target="#b11">[12]</ref>. Similar to the existing methods <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, we split images of 20 classes into four subsets, each of which contains images of five classes, the detailed description can be found in <ref type="table">Table 1</ref>. For these four subsets, three of them are selected as the training set, and the rest one is used as the test set to validate the effectiveness of the proposed method. In the training stage, we <ref type="table">Table 1</ref>. The detailed setting for splitting the sub-dataset to evaluate the few-shot segmentation. There are 4 sub-datasets, and P ASCAL − 5 i represents the ith subset, where i = {0, 1, 2, 3}. When the i-th sub-dataset is selected for evaluation, the rest three datasets are used for training.</p><p>sub-dataset corresponding classes P ASCAL − 5 0 aeroplane,bicycle,bird,boat,bottle P ASCAL − 5 <ref type="bibr" target="#b0">1</ref> bus, car, cat, chair, cow P ASCAL − 5 2 diningtable, dog, horse, motorbike, person P ASCAL − 5 3 potted plant, sheep, sofa, train, tv/monitor randomly select two images for each class, one as a support image and another as a query image until all images of training classes were selected. In the testing stage, in order to make a fair comparison with the existing methods, we use the same random seed in the existing method to sample the same 1000 pairs of images as the test data for each evaluation sub-dataset. We use the mean intersection over union of foreground (mIoU) to measure the performance of our proposed method, which is widely used in few-shot segmentation. In addition, the FB-IoU proposed in co-FCN <ref type="bibr" target="#b3">[4]</ref> is also considered, which includes mean intersection over union of foreground and background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with Benchmarks</head><p>In order to verify the effectiveness of our method, we compare with existing method in 1-shot and 5-shot. We follow CA-Net <ref type="bibr" target="#b6">[7]</ref> to adopt DenseCRF <ref type="bibr" target="#b13">[14]</ref> and multi-scale evaluation strategy to improve the performance, which are always employed in existing <ref type="bibr" target="#b14">[15]</ref> semantic segmentation method. The detailed results can be found in <ref type="table" target="#tab_0">Table 2</ref>, <ref type="table">Table 3 and Table 4</ref>. We can see the values of mIoU by our method achieve at 57.0% in 1-shot and 60.6% in 5-shot, which outperform the state-of-the-art few-shot segmentation method CA-Net [7] by 1.6% and 3.5% <ref type="table">Table 3</ref>. The comparison results (mIoU value) on four evaluation sub-datasets in 5-shot. The best results are in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>P ASCAL − 5 0 P ASCAL − 5 1 P ASCAL − 5 2 P ASCAL − 5   respectively. The improvement in 5-shot indicates the superiority of our method when it comes to more annotations. In addition, the values of FB-IoU in 1-shot and 5-shot achieve at 71.8%, 74.6% respectively, which also outperforms the comparison methods obviously. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation</head><p>In the training stage, the weight λ m , λ a and λ r are set to 1. In order to validate the effectiveness of our three loss functions, ablation experiment is implemented. The detailed results can be found in <ref type="table">Table 5</ref>. We can see L a and L r improve the performance by 1.1% and 1.0% respectively. In addition, we follow CA-Net <ref type="bibr" target="#b6">[7]</ref> to employ DenseCRF <ref type="bibr" target="#b13">[14]</ref> and multi-scale evaluation in our test stage. The ablation of these two strategies is also conducted, and the detailed results can be found in <ref type="table" target="#tab_3">Table 6</ref>. We can see that DenseCRF <ref type="bibr" target="#b13">[14]</ref> and the multi-scale evaluation strategy can improve the mIoU value by 0.4% and 0.3% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Subjective Result</head><p>The subjective results of the proposed method are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. The support image, the ground-truth mask of the support image, the query image, the ground-truth mask of the query image and the segmentation result are displayed from left column to right column, respectively. It is seen that the proposed method segments objects from these images successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposes a new transformation module for few-shot segmentation. Rather than focusing on global cues, the relationships of local features are used to form the transformation. Local feature relationship matrix calculated by the cosine similarity is used to represent the relationships of local features. Linear transformation of relationship matrix based on generalized inverse of the groundtruth matrix is implemented to transform the relationship matrix. We also map the features into a high-dimensional metric embedding space to enhance the generalization of the proposed module. We propose a new few-shot segmentation network based on transformation module, and better results are obtained in terms of both mIoU value and FB-IoU value.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The pipeline of the proposed method. The Feature Extractor extracts the features Fs and Fq. The feature Fs is weighted spatially by the groundtruth mask Gs to obtain the feature F s . The deep features Fq and F s are mapped into an embedding space by Te to get Eq and Es respectively. Simultaneously, the feature Fq is learned by T f to getFq. Based on Es, Eq and Gs, the proposed T ransf ormer outputs attention map A, which weightsFq spatially. The segmentation result is obtained through U psam module finally.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Fig. 3 .</head><label>13</label><figDesc>The detailed information of the proposed transformation module. The embedding Es and Eq are reshaped to matrix. Then, the relationship matrix R is obtained based on Eq and Es by cosine similarity. Finally, the relationship matrix R is transformed linearly by the matrix of generalized inverse matrix of Gs. After reshape operator, the attention map A is obtained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The subjective results of the proposed method. From left to right: query image, ground-truth mask of the query image, the support image, ground-truth mask of the support image and the segmentation result, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>The comparison results (mIoU value) on four evaluation sub-datasets in 1-shot. The best results are in bold.Methods P ASCAL − 5 0 P ASCAL − 5 1 P ASCAL − 5 2 P ASCAL − 5</figDesc><table><row><cell>3 Mean</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>The comparison results (FB-IoU value) on four evaluation sub-datasets in 1-shot and 5-shot. The best results are in bold. The ablation results of three loss functions. The ticking indicates that the loss function is used.</figDesc><table><row><cell cols="8">Methods OSLSM [3] co-FCN [4] PL [11] SG-One [5] A-MCG [6] CA-Net [7] Ours</cell></row><row><cell>1-shot</cell><cell>61.3</cell><cell>60.1</cell><cell>61.2</cell><cell>63.1</cell><cell>61.2</cell><cell>66.2</cell><cell>71.8</cell></row><row><cell>5-shot</cell><cell>61.5</cell><cell>60.2</cell><cell>62.3</cell><cell>65.9</cell><cell>62.2</cell><cell>69.6</cell><cell>74.6</cell></row><row><cell></cell><cell></cell><cell>k-shot Lm</cell><cell>Lr</cell><cell>La</cell><cell>mIoU FB-IoU</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1-shot</cell><cell></cell><cell></cell><cell>55.3 70.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1-shot</cell><cell></cell><cell></cell><cell>55.9 70.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1-shot</cell><cell></cell><cell></cell><cell>56.0 71.1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1-shot</cell><cell></cell><cell></cell><cell>57.0 71.8</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>The effectiveness of Dense-CRF post-processing and Multi-scale strategy are demonstrated. The ticking indicates that the strategy is used.</figDesc><table><row><cell cols="2">k-shot Dense-CRF Multi-scale mIoU FB-IoU</cell></row><row><cell>1-shot</cell><cell>56.1 71.0</cell></row><row><cell>1-shot</cell><cell>56.7 71.5</cell></row><row><cell>1-shot</cell><cell>56.6 71.3</cell></row><row><cell>1-shot</cell><cell>57.0 71.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L I E B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conditional networks for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D A E S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelha-Mer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09091</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-based Multi-Context Guiding for Few-Shot Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advance of Artificial Intelligence</title>
		<meeting>the Association for the Advance of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5217" to="5226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Few-Shot Semantic Segmentation with Prototype Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-11-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-segmentation by composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krhenbhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
		<title level="m">Generalized inverse of a matrix and its applications. Icams Conference</title>
		<imprint>
			<date type="published" when="1972" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
