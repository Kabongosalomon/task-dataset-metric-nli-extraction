<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Learning with Stronger Augmentations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20211">APRIL 2021 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
						</author>
						<title level="a" type="main">Contrastive Learning with Stronger Augmentations</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<date type="published" when="20211">APRIL 2021 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Contrastive Learning</term>
					<term>Distributional Divergence</term>
					<term>Data Augmentation</term>
					<term>Strong Augmentation</term>
					<term>Self-Supervised Learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representation learning has significantly been developed with the advance of contrastive learning methods. Most of those methods have benefited from various data augmentations that are carefully designated to maintain their identities so that the images transformed from the same instance can still be retrieved. However, those carefully designed transformations limited us to further explore the novel patterns exposed by other transformations. Meanwhile, as found in our experiments, the strong augmentations distorted the images' structures, resulting in difficult retrieval. Thus, we propose a general framework called Contrastive Learning with Stronger Augmentations (CLSA) to complement current contrastive learning approaches. Here, the distribution divergence between the weakly and strongly augmented images over the representation bank is adopted to supervise the retrieval of strongly augmented queries from a pool of instances. Experiments on the ImageNet dataset and downstream datasets showed the information from the strongly augmented images can significantly boost the performance. For example, CLSA achieves top-1 accuracy of 76.2% on ImageNet with a standard ResNet-50 architecture with a single-layer classifier fine-tuned, which is almost the same level as 76.5% of supervised results. The code and pre-trained models are available in https://github.com/maple-research-lab/CLSA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks have shown their sweeping successes in learning from large-scale labeled datasets like ImageNet <ref type="bibr" target="#b0">[1]</ref>. However, such successes hinge on the availability of a large number of labeled examples that are expensive to collect. To address this challenge, unsupervised visual representation learning and self-supervised learning have been extensively studied to learn feature representations without labels. Researchers aim to build a framework that can self-learn representations, which does not suffer from the request of many labels and can learn robust and general representations. Among them, the contrastive learning <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, which is one formulation of the instance learning, shows significant potentials to close the performance gap with supervised methods.</p><p>In instance learning <ref type="bibr" target="#b6">[7]</ref>, each image is being considered as an instance, and we wish to train the network so that the representations of different augmented views of the same instance are as close as possible to each other. Meanwhile, the representation of different views from different instances should be distinctive to each other. To achieve this, one widely adopted method is contrastive learning <ref type="bibr" target="#b1">[2]</ref>, which minimizes the similarity between views from the same instance while maximizes the similarity among views from different instances at the same time. To further improve contrastive learning, various methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> are proposed to explore different directions, such as the number of negative examples <ref type="bibr" target="#b8">[9]</ref>, the quality of negative examples <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, data augmentation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>On the one hand, it is worth noting that these methods usually rely on image augmentations that are carefully designated to maintain their instance identities so that the augmentation of an instance can be accurately retrieved from a pool of instances. The big impact of the data augmentation design has been clearly investigated in InfoMin <ref type="bibr" target="#b15">[16]</ref>, which also suggests the potentials of strong augmentations. On the other hand, novel patterns exposed by stronger augmentations can further boost the model's performance, which is clearly demonstrated in supervised and semi-supervised tasks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Hence, we believe the patterns embedded in stronger augmentations could also contribute to self-supervised learning by improving the generalizability of learned representations and eventually close the gap with the fully supervised models. However, directly using stronger augmentations in contrastive learning could deteriorate the performance, because the induced distortions could ridiculously change the image structures and thus the transformed images cannot keep the identity of the original instances. Thus, additional efforts are needed to explore the role of the stronger augmentations to further boost self-supervised learning.</p><p>Thus we propose the CLSA (Contrastive Learning with Stronger Augmentations) framework to address this challenge. Instead of applying strongly augmented views to the contrastive loss, we propose to minimize the distribution divergence between the weakly and strongly augmented images over a representation bank to supervise the retrieval of stronger queries. First, this design avoids an over-optimistic assumption that the strongly augmented view's embedding should be identical with that of the weakly augmented view. Meanwhile, leveraging weakly augmented counterparts' distributions enables our framework to explore the novel pattern carried by strongly augmented views. Most importantly, since it's independent to the contrastive loss, our framework can combine with any contrastive loss-based methods, such as MoCo <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, SimCLR <ref type="bibr" target="#b7">[8]</ref>, BYOL <ref type="bibr" target="#b19">[20]</ref> and so on. Our experiments have shown that our framework can greatly boost performance by introducing the distributional loss, which clearly suggests the novel distributional loss can explore the novel patterns exposed by the stronger augmentations and inherit the knowledge about the relative similarities to the negative samples. Furthermore, our experiments also verify that CLSA does not only improved the feature representation quality of weakly augmented views, but also further enhanced the representations of strongly augmented views at the same time. This property can greatly extend the self-supervised learning application on non-natural images, where the images are not carefully organized and the qualities of images cannot be guaranteed.</p><p>The experiments on various datasets demonstrate that the proposed framework can significantly boost the performance by learning from stronger augmentations. On the ImageNet linear evaluation protocol, we reach a record 76.2% top-1 accuracy with the standard ResNet-50 backbone, which is almost as high as 76.5% top-1 accuracy of the fully supervised model. Meanwhile, it also achieves competitive performances on several downstream tasks. Among them is a top-1 accuracy of 93.6% on VOC07 by the linear classifier with a pre-trained ResNet-50 compared to the previous record of 88.9% top-1 accuracy. For the COCO object detection, the AP S for small object detection has been improved to 24.4% from the previous best AP S of 20.8%. These results show that the CLSA can more effectively capture the semantic information than the previous self-supervised methods on downstream tasks by introducing the strongly augmented images. We also conduct an ablation study to show a naive application of stronger augmentations in contrastive learning would degrade the performances.</p><p>Our contribution can be summarized as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We are the first to explore the stronger augmentations to contribute to self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We propose a distributional loss to transfer knowledge from weakly augmented views to strongly augmented views.</p><p>• CLSA can easily integrate with concurrent contrastive loss-based methods and greatly boost their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We carefully carry the ablation study to verify the impact of the distributional loss.</p><p>• CLSA framework can self-train network to improve representation for weakly augmented images and strongly augmented images simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-Supervised Learning</head><p>Self-supervised learning methods have been widely studied to close the gap with supervised learning and alleviate the time and cost for labeling a large amount of data. These methods can be categorized into five different aspects. Instance Discrimination and Contrastive Learning Each image is considered as an individual class in an instance discrimination setting <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. It can be further formulated as contrastive learning <ref type="bibr" target="#b1">[2]</ref>, whose core idea is to pull the positive pairs together and push the negative pairs away in the embedding space. As studied in many prior works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, it is very crucial to construct high-quality positive and negative pairs to achieve higher performance. In particular, InstDisc <ref type="bibr" target="#b8">[9]</ref> built a memory bank that stores pre-computed representations as negative pairs to increase the size of the negative pair to improve the performance. Following this work, MoCo <ref type="bibr" target="#b4">[5]</ref> used a momentum update mechanism to maintain a long queue of negative examples for contrastive learning. This momentum encoder design greatly improved the quality of negative pairs and led to a big performance increase compared to previous works. SimCLR <ref type="bibr" target="#b7">[8]</ref> further improved by directly utilizing negative samples in the current batch with a much bigger batch size. Meanwhile, it carefully constructed a rich family of data augmentations on cropped images, which significantly boosted classification accuracy. MoCo V2 <ref type="bibr" target="#b3">[4]</ref> verified the improvement by introducing the same data augmentation and MLP layer design to MoCo framework. However, these methods failed to improve the performance by naively applying stronger augmentations to minimize the contrastive loss, which motivated the proposed work.</p><p>Generative Methods The generative methods typically adopt auto-encoders <ref type="bibr" target="#b22">[23]</ref>, and adversarial learning <ref type="bibr" target="#b23">[24]</ref> to train an unsupervised representation. Usually, they focused on the pixel-wise information of images to distinguish images from different classes. For instance, <ref type="bibr" target="#b23">[24]</ref> adopted BiGAN to capture the relationship between latent semantic representations and the input images.</p><p>Clustering Clustering <ref type="bibr" target="#b24">[25]</ref> can also be used to learn visual representations by assigning pseudo cluster labels to individual samples. DeepCluster <ref type="bibr" target="#b24">[25]</ref> generalized k-means by alternating between assigning pseudo-labels and updating networks. Recently, SWAV <ref type="bibr" target="#b5">[6]</ref> is proposed to learn a cluster of prototypes by enforcing consistency between cluster assignments for different views, which has achieved state-of-the-art performance on ImageNet.</p><p>Like contrastive learning, memory bank, large batches, and prototypes queue are also utilized in the clustering-based methods. Moreover, though clustering-based methods do not specifically use negative pairs, the cluster centers can be viewed as negative prototypes. Hence, clustering-based approach can also be viewed as an extension of contrastive learning.</p><p>Consistency Representation Learning Instead of directly using contrastive loss, researchers <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b25">[26]</ref> found consistency representation learning between positive pairs can also enforce the network to learn robust representations. In BYOL <ref type="bibr" target="#b19">[20]</ref>, researchers first found that we can self-train an encoder without using negative examples. It utilized the Siamese architecture, where the query branch has a predictor architecture in addition to the encoder and projector. The encoder can learn good representations by simply minimizing the cosine similarity between query embedding and key embedding. Following BYOL, Simsiam <ref type="bibr" target="#b25">[26]</ref> further removed the momentum key encoder and used the stop-gradient strategy to avoid the collapsing problems. Furthermore, SCRL <ref type="bibr" target="#b26">[27]</ref> further applied the consistency loss for ROI of the intersection region of two views to improve the encoder representation for downstream detection tasks.</p><p>Pretext Tasks In addition to contrastive learning, there exist many alternative methods using different pretext tasks <ref type="bibr" target="#b27">[28]</ref> to train networks with the carefully designed supervised signals from images. For example, <ref type="bibr" target="#b28">[29]</ref> proposed Jigsaw puzzles as supervised signal to train a convolutional neural network. <ref type="bibr" target="#b29">[30]</ref> used the relative positions of two randomly sampled patches as the supervised signal. <ref type="bibr" target="#b30">[31]</ref> used the transformations between two images as supervised signals to guide the representation learning. For more details about these works, please refer to the survey by <ref type="bibr" target="#b31">[32]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STRONGER AUGMENTATIONS</head><p>In this section, we will first review the preliminary work on contrastive learning, and discuss their strength and limitations in Section. 3.1. Then in Section. 3.2, we will present a new distributional divergence loss between weakly and strongly augmented images to self-train the representations by leveraging the underlying visual semantic information from strongly augmented views. After that, the implementation details are explained in Section. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contrastive Learning</head><p>Contrastive learning <ref type="bibr" target="#b1">[2]</ref> is a popular self-supervised idea and made great success in recent years with the advance of computation and various image augmentations. Its goal is to find a parametric function f θ that maps the input image x ∈ R D to a feature representation z = f θ (x) ∈ R d , such that the feature representation z in the feature space can reflect the semantic similarities in the input space. To this end, the contrastive loss is proposed to optimize the network f θ , which encourages z and its positive pair z to be close in the feature space, and pushes away representations of all other negative pairs. After SimCLR <ref type="bibr" target="#b7">[8]</ref>, projector g is imported to further map the representation as z = g θ (f θ (x)) for contrastive pre-training, while we still only use f θ (x) for downstream tasks (classification/detection). This design has been proved to the key to boost performance for contrastive learning <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the latest general framework of contrastive learning methods. In the supervised settings, the contrastive loss can be achieved by defining the same class images as positive pairs while remaining others as negative pairs. Similarly, the definition of positive pairs in contrastive loss is inspired by instance learning <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, where random augmented crops of the same image could be defined as positive pairs and crops from other images are all regarded as negatives. Hence, the contrastive The left is the original image, the middle is the weakly augmented image, and the right is the strongly augmented one with overcontrastive details. loss in self-supervised learning is to maximize the agreement of representations of different views of the same instance while minimizing the agreement with other negative samples.</p><p>Specifically, for each image x in batch B, we apply two transformations T and T to obtain two different views V and V of the same instance x. Then they go through a query encoder f θ and a key encoder f φ respectively, followed with MLP projection layers (g θ /g φ ), resulting in two embedded representations z and z to calculate the constrastive loss in Eq. (1).</p><formula xml:id="formula_0">L C = E i∈B [− log Q(i, i+) Q(i, i+) + K k=1 Q(i, k) ] (1) with      Q(i, i+) = exp(sim(z i , z i )/τ ) Q(i, k) = exp(sim(z i , z k )/τ ) sim(z i , z k ) = z T i z k ||z i ||·||z k || (2)</formula><p>where L C is the contrastive loss, Q(i, i+) is the exponential temperature smoothed similarities of positive pairs between z i and z i , while Q(i, k) is the exponential temperature smoothed similarities of negative pairs between z i and z k , τ is the temperature parameter set to 0.2, and sim denotes the cosine similarity. Here z k is the feature embedding of other instances by f φ and g φ . K is the size of the FIFO queue M (Memory Bank) saving the feature embeddings of other instances from previous batches. In contrastive learning, success is highly dependent on two components:</p><p>1) The design of positive pairs: For the positive pairs, the data augmentation is carefully designed. In SimCLR <ref type="bibr" target="#b7">[8]</ref>, it carefully designed color-jittering, Gaussian blurring transformations to further augment random cropped views. In InfoMin <ref type="bibr" target="#b15">[16]</ref> explored the effect of different data augmentations in contrastive pre-training and has shown some combinations of data augmentation can further improve compared to MoCo <ref type="bibr" target="#b3">[4]</ref> or SimCLR <ref type="bibr" target="#b7">[8]</ref>.</p><p>2) The design of negative pairs: For the negative pairs, researchers have explored a large number of approaches to improve the number and the quality of negative pairs. For example, InstDisc <ref type="bibr" target="#b8">[9]</ref> first deployed memory bank (negative pool) to keep track of feature embeddings of previous batches as negative pairs, which greatly improved the performance by a large pool of negatives. MoCo <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> further improved the quality of negatives by using momentum encoder as key encoder g θ . SimCLR <ref type="bibr" target="#b7">[8]</ref> kept a balance of quality and number of negatives by utilizing large batch online training and using other instances in the same batch serving as negatives for optimizing contrastive loss. The potential and possibility of negatives have been fully explored in various perspectives and greatly improved representation learning. Nevertheless, the potential of positive pairs is still lacking enough studies. As illustrated aforementioned, the two views of the same instance in contrastive learning are not transformed aggressively so that they can still be viewed as the same instance. InfoMin <ref type="bibr" target="#b15">[16]</ref> has proved there still existing unexplored patterns exposed by different combinations of weak augmentations. That inspired us that augmentations for positive pairs can leverage possible semantic information for the encoder to learn. Furthermore, we do not want to rely on the carefully designed augmentation for training. Instead, we aim to explore a general and robust approach to learn from infinite data augmentations.</p><p>However, directly adopting stronger transformations (e.g., with larger rotation angles, more aggressive colorjittering and cutout) in contrastive learning fails to further improve the performance or even deteriorate it for downstream tasks, which is not surprising. Stronger transformations could distort image structures and their perceptual patterns in the learned representation so that strongly augmented counterpart views cannot be viewed as the same instance for training the underlying network. In InfoMin <ref type="bibr" target="#b15">[16]</ref>, they also only explored the combinations of weak augmentations instead of stronger augmentations that may include more semantic information and representative patterns. Since different combinations of weak augmentations provided different clues for obtaining distinctive feature representations, it's highly possible that some useful clues may only exist in the stronger augmentations. In supervised learning <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> and semi-supervised learning <ref type="bibr" target="#b18">[19]</ref>, different stronger data augmentations have been widely studied and greatly boost the performance with the novel pattern exposed by strongly augmented images. The findings in RandAugment <ref type="bibr" target="#b16">[17]</ref> have verified that strongly augmented views can provide more clues even without an explicit augmentation policy. Hence, we believe learning the representations from these novel patterns will pave the last mile to close the gap with the fully supervised representations. That further inspired us to explore novel ways to utilize stronger transformations in selfsupervised learning while avoiding deteriorated performances by naively using them in a contrastive model <ref type="bibr" target="#b3">[4]</ref>.</p><p>By exploring previous approaches and our extensive experiments, we have found it is not a straightforward task to learn the patterns embedded in the stronger augmentations. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, a strongly augmented image may look perceptually different from the original counterpart. Consequently, the representation of a strongly augmented image can be far apart from that of the weakly augmented one. Thus, naively using strongly augmented images in contrastive learning can be over-optimistic since the induced distortions could dramatically ruin their image structures.</p><p>To this end, in Section 3.2, we instead proposed the Distributional Divergence Minimization (DDM) between weakly and strongly augmented images over a representation bank to avoid overfitting the representation of a strongly augmented image with that of the corresponding positive target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distributional Divergence Minimization between Weakly and Strongly Augmented Images</head><p>Due to the limitation mentioned above, learning from the retrieval of a strongly augmented query is infeasible to self-train deep networks. Nevertheless, the distribution of relative similarities can help us understand contrastive learning in a different direction, thus inspired us to propose the distributional divergence minimization (DDM) for learning from stronger augmentations.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1, for</ref> an image x, we can first apply a transformation T to generate a query view V and then obtain its corresponding embedding z = g θ (f θ (V )). Meanwhile, we collect another key view V generated by T and the key embedding z = g φ (f φ (V )). Given a memory bank M of K negative samples {z k |k = 1, · · · , K} accumulated from key embedding z of the past iterations, we can obtain a conditional distribution</p><formula xml:id="formula_1">p(z k |z i ) = exp(sim(z i , z k )/τ ) exp(sim(z i , z i )/τ ) + K k=0 exp(sim(z i , z k )/τ )<label>(3)</label></formula><p>which encodes the likelihood of the query z i being assigned to the embedding z k from the memory bank M .</p><p>Similarly, we can also have the likelihood of positive pairs for the query z i being assigned to its positive counterpart z i from the key encoder:</p><formula xml:id="formula_2">p(z i |z i ) = exp(sim(z i , z i )/τ ) exp(sim(z i , z i )/τ ) + K k=0 exp(sim(z i , z k )/τ )<label>(4)</label></formula><p>Then, the contrastive loss in Eq. (1) can be rewritten in another form as</p><formula xml:id="formula_3">L C = E i∈B [−q(z i |z i ) log p(z i |z i ) − K k=1 q(z k |z i ) log p(z k |z i )]<label>(5)</label></formula><p>where q(z i |z i ) is the ideal distribution of the likelihood, p(z i |z i ) is the distribution learned by network.</p><p>However, we cannot obtain the ideal distribution of the likelihood from the semantic view. It is hard to measure the optimal likelihood between the query image and the key images (positive/negative). To avoid this unknown distribution exploration, contrastive loss regards q as a one-hot distribution, where positive pair has q(z i |z i ) = 1 and negatives satisfy q(z k |z i ) = 0 (k ∈ [1, K]). That means contrastive loss only maximizes the agreement of representations of different views of the same instance while minimizing the agreement with other negative samples. All other underlying complicated relationships between query images and key images are completely ignored. The advantage is that contrastive loss can greatly accelerate the convergence of representation learning and greatly improve the representation feature for classification and detection tasks. However, the information between the query image and negative images is not fully explored and may include useful clues to boost representation learning further.</p><p>Similar to the representation for weakly augmented views, a straightforward solution for exploring the patterns from the stronger augmentation is directly using the strongly augmented image as query and using the weakly augmented image as key in the contrastive loss. However, this too optimistic design preassumes that the strongly augmented view's representation should be close to that of its weakly augmented pair and far from that of the weakly augmented views of other instances. The onehot distribution fails to mimic or even approximate the optimal likelihood distribution and thus cannot help the representation learning anymore. Hence, another alternative distribution q should be proposed to address these limitations of one-hot distribution.</p><p>Though it is almost impossible to obtain the actual likelihood distribution to self-train network in a perfect way, fortunately, we found that the distribution of relative similarities of a weakly augmented image from the same instance over the representation bank can provide useful clues for stronger augmentation learning. In <ref type="figure" target="#fig_2">Fig. 3</ref>, we compared the distribution of positive pair's probabilities p(z i |z i ) (p(z i |z i )) and the distribution of variance of negative pairs' probabilities p(z k |z i ) (p(z k |z i )) for k ∈ [1, K] for weakly (strongly) augmented query. In <ref type="figure" target="#fig_2">Fig. 3A</ref>, it is clear that the initial positive probability and variance of the negative probability of strongly augmented query is identical to that of weakly augmented query given the same key from the same instance. That indicates that weakly query and strongly query shared almost identical distribution for a randomly initialized network. This property inspired us that the distribution of relative similarities of weakly augmented query can be used to supervise that of the strongly augmented query.</p><p>In <ref type="figure" target="#fig_2">Fig. 3</ref>, we further measured the distributions by the pretrained network by one of the most representative contrastive approaches MoCo <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Compared to that of a randomly initialized network, we can clearly find that a strongly augmented query's likelihood distribution becomes much more different from that of a weakly augmented query. This distribution difference suggests that concurrent contrastive methods failed to improve the representations of strongly augmented views at the same time. Our ablative studies found that the representations of strongly augmented views by contrastive methods are much less distinctive than that of weakly augmented views. Thus, we believe the distribution supervision of weakly augmented views can simultaneously improve the representation of strongly augmented views.</p><p>In a nutshell, it does not only avoid directly placing the representation of a strongly augmented image too closely to that of the positive target (like one-hot distribution did), but also allows it to explore the novel patterns of variations exposed by the strong augmentation.</p><p>Formally, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, for an original image x i , we applied another stronger augmentation S to obtain its strongly augmented view V i , and its embedding z i . Meanwhile, we can also obtain its corresponding query embedding z i and key embedding z i for the instance x by weak augmentations T and T . Similar to Eq. (3) and Eq. (4), we obtain a conditional distribution for z i based on its positive target and negative pairs:</p><formula xml:id="formula_4">   p(z k |z i ) = exp(sim(z i ,z k )/τ ) exp(sim(z i ,zi)/τ )+ K k=0 exp(sim(z i ,z k )/τ ) p(z i |z i ) = exp(sim(z i ,zi)/τ ) exp(sim(z i ,zi)/τ )+ K k=0 exp(sim(z i ,z k )/τ )<label>(6)</label></formula><p>Then, we propose to minimize the following distributional divergence between the weak and the strong queries such that By minimizing this divergence, we assume the learned representation z i of the strongly augmented query should inherit the representation z i of the weakly augmented one regarding not only its belief of the query being assigned to the corresponding positive target z i , but also its relations with the negative samples z k in the representation bank through the conditional distribution p(z k |z i ). We believe the distribution p(z k |z i ) is a much better mimic of q(z k |z i ) instead of the one-hot distribution, which is also validated in our ablation study. This will prevent direct overfitting of the strong query representation to the positive target as well as improve the generalization of the learned representation with additional clues from the other examples in the representation pool. In a more general sense, this extends the idea of knowledge distillation <ref type="bibr" target="#b34">[35]</ref>. However, we did not use the predicted labels by a teacher model to supervise a student model's training as in the knowledge distillation. Instead, we used the distribution of the likelihoods of a weak query to supervise the retrieval of a strong query from a pool of representations.</p><formula xml:id="formula_5">L D = E i∈B [−p(z i |z i ) log p(z i |z i )− K k=1 p(z k |z i ) log p(z k |z i )]<label>(7)</label></formula><p>Since the distributional divergence optimization can be viewed as a knowledge distillation process, that also indicates that the success is highly dependent on the distribution of relative similarity of weakly augmented query z . Hence, it is natural to utilize one of the current contrastive methods to learn z through the contrastive loss. That design also enables our DDM loss to be fully independent with concurrent contrastive methods and can complement any of them to further boost them. As illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>, the overall loss to optimize the encoder can be formulated as</p><formula xml:id="formula_6">L = L C + β * L D<label>(8)</label></formula><p>where β is the coefficient to balance the contrastive loss L C and distributional divergence minimization loss L D . Though other values may achieve better results for different contrastive baselines, we use β = 1 to make CLSA more general . As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the pattern from strongly augmented images and weakly augmented images are learned simultaneously. That is a very natural and effective design since contrastive loss can also be viewed as a specific form of distributional divergence minimization as we have shown before, where the supervised distribution is a one-hot distribution. In short, the overall optimization thus can also be interpreted as two simultaneous distributional divergence minimization. The extraordinary performance also supported this simple yet effective design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>In this section, we will discuss the details about the applied strong and weak augmentations for CLSA. InstDisc <ref type="bibr" target="#b8">[9]</ref> 54.0 LocalAgg <ref type="bibr" target="#b12">[13]</ref> 58.8 MoCo <ref type="bibr" target="#b4">[5]</ref> 60.8 SimCLR <ref type="bibr" target="#b7">[8]</ref> 61.9 CPC v2 <ref type="bibr" target="#b14">[15]</ref> 63.8 PCL <ref type="bibr" target="#b35">[36]</ref> 65.9 MoCo v2 <ref type="bibr" target="#b3">[4]</ref> 67.5 InfoMin Aug <ref type="bibr" target="#b15">[16]</ref>  Stronger Augmentations S As explored in the previous works ( <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b27">[28]</ref>), strong augmentations usually have two types: geometric and non-geometric augmentations. Specifically, we considered 14 types of augmentations: ShearX/Y, Transla-teX/Y, Rotate, AutoContrast, Invert, Equalize, Solarize, Posterize, Contrast, Color, Brightness, Sharpness. The magnitude of each augmentation is significant enough to produce as strong augmentations as possible. More details of different transformations are shown in <ref type="table" target="#tab_0">Table 1</ref>. For example, the shear operation is drawn from a range of [-0.3,0.3], resulting in aggressively transformed images that can be hard to retrieve given a counterpart target. In particular, to transform an image, we randomly select an augmentation from the above 14 types of transformations, and apply it to the image with a probability of 0.5. This process is repeated five times and that will strongly augment an image as the example shown in the right panel of <ref type="figure" target="#fig_1">Fig. 2</ref>. Compared to the weakly augmented image in the middle panel, it is clear the image structures of the strongly augmented views are completely changed.</p><p>Weaker Augmentations T Weak augmentations are drawn by following most of existing contrastive learning methods in literature ( <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>): an image is first cropped from an input image and resized to 224×224 pixels. Then random color jittering, Gaussian Blur, grayscale conversion, horizontal flip, channel-wise color normalization are sequentially applied to generate weakly augmented images with an example shown in the middle of <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Technical Details Similar to the previous works ( <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>), we used the ResNet-50 ( <ref type="bibr" target="#b36">[37]</ref>) as our encoder backbones f θ and f φ and a 2-layer MLP (2048-d hidden layer with the ReLU, output FC without ReLU) as the projection head g θ and g φ . The cosine similarity is used in the contrastive loss and DDM loss. The temperature τ is set to 0.2. Following MoCo <ref type="bibr" target="#b4">[5]</ref>, a momentum smoothing factor α of 0.999 is used to update key encoder f φ = α * f φ +(1−α) * f θ and key MLP g φ = α * g φ +(1−α) * g θ . The loss balancing coefficient β is set as 1.0. We set the size K of the queue M to 65536 to store the negative examples used to compute the conditional distribution of weakly and strongly augmented queries and minimize their divergence. We used the same temperature for DDM loss and contrastive loss to simplify the formulation. We believe the performance can be further improved by adjusting different temperatures for L C and L D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Details</head><p>For the unsupervised pretraining on ImageNet with the CLSA, we used the SGD optimizer ( <ref type="bibr" target="#b41">[42]</ref>) with an initial learning rate of  <ref type="bibr" target="#b38">[39]</ref> 61.5 PIRL-800epochs <ref type="bibr" target="#b2">[3]</ref> 63.6 CMC <ref type="bibr" target="#b13">[14]</ref> 66.2 SimCLR-800epochs <ref type="bibr" target="#b7">[8]</ref> 70.0 MoCo v2-800epochs <ref type="bibr" target="#b3">[4]</ref> 71.1 InfoMin Aug-800epochs <ref type="bibr" target="#b15">[16]</ref> 73.0 BYOL-1000epochs <ref type="bibr" target="#b19">[20]</ref> 74.3 SWAV-800epochs <ref type="bibr" target="#b5">[6]</ref> 75.3</p><p>CLSA-800epochs 72.2 CLSA*-800epochs 76.2 Supervised 76.5 0.03, a weight decay of 0.0001 and a momentum of 0.9. We used cosine scheduler ( <ref type="bibr" target="#b42">[43]</ref>) to gradually decay the learning rate to 0. Usually, the batch size is set to 256. When multiple GPU cluster servers are used, the batch size will be multiplied by the same number of servers by convention. To avoid collapsing, the shuffle BN <ref type="bibr" target="#b4">[5]</ref> is also adopted. The learning rate then will be adjusted based on lr = base lr * batch size/256, where base lr is 0.03. Typically, the experiment with a single strong augmentation for each training image takes roughly 70 hours to finish on 8 V100 GPUs.</p><p>For the fine-tuning on ImageNet, we trained a linear classifier on top of the frozen feature vector (2048-D) upon the pre-trained ResNet-50 with CLSA. This linear layer is trained for 100 epochs, with a learning rate of 10 without weight decay. We used the cosine learning rate decay to 0.01 and a batch size of 256.</p><p>For the transfer learning on the VOC dataset, we trained a linear classifier upon the pre-trained Resnet-50 in a similar way for ImageNet -we trained 100 epochs with the SGD optimizer and a learning rate of 0.05, a momentum of 0.9 and no weight decay. The batch size is 256 without a learning rate scheduler.</p><p>Finally, for object detection, we adopted the same protocol in ( <ref type="bibr" target="#b4">[5]</ref>) to fine-tune the pre-trained Resnet-50 backbone based on detectron2 ( <ref type="bibr" target="#b43">[44]</ref>) for the sake of a fair and straight comparison with the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Linear Classification on ImageNet</head><p>For the linear evaluation on ImageNet, we trained the CLSA in two settings. In the first setting named CLSA, we used a single stronger augmentation (see <ref type="table" target="#tab_0">Table 1</ref>) that crops each training image to a smaller size of 96 × 96, which does not incur too much computing overhead in processing these smaller augmented images. In the second setting named CLSA*, we adopted five different stronger augmentations that crop each image into various sizes: 224 × 224, 192 × 192, 160 × 160, 128 × 128, and 96 × 96. The DDM loss in Eq. (7) is the sum over these multiple stronger augmentations. A similar multi-crop strategy has been adopted in contrastive learning literature before. For example, the SWAV <ref type="bibr" target="#b5">[6]</ref> reached state-of-the-art top-1 accuracy by applying such multicrop augmentations. To ensure a fair comparison with the SWAV, we chose five stronger augmentations such that the self-training with CLSA* consumed similar computing time (i.e., 166 hours with a cluster of 8 V100 GPUs for 200 epochs of pre-training with a batch size of 256). <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_3">Table 3</ref>, we compared the performance with the other unsupervised methods. All the experiments are based on a pre-trained ResNet-50 backbone that is finetuned with a linear classifier. <ref type="table" target="#tab_1">Table 2 showed the performance  of different methods pre-trained over 200 epochs, and table 3</ref> reported models pre-trained over more epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>First, under the same contrastive protocol, the CLSA has a higher 69.4% top-1 accuracy than both MoCo v2 (67.5%) and SimCLR (61.9%) with 200 epochs training. With multiple stronger augmentations, CLSA* outperforms the state-of-the-art SWAV model using multi-crops of training images over 200 epochs (73.3% vs. 72.7%). Moreover, as shown in 3, the CLSA outperforms MoCo v2 and SimCLR with the same training epochs. It is also noteworthy that the CLSA* achieves almost the same top-1 accuracy as that of the fully supervised network (76.2% vs. 76.5%). With the advance of more contrastive learning approaches, we believe CLSA can work with them to further improve their performance to beat supervised learning performance with more robust feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transfer Learning Results on Downstream Tasks</head><p>We test the generalizability of the ResNet-50 pre-trained on ImageNet to several downstream tasks. Specifically, we focused on two tasks: cross-dataset image classification and object detection. The pre-trained ResNet-50 was frozen, and we fine-tuned the linear classifier on the VOC07trainval and tested it on the VOC07test. For object detection, we evaluated the pre-trained network on two datasets using the detectron2 <ref type="bibr" target="#b43">[44]</ref> used in the previous methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>. On the VOC dataset, we trained the detection head with VOC07+12 trainval dataset and tested it on VOC07 test dataset. We fine-tuned the network on the train2017 set with 118k images on the COCO dataset and evaluated it on the val2017. For the sake of a fair comparison, the object detection tasks are completed by detectron2 based on the pretrained ResNet-50.</p><p>As shown in <ref type="table" target="#tab_4">Table 4</ref>, the performances on both tasks are much better than the supervised model trained on ImageNet. This suggests that the proposed method has better generalization ability in downstream tasks. The pre-trained network on ImageNet by the CLSA outperformed the compared models after being finetuned on different datasets. Among them is a top-1 accuracy of 93.6% on the VOC07 with the linear classifier on the pre-trained ResNet-50 in comparison with the previous record of 88.9% top-1 accuracy by the SWAV. On the COCO dataset, the AP S for small object detection has been significantly improved to 24.4% from the previously best AP S of 20.8%. As well known, it is much more challenging to detect small objects on the COCO dataset. Thus, the better performance of the CLSA could be attributed to the ability to involve the stronger augmentations that result in many small objects to pre-train the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this section, we analyzed the contribution of DDM loss by comparing it with other designs. Also, we compared CLSA with other baselines using the same running time for a more fair comparison. Moreover, we compared CLSA performance under different strong augmentations. In addition, we carefully addressed our feature representation improvement for both weakly augmented views and strongly augmented views. Last but not least, we further studied   the distribution of probabilities of positive and negative pairs with pre-trained network by CLSA. DDM Loss In the ablation study shown in <ref type="table" target="#tab_5">Table 5</ref>, we studied the role of the proposed DDM loss in the CLSA. First, we naively used the stronger augmentation applied in the CLSA-Single as the query and/or the key in the MoCo V2. Both results (Strong query and Strong query &amp; Strong key) showed the performance could not be improved or even degraded. Second, we replaced the DDM loss in the CLSA-Single with the contrastive loss, which can be viewed as replacing p(z i |z i ) and p(z k |z i ) with a one-hot distribution, and we found it can only achieve a top-1 accuracy of 68.0% compared to that of 69.4% with the DDM loss. That clearly indicates p(z k |z i ) is a more suitable approximation of q(z k |z i ) compared to one-hot distribution. Both studies showed that the proposed CLSA and its DDM loss help us learn from stronger augmentations by avoiding the performance degeneration that augmented images' distortions would incur.</p><p>Running Time To address the concern of extra training time consuming of CLSA compared to MoCo V2 <ref type="bibr" target="#b3">[4]</ref>, we compared the results in <ref type="table" target="#tab_6">Table 6</ref>. It is clearly seen that CLSA (68.3%, 52.5h) can still achieve better results than MoCo V2 (67.5%, 53h) with the same running time and converge much faster. That also indicates that the optimization of contrastive loss and DDM loss at the same time can also benefit the convergence of representation learning.</p><p>The strength of strong augmentation CLSA achieved promising results by introducing stronger augmentations to further  improve self-supervised learning. In contrastive-based approach, the weak augmentations need to be carefully designed to remain the identity. Thus, we designed the stronger augmentations by randomly sampling operations from a large pool of operations with magnitude in a wide range as shown in <ref type="table">Table.</ref> 1. To make the augmentation stronger, we repeated this operation s times. In CLSA, we use s = 5 as default. As shown in <ref type="table">Table.</ref> 7, we trained CLSA with 100 epochs and compared the performance with different augmentation strength s. Here, the comparison is based on the pre-trained model's KNN classification accuracy, where K is set to 20. CLSA did not influence too much with different augmentation strength for stronger augmentation. That suggests the reliability of CLSA framework, which is very independent of the augmentation strength. This property can greatly help CLSA to be adjusted to complement other self-supervised approaches. Representations for weak/strong augmented images Meanwhile, we also want to compare CLSA with baseline MoCo V2 to illustrate our representative features on both weakly augmented images and strongly images. First, following <ref type="bibr" target="#b8">[9]</ref>, we center crop the images to obtain features from the last average pooled layers, and report the accuracy with K = 20 KNN accuracy. As clearly shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, on both 200-epoch (Weak-200) and 800-epoch (Weak-800) pre-trained encoder, we showed much better performance compared to MoCo V2. Moreover, we further transformed images using our strong augmentations S mentioned in Section 3.3 and obtain the features for them for KNN clustering. The comparison is also shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. It is obvious that we showed much better classification performance on those strongly augmented images. CLSA improved 8.2% and 9.9% accuracy compared to MoCo V2 on the 200-epoch (Strong-200) and 800-epoch (Strong-800) pre-trained model, respectively. Those comparisons suggest that CLSA can not only increase the representation for weakly augmented images, but also further greatly boost the representation for strongly augmented images. To some extent, we can say the representation learned by CLSA is more general compared to contrastive approaches.</p><p>Distribution of Relative Similarity by CLSA Similar to the distribution of the probability of positive pairs and the distribution of the variance of the probability of negative pairs shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we further compared that by CLSA in <ref type="figure" target="#fig_2">Fig. 3</ref>. Compared to contrastive approaches, the distribution of strongly query showed a more similar pattern compared to that of the weakly query. This different distribution clearly suggested that DDM can leverage the knowledge of weakly query to contribute to the learning of strong query. Also, the increase of the mean of the distribution of positive pairs indicates that more and more strongly views can be successfully retrieved by its weakly augmented counterpart. Meanwhile, the increase of the mean of the distribution of the variance of negative pairs also suggests the relationships between the representations of strongly query and other representations is more well captured. In a nutshell, both distribution change agrees well that the feature representation of strongly augmented views become more representative by CLSA pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we present CLSA, a novel method that can utilize the distributional divergence to learn the information from strongly augmented images. The proposed method outperforms the stateof-the-art methods on all the datasets and achieved almost the same performance compared to the supervised ImageNet network.</p><p>Meanwhile, it outperforms the previous supervised and selfsupervised methods on downstream tasks, which suggests CLSA learned more reliable and fine-grained features that can contribute to the development of other areas.</p><p>Moreover, as aforementioned comparison with contrastive loss, the DDM loss is independent with concurrent contrastive learning methods. For contrastive loss module in CLSA, we can use it to further improve the performance of any contrastive learning methods, such as MoCo <ref type="bibr" target="#b4">[5]</ref>, SimCLR <ref type="bibr" target="#b7">[8]</ref>, AdCo <ref type="bibr" target="#b20">[21]</ref> and so on, where the performance of CLSA can be further boosted with the contrastive baseline BYOL <ref type="bibr" target="#b19">[20]</ref>. Furthermore, the coefficient and the temperature of contrastive loss and distributional divergence minimization loss can be further explored and may benefit more to learn from the strongly augmented images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Contrastive instance learning framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Comparison of the strongly weakly augmented images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The comparison of the distribution of positive pair's probabilities and variance of negative pairs' probabilities given weakly augmented query and strongly augmented query. A. The distribution with a randomly initialized network. B. The distribution with a pre-trained network by contrastive methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Diagram of distributional divergence minimization. Here the representation bank consists of K stored features z k of previous batches and online features z i from the key encoder. They will be used to calculate the conditional probability of current weakly and strongly augmented query images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of KNN accuracy of MoCo V2 [4] and CLSA. Both results are compared based on pretrained model with 200/800 epochs with single crop. The comparison used the representation of weakly/strongly augmented images, respectively. The neighbor K for KNN here is set to 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>The comparison of the distribution of positive pair's probabilities and variance of negative pairs' probabilities with a pre-trained network by CLSA. A. The distribution comparison of positive pair's probabilities. B. The distribution comparison of variance of negative pairs' probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Various augmentations we applied in experiments to strongly augment training images.</figDesc><table><row><cell>Operation</cell><cell>ShearX(Y)</cell><cell>TranslateX(Y)</cell><cell>Rotate</cell></row><row><cell>Mag Range</cell><cell>[-0.3,0.3]</cell><cell>[-0.3,0.3]</cell><cell>[-30,30]</cell></row><row><cell>Operation</cell><cell>AutoContrast</cell><cell>Invert</cell><cell>Equalize</cell></row><row><cell>Mag Range</cell><cell>0 or 1</cell><cell>0 or 1</cell><cell>0 or 1</cell></row><row><cell>Operation</cell><cell>Solarize</cell><cell>Posterize</cell><cell>Contrast</cell></row><row><cell>Mag Range</cell><cell>[0,256]</cell><cell>[4,8]</cell><cell>[0.05,0.95]</cell></row><row><cell>Operation</cell><cell>Color</cell><cell>Brightness</cell><cell>Sharpeness</cell></row><row><cell>Mag Range</cell><cell>[0.05,0.95]</cell><cell>[0.05,0.95]</cell><cell>[0.05,0.95]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>Top-1 accuracy under the linear evaluation on Ima-geNet with the ResNet-50 backbone with 200 epochs training.</figDesc><table><row><cell>Method</cell><cell>Top 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>Top-1 accuracy under the linear evaluation on Im-ageNet with the ResNet-50 backbone with various numbers of epochs.</figDesc><table><row><cell>Method</cell><cell>Top 1</cell></row><row><cell>BigBiGAN [38]</cell><cell>56.6</cell></row><row><cell>SeLa-400epochs</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc>Transfer learning results on various downstream tasks.</figDesc><table><row><cell></cell><cell>Classification</cell><cell cols="2">Object Detection</cell><cell></cell></row><row><cell></cell><cell>VOC07</cell><cell>VOC07+12</cell><cell cols="2">COCO</cell></row><row><cell>Measurement</cell><cell>Accuracy</cell><cell>AP 50</cell><cell>AP</cell><cell>AP S</cell></row><row><cell>RotNet [40]</cell><cell>64.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NPID++ [9]</cell><cell>76.6</cell><cell>79.1</cell><cell>-</cell><cell>-</cell></row><row><cell>MoCo [5]</cell><cell>79.8</cell><cell>81.5</cell><cell>-</cell><cell>-</cell></row><row><cell>PIRL [3]</cell><cell>81.1</cell><cell>80.7</cell><cell>-</cell><cell>-</cell></row><row><cell>PCL [36]</cell><cell>84.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BoWNet [41]</cell><cell>79.3</cell><cell>81.3</cell><cell>-</cell><cell>-</cell></row><row><cell>SimCLR [8]</cell><cell>86.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MoCov2 [4]</cell><cell>87.1</cell><cell>82.5</cell><cell>42.0</cell><cell>20.8</cell></row><row><cell>SWAV [6]</cell><cell>88.9</cell><cell>82.6</cell><cell>42.1</cell><cell>19.7</cell></row><row><cell>CLSA</cell><cell>93.6</cell><cell>83.2</cell><cell>42.3</cell><cell>24.4</cell></row><row><cell>Supervised</cell><cell>87.5</cell><cell>81.3</cell><cell>40.8</cell><cell>20.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc>Ablation study of the CLSA on ImageNet with 200 epochs of pre-training.</figDesc><table><row><cell>Model</cell><cell>Top-1</cell></row><row><cell>MoCo V2</cell><cell>67.5</cell></row><row><cell>MoCo V2 with Strong query</cell><cell>67.7</cell></row><row><cell cols="2">MoCo V2 with Strong query &amp; Strong key 67.0</cell></row><row><cell>CLSA with contrastive loss</cell><cell>68.0</cell></row><row><cell>CLSA</cell><cell>69.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 :</head><label>6</label><figDesc>Training Time Comparison of CLSA</figDesc><table><row><cell>Model</cell><cell>Time</cell><cell cols="2">Epoch Top-1</cell></row><row><cell cols="2">MoCo V2 53h</cell><cell>200</cell><cell>67.5</cell></row><row><cell>CLSA</cell><cell>35h</cell><cell>100</cell><cell>67.2</cell></row><row><cell>CLSA</cell><cell cols="2">52.5h 150</cell><cell>68.3</cell></row><row><cell>CLSA</cell><cell>70h</cell><cell>200</cell><cell>69.4</cell></row><row><cell cols="4">1 h denotes running hours on a machine</cell></row><row><cell cols="2">with 8 V100 GPUs.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7 :</head><label>7</label><figDesc>Comparison of CLSA under different strong augmentations</figDesc><table><row><cell>Strength s</cell><cell>3</cell><cell>5 (default)</cell><cell>7</cell></row><row><cell>KNN Acc</cell><cell>52.8</cell><cell>53.0</cell><cell>52.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the support of Yuanyuan Zhang for figure plotting.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Guo-Jun Qi Guo-Jun Qi (M14-SM18) is the Chief Scientist leading and overseeing an international R&amp;D team for multiple artificial intelligent services on the Huawei Cloud since August 2018. He was a faculty member in the Department of Computer Science and the director of MAchine Perception and LEarning (MAPLE) Lab at the University of Central Florida since August 2014. Prior to that, he was also a Research Staff Member at IBM T.J. Watson Research Center, Yorktown Heights, NY. His research interests include machine learning and knowledge discovery from multi-modal data sources to build smart and reliable information and decision-making systems. Dr. Qi has published more than 100 papers in a broad range of venues in pattern recognition, machine learning and computer vision. He also has served or will serve as a general co-chair for ICME 2021, </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretextinvariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="570" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Enaet: Self-trained ensemble autoencoding transformations for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adco: Adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08435</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning by cross-level discrimination between instances and groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03813</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06122</idno>
		<title level="m">Spatially consistent representation learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning generalized transformation equivariant representations via autoencoding transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08628</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aet vs. aed: Unsupervised representation learning by auto-encoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2547" to="2555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster autoaugment: Learning augmentation strategies using backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hataya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zdenek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshizoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning representations by predicting bags of visual words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6928" to="6938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Detectron2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
