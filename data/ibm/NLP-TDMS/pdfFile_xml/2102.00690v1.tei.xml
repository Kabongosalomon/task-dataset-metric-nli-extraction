<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ground-aware Monocular 3D Object Detection for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021">2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yixuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Ground-aware Monocular 3D Object Detection for Autonomous Driving</title>
					</analytic>
					<monogr>
						<title level="m">IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY</title>
						<imprint>
							<biblScope unit="issue">1</biblScope>
							<date type="published" when="2021">2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Automation Technologies for Smart Cities</term>
					<term>Deep Learning for Visual Perception</term>
					<term>Object Detection, Segmentation and Categorization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating the 3D position and orientation of objects in the environment with a single RGB camera is a critical and challenging task for low-cost urban autonomous driving and mobile robots. Most of the existing algorithms are based on the geometric constraints in 2D-3D correspondence, which stems from generic 6D object pose estimation. We first identify how the ground plane provides additional clues in depth reasoning in 3D detection in driving scenes. Based on this observation, we then improve the processing of 3D anchors and introduce a novel neural network module to fully utilize such application-specific priors in the framework of deep learning. Finally, we introduce an efficient neural network embedded with the proposed module for 3D object detection. We further verify the power of the proposed module with a neural network designed for monocular depth prediction. The two proposed networks achieve state-of-the-art performances on the KITTI 3D object detection and depth prediction benchmarks, respectively. The code will be published in https://www.github.com/Owen-Liuyuxuan/visualDet3D</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Simultaneously estimating the position, orientation, and dimensions of an object in 3D with a single wellcalibrated RGB camera image in an autonomous driving scene is generally an ill-posed problem. Lidar-based methods and stereo-vision-based methods, which respectively obtain depths and distance information from lidar measurements and triangulation, can achieve superior performance <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b1">[2]</ref>[3] <ref type="bibr" target="#b3">[4]</ref>. Monocular setups are cheaper and more versatile than LiDAR setups and are more robust to variations in extrinsic parameters than stereo cameras. As a result, 3D detection with a single camera is still a heated research direction despite a lack of depth information.</p><p>Recent developments in monocular 3D object detection mainly utilize geometric constraints between the 3D Manuscript received: October, 14, 2020; Revised November, 30, 2020 ; Accepted January, 5, 2021.</p><p>This paper was recommended for publication by Youngjin Choi upon evaluation of the Associate Editor and Reviewers' comments. <ref type="bibr" target="#b0">1</ref>  object and the its projection on a 2D image. ShiftRCNN <ref type="bibr" target="#b4">[5]</ref>, SS3D <ref type="bibr" target="#b5">[6]</ref>, and RTM3D <ref type="bibr" target="#b6">[7]</ref> optimize the estimation of depth and orientation by solving a Perspective-n-Point problem with noisy observation. Most of these ideas come from a more general problem of monocular 6D pose estimation. Monocular 6D pose estimation benchmarks like LINEMOD <ref type="bibr" target="#b7">[8]</ref> are based on the assumption that the CAD models of the objects of interest are known. However, we do not have access to accurate car models for each vehicle in autonomous driving scenes; thus, the performances of monocular 3D object detectors in autonomous driving scenes are limited.</p><p>In autonomous driving and mobile robotics applications, we can generally assume that most important dynamic objects are on a ground plane, and the camera is mounted at a certain height above the ground. Some traditional depth prediction methods also note the importance of ground planes and introduce a similar "floorwall" assumption for indoor environments <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Such perspective priors on the ground plane, not presented in general monocular 6D pose estimation problems, provide a significant amount of information for geometric reasoning for monocular 3D object detection in driving scenes. Few recent works explicitly inject the perspective priors on the ground plane into a neural network. This paper proposes two novel procedures to allow a monocular object detector to reason over the ground plane explicitly.</p><p>The first procedure is anchor filtering, where we explicitly break the invariance in the neural network predictions. Given a prior distance between an anchor and its distance to the camera, we back-project the anchor to 3D. Since all objects of interest are located around the ground plane, we filter out 3D anchors far from the ground plane during training and testing. This operation focuses the network on positions where objects of interest are likely to appear. We will further introduce this procedure in Section III-A.</p><p>The second procedure is a ground-aware convolution module. The motivation of this module is illustrated by <ref type="figure" target="#fig_0">Figure 1</ref>. For a human, ground pixels around a car are useful to estimate the car's 3D position, orientation, and dimensions. For an anchor-based detector, features at the center are responsible for estimating all the car's 3D parameters. However, to infer depth with ground pixels like a human, the network model needs to perform the following steps from the center of an object (e.g. the red 1) identifing the contact points of the object and the ground plane (e.g. the blue curve beneath the car). 2) computing the 3D position of the contact points with perspective geometry. 3) gathering information from these contact points with a receptive field focusing downwards.</p><p>A standard object detection or depth prediction network is built to have a uniform receptive field, and neither perspective geometry priors nor camera parameters are provided to the network. Thus, it is non-trivial to train a standard neural network to make inferences like a human.</p><p>The ground-aware convolution module is designed to guide the network to incorporate the ground-based reasoning in network inferencing. We encode each pixel point's prior depth value as an additional feature map, and we guide each pixel point of the feature map to incorporate features from pixels below them. Details of this module will be introduced in Section III-B.</p><p>Incorporating the two proposed procedures into the network, we propose a one-stage framework with explicit ground plane hypothesis usage. The network is fast thanks to its clean structure and can run at about 20 frames-per-second (FPS) on a modern GPU.</p><p>We further incorporate the ground-aware convolution module in a u-net-based structure on monocular depth prediction. Both networks achieve state-of-the-art (SOTA) performance on the KITTI dataset.</p><p>The contribution of the paper is three-fold.</p><p>• We identify the benefit of learning from the ground plane priors in urban scenes for 3D reasoning from images. • We introduce a processing method and a groundaware convolution module in monocular 3D object detection to use the ground plane hypothesis.</p><p>• We evaluate the proposed module and design methods on the KITTI 3D object detection benchmark and the depth prediction benchmark, and we achieve competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pseudo-LiDAR for Monocular 3D Object Detection</head><p>The idea of pseudo-LiDAR, reconstructing point clouds from mono or stereo images, has led to the recent advances in 3D detection <ref type="bibr" target="#b10">[11]</ref>[12] <ref type="bibr" target="#b12">[13]</ref>[14] <ref type="bibr" target="#b14">[15]</ref>. Pseudo-LiDAR methods usually reconstruct the point cloud from a single RGB image with off-the-shelf depth prediction networks, which limit their performance. Moreover, the current SOTA monocular depth prediction networks generally take about 0.05s per frame, which significantly limits the inference speed of pseudo-lidar detection pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. One-Stage Detection for Monocular 3D Object Detection</head><p>Several recent advances in monocular 3D object detection directly regress 3D bounding boxes in a one-stage object detection framework.</p><p>Optimization-based Methods: SS3D <ref type="bibr" target="#b5">[6]</ref> concurrently estimated 2D bounding boxes, depth, orientation, dimensions, and 3D corners. Nonlinear optimization was applied to merge all these predictions. Shift-RCNN <ref type="bibr" target="#b4">[5]</ref> also estimated 3D information in a 2D anchor and applied a small sub-network instead of a nonlinear solver. More recent methods, SMOKE <ref type="bibr" target="#b15">[16]</ref> and RTM3D <ref type="bibr" target="#b6">[7]</ref> incoperate the aforementioned optimization scheme into the anchorfree object detector CenterNet <ref type="bibr" target="#b16">[17]</ref>.</p><p>3D Anchor-based Methods: M3D-RPN <ref type="bibr" target="#b17">[18]</ref> introduced 3D priors in 2D anchors, and also emphasized the importance of the ground plane hypothesis. It also introduced height-wise convolution while D4LCN <ref type="bibr" target="#b18">[19]</ref> introduced depth-guided convolution. Both techniques came at a high cost to efficiency and only utilized the ground plane hypothesis implicitly.</p><p>We point out that anchor-based methods are still better than anchor-free methods in 3D detection. Anchor-free detectors implicitly require the network to learn the correlation between the object's apparent size and its distance value. In contrast, anchor-based detectors can embed this in an anchor's preprocessing. As a result, we develop our framework upon anchor-based detectors.</p><p>To our knowledge, our proposed framework is the first 3D anchor-based method to explicitly utilize the ground plane hypothesis of driving scenes in monocular 3D detection and achieves the SOTA performance at the time of writing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Supervised Monocular Depth Prediction With Deep Learning</head><p>Supervised monocular depth prediction is another hot research topic closely related to monocular 3D object detection.</p><p>conv conv GAC conv filtering NMS Backbone <ref type="figure">Fig. 2</ref>: Network structure for 3D object detection. We extract features from image I and predict classfication tensor C and regression tensor R. We filter anchors far from the ground before post-processing and produce the final bounding boxes B.</p><p>DORN <ref type="bibr" target="#b19">[20]</ref> and SoftDorn <ref type="bibr" target="#b20">[21]</ref> proposed to treat the depth estimation problem as an ordinal regression problem to improve the convergence rate. BTS <ref type="bibr" target="#b21">[22]</ref> proposed the local planar guidance module and incorporated normal information to constraint the depth prediction results in the scenes. BANet <ref type="bibr" target="#b22">[23]</ref>, meanwhile, proposed a bidirectional attention network to improve the receptive fields and global information understanding of depth prediction networks.</p><p>Many of the methods above focus on depth prediction for multiple datasets and scenarios. Images in datasets like NYUv2 <ref type="bibr" target="#b23">[24]</ref> and DIODE <ref type="bibr" target="#b24">[25]</ref> are taken from various viewpoints, and it is hard to extract floor priors, unlike the cases in driving scenes. As a result, the neural networks mentioned above do not utilize the camera's extrinsic parameters to extract environment priors, and the absolute scale is lacking during the network inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>In this section, we elaborate on the methods applied in this paper. First, we present the formulation of the detection network's inference results and the data preprocessing procedure. Second, we introduce the groundaware convolution module that extracts depth priors from the ground plane hypothesis. Finally, we present the network's architecture with other major modifications in the training and inferencing process.</p><p>A. Anchors Preprocessing 1) Anchors Definition: We follow the idea from YOLO <ref type="bibr" target="#b25">[26]</ref> to densely predict bounding boxes with dense anchors. Each anchor on the image also acts as a proposal of an object in 3D. A 3D anchor consists of a 2D bounding box parameterized by [x, y, w 2d , h 2d ], where (x, y) is the center of the 2D box and (w 2d , h 2d ) is the width and height; 3D centers of an object are presented as [cx, cy, z], where (cx, cy) is the center of the object projected on the image plane and z is the depth; [w 3d , h 3d , l 3d ] corresponds to the width, height and length of the 3D bounding box, and [sin(α), cos(α)] is the sine and cosine value of the observation angle α.</p><p>2) Priors Extraction from Anchors: The shape and size of an anchor or an object are highly correlated with the depth. In some prior methods <ref type="bibr" target="#b17">[18]</ref>, the mean of the depth is computed for each pre-defined anchor box, while variance is computed globally instead. The global variance is only computed to normalize the targets for the neural net.</p><p>We further observe that the variance of the depth z of an anchor is inversely proportional to the object's size in the image. Thus, we consider each anchor as a distribution with individual mean and variance of the object proposal in 3D. To collect prior statistical knowledge in the anchors, we iterate through the training set and collect all objects sharing a large intersection-over-union (IoU) with the box for each anchor box with a different shape. Then we calculate the mean and variance of the depth z, sin(α) and cos(α) for each pre-defined anchor box. We can significantly lower the prior variance of the depth z for large anchor boxes / close objects.</p><p>Since we have considered anchors as distribution of 3D proposals, the associated 3D targets should not deviate much from the expectation. We utilize the fact that most objects of interest should be on the ground plane. Each anchor, centering at (u, v) with pre-computed mean depthẑ, can be back-projected to 3D:</p><formula xml:id="formula_0">x 3d = u − c x f xẑ y 3d = v − c y f yẑ ,<label>(1)</label></formula><p>where (c x , c y ) is the camera's principal point and ( f x , f y ) is the camera's focal length. Anchors with y 3d too far from the ground will be filtered out from training and testing. Such a strategy allows the network to train with 3D anchors around the region of interest and simplify the classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ground-Aware Convolution Module</head><p>Ground-aware convolution is designed to guide the object center to extract features and reason the depth from its contact point; the structure is presented in <ref type="figure">Figure 4</ref>.</p><p>To first inject perspective geometry into the network, we encode the prior depth value z of each pixel point, assuming that it is on the ground. The persepctive geometry foundation is presented in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>According to the ideal pin-hole camera model, the relation between the depth z and height y 3d can be obtained as:</p><formula xml:id="formula_1">z · v = f y · y 3d + c y · z + T y ,<label>(2)</label></formula><p>where f y , c y , and T y are focal lengths, the principal point coordinate and relative translation respectively, and v is the pixel's y-coordinate in the image. Assume we know the expected elevation EL of the camera from the ground (1.65 meters in the KITTI dataset <ref type="bibr" target="#b26">[27]</ref>). The distance from the ground plane pixel to the camera in z can be solved from Equation 2 as:</p><formula xml:id="formula_2">z = f y · EL + T y v − c y .<label>(3)</label></formula><p>We note that the function is not continuous around the vanishing line of the ground plane (v = c y ), and, as When we calculate the vertical offsets δ 0 y , we assume pixels are foreground object centers. When we compute the depth priors z, we assume pixels are on the ground because they are features to be queried.  <ref type="figure">Fig. 4</ref>: Ground-aware convolution. The network predicts the offsets in the vertical direction, and we sample the corresponding features and depth priors from pixels below. Depth priors are computed with perspective geometry with ground plane assumption. indicated in <ref type="figure" target="#fig_1">Figure 3</ref>, physically unachievable for v &lt; c y . To detour from such a problem, we first propose to encode the depth value as the disparity of a virtual stereo setup (baseline B = 0.54m, similar to the KITTI stereo setup), and we derive the virtual disparity</p><formula xml:id="formula_3">d = f y · B v − c y f y · EL + T y<label>(4)</label></formula><p>based on the depth z in Equation 3. Rectified Linear Unit (ReLU) activation (max(x, 0)) is then applied to suppress pixels with disparity smaller than zero, which is physically unachievable for forward-facing cameras. After these two steps, the depth priors of the image becomes spatially continuous and consistent. Inspired by CoordinateConv <ref type="bibr" target="#b27">[28]</ref>, we treat this depth prior as an additional feature map with the same spatial size as the base feature map. Each element in the feature map is now encoded with depth priors assuming it is on the ground.</p><p>As motivated in <ref type="figure" target="#fig_0">Figure 1</ref>, pixels at the center of the object need to query the depth and image features from contact points, which are usually below the object centers.</p><p>Each point p i in the feature map will then dynamically predict an offset δ yi as if it is the center of a foreground object</p><formula xml:id="formula_4">δ yi = δ 0 yi + ∆ i =ĥ 2EL −ĥ · (v − c y ) + ∆ i ,</formula><p>, whereĥ is the height of the object (we fix this to be the average height of foreground objects of the dataset), ∆ i is the residual predicted by the convolution networks. Then, as shown in <ref type="figure">Figure 4</ref>, we extract features f i at position p i + δ yi using linear interpolation. The extracted features f i are merged back to the original point p i with a residual connection.</p><p>The ground-aware convolution module mimics how humans utilize the ground plane in depth perception. It extracts geometric priors and features from pixels beneath. The other part of the network is then responsible for predicting the depth residual between the priors and the targets. The module is differentiable and trained endto-end with the entire network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architecture for Monocular 3D Detection</head><p>The inference structure of the network is presented in <ref type="figure">Figure 2</ref>. We adopt ResNet-101 <ref type="bibr" target="#b29">[30]</ref> as the backbone network, and we only take features at scale 1/16. The feature map is then fed into the classification branch and regression branch.</p><p>The classification branch consists of two convolutional layers, while the regression branch is composed of a ground-aware convolution module followed by a convolutional output layer.</p><p>The shape of the output tensor from the classification branch C is (B, W 16 , H 16 , K * #anchors), where K represents the number of classes and #anchors means the number of anchors per pixel. The output tensor from the regression branch is (B, W 16 , H 16 , 12 * #anchors). There are nine parameters for each anchor: four for 2D bounding box estimation, three for object center predictions, three for dimension predictions, and two more for observation angle predictions.</p><p>1) Loss Functions: The total loss L is the aggregation of classification loss for objectness L cls , and regression loss for other parameters L reg :</p><formula xml:id="formula_5">L = L cls + L reg .</formula><p>We adopt focal loss <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b30">[31]</ref> for classification of objectness and cross-entropy loss for the multi-bin classification of width, height, and length. Other parameters, [x 2d , y 2d , w 2d , h 2d , cx, cy, z, w 3d , h 3d , l 3d , sin(α), cos(α)], are normalized based on the anchors' prior parameters and optimized through smoothed-L1 loss <ref type="bibr" target="#b31">[32]</ref>.</p><p>2) Post Optimization: We follow <ref type="bibr" target="#b17">[18]</ref> to apply hillclimbing algorithms as a post-optimization procedure. By perturbating the observation angle and depth estimation, the algorithm incrementally maximizes the IoU between the directly estimated 2D bounding box and the 2D bounding box projected from the 3D bounding box to the image plane. The original implementation optimizes the depth and observation angle concurrently. With repeated experiments, we find that optimizing only the observation angle produces even better results in the validation set. Concurrently optimizing two variables could overfit to the sparse 3D-2D constraints and affect the accuracy of the 3D prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Architecture for Monocular Depth Prediction</head><p>We adopt a U-Net <ref type="bibr" target="#b32">[33]</ref> structure for supervised dense depth prediction. We select a pretrained ResNet-34 <ref type="bibr" target="#b29">[30]</ref> as the backbone encoder.</p><p>In the decoding phase, the features are bilinearly upsampled, followed by two convolution layers and concat with the skip connections. We add a groundaware convolution module before the two convolution layers in the decoder. The depth prediction network densely predicts the logarithm of depth from each image with a (B, 1, H, W) tensor y = log z. We provide supervision on each output scale l. The total loss is the sum of a scale-invariant (SI) loss L SI <ref type="bibr" target="#b20">[21]</ref> and a smoothness loss L smooth <ref type="bibr" target="#b33">[34]</ref> with hyperparameter α:</p><formula xml:id="formula_6">L = ∑ l (L SI + αL smooth ).<label>(5)</label></formula><p>SI loss is commonly used to simultaneously minimize the mean-square-error (MSE) and improve global consistency. Smoothness loss is needed because the supervision from the KITTI dataset <ref type="bibr" target="#b26">[27]</ref> is sparse and lacks local consistency. The SI loss and smoothness loss are computed with the following equations:</p><formula xml:id="formula_7">L SI = 1 n ∑ i d 2 i − λ n 2 ( ∑ i d i ) 2 (6) L smooth = 1 N ∑ i |∂ x z l i |e −||∂ x I l i || + |∂ y z l i |e −||∂ y I l i || ,<label>(7)</label></formula><p>where d i = log z i − log z * i , n is the number of valid pixels, λ ∈ [0, 1] is a hyperparameter balancing the absolute MSE loss and relative scale loss, N is the number of total pixels, and ∂ x I and ∂ y I are the gradients of the input images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Dataset and Training Setups</head><p>We first evaluate the proposed monocular 3D detection network on the KITTI benchmark <ref type="bibr" target="#b26">[27]</ref>. The dataset consists of 7,481 training frames and 7,518 test frames. Chen et al. <ref type="bibr" target="#b34">[35]</ref> further splits the training set into 3,712 training frames and 3,769 validation frames.</p><p>We first determine the hyperparameters of the network with a family of smaller networks fine-tuned on Chen's split <ref type="bibr" target="#b34">[35]</ref>. Then, we retrain the final network on the entire training set with the same hyperparameters before uploading the result for testing on the KITTI server. The ablation study that follows is also conducted on the validation set of Chen's split.</p><p>Similar to RTM3D <ref type="bibr" target="#b6">[7]</ref>, we double the training set by utilizing images both from the left and right RGB cameras (only RGB images from the left camera are used in validation and final testing) and use random horizontal mirroring as data augmentation (not applied in validation and testing), which significantly enlarges the training set and improve performance. The top 100 pixels of each image are cropped to speed up inference, and the cropped input images are scaled to 288 × 1280 for the model submitted to the KITTI server, which is similar to the original scale of the images. The feature map produced by the backbone, therefore, has a shape of 18 × 80. Regression loss and classification loss that are too small in magnitude (1e-3) are clipped to prevent overfitting. The network is trained with a batch size of 8 on a single Nvidia 1080Ti GPU. During inference, the network is fed one image at a time, and the total average processing time, including file IO and post-optimization, is 0.05s per frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metric and Results for 3D Detection</head><p>As pointed out by Simonelli et al. <ref type="bibr" target="#b28">[29]</ref> and the KITTI team, evaluating performance with 40 recall positions (AP 40 ) instead of the 11 recall positions (AP 11 ) proposed in the original Pascal VOC benchmark <ref type="bibr" target="#b37">[38]</ref> could eliminate the problematic results presented in the lowest recall bin. Therefore, we present our results on the test set and also ablation study based on AP <ref type="bibr" target="#b39">40</ref> .</p><p>The results are presented in <ref type="table" target="#tab_1">Table I</ref> alongside those of other SOTA monocular 3D detection methods based on the KITTI benchmark.</p><p>The proposed network significantly outperforms existing methods on easy and moderate vehicles. We do expect ground-aware convolutions to produce more accurate predictions for close-up vehicles with clear borders with the ground plane.</p><p>Qualitative results are presented in <ref type="figure" target="#fig_3">Figure 5</ref>. The model shown here shares the same hyperparameters as the model submitted to the KITTI server but is only trained on the training sub-split. In the images on the left-hand side of the figure, cars are mostly detected and estimated accurately. The effect of the GAC module is also visualized.</p><p>We present several typical failure cases on the righthand side of <ref type="figure" target="#fig_3">Figure 5</ref>, and in the top-right image, the network does not detect a heavily obscured car. In the middle-right image, truncated cars and a car that is quite far away are not detected. We acknowledge that the network could still have trouble detecting small objects. We show the bottom-right image to demonstrate cases in which the network give an inaccurate estimation of the 3D dimensions of a car because, as stated in Section III, it is still difficult to estimate the width, length, and height of an object merely by semantic information in the image.</p><p>We provide an ablation study of the model in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on Monocular Depth Prediction</head><p>We further evaluate the proposed depth prediction network in the KITTI depth prediction benchmark <ref type="bibr" target="#b26">[27]</ref>.</p><p>The dataset for monocular depth prediction consists of 42949 training frames, 1000 validation samples, and 500 test samples, annotated with sparse point clouds.</p><p>The input images are cropped to 352 × 1216 during training and testing. In the loss function, we applied α= 0.3, λ=0.3 through grid-search on the validation set. The network is also trained with a batch size of 8 on a single Nvidia 1080Ti GPU.</p><p>Scale-invariant log error (SILog) is the primary metric used in the KITTI benchmark to evaluate depth prediction algorithms.</p><p>The results are presented in <ref type="table" target="#tab_1">Table II</ref>. The proposed network produces one of the best performances on the KITTI dataset, providing competitive results compared with SOTA methods. We also show that the network improves significantly against the baseline U-Net Model.</p><p>Qualitative results are presented in <ref type="figure" target="#fig_4">Figure 6</ref>. Depth predictions inside the range of LiDAR are generally consistent. Depth predictions along long, vertical objects like trees are consistent thanks to the ground aware convolution module. We point out that there are still artifacts around the edge of objects and areas without supervision because the network receives no post-processing and little pre-training. The depth prediction results show that the proposed module and the proposed network can improve depth inferencing from monocular images in autonomous driving scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. MODEL ANALYSIS AND DISCUSSION</head><p>In this section, we further analyze the performance of the proposed method and discuss the effectiveness of each design choice. The experiments will focus more on monocular 3D detection. We conduct ablation studies to validate the contribution of anchor preprocessing and the ground-aware convolution module.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Anchor Preprocessing</head><p>We first conduct experiments on anchor filtering. In the experiment, we do not filter out unnecessary anchors during training and testing. We notice that the proposed filtering will filter out half of the negative anchors, so we also conduct an experiment against Online Hard Example Mining (OHEM), where we filter out half of the easy negative anchors during training <ref type="bibr" target="#b38">[39]</ref>.</p><p>As shown in <ref type="table" target="#tab_1">Table III</ref>, the baseline model outperforms the ablated one and OHEM. The baseline model performs better at 3D inference. We also point out that there is almost no difference in 2D detection between the two models.</p><p>Generally, a one-stage single-scale object detector not only needs to classify background from the foreground but also needs to select anchors with the correct scales at foreground pixels, which also means selecting the proper depth prior. Filtering off-the-ground anchors during training and testing significantly lowers the learning burden for the classification branch of the object detector. Thus the classification branch can focus more on selecting the right anchors for foreground pixels. Such a method, as a result, also outperforms position-invariant filtering methods like OHEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ground-Aware Convolution Module</head><p>Intuitively, basic convolutions provide a uniform receptive field for each pixel, and the network could implicitly learn to adjust its receptive field by fine-tuning the weights of multiple convolution layers. Deformable convolutions <ref type="bibr" target="#b39">[40]</ref> further explicitly encourage the network to adapt its receptive field according to each pixel's surrounding context. Compared with deformable convolutions, the ground-aware convolution module fixes the search direction and allows a larger search range.</p><p>We substitute the proposed module with basic convolutions, disparity-conditioned convolutions (i.e., convolution with the depth prior as an additional feature map), and deformable convolutions to examine the performance. The results are shown in <ref type="table" target="#tab_1">Table III</ref>. The experiments with deformable convolutions demonstrate better 2D detection results.</p><p>Deformable convolution can enhance the performance with a generally larger receptive field. While disparityconditioned convolution provides the network with prior depths, the receptive field of the network is lacking. These two modules improve the performance, but the proposed module has better results by a considerable margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we presented ground-aware monocular 3D object detection for autonomous driving scenes. First, we improved the problem setup for monocular 3D detection and introduced an anchor filtering procedure to inject ground plane priors and statistical priors in anchors. Second, we introduced a ground-aware convolution module, providing sufficient hints and geometric priors for the network to reason based on ground plane priors. The proposed monocular 3D object detection network was tested on the KITTI detection benchmark and achieved SOTA performance among monocular methods. We further tested the ground-aware convolution module in the monocular depth prediction task, and it also produced competitive results on the KITTI depth prediction benchmark.</p><p>We note that the "floor-wall" assumption is limited to scenes with specific camera poses, and it only partially holds in a complex driving scene. The proposed methods still do not reason explicitly based on the boundaries of the ground and other objects. Instead, we encode sufficient information and priors into the network and adopt a data-driven approach.</p><p>Nevertheless, the proposed method pushes the boundary of 3D detection and depth inference from images and produces powerful neural network models for autonomous driving and mobile robotics scenes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Contact points with the ground plane are important in inferreing 3D information of an object. Predicting depths of background pixels (e.g., the brown point) also rely on the geometry of the ground plane. Best viewed in color. dot in thefigure)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Perspective geometry for the GAC module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative examples from validation sets. Blue boxes, pink boxes and yellow boxes indicate the ground truth 2D bounding box, estimated 2D bounding box, and estimated 3D bounding box respectively. Red points are object centers and green points visualize the offsets δ yi in the GAC module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative examples of depth prediction from validation sets. The depth maps on the right are rendered with the official color map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Yuxuan Liu and Ming Liu are with the Robotics and Multi-Perception Laborotary, Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology yliuhb@connect.ust.hk ,eelium@ust.hk 2 Yuan Yixuan is with the Department of Electrical Engineering, City University of Hong Kong, Hong Kong, China.</figDesc><table /><note>yxyuan.ee@cityu.edu.hk Digital Object Identifier (DOI): see top of this page.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>3D Object Detection Results of Car on KITTI Test Set</figDesc><table><row><cell>Methods</cell><cell>3D Easy</cell><cell>3D Moderate</cell><cell>3D Hard</cell><cell>BEV Easy</cell><cell>BEV Moderate</cell><cell>BEV Hard</cell><cell>Time</cell></row><row><cell>MonoPSR[15]</cell><cell>10.76 %</cell><cell>7.25 %</cell><cell>5.85 %</cell><cell>18.33 %</cell><cell>12.58 %</cell><cell>9.91 %</cell><cell>0.2s</cell></row><row><cell>PLiDAR[14]</cell><cell>10.76 %</cell><cell>7.50 %</cell><cell>6.10 %</cell><cell>21.27 %</cell><cell>13.92 %</cell><cell>11.25 %</cell><cell>0.1s</cell></row><row><cell>SS3D[6]</cell><cell>10.78 %</cell><cell>7.68 %</cell><cell>6.51 %</cell><cell>16.33 %</cell><cell>11.52 %</cell><cell>9.93 %</cell><cell>0.05s</cell></row><row><cell>MonoDIS[29]</cell><cell>10.37 %</cell><cell>7.94 %</cell><cell>6.40 %</cell><cell>17.23 %</cell><cell>13.19 %</cell><cell>11.12 %</cell><cell>0.1s</cell></row><row><cell>M3D-RPN[18]</cell><cell>14.76 %</cell><cell>9.71 %</cell><cell>7.42 %</cell><cell>21.02 %</cell><cell>13.67 %</cell><cell>10.42 %</cell><cell>0.16s</cell></row><row><cell>RTM3D[7]</cell><cell>14.41 %</cell><cell>10.34 %</cell><cell>8.77 %</cell><cell>19.17 %</cell><cell>14.20 %</cell><cell>11.99 %</cell><cell>0.05s</cell></row><row><cell>AM3D[12]</cell><cell>16.50 %</cell><cell>10.74 %</cell><cell>9.52 %</cell><cell>25.03 %</cell><cell>17.32 %</cell><cell>14.91 %</cell><cell>0.4s</cell></row><row><cell>D4LCN[19]</cell><cell>16.65 %</cell><cell>11.72 %</cell><cell>9.51 %</cell><cell>22.51 %</cell><cell>16.02 %</cell><cell>12.55 %</cell><cell>0.2s</cell></row><row><cell>Ours</cell><cell>21.65 %</cell><cell>13.25 %</cell><cell>9.91 %</cell><cell>29.81 %</cell><cell>17.98 %</cell><cell>13.08 %</cell><cell>0.05s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Depth Prediction Results on KITTI Test Set</figDesc><table><row><cell>Methods</cell><cell>SILog</cell><cell>sqErrorRel</cell><cell>absErrorRel</cell><cell>iRMSE</cell></row><row><cell>PAP[36]</cell><cell>13.08</cell><cell>2.72 %</cell><cell>10.27 %</cell><cell>13.95</cell></row><row><cell>VNL[37]</cell><cell>12.65</cell><cell>2.46 %</cell><cell>10.15 %</cell><cell>13.02</cell></row><row><cell>SoftDorn[21]</cell><cell>12.39</cell><cell>2.49 %</cell><cell>10.10 %</cell><cell>13.48</cell></row><row><cell>Base U-Net</cell><cell>12.78</cell><cell>3.11 %</cell><cell>10.12 %</cell><cell>13.46</cell></row><row><cell>Ours</cell><cell>12.13</cell><cell>2.61 %</cell><cell>9.41 %</cell><cell>12.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>3D Detection Ablation Study Results of Car on KITTI Validation Set</figDesc><table><row><cell>Methods</cell><cell>IoU ≥ 0.7 3D Easy/Moderate/Hard</cell><cell>IoU ≥ 0.5 3D Easy/Moderate/Hard</cell></row><row><cell>Baseline Model</cell><cell>23.63 %/ 16.16 %/ 12.06 %</cell><cell>60.92 %/ 42.18 %/ 32.02 %</cell></row><row><cell>w/o Anchor Filtering</cell><cell>21.39 %/ 14.35 %/ 11.11 %</cell><cell>59.76 %/ 41.00 %/ 31.10 %</cell></row><row><cell>w OHEM</cell><cell>22.45 %/ 15.10 %/ 11.29 %</cell><cell>60.71 %/ 42.01 %/ 31.88 %</cell></row><row><cell>w Conv</cell><cell>21.57 %/ 15.26 %/ 11.35 %</cell><cell>58.17 %/ 41.17 %/ 32.58 %</cell></row><row><cell>w DisparityConv</cell><cell>22.13 %/ 15.42 %/ 11.34 %</cell><cell>60.13 %/ 41.62 %/ 33.07 %</cell></row><row><cell>w Deformable Conv</cell><cell>22.16 %/ 15.71 %/ 11.75 %</cell><cell>62.24 %/ 43.93 %/ 33.76 %</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Focal loss in 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1809.06065</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pointtracknet: An end-to-end network for 3-d object detection and tracking from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Sukai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Yuxiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Chengju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="issue">2020</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image detector based automatic 3d data labeling and training for vehicle detection on point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="1408" to="1413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Stereo R-CNN based 3d object detection for autonomous driving. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Shift R-CNN: deep monocular 3d object detection with closed-form geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andretti</forename><surname>Naiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Paunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongmo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongmoon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
		<idno>abs/1905.09970</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection and box fitting trained end-to-end using intersection-over-union loss. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eskil</forename><surname>Jörgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Xuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaici</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feidao</forename><surname>Cao</surname></persName>
		</author>
		<idno>abs/2001.03343</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A dynamic bayesian network model for autonomous 3d reconstruction from a single indoor image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Delage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2418" to="2428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Floor detection based depth estimation from a single indoor scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3358" to="3362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07179</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Accurate monocular 3d object detection via colorembedded 3d reconstruction for autonomous driving. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Refinedmpl: Refined monocular pseudolidar for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Vianney Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aich</forename><surname>Uwabeza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Shubhra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bingbing</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection with pseudo-lidar point cloud. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">SMOKE: Single-stage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Tóth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10111</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">M3D-RPN: monocular 3d region proposal network for object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1806.02446</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Soft labels for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bidirectional attention network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhra</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean Marie Uwabeza</forename><surname>Vianney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md Amirul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00743</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet Kohli Nathan</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">DIODE: A Dense Indoor and Outdoor DEpth Dataset. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">F</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolov3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<idno>abs/1807.03247</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Disentangling monocular 3d object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>López-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1504.08083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4106" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5683" to="5692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/1604.03540</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deformable convnets v2: More deformable, better results. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>abs/1811.11168</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
