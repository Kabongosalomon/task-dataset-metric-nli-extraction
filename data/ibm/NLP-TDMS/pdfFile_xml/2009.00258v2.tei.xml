<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distinctive 3D local deep descriptors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Poiesi</surname></persName>
							<email>poiesi@fbk.eu</email>
							<affiliation key="aff0">
								<orgName type="department">Technologies of Vision</orgName>
								<orgName type="institution">Fondazione Bruno Kessler</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
							<email>dboscaini@fbk.eu</email>
							<affiliation key="aff0">
								<orgName type="department">Technologies of Vision</orgName>
								<orgName type="institution">Fondazione Bruno Kessler</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distinctive 3D local deep descriptors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a simple but yet effective method for learning distinctive 3D local deep descriptors (DIPs) that can be used to register point clouds without requiring an initial alignment. Point cloud patches are extracted, canonicalised with respect to their estimated local reference frame and encoded into rotation-invariant compact descriptors by a PointNet-based deep neural network. DIPs can effectively generalise across different sensor modalities because they are learnt end-to-end from locally and randomly sampled points. Because DIPs encode only local geometric information, they are robust to clutter, occlusions and missing regions. We evaluate and compare DIPs against alternative hand-crafted and deep descriptors on several indoor and outdoor datasets consisting of point clouds reconstructed using different sensors. Results show that DIPs (i) achieve comparable results to the state-of-the-art on RGB-D indoor scenes (3DMatch dataset), (ii) outperform state-of-the-art by a large margin on laser-scanner outdoor scenes (ETH dataset), and (iii) generalise to indoor scenes reconstructed with the Visual-SLAM system of Android ARCore. Source code: https://github.com/fabiopoiesi/dip.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Encoding local 3D geometric information (e.g. coordinates, normals) into compact descriptors is key for shape retrieval <ref type="bibr" target="#b0">[1]</ref>, face recognition <ref type="bibr" target="#b1">[2]</ref>, object recognition <ref type="bibr" target="#b2">[3]</ref> and rigid (six degrees-of-freedom) registration <ref type="bibr" target="#b3">[4]</ref>. Learning such encoding from examples using deep neural networks has outperformed hand-crafted methods <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b11">[12]</ref>. These approaches have been designed to encode geometric information either from meshes <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> or from point clouds <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>. Our method belongs to the latter category and can be used to rigidly register point clouds without requiring an initial alignment ( <ref type="figure">Fig. 1)</ref>.</p><p>Existing solutions to compute compact 3D descriptors can be categorised into one-stage <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref> and two-stage <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref> methods. Although both categories share the objective of making descriptors invariant to pointcloud rigid transformations, one-stage methods encode the local geometric information of a patch (collection of locally sampled points) using the points of the patch directly. Differently, twostage methods firstly estimate a local reference frame (LRF) from the points within the patch to rigidly transform the patch to a canonical frame, then they encode the information of the canonicalised points into a compact descriptor. Given two corresponding patches of two non-aligned point clouds, if we canonicalise them through their respective LRFs we should obtain two identical overlapping patches. Hence, the encoding method should be simpler to design than that of onestage methods. However, noise, occlusions and point clouds reconstructed with different sensors make the LRF estimation challenging <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Modelling descriptors to be robust to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB-D reconstruction -indoors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laser scanner reconstruction -outdoors</head><p>Mobile Visual-SLAM (ARCore) reconstruction -indoors descriptor extraction descriptor matching rigid transformation <ref type="figure">Fig. 1</ref>. Point cloud registration typically involves three steps: description extraction, descriptor matching and rigid transformation. We focus on the first step. DIPs can be used to register point clouds reconstructed with different sensor modalities in different environments. DIPs are local, rotation-invariant and compact descriptors that are learnt from examples using a PointNetbased deep neural network <ref type="bibr" target="#b14">[15]</ref>. The correspondences between points of corresponding patches (magenta and green) are implicitly learnt by the network.</p><p>The key aspect of DIPs is their generalisation ability. different sensors (e.g. RGB-D, laser scanner) and to different environments (e.g. indoor, outdoor) is also a challenge <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>. One-stage learning-based methods can achieve rotation invariance by encoding the local geometric information with a set of geometric relationships, such as points, normals and point pair features <ref type="bibr" target="#b5">[6]</ref>, and then by learning descriptors via a PointNet-based deep network in order to achieve permutation invariance with respect to the set of the input points <ref type="bibr" target="#b14">[15]</ref>. Alternatively, 3D convolutional neural networks (ConvNets) can be used locally, to process patches around interest points <ref type="bibr" target="#b3">[4]</ref>, or globally, to process whole point clouds <ref type="bibr" target="#b10">[11]</ref>. In twostep methods, LRFs can be computed with hand-crafted <ref type="bibr" target="#b19">[20]</ref> or learning-based <ref type="bibr" target="#b17">[18]</ref> methods. After LRF canonicalisation, points can be transformed into a voxel grid, where each voxel encodes the density of the points within <ref type="bibr" target="#b9">[10]</ref>. Then, descriptors can be encoded from this voxel representations using a 3D ConvNet learnt with a Siamese approach <ref type="bibr" target="#b20">[21]</ref>. In this paper we present a novel two-stage method where compact descriptors are learnt end-to-end from canonicalised patches. To mitigate the problem of incorrectly estimated LRFs, we learn an affine transformation that refines the canonicalisation operation by minimising the Euclidean distance between points through the Chamfer loss <ref type="bibr" target="#b8">[9]</ref>. Similarly to <ref type="bibr" target="#b5">[6]</ref>, we learn descriptors with a PointNet-based deep neural network through a Siamese approach, but differently from <ref type="bibr" target="#b5">[6]</ref> (i) we use LRFs to canonicalise patches, (ii) our descriptors encode local information only, thus promoting robustness to clutter, occlusions, and missing regions, and (iii) we use a hardest contrastive loss to mine for quadruplets <ref type="bibr" target="#b10">[11]</ref>, thus improving metric learning. Differently from <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b9">[10]</ref>, points are consumed directly by our network without adding augmented hand-crafted features or performing prior voxelisations. We train our network using the 3DMatch dataset that consists of indoor scenes reconstructed with RGB-D sensors <ref type="bibr" target="#b3">[4]</ref>. We achieve state-of-the-art results on the 3DMatch test set and on its augmented version, namely 3DMatchRotated <ref type="bibr" target="#b6">[7]</ref>, employed to assess descriptor rotation invariance. We significantly outperform existing approaches in terms of generalisation ability to different sensor modalities (RGB-D → laser scanner) and to different environments (indoor bedrooms → outdoor forest) using the ETH dataset <ref type="bibr" target="#b21">[22]</ref>. Moreover, we validate DIP generalisation ability to another sensor modality (RGB-D → smartphone) by capturing three overlapping indoor point clouds with the Visual-SLAM system <ref type="bibr" target="#b22">[23]</ref> of an ARCore-based App <ref type="bibr" target="#b23">[24]</ref> we have developed to reconstruct the environment. Notably, DIPs can successfully and robustly be used also to align these point clouds. The source code and the reconstruction App are publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. OUR APPROACH</head><p>Given a point cloud P ⊂ R 3 , we define a local patch X = {x} ⊂ P as an unordered set of 3D points x with cardinality |X | = n. We design a deep neural network Φ Θ that generates DIPs such that f = Φ Θ (X ), where f ⊂ R d and Θ is the set of learnable parameters. Without loss of generality we use the 3D coordinates of the points as input to Φ Θ , i.e. x = (x, y, z).  <ref type="bibr" target="#b14">[15]</ref>, where the three main modifications that allow us to produce DIPs are in the Transformation Network, the Bottleneck and the Local Response Normalisation layer. Transformation Network The patch X is firstly passed through the Transformation Network (TNet) that predicts the affine transformation A ∈ R 3×3 and that is applied to each x ∈ X . In our experiments we explored the possibility of constraining TNet to be close to an orthogonal matrix, hence a rigid transformation, via the regularisation term reg = I−AA 2 F [15], <ref type="bibr" target="#b24">[25]</ref>. We observed that while det(A) → 1, which is a necessary condition for A to be a rigid transformation, A → I as well, thus making the contribution of A negligible. We empirically observed performance improvements without constraining TNet to be orthogonal. There also exists the possibility of using an Iterative Transformer Network, to perform patch canonicalisation iteratively through a series of 3D rigid transformations <ref type="bibr" target="#b25">[26]</ref>. This could be an interesting extension of our architecture, but it is out of the scope of our paper. TNet has a similar architecture to the rest of the network, except for the last MLP layer that outputs nine values that are opportunely reshaped to form A. Although TNet is also used in the original version of PointNet <ref type="bibr" target="#b14">[15]</ref>, we have to train it carefully to achieve the desired behaviour for DIPs. We explain how we train TNet in Sec. II-B and III-B. Applying A to x results inx = Ax.X = {x} is then processed through the MLP layers with shared weights before reaching the bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network architecture</head><p>Bottleneck The bottleneck is modelled as a symmetric function that produces permutation-invariant outputs <ref type="bibr" target="#b14">[15]</ref>. Although modelling this function with an average pooling operation has shown to be effective for 6DoF registration applications <ref type="bibr" target="#b26">[27]</ref>, we empirically found that max pooling provides superior performance <ref type="bibr" target="#b14">[15]</ref>. Let m be the number of channels in output from the layer before the bottleneck, the max pooling operation is defined as max :</p><formula xml:id="formula_0">R n×m → R m such that γ = max X Φ Θj (X ) ,<label>(1)</label></formula><p>where γ = (g 1 , g 2 , . . . , g m ) is a global signature of the input patch, g i is the i th element of γ and Φ Θj is an intermediate output of the network at the j th layer. We observed that γ can be used to predict how informative DIP is. Let us define the function that returns the indices of the γ values as argmax :</p><formula xml:id="formula_1">R n×m → [1, n] m such that α = argmax X Φ Θj (X ) .<label>(2)</label></formula><p>Next, let us take two corresponding patches X and X extracted from two overlapping point clouds P and P , and then compute γ, α, and γ , α , respectively. α (α ) will be the same regardless of the permutations of the points in X (X ). We observed that the corresponding values of α and α can be interpreted as the correspondences between points in X and X . Accordingly, their corresponding max values γ and γ quantify how reliable these correspondences are. Then, we found that the norm of γ can be effectively used to quantify the reliability of γ, e.g. to lower the importance of, or discard, patches extracted from flat surfaces. It turns out that good descriptors can be selected imposing the condition</p><formula xml:id="formula_2">ρ = γ 2 &gt; τ ρ ,<label>(3)</label></formula><p>where τ ρ is a threshold. <ref type="figure" target="#fig_1">Fig. 3</ref> shows an example of global signatures computed from two pairs of corresponding patches (green) that are extracted from two overlapping point clouds (blue and grey) from the 3DMatch dataset <ref type="bibr" target="#b3">[4]</ref>. The first case shows two patches extracted from a flat surface (wall), whereas the patches in the second case are extracted from more structured surfaces (bed). From each patch we randomly sample 256 points and pass them through the network to obtain their respective global signatures (Eq. 1). This figure shows the correspondences between the points of the corresponding patches such that g i &gt; τ ρ ∀ i = 1, . . . , m, where τ ρ = 0.15. There are a few things we can observe in this example. First, values of γ are on average higher when the patches are extracted on structured surfaces. Second, the percentage of correspondences above the threshold is larger when the patches are extracted on structured surfaces (28% vs. 18%). Lastly, we can see that the patches extracted on flat surfaces have lower ρ. <ref type="figure">Fig. 4</ref> shows the distribution of the ρ values for 20K patches randomly sampled from three point clouds. We can see that low values of ρ are distributed on flat surfaces (poor information) and along borders (incomplete <ref type="figure">Fig. 4</ref>. Heatmaps of the ρ values for 20K patches randomly sampled from three point clouds of the 3DMatch dataset <ref type="bibr" target="#b3">[4]</ref>. Each point is the centre of a patch with a radius of 0.3 √ 3m <ref type="bibr" target="#b9">[10]</ref>. The more structured the surface enclosed in a patch is, the higher the value of ρ is. information). Differently, ρ has higher value near corners and on objects.</p><p>It may be impractical to find a single τ ρ that generalises across different input data or different network architecture. We thereby propose to infer it from the distribution of the ρ values. Let R be the set of ρ values computed from the patches extracted from P, and let f (ρ) be the probability density function of the ρ values. We determine τ ρ as the p th ρ percentile through the cumulative density function, such that</p><formula xml:id="formula_3">p ρ = 100 · τρ −∞ f (ρ)dρ,<label>(4)</label></formula><p>i.e. the area under the probability density function f (ρ) to the left of τ ρ is p ρ /100. Metric layers and Local Response Normalisation After max pooling, γ is processed by a series of MLP layers acting as metric layers to learn distinctive embeddings for our descriptors. We use a Local Response Normalisation (LRN) layer to produce unitary-length descriptors as we found it works well in practice <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b20">[21]</ref>. LRN consists of a L2 normalisation of the last MLP layer's d-dimensional output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss functions</head><p>The objective of our training is to produce descriptors whose reciprocal distance in the embedding space is minimised for corresponding patches of different point clouds. To this end, we train our network following a Siamese approach that processes pairs of corresponding descriptors using two branches with shared weights <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Each branch independently calculates a descriptor for a given patch. We learn the parameters of the network by minimising the linear combination of two losses, aiming at two different goals. The first goal is to geometrically align two patches under the learnt affine transformation. The second goal is to produce compact and distinctive descriptors via metric learning. Chamfer loss Given two patches X , X , we want to minimise the distance between each point x ∈ X and its nearest neighbour x ∈ X . Therefore we use the Chamfer loss <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b28">[29]</ref> on the output of TNet as Two overlapping point clouds are aligned using the ground-truth transformation. A set of b points (red) belonging to the overlap region (cyan) is sampled using the Farthest Point Sampling method <ref type="bibr" target="#b27">[28]</ref>. We use a Siamese approach to train two deep neural networks with shared parameters concurrently. For each branch, we perform the following operations: (i) for each point a patch (orange) with radius τr is extracted and the corresponding Local Reference Frame (LRF) <ref type="bibr" target="#b9">[10]</ref> is computed using the points of the patch; (ii) this patch is rigidly transformed using the LRF and n points are randomly sampled from the patch (yellow points); (iii) the coordinates of these n points are expressed relative to the patch centre and normalised in order to have a unitary radius; (iv) these n points are given to the deep network as input to learn the descriptor. We compute the final loss as the linear combination of the Chamfer loss <ref type="bibr" target="#b8">[9]</ref> applied to the TNet's output and of the hardest-contrastive loss <ref type="bibr" target="#b10">[11]</ref> applied to the network's output.</p><formula xml:id="formula_4">c (X ) = 1 2n x∈X min x ∈X Ax − A x 2 (5) + x ∈X min x∈X Ax − A x 2 .</formula><p>Hardest-contrastive loss Our metric learning is performed through negative mining using the hardest-contrastive loss <ref type="bibr" target="#b10">[11]</ref>. Given a pair of anchors (f , f ), we mine the hardest-negatives (f -, f -) and define the loss as</p><formula xml:id="formula_5">h = 1 b (f ,f )∈C+ 1 |C + | [d(f , f ) − m + ] 2 +<label>(6)</label></formula><formula xml:id="formula_6">+ 1 2|C -| [m -− miñ f ∈C- d(f ,f ) d(f ,f-) ] 2 + + 1 2|C -| [m -− miñ f ∈C- d(f ,f ) d(f ,f -) ] 2 + ,</formula><p>where C + is the set of the anchor pairs and Cis the set of descriptors (opportunely sampled) used for the hardest-negative mining extracted from a minibatch. m + and mare the margins for positive and negative pairs, respectively.</p><p>[·] + takes the positive part of its argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL VALIDATION</head><p>We evaluate the distinctiveness of DIPs using the indoor 3DMatch dataset <ref type="bibr" target="#b3">[4]</ref>, and assess DIP generalisation on the outdoor ETH dataset <ref type="bibr" target="#b21">[22]</ref> and on a new indoor dataset we collected with a smartphone. We explain how patches are extracted and given as input to our deep network. The training pipeline is shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. Our method is developed in Pytorch 1.3.1 <ref type="bibr" target="#b29">[30]</ref>. We compare our method with 14 state-of-the-art methods and carry out a thorough ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Patch extraction</head><p>DIPs are learnt from patches that are extracted from point cloud pairs (P, P ) whose overlap region is greater than a threshold τ o . Let O ⊂ P and O ⊂ P be the overlap regions. During training we know the ground-truth transformation T ∈ SE(3) that register P to P. Point correspondences between O and O can be determined either by using the 3DMatch toolbox <ref type="bibr" target="#b3">[4]</ref>, or by using a nearest neighbourhood search after applying T to P <ref type="bibr" target="#b9">[10]</ref>. We use the latter approach by seeking nearest points from O to O within a radius of 10cm. Corresponding points in O and O are the candidate anchors used by the hardest-contrastive loss (Eq. 6).</p><p>Farthest Point Sampling Anchor sampling is key to allow for an effective minimisation of Eq. 6, and typically this is carried out with random sampling <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Such random sampling may lead to cases where anchors and negatives are sampled spatially close to each other. A solution can be disregarding negatives within a certain radius from an anchor by computing the Euclidean distances amongst all the anchors within a minibatch in order to determine whether to penalise for the distance between descriptors in the embedding space (Eq. 5 in <ref type="bibr" target="#b10">[11]</ref>). Including points within the radius would force the network to learn distinctive descriptors of region with similar geometric structures, thus making training unstable. Therefore to avoid computing the Euclidean distances amongst all the anchors within the minibatch <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, we efficiently sample anchors having the largest distance amongst themselves using Farthest Point Sampling (FPS) <ref type="bibr" target="#b27">[28]</ref>. Specifically, we sample b points within O using FPS and then search for the nearest neighbour counterparts in O . These points are the anchors that construct the minibatch on which the hardest-negative mining is performed. In our experiments we use b = 256.</p><p>Local patch Given a point c ∈ O sampled with FPS, and its nearest neighbour c ∈ O , we build the set Y = {y ∈ X : y − c 2 ≤ τ r }, and similarly the set Y for c . τ r is the radius of our patch. We use the points in Y and Y to compute their own Local Reference Frame (LRF) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Each LRF is constructed independently by computing the three orthogonal axes: the z-axis is computed as the normal of the local surface defined by the points of Y (Y ); the x-axis is computed as a weighted sum of the vectors constructed as the projection of vectors between c and the points in Y \c on the plane orthogonal to the z-axis; the y-axis is computed as the cross-product between the z-axis and the x-axis. We implement the LRF following <ref type="bibr" target="#b9">[10]</ref>. Let L, L ∈ R 3×3 be the LRFs of Y and Y , respectively. Y and Y may contain a large number of points, typically a few thousands, thus it is impractical to process them all with a deep network. Similarly to <ref type="bibr" target="#b14">[15]</ref>, we randomly sample n = 256 points. This also helps regularisation during training and generalisation. Next, we recalculate the coordinate of each of the n points relative to their patch centre and normalise the radius of the sphere that contains them. Formally, let Q(Y) = {ŷ :ŷ = (y − c)/τ r , y ∈ Y} be the set of randomly-sampled and normalised points from Y where |Q(Y)| = n. Lastly, we apply L to Q(Y) to rotate the points with respect to their LRFs, such that</p><formula xml:id="formula_7">X = L ⊗ Q(Y),<label>(7)</label></formula><p>where the operation ⊗ defines the application of L to each element of Q(Y) such that x = Lŷ. Analogously, the same operations are performed for Q(Y ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets, training and testing setup 3DMatch dataset</head><p>We learn DIPs from the point clouds of the 3DMatch dataset <ref type="bibr" target="#b3">[4]</ref>. 3DMatch is composed of 62 real-world indoor scenes collected from Analysis-by-Synthesis <ref type="bibr" target="#b30">[31]</ref>, 7-Scenes <ref type="bibr" target="#b31">[32]</ref>, SUN3D <ref type="bibr" target="#b32">[33]</ref>, RGB-D Scenes v.2 <ref type="bibr" target="#b33">[34]</ref>, and Halber and Funkhouser <ref type="bibr" target="#b34">[35]</ref>. The official split consists of 54 scenes for training and 8 for testing. Each scene is split into partially overlapping and registered point cloud pairs. As in <ref type="bibr" target="#b9">[10]</ref>, we train our deep network with the pairs whose overlap is more than τ o = 30%. The b = 256 points are sampled from each of these overlap regions and we centre the patches on these points to construct each minibatch. As in <ref type="bibr" target="#b10">[11]</ref>, we set m + = 0.1 and m -= 1.4 (Eq. 6). Each epoch consists of 16602 iterations. Each iteration is for a point cloud pair. We train for 40 epochs. We use Dropout with probability 0.3 at the last MLP layer. We subsample point clouds using a voxel size of 0.01m. As <ref type="bibr" target="#b9">[10]</ref>, we set τ r = 0.3 √ 3m. Our training aims to minimise the linear combination of h (Eq. 6) and c (Eq. 5) as</p><formula xml:id="formula_8">= h + 1 b X ∈P c (X ).<label>(8)</label></formula><p>We use Stochastic Gradient Descent with an initial learning rate of 10 −3 that decreases by a factor 0.1 every 15 epochs. The eight test scenes consists of 1117 point cloud pairs. As in <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>, testing is performed by randomly sampling 5K points from each point cloud. To evaluate DIP's rotation invariance ability, we follow the evaluation of <ref type="bibr" target="#b9">[10]</ref> and create an augmented version of 3DMatch, namely 3DMatchRotated: each point cloud is rotated by an angle sampled uniformly between [0, 2π] around all the three axes independently. Unless otherwise stated we use p ρ = 5. ETH dataset We use the ETH dataset to assess the ability of DIPs to generalise across sensor modalities (RGB-D → laser scanner) and on different scenes (indoor → outdoor) <ref type="bibr" target="#b21">[22]</ref>. To this end we use the same model trained on the 3DMatch dataset (no fine tuning). The ETH dataset consists of four outdoor scenes, namely Gazebo-Summer, Gazebo-Winter, Wood-Summer and Wood-Autumn, containing partially overlapping, sparse and dense vegetation point clouds. Differently from the 3DMatch dataset we subsample point clouds using a voxel size of 0.06m. We set the patch kernel size τ r = 0.6 √ 3m. For a fair comparison, the evaluation procedure follows verbatim <ref type="bibr" target="#b9">[10]</ref>, i.e. random sampling of 5K points. VigoHome dataset To evaluate DIPs on another sensor modality (RGB-D → smartphone RGB), we have created a new dataset, namely VigoHome, by reconstructing the inside of a house using a Visual-SLAM smartphone App we developed with ARCore (Android) <ref type="bibr" target="#b23">[24]</ref>. We captured three zones, namely livingroom-downstairs (94K points), bedroomupstairs (43K points), and bathroom-upstairs (83K points). The stairs between the three zones is the overlapping region of the point clouds. We calculated their transformations to a common reference frame and determined the point correspondences to evaluate the registration: two points of a point cloud pair are corresponding if they are nearest neighbours within a 0.1mradius. We subsample point clouds using voxels of 0.01m and set τ r = 0.6 √ 3m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison and ablation study setup</head><p>We compare DIPs against 14 alternative descriptors: Spin <ref type="bibr" target="#b2">[3]</ref>, SHOT <ref type="bibr" target="#b16">[17]</ref>, FPFH <ref type="bibr" target="#b15">[16]</ref>, USC <ref type="bibr" target="#b35">[36]</ref>, CGF <ref type="bibr" target="#b36">[37]</ref>, 3DMatch <ref type="bibr" target="#b3">[4]</ref>, Folding <ref type="bibr" target="#b4">[5]</ref>, PPFNet <ref type="bibr" target="#b5">[6]</ref>, PPF-FoldNet <ref type="bibr" target="#b6">[7]</ref>, DirectReg <ref type="bibr" target="#b7">[8]</ref>, Cap-suleNet <ref type="bibr" target="#b8">[9]</ref>, PerfectMatch <ref type="bibr" target="#b9">[10]</ref>, FCGF <ref type="bibr" target="#b10">[11]</ref>, and D3Feat <ref type="bibr" target="#b11">[12]</ref>. In our ablation study we train the model for five epochs on a subset of 3DMatch's scenes, i.e. Chess and Fire, and test on Home2 and Hotel3. We chose these test scenes because we found them to be sufficiently challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation metrics</head><p>Feature-matching recall We use the feature-matching recall (FMR) to quantify the descriptor quality <ref type="bibr" target="#b5">[6]</ref>. FMR does not require RANSAC <ref type="bibr" target="#b37">[38]</ref> as it directly averages the number of correctly matched point clouds across datasets. Only recall is measured, as the precision can be improved by pruning correspondences <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b38">[39]</ref>. FMR is defined as</p><formula xml:id="formula_9">Ξ = 1 |F| |F | s=1 1 1 |Ω s | (x,x )∈Ωs 1( x − T s x 2 &lt; τ 1 ) ξΩ s &gt; τ 2 ,<label>(9)</label></formula><p>where |F| is the number of matching point cloud pairs having τ o ≥ 30% (overlap between each other). (x, x ) is a pair of corresponding points found in the descriptor (embedding) space via a mutual nearest-neighbour search <ref type="bibr" target="#b9">[10]</ref>. Ω s is the set that contains all the found pairs (x, x ) in the overlap regions O ⊂ P and O ⊂ P , respectively. T s is the ground-truth transformation alignment between P and P. 1(·) is the indicator function. τ 1 = 10cm and τ 2 = 0.05 are set based on the theoretical analysis that RANSAC will find at least three corresponding points that can provide the correct T s with probability 99.9% using no more than ≈ 55K iterations <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>. In addition to Ξ, we also report mean (µ ξ ) and standard deviation (σ ξ ) of ξ Ωs before applying τ 2 . Registration recall We measure the registration recall for the transformation estimated with RANSAC <ref type="bibr" target="#b10">[11]</ref>. The registration  <ref type="bibr" target="#b16">[17]</ref> .238 .109 .234 .095 352 .279 FPFH <ref type="bibr" target="#b15">[16]</ref> .359 .134 .364 .136 33 .032 USC <ref type="bibr" target="#b35">[36]</ref> .400 .125 --1980 3.712 CGF <ref type="bibr" target="#b36">[37]</ref> .582 .142 .585 .140 32 1.463 3DMatch <ref type="bibr" target="#b3">[4]</ref> .596 .088 .011 .012 512 3.210 Folding <ref type="bibr" target="#b4">[5]</ref> .613 .087 .023 .010 512 .352 PPFNet <ref type="bibr" target="#b5">[6]</ref> .623 .108 .003 .005 64 2.257 PPF-FoldNet <ref type="bibr" target="#b6">[7]</ref> .718 .105 .731 .104 512 .794 DirectReg <ref type="bibr" target="#b7">[8]</ref> .746 .094 --512 .794 CapsuleNet <ref type="bibr" target="#b8">[9]</ref> .807 .062 .807 .062 512 1.208 PerfectMatch <ref type="bibr" target="#b9">[10]</ref> .947 .027</p><p>.949 .024 32 5.515 FCGF <ref type="bibr" target="#b10">[11]</ref> .952 .029 .953 .033 32 .009 D3Feat <ref type="bibr" target="#b11">[12]</ref> . recall quantifies the miss-rate by measuring the distance between corresponding points for each point cloud pair using the estimated transformation based on ground-truth point correspondence information. The registration recall is defined as</p><formula xml:id="formula_10">Υ = 1 |F| |F | s=1 1 1 |Ω s | (x,x )∈Ωs x −Tx 2 2 &lt; 0.2m ,<label>(10)</label></formula><p>whereT is the estimated transformation. Only point clouds pairs with at least 30% overlap are evaluated. 0.2m is set as in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. We configure RANSAC based on the FMR formulation and use its Open3D implementation <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Quantitative analysis and comparison</head><p>Tab. I reports DIP results in comparison with alternative descriptors on 3DMatch and 3DMatchRotated datasets. Results show that DIPs achieve state-of-the art results and that are rotation invariant as FMR is almost the same for both datasets. When FMR is measured at different values of τ 2 , DIPs are more distinctive than the alternatives <ref type="figure" target="#fig_3">(Fig. 6</ref>). Interestingly, DIPs largely outperform PPFNet descriptors that are also computed with a PointNet-based backbone. We believe that this occurs on the one hand as a result of DIP's LRF canonicalisation, in fact PPFNet's FMR drops in 3DMatchRotated as no canonicalisation is performed, and on the other hand because DIPs encode only the local geometric information, which makes them more generic and distinctive across different scenes as opposed to PPFNet descriptors that instead encode contextual information too. Amongst the eight tested scenes, the worst performing one is Lab that will analyse in detail later. Our Python implementation takes 4.87ms to process a DIP using an i7-8700 CPU at 3.20GHz with a NVIDIA GTX 1070 Ti GPU and 16GB RAM. 93% of the execution time is for the LRF estimation. Once a patch is canonicalised the deep network processes the descriptor in 0.37ms. Note that the LRF estimation can be parallelised and implemented in C++ to reduce the execution time.  Tab. II reports the registration recall results, where the descriptor distinctiveness we have observed in <ref type="figure" target="#fig_3">Fig. 6</ref> is reflected on the estimated transformations. On average, DIPs outperform all the other descriptors. We can see that the Lab scene mentioned before is the worst performing one. This occurs because Lab contains several point clouds of partially reconstructed objects and flat surfaces. Two examples are shown in <ref type="figure">Fig. 7</ref>. The first one is a failed registration due to the lack of informative geometries in the scene. The second one is a successful registration, where the kitchen appliances produced more distinctive descriptors than the first case.</p><p>We further evaluate the registration recall and assess DIP robustness following the comparative ablation study proposed in <ref type="bibr" target="#b11">[12]</ref>, where the registration recall is measured as a function of a decreasing number of sampled points used by RANSAC to estimate the transformation. Tab. III shows that DIPs on average have a superior robustness than the alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ablation study</head><p>Tab. IV reports the results of our ablation study on the implementation choices. Here we can see how the three modules, i.e. TNet, LRF and LRN, affect FMR. TNet learns to compensate for incorrectly estimated LRFs. But we can see that without LRF, TNet cannot learn the complete transformation to canonicalise the patches (FMR drops on 3DMatchRotated). LRF is key to make DIPs rotation invariant. Following <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b10">[11]</ref>, we can notice how learning unitary-length descriptors improve FMR. Lastly, as expected, the more the capacity to encode the information in descriptors of larger dimension, the better the performance. However, we used 32-dimensional descriptors throughout all the experiments in order to compare results with existing descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Generalisation ability: comparison and analysis</head><p>Tab. V reports the results obtained on the ETH dataset <ref type="bibr" target="#b21">[22]</ref>. We can see that DIPs on average largely outperform the alternative descriptors. Second to DIPs are PerfectMatch's descriptors <ref type="bibr" target="#b9">[10]</ref> that, as DIPs, use LRF canonicalisation. However, differently from DIPs, PerfectMatch's descriptors are learnt from hand-crafted representations, namely voxelised smoothed density value <ref type="bibr" target="#b9">[10]</ref>. We argue that letting the network learn the encoding from the points directly (end-to-end), leads to a much greater robustness and generalisation ability. We can also observe that the application of p ρ improves the performance. <ref type="figure">Fig. 8</ref> shows an example of result from Gazebo-Summer. Although the sensor modality and the structure of the environment is very different from that of 3DMatch, DIPs maintain their distinctiveness and can be successfully used to register two point clouds reconstructed with a laser scanner. <ref type="figure">Fig. 9</ref> shows results on our dataset, i.e. VigoHome. In each point cloud we included the corresponding reference frame, which is where each mapping session started. The result of a successful registration estimated using DIPs is shown in the bottom-right corner. We can notice that the structure of the environment largely differs from that of 3DMatch and ETH datasets, and that the distribution of the points on the surfaces is much noisier that that in the 3DMatch dataset. To quantify the registration results, we used a similar evaluation of that used in Sec. III-F. For each number of sampled points we run RANSAC 100 times and compute the registration recall. Tab. VI shows  <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Gazebo Wood Average Summer Winter Autumn Summer FPFH <ref type="bibr" target="#b15">[16]</ref> .386 .142 .148 .208 .221 SHOT <ref type="bibr" target="#b16">[17]</ref> .739 .457 .609 .640 .611 3DMatch <ref type="bibr" target="#b3">[4]</ref> .228 .083 .139 .224 .169 CGF <ref type="bibr" target="#b36">[37]</ref> .375 .138 .104 .192 .202 PerfectMatch <ref type="bibr" target="#b9">[10]</ref> .913 .841 .678 .728 .790 FCGF <ref type="bibr" target="#b10">[11]</ref> .228 .100 .148 .168 .161 D3Feat <ref type="bibr" target="#b11">[12]</ref> . that with only 5K points sampled from each point cloud, 85% of the times the three point clouds are correctly registered. A correct registration takes about 54s to be processed. We deem this a great result because it is achieved with DIPs learnt on the 3DMatch dataset. As additional comparison, we have also quantified the registration recall using FPFH descriptors <ref type="bibr" target="#b15">[16]</ref>. However, we found that the registration fails regardless the parameters used. So we have intentionally not included the results obtained with FPFH in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSIONS</head><p>We presented a novel approach to learn local, compact and rotation invariant descriptors end-to-end through a PointNetbased deep neural network using canonicalised patches. The affine transformation embedded in our network is learnt with the specific goal of improving patch canonicalisation. We showed the importance of this step through our ablation study. Results showed that DIPs achieve comparable performance to the stateof-the-art on the 3DMatch dataset, but that outperform the state-of-the-art by a large margin in terms of generalisation to different sensors and scenes. We further confirmed this by capturing a new indoor dataset using the Visual-SLAM system of ARCore (Android) running on an off-the-shelf smartphone. We observed that we can achieve good generalisation because DIPs are learnt end-to-end from the points without any handcrafted preprocessing after canonicalisation. Our future research direction is to improve the canonicalisation operation <ref type="bibr" target="#b17">[18]</ref>. <ref type="figure">Fig. 8</ref>. Estimated rigid transformation of a point cloud pair from the ETH dataset <ref type="bibr" target="#b21">[22]</ref>. This example shows that DIPs learnt on the 3DMatch dataset (RGB-D reconstructions) can be successfully used to register point clouds reconstructed with a laser scanner in outdoor scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>livingroom-downstairs</head><p>bedrooms-upstairs bathroom-upstairs rigid registration result <ref type="figure">Fig. 9</ref>. Estimated rigid transformations of three point clouds from our VigoHome dataset. DIPs can also generalise to point clouds reconstructed with the Visual-SLAM system of ARCore (Android) running on an off-the-shelf smartphone (Xiaomi Mi8). Three different overlapping zones of the inside of a house have been reconstructed. The reference frame of each point cloud, that is where the reconstruction session has started, is shown for each zone. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>shows our PointNet-based architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Point correspondences computed from the global signatures of two pairs of corresponding patches (green) that are extracted from two overlapping point clouds (blue and grey) from the 3DMatch dataset [4]. (top) Patches extracted from flat surfaces. (bottom) Patches extracted from structured surfaces.256 points are randomly sampled from each patch, which are given to our deep neural network as input to produce the global signature. Only the correspondences that satisfy the condition g i &gt; τρ = 0.15 are drawn. The percentage of these correspondences is reported. Points are colour-coded based on their respective g i value. Minimum, maximum and average g i s values are reported for each case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>DIP's training pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Feature-matching recall as a function of (left) τ 2 and (right) τ 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I FEATURE</head><label>I</label><figDesc>-MATCHING RECALL ON THE 3DMATCH DATASET<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table><row><cell>Method</cell><cell>3DMatch Ξ std</cell><cell cols="2">3DMatchRotated Ξ std</cell><cell>Feat. dim.</cell><cell>Time [ms]</cell></row><row><cell>Spin [3]</cell><cell>.227 .114</cell><cell>.227</cell><cell>.121</cell><cell>153</cell><cell>.133</cell></row><row><cell>SHOT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="8">REGISTRATION RECALL ON THE 3DMATCH DATASET [4].</cell></row><row><cell>Method</cell><cell cols="7">Kitchen Home1 Home2 Hotel1 Hotel2 Hotel3 Study Lab Average</cell></row><row><cell>FPFH [16]</cell><cell>.36</cell><cell>.56</cell><cell>.43</cell><cell>.29</cell><cell>.36</cell><cell>.61</cell><cell>.31 .31 .40</cell></row><row><cell>USC [36]</cell><cell>.52</cell><cell>.35</cell><cell>.47</cell><cell>.53</cell><cell>.20</cell><cell>.38</cell><cell>.46 .49 .43</cell></row><row><cell>CGF [37]</cell><cell>.72</cell><cell>.69</cell><cell>.46</cell><cell>.55</cell><cell>.49</cell><cell>.65</cell><cell>.48 .42 .56</cell></row><row><cell cols="2">3DMatch [4] .85</cell><cell>.78</cell><cell>.61</cell><cell>.79</cell><cell>.59</cell><cell>.58</cell><cell>.63 .51 .67</cell></row><row><cell>PPFNet [6]</cell><cell>.90</cell><cell>.58</cell><cell>.57</cell><cell>.75</cell><cell>.68</cell><cell>.88</cell><cell>.68 .62 .71</cell></row><row><cell>FCGF [11]</cell><cell>.93</cell><cell>.91</cell><cell>.71</cell><cell>.91</cell><cell>.87</cell><cell>.69</cell><cell>.75 .80 .82</cell></row><row><cell>DIP</cell><cell>.98</cell><cell>.94</cell><cell>.85</cell><cell>.98</cell><cell>.92</cell><cell>.89</cell><cell>.80 .75 .89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDY USING THE REGISTRATION RECALL AS A FUNCTION OF THE NUMBER OF SAMPLED POINTS ON THE 3DMATCH DATASET<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="4"># sampled points 5000 2500 1000 500</cell><cell>250</cell><cell>Average</cell></row><row><cell>PerfectMatch [10]</cell><cell>.803</cell><cell>.775</cell><cell>.734</cell><cell cols="2">.648 .509</cell><cell>.694</cell></row><row><cell>FCGF [11]</cell><cell>.873</cell><cell>.858</cell><cell>.858</cell><cell cols="2">.810 .730</cell><cell>.826</cell></row><row><cell>D3Feat [12]</cell><cell>.822</cell><cell>.844</cell><cell>.849</cell><cell>.825</cell><cell>.793</cell><cell>.827</cell></row><row><cell>DIP</cell><cell>.889</cell><cell>.890</cell><cell>.878</cell><cell>.866</cell><cell>.774</cell><cell>.859</cell></row><row><cell cols="7">Fig. 7. Estimated rigid transformations of two point cloud pairs from the</cell></row><row><cell cols="7">3DMatch dataset [4]. (top) Incorrectly estimated transformation due to the</cell></row><row><cell cols="7">lack of structured surfaces. (bottom) Correctly estimated transformation thanks</cell></row><row><cell cols="6">to the structured elements given by the kitchen appliances.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY ON DIP'S IMPLEMENTATION CHOICES. 32 .886 .272 .207 .877 .271 .207 32 .831 .215 .175 .834 .214 .174 32 .824 .237 .195 .215 .039 .054 16 .903 .264 .193 .906 .264 .193 32 .919 .318 .218 .926 .317 .217 64 .916 .327 .221 .928 .325 .220 128 .933 .335 .222 .924 .334 .222 TABLE V FEATURE-MATCHING RECALL ON THE ETH DATASET</figDesc><table><row><cell>d</cell><cell>TNet LRF LRN</cell><cell>Ξ</cell><cell>3DMatch µ ξ</cell><cell>σ ξ</cell><cell>3DMatchRotated Ξ µ ξ σ ξ</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI REGISTRATION</head><label>VI</label><figDesc>RECALL ON THE VIGOHOME DATASET AS A FUNCTION OF THE NUMBER OF SAMPLED POINTS.</figDesc><table><row><cell>Method</cell><cell cols="3"># sampled points 15000 10000 5000 2500 1000 500</cell></row><row><cell>DIP</cell><cell>.97</cell><cell>.90</cell><cell>.85 .68 .44 .16</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This research has received funding from the Fondazione CARITRO -Ricerca e Sviluppo programme 2018-2020.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">One point isometric matching with the heat kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Merigot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Memoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SGP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A two-phase weighted collaborative representation for 3D partial face recognition with single sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3D scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3DMatch: Learning the matching of local 3D geometry in range scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FoldingNet: Interpretable unsupervised learning on 3D point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PPFNet: Global context aware local features for robust 3D point matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of rotation invariant 3D local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">--</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Ppf-Foldnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D local features for direct pairwise registration</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D point capsule networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The perfect match: 3D point cloud matching with smoothed densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andreas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully Convolutional Geometric Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Anisotropic Diffusion Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fast point feature histograms (FPFH) for 3D registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SHOT: Unique signatures of histograms for surface and texture description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Di Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GFrames: Gradient-Based Local Reference Frame for 3D Shape Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Spezialetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">LRF-Net: Learning Local Reference Frames for 3D Local Shape Description and Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07832</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TOLDI: An effective and robust approach for 3D local shape description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">L2-Net: deep learning of discriminative patch descriptor in Euclidean space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Challenging data sets for point cloud registration algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ORB-SLAM: a versatile and accurate monocular SLAM system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TRO</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">ARCore</title>
		<ptr target="https://developers.google.com/ar" />
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Metrics for 3D rotations: comparison and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Iterative Transformer Network for 3D Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin-Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Category-Specific Symmetric 3D Keypoints from Point Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Labrador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chhatkuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Demonceaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07619</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<ptr target="http://pytorch.org" />
		<title level="m">Pytorch</title>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to navigate the energy landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SUN3D: A Database of Big Spaces Reconstructed using SfM and Object Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shape2Pose: human-centric shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Structured global registration of RGB-D scans in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08539</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unique shape context for 3D data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning compact geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust reconstruction of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Open3D: A modern library for 3D data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
