<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudra</forename><forename type="middle">P K</forename><surname>Poudel</surname></persName>
							<email>rudra.poudel@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Toshiba Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujwal</forename><surname>Bonde</surname></persName>
							<email>ujwal.bonde@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Toshiba Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Liwicki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toshiba Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
							<email>christopher.m.zach@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Toshiba Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern deep learning architectures produce highly accurate results on many challenging semantic segmentation datasets. State-ofthe-art methods are, however, not directly transferable to real-time applications or embedded devices, since naïve adaptation of such systems to reduce computational cost (speed, memory and energy) causes a significant drop in accuracy. We propose ContextNet, a new deep neural network architecture which builds on factorized convolution, network compression and pyramid representation to produce competitive semantic segmentation in real-time with low memory requirement. Con-textNet combines a deep network branch at low resolution that captures global context information efficiently with a shallow branch that focuses on high-resolution segmentation details. We analyse our network in a thorough ablation study and present results on the Cityscapes dataset, achieving 66.1% accuracy at 18.3 frames per second at full (1024 × 2048) resolution (41.9 fps with pipelined computations for streamed data).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation provides detailed pixel-level classification of images, which is particularly suited for autonomous vehicles and driver assistance, as these applications often require accurate road boundaries and obstacle detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Modern systems produce highly accurate segmentation results, but often at the cost of reduced computational efficiency. In this paper, we propose ContextNet to address competitive semantic segmentation for autonomous driving tasks, requiring real-time processing and memory efficiency.</p><p>Deep neural networks (DNNs) are becoming the preferred approach for semantic image segmentation in recent years. High performance segmentation methods adopt state-of-the-art classification architecture using fully convolutional network (FCN) <ref type="bibr" target="#b3">[4]</ref> or encoder-decoder techniques <ref type="bibr" target="#b4">[5]</ref>. In particular, DeepLab <ref type="bibr" target="#b5">[6]</ref> employs an increased number of layers to extract complex and abstract features, leading to increased accuracy. PSPNet <ref type="bibr" target="#b6">[7]</ref> combines multiple levels of information through context aggregation from multiple feature resolutions, and benchmarks as one of the most accurate DNNs. The accuracy of these architectures comes at a high computational cost. Semantic segmentation of a single image requires more than a second, even on modern high-end GPUs (e.g.Nvidia Titan X) and hinders their deployment for driverless cars.  Recent interest in embedded devices, wearable devices and autonomous vehicles has sparked growing focus on semantic segmentation in real-time with low energy and memory requirements <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Common techniques include convolutional factorization <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, network pruning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and weight quantization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Convolution Factorization: Ordinary convolutions perform cross-channel and spatial correlations simultaneously. In contrast, convolution factorization employs multiple sub-operations to reduce computation cost and memory. Examples include Inception <ref type="bibr" target="#b18">[19]</ref>, Xception <ref type="bibr" target="#b15">[16]</ref> and MobileNet <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. In particular, MobileNet <ref type="bibr" target="#b13">[14]</ref> decomposes a standard convolution into a depth-wise convolution (also known as spatial or channel-wise convolution) and a 1 × 1 point-wise convolution. MobileNetV2 <ref type="bibr" target="#b14">[15]</ref> further improves this framework by introducing a bottleneck block and residual connections <ref type="bibr" target="#b19">[20]</ref>. Network Compression: Network compression is orthogonal to convolution factorization. Network hashing or pruning is applied to reduce the size of a pretrained network, resulting in faster test-time execution and a smaller parameter set and memory footprint <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Network Quantization: The runtime of a network can be further reduced using quantization techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. These techniques reduce the size/memory requirements of a network by encoding parameters in low-bit representations. Moreover, runtime is further improved if binary weights and activation functions are employed since efficient XNOR and bit-count operations replace costly floating point operations of standard DNNs.</p><p>Despite these known techniques, notably, only ENet <ref type="bibr" target="#b7">[8]</ref> implements a network that runs in real-time on the full resolution images of the popular Cityscapes dataset <ref type="bibr" target="#b0">[1]</ref>, but with significantly reduced accuracy in comparison to the stateof-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>In this paper we introduce ContextNet, a competitive network for semantic segmentation running in real-time with low memory footprint ( <ref type="figure" target="#fig_1">Figure 1</ref>). Con-textNet builds on the following two observations of previous work:</p><p>1. An increased number of layers helps to learn more complex and abstract features, leading to increased accuracy but also increased running time <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. 2. Aggregation of context information from multiple resolutions is beneficial, since it combines multiple levels of information for improved performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Consequently, we propose a network architecture with branches at two resolutions. In order to obtain real-time performance, the network at low resolution is deep, while the network at high resolution is rather shallow. As such, our network perceives context at low resolution and refines it for high resolution results. The architecture is designed to ensure the low resolution branch is mainly responsible for providing the correct label, whereas the high-resolution branch refines the segmentation boundaries. Our design is related to pyramid representations <ref type="bibr" target="#b20">[21]</ref> which have been employed recently in many DNNs. RefineNet <ref type="bibr" target="#b21">[22]</ref> uses multiple paths over which information from different resolutions is carefully combined to obtain highresolution segmentation results. Ghiasi and Fowlkes <ref type="bibr" target="#b22">[23]</ref> use a class specific multi-level Laplacian pyramid reconstruction technique for the segmentation task. However, neither achieve real-time performance.</p><p>ICNet <ref type="bibr" target="#b8">[9]</ref> employs a subset of ResNet <ref type="bibr" target="#b19">[20]</ref> at three resolution levels (full, half and one fourth of the original input resolution) which are later combined to provide semantic segmentation results. Here we emphasize, while our implementation of ICNet confirms accuracy in <ref type="bibr" target="#b8">[9]</ref>, the reported runtime is achieved only at half resolution images of Cityscapes <ref type="bibr" target="#b0">[1]</ref>. In contrast, we experimentally show that it is possible to capture global context (semantically rich features) with only a single deeper network on smaller input size and local context (detailed spatial features) with a shallow network on full resolution.</p><p>We employ efficient bottleneck residual blocks <ref type="bibr" target="#b14">[15]</ref> and network pruning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> to present a novel DNN architecture for real-time semantic segmentation with full floating point operations. Network quantization is not explored and is left for future work. In the following sections, we provide design choices of our proposed ContextNet and describe our detailed ablation analysis and experiments on Cityscapes [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Context Network (ContextNet)</head><p>The proposed ContextNet is visualized in <ref type="figure" target="#fig_1">Figure 1</ref>. ContextNet produces cost efficient accurate segmentation at low resolution, which is then combined with a sub-network at high resolution to provide detailed segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation</head><p>Combining different levels of context information is advantageous for the semantic segmentation task <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref>. PSPNet <ref type="bibr" target="#b6">[7]</ref> employs an explicit pyramid pooling module to improve performance by capturing global and local context at different feature resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Operator <ref type="table">Table 1</ref>. Bottleneck residual block transferring the input from c to c channels with h height, w width, expansion factor t, convolution type kernel-size/stride s and non-linear function f .</p><formula xml:id="formula_0">Output h × w × c Conv2d 1/1, f h × w × tc h × w × tc DWConv 3/s, f h s × w s × tc h s × w s × tc Conv2d 1/1, − h s × w s × c</formula><p>Another noticeable trend is that state-of-the-art DNNs have grown deeper (e.g. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6]</ref>), since this can capture more complex and abstract features, and increase the receptive field. Unfortunately, higher number of layers ultimately increase runtime and memory requirements.</p><p>ContextNet combines both, deep networks and multi-resolution architectures. In order to achieve fast runtime we restrict our multi-scale input to two branches, where global information at low resolution is refined by a shallow network at high resolution to produce the final segmentation results in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Design</head><p>We now describe the main building blocks, the overall architecture and discuss our design.</p><p>Depth-wise Convolution to Improve Run-time Depth-wise separable convolutions factorize standard convolution (Conv2d) into a depth-wise convolution (DWConv), also known as spatial or channel-wise convolution, followed by a 1 × 1 point-wise convolution layer <ref type="bibr" target="#b13">[14]</ref>. Cross-channel and spatial correlation is therefore computed independently, which drastically reduces the number of parameters, resulting in fewer floating point operations and fast execution time.</p><p>ContextNet utilizes DWConv, as we design its two main building blocks accordingly ( <ref type="figure" target="#fig_1">Figure 1</ref>). The sub-network with down-sampled input uses bottleneck residual blocks with DWConv <ref type="bibr" target="#b14">[15]</ref>  <ref type="table">(Table 1</ref>). In the sub-network at full resolution depth-wise separable convolutions are directly employed. We omit the nonlinearity between depth-wise and point-wise convolutions in our full resolution branch, since it had limited impact on accuracy in our initial experiments.</p><p>Capturing Global and Local Context ContextNet has two branches, one for full resolution (h × w) and one for lower resolution (e.g. h 4 × w 4 ), with input image height h and width w ( <ref type="figure" target="#fig_1">Figure 1)</ref>. Each branch has different responsibilities; the latter captures the global context of the image, and the former provides the detail information for the higher resolution segmentation. In particular, our design choices are motivated as follows:</p><p>1. For fast feature extraction, semantically rich features are extracted only from the lowest possible resolution.  <ref type="table">Table 2</ref>. Branch-4 for compressed input. Repeated block use stride 1 after first block/layer. 2. Features for local context are extracted separately from full resolution input by a very shallow branch, and are then combined with low-resolution results.</p><formula xml:id="formula_1">Branch-1 Branch-4 - Upsample × 4 - DWConv (dilation 4) 3/1, f Conv2d 1/1, − Conv2d 1/1, − add,f</formula><p>Hence, significantly faster computation of image segmentation is possible in Con-textNet.</p><p>Capturing context: The detail structure of the lower resolution branch is shown in <ref type="table">Table 2</ref>. This sub-network consists of two convolution layers and 12 bottleneck residual blocks. Similar to MobileNetV2 <ref type="bibr" target="#b14">[15]</ref>, we employ residual connections for bottleneck residual blocks when input and output are of the same size. While the low resolution branches of ICNet <ref type="bibr" target="#b8">[9]</ref> requires 50 costly layers of ResNet <ref type="bibr" target="#b19">[20]</ref>, in ContextNet a total of only 38 highly efficient layers are used to describe global context. Spatial detail: The sub-network of the full resolution branch is kept as shallow as possible and only consists of four layers. Its objective is to refine the results with local context. In particular, the number of feature maps are 32, 64, 128 and 128 respectively. The first layer uses standard convolution while all other layers use depth-wise separable convolutions with kernel size 3 × 3. The stride is 2 for all but the last layer, where it is 1.</p><p>We use fusion unit shown in <ref type="table" target="#tab_1">Table 3</ref> to merge the features from both branches. Since runtime is of concern, we use feature addition instead of concatenation. Finally, we use a simple 1 × 1 convolution layer for the final soft-max based classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design Choices</head><p>We have conducted several initial experiments before deciding on the final model of ContextNet using train and validation sets of the Cityscapes dataset. We have found that the use of a pyramid pooling module <ref type="bibr" target="#b6">[7]</ref> after the low-resolution branch increases accuracy. Also, learning global context using down-sampled input is more efficient than learning with asymmetric convolution (for examples, k × 1, 1 × k, where k = 5/7/9) on higher resolution inputs. Class weight balancing technique did not help, when we increased batch-size to 16 or more.</p><p>Empirically, we found a weighted auxiliary loss for the low-resolution branch is beneficial. We think, the auxiliary loss ensures that meaningful features for semantic segmentation are extracted by the branch for global context, and are learned independently from the other branch. The weight of the auxiliary loss was set to 0.4. Following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, a cross-entropy loss is employed as auxiliary and final loss of ContextNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In our evaluation, we present a detailed ablation study of ContextNet on the validation set of the Cityscapes dataset <ref type="bibr" target="#b0">[1]</ref> , and report its performance on the Cityscapes benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>All our experiments are performed on a workstation with Nvidia Titan X (Maxwell, 3072 CUDA cores), CUDA 8.0 and cuDNN V6. We use ReLU6 as nonlinearity function due to its robustness when used with low-precision computations <ref type="bibr" target="#b13">[14]</ref>. During training, batch normalization is used at all layers and dropout is used before the soft-max layer only. During inference, parameters of batch normalization are merged with the weights and bias of parent layers. In the depth-wise convolution layers, we found that 2 regularization is not necessary, which is consistent with the findings in <ref type="bibr" target="#b13">[14]</ref>. Since labelled training data is limited, we apply standard data augmentation techniques in all experiments: random scale 0.5 to 2, horizontal flip, varied hue, saturation, brightness and contrast.</p><p>The models of ContextNet are trained with TensorFlow <ref type="bibr" target="#b24">[25]</ref> using RMSprop <ref type="bibr" target="#b25">[26]</ref> with a discounting factor of 0.9, momentum of 0.9 and epsilon parameter equal to 1. Additionally, we apply a poly learning rate <ref type="bibr" target="#b5">[6]</ref> with base rate 0.045 and power 0.98. The maximum number of epochs is set to 1000, as no pre-training is used.</p><p>Results are reported as mean intersection-over-union (mIoU) <ref type="bibr" target="#b0">[1]</ref> and runtime considers single threaded CPU with sequential CPU to GPU memory transfer, kernel execution, and GPU to CPU memory exchange.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cityscapes Dataset</head><p>Cityscapes is a large-scale dataset for semantic segmentation that contains a diverse set of images in street scenes from 50 different cities in Germany <ref type="bibr" target="#b0">[1]</ref>. In total, it consists of 25,000 annotated 1024 × 2048px images of which 5,000 have labels at high pixel accuracy and 20,000 are weakly annotated. In our cn18 cn14 cn12 cn124 cn14-500 cn14-160 Accuracy (mIoU in %) 60.  <ref type="table">Table 4</ref>. ContextNet (cn14) compared to its version with half resolution (cn12) and eighth resolution (cn18) at the low resolution branch, and with multiple levels at quarter, half and full resolution (cn124) on Cityscapes validation set. Implementations with smaller memory footprint are also shown (cn14-500 and cn14-160).</p><p>experiments we only use the 5,000 images with high label quality: a training set of 2,975 images, validation set of 500 images and 1,525 test images which can be evaluated on the Cityscapes evaluation server. No pre-training is employed.</p><p>Ablation Study In our ablation study weights are learned solely from the Cityscapes training set, and we report the performance on the validation set.</p><p>In particular, we present effects on different resolution factors of the low resolution branch, introducing multiple branches, and modifications on the number of parameters. Finally, we analyse the two branches in detail.</p><p>Input resolution: The input image resolution is the most critical factor for the computation time. Our low resolution branch takes images of a quarter size at 256 × 512px for Cityscapes images (denoted cn14). Alternatively, half (cn12) or one eighth (cn18) resolution could be used. <ref type="table">Table 4</ref> shows how the different options affect the results. Overall, larger resolution in the deeper context branch produce better segmentation results. However, these improvements come at the cost of running time. <ref type="table">Table 5</ref> lists the IoU in more detail. As expected, accuracy of small-size classes (i.e.fence, pole, traffic light and traffic sign), classes with fine detail (i.e.person, rider, motorcycle and bicycle) and rare classes (i.e.bus, train and truck) benefit from increased resolution at the global context branch. Other classes are often of larger size, and can therefore be captured at lower resolution. We conclude that cn14 is fairly balanced at 18.3 frames per seconds (fps) and 65.9% mIoU. Multiple Branches: We designed ContextNet under consideration of runtime using only two branches. However, branches at multiple resolutions could be employed. In addition to previous results, <ref type="table">Table 4</ref> and <ref type="table">Table 5</ref> also include a version of ContextNet with two shallow branches at half and full resolution (cn124). In comparison to cn14, cn124 improves accuracy by 1.4% which confirms earlier results on multi-scale feature fusion <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b8">9]</ref>. Runtime however is reduced more than twice from 18.3 fps to 7.6 fps. Furthermore we note, cn12 which has a deep sub-network at half resolution outperforms cn124 in terms of accuracy and speed. These results show that using deep sub-network on higher resolution is more beneficial than on lower resolution. Further, the number of layers have positive effect on accuracy and negative effect on run-time. We therefore conclude that a two branch architecture is most promising in our implementation of ContextNet to run in real-time.  <ref type="table">Table 5</ref>. Detailed IoU of ContextNet (cn14) compared to version with half (cn12) and eighth (cn18) resolution, and its multi-level version (cn124). Small-size classes (green), classes with fine detail (blue), and classes with very few samples (red) benefit from high resolution.</p><p>Number of Parameters: Apart from runtime, memory footprint is an important consideration for implementations on embedded devices. <ref type="table">Table 4</ref> includes two versions of ContextNet with drastically reduced number of parameters, denoted as cn14-500 and cn14-160. Surprisingly, ContextNet with only 159,387 and 490,459 parameters achieved 57.7% and 62.1% mIoU, respectively, on the Cityscapes validation set. These results show that the ContextNet design is flexible enough to adapt the computation time and memory requirements of the given system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ContextNet vs Ensemble Nets:</head><p>Global context with one fourth resolution and detail branches trained independently obtained 63.3% and 25.1% mIoU respectively. As expected, we found that context branch is not performing well on small-size classes and receptive field of the detail branch is not large enough for reasonable performance. Outputs (softmax) of context and detail networks are averaged to create an ensemble of networks, which are trained independently. Ensemble of both branches obtained 60.3% mIoU, which is 6.6% less than cn14 and 3% less than using the context branch alone. This provides further evidence that ContextNet architecture is a better choice for multi-scale features fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Branch Analysis:</head><p>We have zeroed the output of either the detail branch or the context branch to observe their individual contributions. The results are shown in <ref type="figure">Figure 2</ref>. In the first column, we see that the global context branch detects larger objects correctly (for example sky or trees) but fails around boundaries and thin regions. In contrast, the detail branch detects object boundaries correctly but fails at the centre region of objects. One exception is the centre region of trees, which is classified correctly, probably due to the discriminative nature of tree texture. Finally, we can see that ContextNet correctly combines both information to produce improved results in last row. Similarly, in the middle column we can see that segmentation of the pedestrians are refined by ContextNet over the global context branch output. In the last column, even though detail branch detects poles and traffic signs, ContextNet fails to effec- <ref type="figure">Figure 2</ref>. Visual comparison on Cityscape validation set <ref type="bibr" target="#b0">[1]</ref>. First row: input RGB images; second row: ground truths; third row: context branch outputs; fourth row: detail branch outputs; and last row: ContextNet outputs using both context and detail branches. ContextNet obtained 65.9% mIoU, while global context with one fourth resolution and detail branches trained independently obtained 63.3% and 25.1% mIoU respectively tively combine some of these with the global context branch. Overall we observe that the context branch and the detail branch learn complementary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cityscape Benchmark Results</head><p>We evaluate ContextNet on the withheld test-set of Cityscapes <ref type="bibr" target="#b0">[1]</ref>. <ref type="table" target="#tab_5">Table 6</ref> shows the results in comparison to current state-of-the-art real-time segmentation networks (SegNet <ref type="bibr" target="#b4">[5]</ref>, ENet <ref type="bibr" target="#b7">[8]</ref>, ICNet <ref type="bibr" target="#b8">[9]</ref> and ERFNet <ref type="bibr" target="#b9">[10]</ref>), and offline methods (PSPNet <ref type="bibr" target="#b6">[7]</ref> and DeepLab-v2 <ref type="bibr" target="#b5">[6]</ref>).   ERFNet <ref type="bibr" target="#b9">[10]</ref> achieve 69.5% and 68.0% mIoU, respectively, but are considerably slower than ContextNet. <ref type="bibr" target="#b0">1</ref> We emphasize that our runtime evaluation includes the complete CPU and GPU pipeline including memory transfers. If parallel memory transfer and kernel execution are employed, our run time improves to 41.9 fps. Finally, in <ref type="table" target="#tab_4">Table 7</ref> we observe that ContextNet scales well for smaller input resolution sizes, and therefore can be tuned for the task at hand and the available resources. The results of ContextNet are displayed in <ref type="figure">Figure 2</ref> for qualitative analysis. ContextNet is able to segment even small objects at far distances adequately. Network Pruning: Network pruning is usually employed to reduce network parameters <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9]</ref>. Following the "lottery ticket" intuition <ref type="bibr" target="#b27">[28]</ref> we start with wider networks (larger number of feature maps) and use pruning to obtain "skinnier" networks in order to increase the accuracy of our model. First, we train our network with twice the (target) number of feature maps to obtain improved results. We then decrease parameters progressively by pruning to 1.5, 1.25 and 1 times the original size. Following <ref type="bibr" target="#b17">[18]</ref>, we pruned filters with lowest 1 sum. Via pruning we improve the mIoU from 64.2% to 66.1% on the Cityscape test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>In this work we proposed a real-time semantic segmentation model called Con-textNet, which combines a deep network branch at low resolution but with large receptive field with a shallow and therefore efficient full-resolution branch to enhance the segmentation details. ContextNet further extensively leverages depthwise convolutions and bottleneck residual blocks for maximum memory and runtime efficiency.</p><p>Our ablation study shows that ContextNet effectively combines global and local context to achieve competitive result and outperforms other state-of-theart real-time methods. We also empirically show that model pruning (in order to reach given targets of network size and real-time performance) leads to improved segmentation accuracy. Demonstrating that the ContextNet architecture is beneficial for other tasks relevant for autonomous systems such as single-image depth prediction is part of future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:1805.04554v4 [cs.CV] 5 Nov 2018</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>ContextNet combines a deep network at small resolution with a shallow network at full resolution to achieve accurate and real-time semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Features fusion unit of ContextNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>72.4 87.4 45.9 44.1 32.1 38.4 54.3 87.9 54.1 91.0 62.3 36.1 88.3 57.6 59.1 34.2 41.</figDesc><table><row><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bicycle</cell></row><row><cell cols="19">cn18 96.2 2 58.4</cell></row><row><cell cols="19">cn14 96.8 76.6 88.6 46.4 49.7 38.0 45.3 60.5 89.0 59.3 91.4 67.5 41.7 90.0 63.5 71.7 57.1 41.5 64.6</cell></row><row><cell cols="19">cn12 97.2 78.9 89.2 47.2 54.4 39.5 55.3 63.8 89.4 59.8 91.5 70.2 47.5 91.1 70.2 76.2 63.7 51.36 67.8</cell></row><row><cell cols="19">cn124 97.4 79.6 89.5 44.1 49.8 45.5 50.6 64.6 90.2 59.4 93.4 70.9 43.1 91.8 65.2 71.9 64.5 41.95 66.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 compares</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Class mIoU (in %) Category mIoU (in %) Parameters (in millions)</cell></row><row><cell>DeepLab-v2 [6]*</cell><cell>70.4</cell><cell>86.4</cell><cell>44.</cell></row><row><cell>PSPNet [7]*</cell><cell>78.4</cell><cell>90.6</cell><cell>65.7</cell></row><row><cell>SegNet [5]</cell><cell>56.1</cell><cell>79.8</cell><cell>29.46</cell></row><row><cell>ENet [8]</cell><cell>58.3</cell><cell>80.4</cell><cell>00.37</cell></row><row><cell>ICNet [9]*</cell><cell>69.5</cell><cell>-</cell><cell>06.68</cell></row><row><cell>ERFNet [10]</cell><cell>68.0</cell><cell>86.5</cell><cell>02.1</cell></row><row><cell>ContextNet (Ours)</cell><cell>66.1</cell><cell>82.7</cell><cell>00.85</cell></row></table><note>the runtime at full, half and quarter resolution on images of Cityscapes. ContextNet achieves 64.2% before, and 66.1% mIoU after pruning (explained below), and runs at 18.3 fps in a single CPU thread of TensorFlow [25]. ENet [8] has a similar run-time but achieves only 58.3% accuracy. ICNet [9] and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Cityscape benchmark results for the proposed ContextNet and similar networks. DeepLab-v2<ref type="bibr" target="#b5">[6]</ref> and PSPNet<ref type="bibr" target="#b6">[7]</ref> are considered offline approaches.</figDesc><table><row><cell>Runtime of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Runtime on Nvidia Titan X (Maxwell, 3,072 CUDA cores) with Tensor-Flow<ref type="bibr" target="#b24">[25]</ref>, including sequential CPU/GPU memory transfer and kernel execution. (Results with '*' are taken from existing literature -it is not known if memory transfer is considered. Our measure with ' †' denotes kernel execution time alone.)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Although our implementation of ICNet<ref type="bibr" target="#b8">[9]</ref> achieves similar accuracy, we do not achieve the timing mentioned by the authors. This might be caused by differences in software environment and the employed testing protocols.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A highdefinition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<title level="m">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<title level="m">ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08545</idno>
		<title level="m">ICNet for Real-Time Semantic Segmentation on High-Resolution Images</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Binarized Neural Networks. In: NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. In: ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training and Inference with Integers in Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04381[cs]</idno>
		<title level="m">Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02357</idno>
		<title level="m">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<title level="m">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. In: ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pruning Filters for Efficient ConvNets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep Residual Learning for Image Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Laplacian Pyramid as a Compact Image Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Readings in Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="671" to="679" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02264</idno>
		<title level="m">Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MICCAI.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Lecture 6.5-RMSProp, Coursera: neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Training pruned neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
