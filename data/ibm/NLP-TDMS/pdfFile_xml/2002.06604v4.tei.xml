<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Key Points Estimation and Point Instance Segmentation Approach for Lane Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Yeongmin</forename><surname>Ko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Younkwan</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE, Farzeen Munir, Senior Member, IEEE</roleName><forename type="first">Shoaib</forename><surname>Azam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moongu</forename><surname>Jeon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Witold</forename><surname>Pedrycz</surname></persName>
						</author>
						<title level="a" type="main">Key Points Estimation and Point Instance Segmentation Approach for Lane Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Perception techniques for autonomous driving should be adaptive to various environments. In the case of traffic line detection, an essential perception module, many condition should be considered, such as number of traffic lines and computing power of the target system. To address these problems, in this paper, we propose a traffic line detection method called Point Instance Network (PINet); the method is based on the key points estimation and instance segmentation approach. The PINet includes several stacked hourglass networks that are trained simultaneously. Therefore the size of the trained models can be chosen according to the computing power of the target environment. We cast a clustering problem of the predicted key points as an instance segmentation problem; the PINet can be trained regardless of the number of the traffic lines. The PINet achieves competitive accuracy and false positive on the TuSimple and Culane datasets, popular public datasets for lane detection. Our code is available at https://github.com/koyeongmin/PINet new Index Terms-Lane detection, autonomous driving, deep learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. System overview. The proposed framework predicts key points on traffic lines and distinguishes individual instances regardless of the number of traffic lines. In addition, if user wants to run the trained model on a system with weak computing power, like an embedded board, the network can be clipped and transferred without additional training.</p><p>proposed network, distinguishes key points into individual instances. In addition, the proposed network is trained endto-end, and the network size can be modified according to the computing power of the target system without any change of the network architecture or additional training.</p><p>Most traditional methods of traffic line detection extract low-level traffic line features using various hand-craft features like color <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, or edges <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. These low-level features can be combined using a Hough transform <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> or Kalman filter <ref type="bibr" target="#b12">[13]</ref>; the combined features generate traffic line segment information. These methods are simple and can be adapted to various environments without significant modification. Still, the performance of these methods depends on condition of the testing environment such as lighting and occlusion.</p><p>Deep learning methods show outstanding performance for complex scenes. Among deep learning methods, Convolutional Neural Network (CNN) methods are primarily applied for feature extraction in computer vision <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Semantic segmentation methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, the major research area in computer vision, are frequently applied to traffic line detection problems to make inferences about shapes and locations <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Some methods use multiclass approaches to distinguish individual traffic line instances. Therefore, even though these methods can achieve outstanding performance, they can only be applied to scenes that consist of fixed numbers of traffic lines. As a solution to this problem, instance segmentation methods are applied to distinguish individual instance. These semantic segmentation based traffic line <ref type="bibr">Fig. 2</ref>. Proposed framework with three main parts. 512 × 256 size input data is compressed by the resizing network; the compressed input is fed to the predicting network, which includes four hourglass modules. Three output branches are applied at the ends of each hourglass block; they predict confidence, offset, and embedding feature. The loss function can be calculated from the outputs of each hourglass block. By clipping several hourglass modules, required computing resources can be adjusted. detection methods require some post-processing to estimate the exact location values of the predicted traffic lines. Avoiding this post-processing of the semantic segmentation approach, several other methods directly predict traffic line location <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>The existing methods have certain limitations. The semantic segmentation methods require the labeling or pre-processing at the pixel level for training, which is cumbersome. These methods also predict many unnecessary points because semantic segmentation generates classified pixel images with sizes identical to the given input image, even though only a few points are required to recognize traffic lines. In addition, existing methods are not adaptive to various environments according to available computing power. To apply them to light systems like embedded boards, the entire architecture should be modified and trained again.</p><p>To overcome these limitations, our proposed method uses a deep learning model inspired by a stacked hourglass network to predict a few key points on traffic lines. The stacked hourglass network <ref type="bibr" target="#b24">[25]</ref> is usually applied in key points estimation fields such as pose estimation <ref type="bibr" target="#b25">[26]</ref> and object detection <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Using sequence of down-sampling and up-sampling, the stacked hourglass network can extract information about various scales. Because the stacked hourglass network includes several hourglass modules that are trained by the same loss function, we can simultaneously obtain various models that have different parameter sizes by clipping some bays from the whole structure. Using the simple method inspired by point cloud instance segmentation, each key point is distinguished into individual instance <ref type="bibr" target="#b28">[29]</ref>.</p><p>Camera-based traffic line detection has been actively developed, and many state-of-the-art methods <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b23">[24]</ref> are almost completely effective for public data sets. However, some methods have higher rates of false positive. False negatives, traffic lines that the module fails to detect, do not suddenly change the control values, and correct control values can be predicted from other detected traffic lines or previous results.</p><p>However, false positives can lead to severe risks; incorrect identification of traffic lines by the module can cause rapid changes of the control values.</p><p>In summary, <ref type="figure">Fig. 2</ref> shows our proposed framework for traffic line detection. It has three output branches and predicts the exact location and instance features of points on traffic lines. More details are introduced in section III. These are the primary contributions of this study:</p><p>• Using the key points estimation approach, we propose a novel method for traffic line detection. It produces a more compact size prediction output than those of other semantic segmentation-based methods. • The framework consists of several hourglass modules, and so we can obtain various models that have different sizes by simple clipping because each hourglass module is trained simultaneously using the same loss function. • The proposed method can be applied to various scenes that include any orientation of traffic lines, such as vertical or horizontal traffic lines, and arbitrary numbers of traffic lines. • The proposed method has lower false positives and the noteworthy accuracy performance. It guarantees the stability of the autonomous driving car.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Traffic Line Detection</head><p>Lane detection is an important research area in autonomous driving. Lane detection modules recognize drivable areas on roads from input data. Traffic line detection is considered a main method for lane detection. Traffic line detection usually localizes line markings that distinguish drivable areas on roads. Especially regarding RGB images as input data, various handcrafted features have been proposed to detect traffic lines <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. However, these methods show limitations in complex scenarios.</p><p>Recently, deep learning has become a dominant method in computer vision research. Semantic segmentation <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, [18] <ref type="bibr" target="#b35">[36]</ref> is a major topic in perception research; it can classify pixels of the input image into individual class. Generative methods <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> can also perform a similar function. Therefore, semantic segmentation methods and generative methods are suitable for expressing complex shapes of lines. <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b38">[39]</ref>, and <ref type="bibr" target="#b39">[40]</ref> show applications of semantic segmentation and the generative model for traffic line detection. Some methods use multi-class approaches to distinguish each instance; however, multi-class approaches can classify only fixed numbers of instances. Instance segmentation approaches are proposed as solutions to this limitation. Neven et al. <ref type="bibr" target="#b40">[41]</ref> attempted to solve this problem of multi-class approaches with instance segmentation. Their proposed LaneNet has a shared encoder and two decoders. One of these decoders performs binary lane segmentation; the other predicts embedding features for instance segmentation.</p><p>Although semantic segmentation methods can predict lines that have complex shapes, during training and testing they require pixel-level labeled data and post-processing to extract exact points on lines. Some direct methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> directly generate exact points on lines. <ref type="bibr" target="#b22">[23]</ref> predicts exact starting and terminal points, and x-axis values of the fixed y-axis values for each traffic line. <ref type="bibr" target="#b23">[24]</ref> presents the Line Proposal Unit (LPU) inspired by the Region Proposal Network (RPN) of Faster R-CNN <ref type="bibr" target="#b41">[42]</ref>. LPU predicts horizontal offsets for fixed y-axis values along certain pre-defined line proposals.</p><p>These approaches, the semantic segmentation method, the generative method, and the direct method, produce many unnecessary output values. In semantic segmentation and generative method, not all pixels are required to recognize traffic lines; an exact line can be predicted from a few key points. Direct methods also have certain unnecessary predictions like the length, starting points, and terminal points of the given target traffic lines that are unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Key Points Estimation</head><p>Key points estimation techniques predict from input images certain important points called key points. Human pose estimation <ref type="bibr" target="#b25">[26]</ref> is a major research topic in the key points estimation area. Stacked hourglass networks <ref type="bibr" target="#b24">[25]</ref> consists of several hourglass modules that are trained simultaneously. The hourglass module can transfer various scales' information to deeper layers, helping the whole network obtain both global and local features. Because of this property, an hourglass network is frequently utilized to detect centers or corners of objects in the object detection area. Not only network architecture or loss function but also refinement methods adapted to existing networks are developed for key point estimation. <ref type="bibr" target="#b42">[43]</ref> suggests a feature aggregation and coarse-to-fine supervision method that can be applied to other multi-stage methods. <ref type="bibr" target="#b43">[44]</ref> proposes the refinement network that improves the results of other existing models. In this paper, these refinement methods are not applied to indicate performance of our proposed framework; however, they can be applied to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>For lane detection, we train a neural network that consists of several hourglass modules. The network, which we will refer to as the Point Instance Network (PINet), generates points on lanes and distinguishes predicted points into individual instance. To achieve these tasks, our proposed neural network includes three output branches, a confidence branch, offset branch, and embedding branch. The confidence and offset branches predict exact points of traffic lines; loss functions inspired from YOLO <ref type="bibr" target="#b44">[45]</ref> are applied. The embedding branch generates the embedding features of each predicted point; the embedding feature is fed to the clustering process to distinguish each instance. The loss function of the embedding branch is inspired by an instance segmentation method. The Similarity Group Proposal Network (SPGN) <ref type="bibr" target="#b28">[29]</ref>, an instance segmentation frameworks for 3D point cloud, introduces a simple technique and a loss function for instance segmentation. Based on the contents proposed by SPGN, we design a loss function fitting to discriminate each instance of the predicted traffic lines. Section II-A introduces details of the main archi- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>tecture; Section II-B consists of details about the loss function;</head><p>and Section II-C shows the implementation in detail.</p><p>A. Architecture <ref type="figure">Fig. 2</ref> shows the proposed framework of the network. Input RGB image size is 512×256; it is fed to the resizing network. This image is compressed to a smaller size (64 × 32) by the sequence of convolution layers in the resizing network; the output of the resizing network is fed to the predicting network. An arbitrary number of hourglass modules can be included in the predicting network; four hourglass modules are used in this study. All hourglass modules are trained simultaneously by the same loss function. After the training step, user can choose how many hourglass modules to use according to the computing power, without any additional training. The following sections provide details about each network.</p><p>1) Resizing Network: The resizing network reduces the input image's size to save memory and inference time. First, the input RGB image size is 512 × 256. This network consists of three convolution layers. All convolution layers are applied with filter size 3×3, stride 2, and padding size 1. Prelu <ref type="bibr" target="#b45">[46]</ref> and batch normalization <ref type="bibr" target="#b46">[47]</ref> are utilized after each convolution layer. Finally, this network generates resized output with 64 × 32 size. <ref type="table" target="#tab_0">Table I</ref> shows details of the constituent layers. 2) Predicting Network: The resizing network output is fed to the prediction part, which will be described in this section. This part predicts the exact points on the traffic lines and the embedding features for instance segmentation. This network consists of several hourglass modules, each including an encoder, decoder, and three output branches, as shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. Some skip-connections transfer the information of the various scales to deeper layers. Each colored block in <ref type="figure" target="#fig_0">Fig. 3</ref> is a bottle-neck module; these bottle-neck modules are described in <ref type="figure" target="#fig_1">Fig. 4</ref>. There are three kinds of bottle-neck: same, down, and up bottle-necks. The same bottle-neck generates output that has the same size as the input. The down bottle-neck is applied for down-sampling in the encoder; the first layer of the down bottle-neck is replaced by a convolution layer with filter size 3, stride 2, and padding 1. The transposed convolution layer with filter size 3, stride 2, and padding 1 is applied for the up bottle-neck in the up-sampling layers. Each output branch has three convolution layers, and generates a 64 × 32 grid. Confidence values about key point existence, offset, and embedding feature of each cell in the output grid are predicted by the output branches. <ref type="table" target="#tab_0">Table II</ref> shows details of the predicting network. Because a deeper network has better performance <ref type="bibr" target="#b24">[25]</ref>, it can act as a teacher network. Therefore, using knowledge distillation techniques, we can expect better performance for clipped short networks. The channel of each output branch is different (confidence: 1, offset: 2, embedding: 4), and the corresponding loss function is applied according to the goal of each output branch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss Function</head><p>For training, four loss functions are applied to each output branch of the hourglass networks. The following sections provide details of each loss function. As in <ref type="table" target="#tab_0">Table II</ref>, the output branch generates a 64 grid, and each cell in the output grid consist of the predicted values of 7 channels, including the confidence value (1 channel), offset (2 channel) value, and embedding feature (4 channel). Confidence value determines whether or not key points of the traffic line exist; offset value localizes the exact position of the key points predicted by the confidence value, and the embedding feature is utilized to distinguish key points into individual instance. Therefore, three loss functions, except for the distillation loss function, are applied to each cell of the output grid. The distillation loss function to distillate the knowledge of the teacher network is adapted to the distillation layer of each encoder, as shown in <ref type="table" target="#tab_0">Table II</ref>. Details of each predicted value and feature are included by the following sections.</p><p>1) Confidence Loss: The confidence output branch predicts the confidence value of each cell. If a key point is present in the cell, the confidence value is close to 1, if not, it is 0. The output of the confidence branch has 1 channel, and it is fed to the next hourglass module. The confidence loss consists of two parts, existence loss and non-existence loss. The existence loss is applied to cells that include key points; the non-existence loss is utilized to reduce the confidence value of each background cell. The non-existence loss is computed at cells that predict confidence values higher than 0.01. Because cells away from key points converge rapidly, this technique helps the training concentrate on cells closer to the key points. The following shows the loss function of the confidence branch:</p><formula xml:id="formula_0">L exist = 1 N e cc∈Ge (c * c − c c ) 2 ,<label>(1)</label></formula><formula xml:id="formula_1">L non exist = 1 N n cc∈Gn cc&gt;0.01 (c * c − c c ) 2 + 0.00001 · cc∈Gn c 2 c ,<label>(2)</label></formula><p>where N e denotes the number of cells that include key points, N n denotes the number of cells that do not include any key points, G e denotes a set of cells that consist of key points, G n denotes a set of cells that consist of points, c c denotes the predicted value of each cell in the confidence output branch, and c * c denotes the ground-truth value. The ground truth value of the cell that has key point is 1; otherwise it is 0. At inference time, if the confidence value is bigger than the pre-defined threshold, we consider that a key point exists at the cell. The second term of L non exist is a regularization term.</p><p>2) Offset Loss: From the offset branch, PINet predicts the exact location of the key points for each output cell. The output of each cell has a value between 0 and 1; this value indicates the position related to the corresponding cell. In this paper, a cell is matched to 8 pixels of the input image. For example, if the predicted offset value is 0.5, the real position of the key point is 4 pixels away from the edge of the cell. The offset branch has two channels for predicting the x-axis and y-axis offsets. Equation 2 shows the loss function:</p><formula xml:id="formula_2">L of f set = 1 N e cx∈Ge (c * x − c x ) 2 + 1 N e cy∈Ge (c * y − c y ) 2 .<label>(3)</label></formula><p>Because the ground truth does not exist at cells that include no key points, these cells are ignored when the offset loss is calculated.</p><p>3) Embedding Feature Loss: The loss function of this branch is inspired by SGPN, a 3D points cloud instance segmentation method <ref type="bibr" target="#b28">[29]</ref>. The branch is trained to make the embedding feature of each cell closer if the embedding features are the same in this instance. Equations 3 and 4 show the loss function of the feature branch:</p><formula xml:id="formula_3">L f eature = 1 N 2 e Ne i Ne j l(i, j), l(i, j) = ||F i − F j || 2 if I ij = 1 max(0, K − ||F i − F j || 2 ) if I ij = 0 ,<label>(4)</label></formula><p>where F i denotes the predicted embedding feature of a cell i, I ij indicates whether cell i and cell j are same instance or not, and K is a constant such that K &gt; 0. If I ij = 1, the cells are the same instance, and if I ij = 0, these cells are different instances. When the network is trained, the loss function makes features closer when each cell belongs to the same instance; it distributes features when cells belong to different instances. We can distinguish key points into individual instance using the simple distance-based clustering technique. In this study, if embedding features of certain predicted key points are within a certain distance, we consider that they are the same instance. The feature size is set at 4 in this study, but this size is observed to have no major effect on the performance. 4) Distillation Loss: According to Newell et al. <ref type="bibr" target="#b24">[25]</ref>, better performance is observed when more hourglass modules are stacked. Therefore, the deepest hourglass module can be a teacher network, and we expect that clipped short networks that are lighter than the teacher network will show better performance if a knowledge distillation method is applied. Zagoruyko &amp; Komodakis <ref type="bibr" target="#b47">[48]</ref> proposed a simple knowledge distillation method that can be applied to the CNN model. This method allows a student network to imitate a teacher network; Hou et al. <ref type="bibr" target="#b29">[30]</ref> show that the method can improve the performance of the whole framework. Equation 5. shows the loss function for distillation:</p><formula xml:id="formula_4">L distillation = M m D(F (A M ) − F (A m )), F (A M ) = S(G(A m )), S : spatial sof tmax, G(A m ) = C i=1 | A mi | 2 , G : R C×H×W → R H×W ,<label>(5)</label></formula><p>where D denotes the sum of square, A m denotes the distillation layer output at the m-th hourglass modules, as shown in <ref type="table" target="#tab_0">Table II</ref>, M denotes the number of hourglass modules, A m i denotes the i-th channel of A m , and all operators like sum, power, and absolute value (| · |) are elementwise.</p><p>The total loss L total is equal to the weighted sum of the above four loss terms, and the whole network is trained using an end-to-end procedure with the following total loss:</p><formula xml:id="formula_5">L total = γ e L exist + γ n L non−exist + γ o L of f set + γ f L f eature + γ d L distillation .<label>(6)</label></formula><p>In the training step, we set γ o to 0.2, γ f to 0.5, and γ d to 0.1. γ e and γ n are described at Section IV. The proposed loss function is adapted to the output branch of each hourglass module; this helps the whole network to be trained stably.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Detail</head><p>All input images are resized to 512 × 256 size and normalized from values of RGB of 0 ∼ 255 to values of 0 ∼ 1 before the data are fed to the proposed network in both training and testing. The two public datasets used for the evaluation of the proposed method, TuSimple <ref type="bibr" target="#b48">[49]</ref> and CULane <ref type="bibr" target="#b19">[20]</ref>, provide x-axis values of traffic lines according to the fixed yaxis values. Due to the annotation method, some traffic lines close to the horizontal line are annotated sparsely. To solve this problem, we make additional annotations every 10 pixels of the x-axis by linear regression from the original data. Various data augmentation methods like shadowing, adding noise, flipping, translation, rotation, and intensity changing are also applied; these methods are shown in <ref type="figure" target="#fig_2">Fig. 5</ref>.</p><p>Additionally, the two public datasets include a lot of image frames; however, the data are imbalanced. For example, the testing set of the CULane dataset consists of various categories such as normal, night, and crossroad; the numbers of category frames are vary widely. The exact ratios of the CULane category can be found in Section IV-B, the results section. To resolve this issue, we sample hard data that show poor loss values in the training step, and increase the selection ratio of the hard data. The concept is similar to the hard negative mining technique.</p><p>We use one GPU (GTX 2080ti 11GB) for training and testing; source code is written in Pytorch. In the training step, each batch contains six images; hyper-parameters like thresholds and coefficients are determined experimentally.</p><p>The exact values of the hyper-parameters are shown in the following section. PINet predicts the exact position of key points on traffic lines, and the spline curve fitting method is applied to obtain a smoother curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate PINet on two public datasets, TuSimple <ref type="bibr" target="#b48">[49]</ref> and CULane <ref type="bibr" target="#b19">[20]</ref>. The following Section A introduces the overview and evaluation metric used for each dataset in the official evaluation methods. Section B shows the evaluation results of PINet; Section C includes an ablation study on the effect of the knowledge distillation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>Our proposed network, PINet, is trained on both TuSimple and CULane. Table III summarizes information of the two datasets. TuSimple is relatively simpler than CULane because the TuSimple dataset consists of only the highway environment and fewer obstacles. We use the official evaluation source codes to evaluate PINet; the details of the datasets and evaluation metrics are described in the following section.</p><p>1) TuSimple: TuSimple dataset consists of 3,626 training sets and 2,782 testing sets. Accuracy is the main evaluation metric of the TuSimple dataset, defined by the following equation according to the average number of correct points:</p><formula xml:id="formula_6">accuracy = clip C clip S clip ,<label>(7)</label></formula><p>where C clip denotes the number of points correctly predicted by the trained module for the given image clip, and S clip denotes the number of ground-truth points in the clip. The rates false negative (FN) and the false positive (FP) are also provided by the following equation:</p><formula xml:id="formula_7">F P = F pred N pred ,<label>(8)</label></formula><formula xml:id="formula_8">F N = M pred N gt ,<label>(9)</label></formula><p>where F pred denotes the number of wrongly predicted lanes, N pred denotes the number of predicted lanes, M pred denotes the number of missed lanes, and N gt denotes the number of ground-truth lanes.</p><p>2) CULane: The CULane dataset includes 88,880 training images and 34,680 testing images. Unlike the TuSimple dataset, various road types such as urban and night are shown in the CULane dataset. We follow the official evaluation metric <ref type="bibr" target="#b19">[20]</ref> for evaluation of the CULane dataset. According to <ref type="bibr" target="#b19">[20]</ref>, each traffic line is assumed to have 30 pixel width and we calculate the intersection-over-union(IoU) between the prediction of the evaluated model and the ground truth. In CULane dataset, F1-measure is the major evaluation metric; it is defined as the following equation.</p><formula xml:id="formula_9">F 1 measure = 2 * P recision * Recall P recision + Recall ,<label>(10)</label></formula><p>where P recision = T P T P +F P and Recall = T P T P +F N . TP is a the true positive, which means a prediction that has larger IoU than the threshold, 0.5. FP is a false positive and FN is a false negative.</p><p>B. Result 1) TuSimple: Evaluation of the TuSimple dataset requires exact x-axis values for certain fixed y-axis values. The detailed evaluation results can be seen in <ref type="table" target="#tab_4">Table V</ref>; <ref type="figure" target="#fig_3">Fig. 6</ref> shows certain results for the TuSimple dataset. The value nH in Tables IV -VI means that the network consists of n hourglass modules. Though pre-trained weights and extra datasets are not used, PINet also shows high performance in term of accuracy and false positive rate. The false negative rate also shows a reasonable value. <ref type="table" target="#tab_0">Table VI</ref> shows the number of parameters and the fps on the GTX 2080ti GPU according to the number of hourglass modules. Most components of PINet are built of bottle-neck layers. This architecture can save a lot of memory. PINet can run at 25 fps when all hourglass networks are used, and if only one hourglass network is applied, the network works about 40 fps. When the short network is evaluated, the network is just clipped from the whole trained network, without any additional training. The deepest network has higher performance, but the performances of the clipped short networks show subtle differences from that of deepest network. The distance threshold is 0.08 to distinguish each instance; confidence thresholds are 0.35 (4H), 0.32 (3H), 0.30 (2H), and 0.52 (1H); γ e and γ n are 1.0 and 1.0.</p><p>2) CULane: <ref type="table" target="#tab_0">Table IV</ref> and <ref type="figure" target="#fig_4">Fig. 7</ref> show detailed results of PINet on the CULane dataset. We observe three features in the result. The first is that PINet shows a particularly low false positive rate on the CULane dataset. This means that wrong prediction of lanes by our PINet is rarer than in other methods; this guarantees the safety performance. Second, the clipped networks 2H and 3H show a performance similar to that of the whole network; only 1H has poor performance. It </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc FP FN SCNN <ref type="bibr" target="#b19">[20]</ref> 96.53% 0.0617 0.0180 LaneNet(+H-net) <ref type="bibr" target="#b40">[41]</ref> 96.38% 0.0780 0.0244 PointLaneNet(MoblieNet) <ref type="bibr" target="#b22">[23]</ref> 96.34% 0.0467 0.0518 ENet-SAD <ref type="bibr" target="#b29">[30]</ref> 96.64% 0.0602 0.0205 ERFNet-E2E <ref type="bibr" target="#b49">[50]</ref> 96.02% 0.0321 0.0428 Line-CNN <ref type="bibr" target="#b23">[24]</ref> 96 ; γ e and γ n are initially set by 1.0 and 1.0. γ e is changed from 1.0 to 2.5 at the last 40 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>We investigate the effects of the knowledge distillation method, whose purpose of this knowledge distillation method  is to reduce the gap between the clipped short network and the deepest network that acts as a teacher network. <ref type="table" target="#tab_0">Table  VII</ref> shows the results of the ablation study. The average performance gap is calculated using the following equation:</p><formula xml:id="formula_10">AG n = 1 N N i P 4H i − P nH i ,<label>(11)</label></formula><p>where AG n denotes the average performance gap between 4H and nH, N denotes the total number of training epochs for this ablation study, and P nH i denotes the performance of nH at the i-th epoch. The performance is evaluated on the tuSimple test set; we collect data for the first 30 epochs. When the distillation method is applied, the average performance gap between the whole network and the clipped short networks is lower when the distillation method is not applied. This means that the distillation method helps the clipped short network to mimic the teacher network well.</p><p>V. CONCLUSION In this study, we have proposed a novel lane detection method, PINet, combining with the point estimation and the  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Details of hourglass block consisting three types of bottle-neck layers: same bottle-necks, down bottle-necks, and up bottle-necks. Output branches are applied at ends of hourglass layers; confidence output is forwarded to the next block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Details of bottle-neck. The three kinds of bottle-neck have different first layers according to their purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Data augmentation methods. (a) is the original image, and (b), (c), (d), (e), (f), and (g) show examples of the applied data augmentation methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Results for TuSimple dataset. First row is ground truth; the second row is predicted results of PINet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Results of CULane dataset. First row is ground truth; the second row is predicted results of PINet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Moongu</head><label></label><figDesc>Jeon received the B.S. degree in architectural engineering from Korea University, Seoul, South Korea, in 1988, and the M.S. and Ph.D. degrees in computer science and scientific computation from the University of Minnesota, Minneapolis, MN, USA, in 1999 and 2001, respectively. As the masters degree researcher, he was involved in optimal control problems with the University of California at Santa Barbara, Santa Barbara, CA, USA, from 2001 to 2003, and then moved to the National Research Council of Canada, where he was involved in the sparse representation of high-dimensional data and the image processing, until July 2005. In 2005, he joined the Gwangju Institute of Science and Technology, Gwangju, South Korea, where he is currently a Full Professor with the School of Electrical Engineering and Computer Science. His current research interests include machine learning, computer vision, and artificial intelligence. Witold Pedrycz received the M.Sc., Ph.D., and D.Sc. degrees from the Silesian University of Technology, Gliwice, Poland. He is a Professor and the Canada Research Chair of Computational Intelligence with the Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada. He is also with the Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland. Dr. Pedrycz is a Foreign Member of the Polish Academy of Sciences and a fellow of the Royal Society of Canada. He has authored 17 research monographs and edited volumes covering various aspects of computational intelligence, data mining, and software engineering. His current research interests include computational intelligence, fuzzy modeling and granular computing, knowledge discovery and data science, fuzzy control, pattern recognition, knowledge-based neural networks, relational computing, and software engineering. Dr. Pedrycz was a recipient of the Prestigious Norbert Wiener Award from the IEEE Systems, Man, and Cybernetics Society in 2007; the IEEE Canada Computer Engineering Medal; the Cajastur Prize for Soft Computing from the European Centre for Soft Computing; the Killam Prize; and the Fuzzy Pioneer Award from the IEEE Computational Intelligence Society. He is vigorously involved in editorial activities. He is an Editor-in-Chief of Information Sciences, Editor-in-Chief of WIREs Data Mining and Knowledge Discovery (Wiley), and International Journal of Granular Computing (Springer). He currently serves on the Advisory Board of IEEE Transactions on Fuzzy Systems and is a member of a number of editorial boards of other international journals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DETAILS</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">OF RESIZING NETWORK</cell></row><row><cell>Layer</cell><cell cols="2">Size/Stride Output size</cell></row><row><cell>Input data</cell><cell></cell><cell>3*512*256</cell></row><row><cell>Conv+Prelu+bn</cell><cell>3/2</cell><cell>32*256*128</cell></row><row><cell>Conv+Prelu+bn</cell><cell>3/2</cell><cell>64*128*64</cell></row><row><cell>Conv+Prelu+bn</cell><cell>3/2</cell><cell>128*64*32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DETAILS</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">OF PREDICTING NETWORK</cell><cell></cell></row><row><cell></cell><cell>Layer</cell><cell cols="2">Size/Stride Output size</cell></row><row><cell></cell><cell>Input data</cell><cell></cell><cell>128*64*32</cell></row><row><cell>Encoder</cell><cell>Bottle-neck(down)</cell><cell></cell><cell>128*32*16</cell></row><row><cell></cell><cell>Bottle-neck(down)</cell><cell></cell><cell>128*16*8</cell></row><row><cell></cell><cell>Bottle-neck(down)</cell><cell></cell><cell>128*8*4</cell></row><row><cell></cell><cell>Bottle-neck(down)</cell><cell></cell><cell>128*4*2</cell></row><row><cell></cell><cell>Bottle-neck</cell><cell></cell><cell>128*4*2</cell></row><row><cell>(Distillation layer)</cell><cell>Bottle-neck</cell><cell></cell><cell>128*4*2</cell></row><row><cell></cell><cell>Bottle-neck</cell><cell></cell><cell>128*4*2</cell></row><row><cell></cell><cell>Bottle-neck</cell><cell></cell><cell>128*4*2</cell></row><row><cell>Decoder</cell><cell>Bottle-neck(up)</cell><cell></cell><cell>128*8*4</cell></row><row><cell></cell><cell>Bottle-neck(up)</cell><cell></cell><cell>128*16*8</cell></row><row><cell></cell><cell>Bottle-neck(up)</cell><cell></cell><cell>128*32*16</cell></row><row><cell></cell><cell>Bottle-neck(up)</cell><cell></cell><cell>128*64*32</cell></row><row><cell>Output branch</cell><cell>Conv+Prelu+bn</cell><cell>3/1</cell><cell>64*64*32</cell></row><row><cell></cell><cell>Conv+Prelu+bn</cell><cell>3/1</cell><cell>32*64*32</cell></row><row><cell></cell><cell>Conv</cell><cell>1/1</cell><cell>C*64*32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc>5900 urban, rual, highway, various light condition and weather</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>DATASET SUMMARY</cell><cell></cell></row><row><cell>Dataset</cell><cell>Train</cell><cell>Test</cell><cell>Resolution</cell><cell>Type</cell></row><row><cell>TuSimple</cell><cell>3,626</cell><cell>2,782</cell><cell>1280 × 720</cell><cell>highway</cell></row><row><cell>CULane</cell><cell>88,880</cell><cell>34,680</cell><cell>1640 ×</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV EVALUATION</head><label>IV</label><figDesc>RESULTS FOR CULANE DATASET. (FIRST AND SECOND BEST RESULTS ARE HIGHLIGHTED IN RED AND BLUE.)</figDesc><table><row><cell>Category</cell><cell cols="3">Proportion PINet(1H) PINet(2H)</cell><cell>PINet(3H)</cell><cell>PINet(4H)</cell><cell cols="3">SCNN [20] R-101-SAD [30] ERFNet-E2E [50]</cell></row><row><cell>Normal</cell><cell>27.7%</cell><cell>85.8</cell><cell>89.6</cell><cell>90.2</cell><cell>90.3</cell><cell>90.6</cell><cell>90.7</cell><cell>91.0</cell></row><row><cell>Crowed</cell><cell>23.4%</cell><cell>67.1</cell><cell>71.9</cell><cell>72.4</cell><cell>72.3</cell><cell>69.7</cell><cell>70.0</cell><cell>73.1</cell></row><row><cell>Night</cell><cell>20.3%</cell><cell>61.7</cell><cell>67.0</cell><cell>67.7</cell><cell>67.7</cell><cell>66.1</cell><cell>66.3</cell><cell>67.9</cell></row><row><cell>No Line</cell><cell>11.7%</cell><cell>44.8</cell><cell>49.3</cell><cell>49.6</cell><cell>49.8</cell><cell>43.4</cell><cell>43.5</cell><cell>46.6</cell></row><row><cell>Shadow</cell><cell>2.7%</cell><cell>63.1</cell><cell>67.0</cell><cell>68.4</cell><cell>68.4</cell><cell>66.9</cell><cell>67.0</cell><cell>74.1</cell></row><row><cell>Arrow</cell><cell>2.6%</cell><cell>79.6</cell><cell>84.2</cell><cell>83.6</cell><cell>83.7</cell><cell>84.1</cell><cell>84.4</cell><cell>85.8</cell></row><row><cell>Dazzle Light</cell><cell>1.4%</cell><cell>59.4</cell><cell>65.2</cell><cell>66.4</cell><cell>66.3</cell><cell>58.5</cell><cell>59.9</cell><cell>64.5</cell></row><row><cell>Curve</cell><cell>1.2%</cell><cell>63.3</cell><cell>66.2</cell><cell>65.4</cell><cell>65.6</cell><cell>64.4</cell><cell>65.7</cell><cell>71.9</cell></row><row><cell>Crossroad</cell><cell>9.0%</cell><cell>1534</cell><cell>1505</cell><cell>1486</cell><cell>1427</cell><cell>1990</cell><cell>2052</cell><cell>2022</cell></row><row><cell>Total</cell><cell>-</cell><cell>69.4</cell><cell>73.8</cell><cell>74.3</cell><cell>74.4</cell><cell>71.6</cell><cell>71.8</cell><cell>74.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell>EVALUATION RESULTS FOR TUSIMPLE DATASET.</cell></row><row><cell>(FIRST AND SECOND BEST RESULTS ARE HIGHLIGHTED IN RED AND</cell></row><row><cell>BLUE.)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII AVERAGE</head><label>VII</label><figDesc>PERFORMANCE GAP BETWEEN WHOLE NETWORK AND CLIPPED SHORT NETWORK ON TUSIMPLE DATASET (LOWER IS BETTER). In addition, PINet can be clipped according to the computing power of the target system; the clipped network can be applied directly without any additional training. PINet achieves high performance and a lower rate of false positives; the low false positive rate guarantees the safety performance of autonomous driving cars because wrongly predicted lanes rarely occur. Particularly, PINet show better performance than other methods in difficult light conditions such as night, shadow, and dazzling light; however, PINet has limitations when local occlusions or unclear traffic lines exist. We have shown by ablation study that the knowledge distillation method improves the performance of the clipped short network. As a result, we have observed that the clipped short network's performance is close to that of the whole network's performance.Ko  received the B.S. degree in School of Electrical Engineering from Gwangju Institute of Science and Technology (GIST), Gwangju, South Korea, in 2017. He is currently pursuing the Ph.D. degree with the School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology. His current research interests include computer vision, self-driving, and deep learning. Farzeen Munir received the B.S degree in Electrical Engineering from Pakistan Institute of Engineering and Applied Sciences, Pakistan in 2013, and MS degree in System Engineering from Pakistan Institute of Engineering and Applied Sciences, Pakistan in 2015. Now she is pursing her PhD degree at Gwangju Institute of Science and Technology, Korea in Electrical Engineering and Computer Science. Her current research interest include, machine Learning, deep neural network, autonomous driving and computer vision.</figDesc><table><row><cell>4H-3H 0.0096 0.0130 73.85 point instance segmentation method. Method can work in 4H-2H 4H-1H Distillation (a) 0.0234 0.0739 No distillation (b) 0.0327 0.1092 a/b (%) 71.56 67.67 real-time. Yeongmin Younkwan Lee received the B.S. degree in com-puter science from Korea Aerospace University, Gyeonggi, South Korea, in 2016. He is currently pur-suing the Ph.D. degree with the School of Electrical Engineering and Computer Science, Gwangju Insti-tute of Science and Technology (GIST), Gwangju, South Korea. His current research interests include computer vision, machine learning, and deep learn-ing. Shoaib Azam received the B.S. degree in Engi-</cell></row><row><cell>neering Sciences from Ghulam Ishaq Khan Institute</cell></row><row><cell>of Science and Technology, Pakistan in 2010, and</cell></row><row><cell>MS degree in Robotics and Intelligent Machine En-</cell></row><row><cell>gineering from National University of Science and</cell></row><row><cell>Technology, Pakistan in 2015. He is currently pur-</cell></row><row><cell>suing the Ph.D. degree with the Department of Elec-</cell></row><row><cell>trical Engineering and Computer Science, Gwangju</cell></row><row><cell>Institute of Science and Technology, Gwangju, South</cell></row><row><cell>Korea. His current research interests include artificial</cell></row><row><cell>intelligence and machine learning, computer vision,</cell></row><row><cell>robotics and autonomous driving.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Snider: Single noisy image denoising and rectification for improving license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Where am i: Localization and 3d maps for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Munir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Sheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VEHITS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="452" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lane detection and tracking using b-snake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="269" to="280" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust lane detection and tracking in challenging scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="26" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Road detection algorithm for autonomous navigation systems based on dark channel prior and vanishing point in complex road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised drivable area and road anomaly segmentation using rgb-d data for robotic wheelchairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4386" to="4393" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Color-based road detection in urban traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="318" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lane detection using color-based segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Proceedings. Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="706" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lane detection using catmull-rom spline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Teoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Vehicles</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="51" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust lane detection and tracking for real-time applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4043" to="4048" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Use of the hough transformation to detect lines and curves in pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="15" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiple lane detection via combining complementary structural constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust lane detection and tracking with ransac and kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 16th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3261" to="3264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved lane detection with multilevel features in branch convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="173148" to="173156" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end lane detection through differentiable leastsquares fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust lane detection from continuous driving scenes using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on vehicular technology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="54" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointlanenet: Efficient end-to-end cnns for accurate real-time lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2563" to="2568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Line-cnn: End-to-end traffic line detection with line proposal unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="248" to="258" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1281" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6568" to="6577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A random finite set approach to multiple lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szczot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 15th International IEEE Conference on Intelligent Transportation Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="270" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Three-feature based automatic lane detection algorithm (tfalda) for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">U</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="219" to="225" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A novel lane detection system with efficient ground truth generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="365" to="374" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A novel strategy for road lane detection and tracking based on a vehicles forward monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H Z</forename><surname>Neme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Margraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Omoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Farinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Tusset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Okida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D S</forename><surname>Amaral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1497" to="1507" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining priors, appearance, and context for road detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lpez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lumbreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1168" to="1178" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adfnet: Accumulated decoder features for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joonmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-class lane semantic segmentation using efficient convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Elgan: Embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE intelligent vehicles symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Rethinking on multi-stage networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00148</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7773" to="7781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">448456</biblScope>
		</imprint>
		<respStmt>
			<orgName>JMLR.org</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The tusimple lane challenge</title>
		<ptr target="http://benchmark.tusimple.ai/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">End-to-end lane marker detection via row-wise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D. Hoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1006" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
