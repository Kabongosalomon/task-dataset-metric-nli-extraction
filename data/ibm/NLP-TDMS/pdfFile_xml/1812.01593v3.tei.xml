<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Semantic Segmentation via Video Propagation and Label Relaxation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California at Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
							<email>ksapra@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Nvidia Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fitsum</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
							<email>freda@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Nvidia Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
							<email>kshih@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Nvidia Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California at Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
							<email>atao@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Nvidia Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
							<email>bcatanzaro@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Nvidia Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Semantic Segmentation via Video Propagation and Label Relaxation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation requires large amounts of pixelwise annotations to learn accurate models. In this paper, we present a video prediction-based methodology to scale up training sets by synthesizing new training samples in order to improve the accuracy of semantic segmentation networks. We exploit video prediction models' ability to predict future frames in order to also predict future labels. A joint propagation strategy is also proposed to alleviate mis-alignments in synthesized samples. We demonstrate that training segmentation models on datasets augmented by the synthesized samples leads to significant improvements in accuracy. Furthermore, we introduce a novel boundary label relaxation technique that makes training robust to annotation noise and propagation artifacts along object boundaries. Our proposed methods achieve state-of-the-art mIoUs of 83.5% on Cityscapes and 82.9% on CamVid. Our single model, without model ensembles, achieves 72.8% mIoU on the KITTI semantic segmentation test set, which surpasses the winning entry of the ROB challenge 2018. Our code and videos can be found at https://nv-adlr.github. io/publication/2018-Segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is the task of dense per pixel predictions of semantic labels. Large improvements in model accuracy have been made in recent literature <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10]</ref>, in part due to the introduction of Convolutional Neural Networks (CNNs) for feature learning, the task's utility for selfdriving cars, and the availability of larger and richer training datasets (e.g., Cityscapes <ref type="bibr" target="#b14">[15]</ref> and Mapillary Vista <ref type="bibr" target="#b31">[32]</ref>). While these models rely on large amounts of training data to achieve their full potential, the dense nature of semantic segmentation entails a prohibitively expensive dataset annotation process. For instance, annotating all pixels in a 1024 Ã— 2048 Cityscapes image takes on average 1.5 hours * indicates equal contribution.  <ref type="figure">Figure 1</ref>: Framework overview. We propose joint image-label propagation to scale up training sets for robust semantic segmentation. The green dashed box includes manually labelled samples, and the red dashed box includes our propagated samples. T is the transformation function learned by the video prediction models to perform propagation. We also propose boundary label relaxation to mitigate label noise during training. Our framework can be used with most semantic segmentation and video prediction models. <ref type="bibr" target="#b14">[15]</ref>. Annotation quality plays an important role for training better models. While coarsely annotating large contiguous regions can be performed quickly using annotation toolkits, finely labeling pixels along object boundaries is extremely challenging and often involves inherently ambiguous pixels.</p><p>Many alternatives have been proposed to augment training processes with additional data. For example, Cords et al. <ref type="bibr" target="#b14">[15]</ref> provided 20K coarsely annotated images to help train deep CNNs, an annotation cost effective alternative used by all top 10 performers on the Cityscapes benchmark. Nevertheless, coarse labeling still takes, on average, 7 minutes per image. An even cheaper way to obtain more labeled samples is to generate synthetic data <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b44">45]</ref>. However, model accuracy on the synthetic data often does not generalize to real data due to the domain gap between synthetic and real images. Luc et al. <ref type="bibr" target="#b27">[28]</ref> use a state-of-theart image segmentation method <ref type="bibr" target="#b41">[42]</ref> as a teacher to generate extra annotations for unlabelled images. However, their performance is bounded by the teacher method. Another approach exploits the fact that many semantic segmentation datasets are based on continuous video frame sequences sparsely labeled at regular intervals. As such, several works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref> propose to use temporal consistency constraints, such as optical flow, to propagate ground truth labels from labeled to unlabeled frames. However, these methods all have different drawbacks which we will describe in Sec. <ref type="bibr">2.</ref> In this work, we propose to utilize video prediction models to efficiently create more training samples (image-label pairs) as shown in <ref type="figure">Fig. 1</ref>. Given a sequence of video frames having labels for only a subset of the frames in the sequence, we exploit the prediction models' ability to predict future frames in order to also predict future labels (new labels for unlabelled frames). Specifically, we propose leveraging such models in two ways. 1) Label Propagation (LP): We create new training samples by pairing a propagated label with the original future frame. 2) Joint image-label Propagation (JP): We create a new training sample by pairing a propagated label with the corresponding propagated image. In approach <ref type="bibr" target="#b1">(2)</ref>, it is of note that since both past labels and frames are jointly propagated using the same prediction model, the resulting image-label pair will have a higher degree of alignment. As we will show in later sections, we separately apply each approach for multiple future steps to scale up the training dataset.</p><p>While great progress has been made in video prediction, it is still prone to producing unnatural distortions along object boundaries. For synthesized training examples, this means that the propagated labels along object boundaries should be trusted less than those within an object's interior. Here, we present a novel boundary label relaxation technique that can make training more robust to such errors. We demonstrate that by maximizing the likelihood of the union of neighboring class labels along the boundary, the trained models not only achieve better accuracy, but are also able to benefit from longer-range propagation.</p><p>As we will show in our experiments, training segmentation models on datasets augmented by our synthesized samples leads to improvements on several popular datasets. Furthermore, by performing training with our proposed boundary label relaxation technique, we achieve even higher accuracy and training robustness, producing stateof-the-art results on the Cityscapes, CamVid, and KITTI semantic segmentation benchmarks. Our contributions are summarized below:</p><p>â€¢ We propose to utilize video prediction models to propagate labels to immediate neighbor frames.</p><p>â€¢ We introduce joint image-label propagation to alleviate the mis-alignment problem.</p><p>â€¢ We propose to relax one-hot label training by maximizing the likelihood of the union of class probabilities along boundary. This results in more accurate models and allows us to perform longer-range propagation.</p><p>â€¢ We compare our video prediction-based approach to standard optical flow-based ones in terms of segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Here, we discuss additional work related to ours, focusing mainly on the differences. Label propagation There are two main approaches to propagating labels: patch matching <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> and optical flow <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref>. Patch matching-based methods, however, tend to be sensitive to patch size and threshold values, and, in some cases, they assume prior-knowledge of class statistics. Optical flow-based methods rely on very accurate optical flow estimation, which is difficult to achieve. Erroneous flow estimation can result in propagated labels that are misaligned with their corresponding frames.</p><p>Our work falls in this line of research but has two major differences. First, we use motion vectors learned from video prediction models to perform propagation. The learned motion vectors can handle occlusion while also being class agnostic. Unlike optical flow estimation, video prediction models are typically trained through self-supervision. The second major difference is that we conduct joint image-label propagation to greatly reduce the mis-alignments. Boundary handling Some prior works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref> explicitly incorporate edge cues as constraints to handle boundary pixels. Although the idea is straightforward, this approach has at least two drawbacks. One is the potential error propagation from edge estimation and the other is fitting extremely hard boundary cases may lead to over-fitting at the test stage. There is also literature focusing on structure modeling to obtain better boundary localization, such as affinity field <ref type="bibr" target="#b20">[21]</ref>, random walk <ref type="bibr" target="#b4">[5]</ref>, relaxation labelling <ref type="bibr" target="#b36">[37]</ref>, boundary neural fields <ref type="bibr" target="#b3">[4]</ref>, etc. However, none of these methods deals directly with boundary pixels but they instead attempt to model the interactions between segments along object boundaries. The work most similar to ours is <ref type="bibr" target="#b21">[22]</ref> which proposes to incorporate uncertainty reasoning inside Bayesian frameworks. The authors enforce a Gaussian distribution over the logits to attenuate loss when uncertainty is large. Instead, we propose a modification to class label space that allows us to predict multiple classes at a boundary pixel. Experimental results demonstrate higher model accuracy and increased training robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We present an approach for training data synthesis from sparsely annotated video frame sequences. Given an input video I âˆˆ R nÃ—W Ã—H and semantic labels L âˆˆ R mÃ—W Ã—H , where m â‰¤ n, we synthesize k Ã— m new training samples (image-label pairs) using video prediction models, where k is the length of propagation applied to each input image- label pair (I i , L i ). We will first describe how we use video prediction models for label synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video Prediction</head><p>Video prediction is the task of generating future frames from a sequence of past frames. It can be modeled as the process of direct pixel synthesis or learning to transform past pixels. In this work, we use a simple and yet effective vector-based approach <ref type="bibr" target="#b33">[34]</ref> that predicts a motion vector (u, v) to translate each pixel (x, y) to its future coordinate. The predicted future frame I t+1 is given by,</p><formula xml:id="formula_0">I t+1 = T G I 1:t , F 2:t , I t ,<label>(1)</label></formula><p>where G is a 3D CNN that predicts motion vectors (u, v) conditioned on input frames I 1:t and estimated optical flows F i between successive input frames I i and I iâˆ’1 . T is an operation that bilinearly samples from the most recent input I t using the predicted motion vectors (u, v). Note that the motion vectors predicted by G are not equivalent to optical flow vectors F. Optical flow vectors are undefined for pixels that are visible in the current frame but not visible in the previous frame. Thus, performing past frame sampling using optical flow vectors will duplicate foreground objects, create undefined holes or stretch image borders. The learned motion vectors, however, account for disocclusion and attempt to accurately predict future frames. We will demonstrate the advantage of learned motion vectors over optical flow in Sec. 4.</p><p>In this work, we propose to reuse the predicted motion vectors to also synthesize future labels L t+1 . Specifically:</p><formula xml:id="formula_1">L t+1 = T G I 1:t , F 2:t , L t ,<label>(2)</label></formula><p>where a sampling operation T is applied on a past label L t . G in equation 2 is the same as in equation 1 and is pretrained on the underlying video frame sequences for the task of accurately predicting future frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Joint Image-Label Propagation</head><p>Standard label propagation techniques create new training samples by pairing a propagated label with the original future frame as I i+k , L i+k , with k being the propagation length. For regions where the frame-to-frame correspondence estimation is not accurate, we will encounter mis-alignment between I i+k and L i+k . For example, as we see in <ref type="figure" target="#fig_1">Fig. 2</ref>, most regions in the propagated label (row 2) correlate well with the corresponding original video frames (row 1). However, certain regions, like the pole (red) and the leg of the pedestrian (green), do not align with the original frames due to erroneous estimated motion vectors.</p><p>To alleviate this mis-alignment issue, we propose a joint image-label propagation strategy; i.e., we jointly propagate both the video frame and the label. Specifically, we apply equation 2 to each input training sample (I i , L i ) for k future steps to create k Ã— m new training samples by pairing a predicted frame with a predicted label as ( I i+k , L i+k ). As we can see in <ref type="figure" target="#fig_1">Fig. 2</ref>, the propagated frames (row 3) correspond well to the propagated labels (row 2). The pole and the leg experience the same distortion. Since semantic segmentation is a dense per-pixel estimation problem, such good alignment is crucial for learning an accurate model.</p><p>Our joint propagation approach can be thought of as a special type of data augmentation because both the frame and label are synthesized by transforming a past frame and the corresponding label using the same learned transformation parameters (u, v). It is an approach similar to standard data augmentation techniques, such as random rotation, random scale or random flip. However, joint propagation uses a more fundamental transformation which was trained for the task of accurate future frame prediction.</p><p>In order to create more training samples, we also perform reversed frame prediction. We equivalently apply joint propagation to create additional k Ã— m new training samples as ( I iâˆ’k , L iâˆ’k ). In total, we can scale the training dataset by a factor of 2k + 1. In our study, we set k to be Â±1, Â±2, Â±3, Â±4 or Â±5, where + indicates a forward propagation, and âˆ’ a backward propagation.</p><p>We would like to point out that our proposed joint propagation has broader applications. It could also find application in datasets where both the raw frames and the corresponding labels are scarce. This is different from label propagation alone for synthesizing new training samples for typical video datasets, for instance Cityscapes <ref type="bibr" target="#b14">[15]</ref>, where raw video frames are abundant but only a subset of the frames have human annotated labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Video Reconstruction</head><p>Since, in our problem, we know the actual future frames, we can instead perform not just video prediction but video reconstruction to synthesize new training examples. More specifically, we can condition the prediction models on both the past and future frames to more accurately reconstruct "future" frames. The motivation behind this reformulation is that because future frames are observed by video reconstruction models, they are, in general, expected to produce better transformation parameters than video prediction models which only observe only past frames. Mathematically, a reconstructed future frameÃŽ t+1 is given by,ÃŽ</p><formula xml:id="formula_2">t+1 = T G I 1:t+1 , F 2:t+1 , I t .<label>(3)</label></formula><p>In a similar way to equation 2, we also apply G from equation 3 (which is learned for the task of accurate future frame reconstruction) to generate a future labelL t+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Boundary Label Relaxation</head><p>Most of the hardest pixels to classify lie on the boundary between object classes <ref type="bibr" target="#b24">[25]</ref>. Specifically, it is difficult to classify the center pixel of a receptive field when potentially half or more of the input context could be from a different class. This problem is further compounded by the fact that the annotations are nowhere near pixel-perfect along the edges.</p><p>We propose a modification to class label space, applied exclusively during training, that allows us to predict multiple classes at a boundary pixel. We define a boundary pixel as any pixel that has a differently labeled neighbor. Suppose we are classifying a pixel along the boundary of classes A and B for simplicity. Instead of maximizing the likelihood of the target label as provided by annotation, we propose to maximize the likelihood of P (A âˆª B). Because classes A and B are mutually exclusive, we aim to maximize the union of A and B:</p><formula xml:id="formula_3">P (A âˆª B) = P (A) + P (B),<label>(4)</label></formula><p>where P () is the softmax probability of each class. Specifically, let N be the set of classes within a 3Ã—3 window of a pixel. We define our loss as:</p><formula xml:id="formula_4">L boundary = âˆ’log CâˆˆN P (C).<label>(5)</label></formula><p>Note that for |C| = 1, this loss reduces to the standard onehot label cross-entropy loss.</p><p>One can see that the loss over the modified label space is minimized when CâˆˆN P (C) = 1 without any constraints on the relative values of each class probability. We demonstrate that this relaxation not only makes our training robust to the aforementioned annotation errors, but also to distortions resulting from our joint propagation procedure. As can be seen in <ref type="figure" target="#fig_2">Fig. 3</ref>, the propagated label (three frames away from the ground truth) distorts along the moving car's boundary and the pole. Further, we can see how much the model is struggling with these pixels by visualizing the model's entropy over the class label . As the high entropy would suggest, the border pixel confusion contributes to a large amount of the training loss. In our experiments, we show that by relaxing the boundary labels, our training is more robust to accumulated propagation artifacts, allowing us to benefit from longer-range training data propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our proposed method on three widely adopted semantic segmentation datasets, including Cityscapes <ref type="bibr" target="#b14">[15]</ref>, CamVid <ref type="bibr" target="#b6">[7]</ref> and KITTI <ref type="bibr" target="#b0">[1]</ref>. For all three datasets, we use the standard mean Intersection over Union (mIoU) metric to report segmentation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>For the video prediction/reconstruction models, the training details are described in the supplementary materials. For semantic segmentation, we use an SGD optimizer and employ a polynomial learning rate policy <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b12">13]</ref>, where the initial learning rate is multiplied by</p><formula xml:id="formula_5">(1 âˆ’ epoch max epoch ) power .</formula><p>We set the initial learning rate to 0.002 and power to 1.0. Momentum and weight decay are set to 0.9 and 0.0001 respectively. We use synchronized batch normalization (batch statistics synchronized across each GPU) <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b42">43]</ref> with a batch size of 16 distributed over 8 V100 GPUs. The number of training epochs is set to 180 for Cityscapes, 120 for Camvid and 90 for KITTI. The crop size is 800 for Cityscapes, 640 for Camvid and 368 for KITTI due to different image resolutions. For data augmentation, we randomly scale the input images (from 0.5 to 2.0), and apply horizontal flipping, Gaussian blur and color jittering during training. Our network architecture is based on DeepLabV3Plus <ref type="bibr" target="#b13">[14]</ref> with output stride equal to 8. For the network backbone, we use ResNeXt50 <ref type="bibr" target="#b38">[39]</ref> for the ablation studies, and WideResNet38 <ref type="bibr" target="#b37">[38]</ref> for the final test-submissions. In addition, we adopt the following two effective strategies. Mapillary Pre-Training Instead of using ImageNet pretrained weights for model initialization, we pre-train our model on Mapillary Vistas <ref type="bibr" target="#b31">[32]</ref>. This dataset contains street-level scenes annotated for autonomous driving, which is close to Cityscapes. Furthermore, it has a larger training set (i.e., 18K images) and more classes (i.e., 65 classes). Class Uniform Sampling We introduce a data sampling strategy similar to <ref type="bibr" target="#b9">[10]</ref>. The idea is to make sure that all classes are approximately uniformly chosen during training. We first record the centroid of areas containing the class of interest. During training, we take half of the samples from the standard randomly cropped images and the other half from the centroids to make sure the training crops for all classes are approximately uniform per epoch. In this case, we are actually oversampling the underrepresented categories. For Cityscapes, we also utilize coarse annotations based on class uniform sampling. We compute the class centroids for all 20K samples, but we can choose which data to use. For example, classes such as fence, rider, train are underrepresented. Hence, we only augment these classes by providing extra coarse samples to balance the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cityscapes</head><p>Cityscapes is a challenging dataset containing high quality pixel-level annotations for 5000 images. The standard dataset split is 2975, 500, and 1525 for the training, validation, and test sets respectively. There are also 20K coarsely annotated images. All images are of size 1024Ã—2048. Cityscapes defines 19 semantic labels containing both objects and stuff, and a void class for do-not-care regions. We perform several ablation studies below on the validation set to justify our framework design.</p><p>Stronger Baseline First, we demonstrate the effectiveness of Mapillary pre-training and class uniform sampling. As shown in <ref type="table" target="#tab_0">Table 1</ref>, Mapillary pre-training is highly beneficial and improves mIoU by 1.72% over the baseline (76.60% 78.32%). This makes sense because the Mapillary Vista dataset is close to Cityscape in terms of domain similarity, and thus provides better initialization than ImageNet. We also show that class uniform sampling is an effective data sampling strategy to handle class imbalance problems. It brings an additional 1.14% improvement (78.32% 79.46%). We use this recipe as our baseline.</p><p>Label Propagation versus Joint Propagation Next, we show the advantage of our proposed joint propagation over label propagation. For both settings, we use the motion vectors predicted by the video prediction model to perform propagation. The comparison results are shown in <ref type="table" target="#tab_1">Table 2</ref>. Column 0 in <ref type="table" target="#tab_1">Table 2</ref> indicates the baseline ground-truth-only training (no augmentation with synthesized data). Columns 1 to 5 indicate augmentation with sythesized data from timesteps Â±k, not including intermediate sythesized data from timesteps &lt; |k|. For example, Â±3 indicates we are using +3, âˆ’3 and the ground truth samples, but not Â±1 and Â±2. Note that we also tried the accumulated  case, where Â±1 and Â±2 is included in the training set. However, we observed a slight performance drop. We suspect this is because the cumulative case significantly decreases the probability of sampling a hand-annotated training example within each epoch, ultimately placing too much weight on the synthesized ones and their imperfections. Comparisons between the non-accumulated and accumulated cases can be found in the supplementary materials. As we can see in <ref type="table" target="#tab_1">Table 2</ref> (top two rows), joint propagation works better than label propagation at all propagation lengths. Both achieve highest mIoU for Â±1, which is basically using information from just the previous and next frames. Joint propagation improves by 0.8% mIoU over the baseline (79.46% 80.26%), while label propagation only improves by 0.33% (79.46% 79.79%). This clearly demonstrates the usefulness of joint propagation. We believe this is because label noise from mis-alignment is outweighed by additional dataset diversity obtained from the augmented training samples. Hence, we adopt joint propagation in subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Prediction versus Video Reconstruction</head><p>Recall from Sec. 3.1 that we have two methods for learning the motion vectors to generate new training samples through propagation: video prediction and video reconstruction. We experiment with both models in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>As shown in <ref type="table" target="#tab_1">Table 2</ref> (bottom two rows), video reconstruction works better than video prediction at all propagation lengths, which agrees with our expectations. We also find that Â±1 achieves the best result. Starting from Â±4, the model accuracy starts to drop. This indicates that the quality of the augmented samples becomes lower as we propagate further. Compared to the baseline, we obtain an absolute improvement of 1.08% (79.46% 80.54%). Hence, we use the motion vectors produced by the video reconstruc- Effectiveness of Boundary Label Relaxation Theoretically, we can propagate the labels in an auto-regressive manner for as long as we want. The longer the propagation, the more diverse information we will get. However, due to abrupt scene changes and propagation artifacts, longer propagation will generate low quality labels as shown in <ref type="figure" target="#fig_1">Fig. 2.</ref> Here, we will demonstrate how the proposed boundary label relaxation technique can help to train a better model by utilizing longer propagated samples. We use boundary label relaxation on datasets created by video prediction (red) and video reconstruction (blue) in <ref type="figure" target="#fig_4">Fig. 4</ref>. As we can see, adopting boundary label relaxation leads to higher mIoU at all propagation lengths for both models. Take the video reconstruction model for example. Without label relaxation (dashed lines), the best performance is achieved at Â±1. After incorporating relaxation (solid lines), the best performance is achieved at Â±3 with an improvement of 0.81% mIoU (80.54% 81.35%). The gap between the solid and dashed lines becomes larger as we propagate longer. The same trend can be observed for the video prediction models. This demonstrates that our boundary label relaxation is effective at handling border artifacts. It helps our model obtain more diverse information from Â±3, and at the same time, reduces the impact of label noise brought by long propagation. Hence, we use boundary label relaxation for the rest of the experiments.</p><p>Note that even for no propagation (x-axis equal to 0) in <ref type="figure" target="#fig_4">Fig. 4</ref>, boundary label relaxation improves performance by a large margin (79.46% 80.85%). This indicates that our boundary label relaxation is versatile. Its use is not limited to reducing distortion artifacts in label propagation, but it can also be used in normal image segmentation tasks to handle ambiguous boundary labels. Our Proposed GT Our Baseline Frame <ref type="figure">Figure 6</ref>: Visual comparisons on Cityscapes. The images are cropped for better visualization. We demonstrate our proposed techniques lead to more accurate segmentation than our baseline. Especially for thin and rare classes, like street light and bicycle (row 1), signs (row 2), person and poles (row 3). Our observation corresponds well to the class mIoU improvements in <ref type="table" target="#tab_2">Table 3.</ref> from the video reconstruction model and optical flow, to show why optical flow is not preferred. For optical flow, we use the state-of-the-art CNN flow estimator FlowNet2 <ref type="bibr" target="#b19">[20]</ref> because it can generate sharp object boundaries and generalize well to both small and large motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learned Motion Vectors versus Optical Flow Here, we perform a comparison between the learned motion vectors</head><p>First, we show a qualitative comparison between the learned motion vectors and the FlowNet2 optical flow. As we can see in <ref type="figure" target="#fig_5">Fig. 5a</ref>, FlowNet2 suffers from serious doubling effects caused by occlusion. For example, the dragging car (left) and the doubling rider (right). In contrast, our learned motion vectors can handle occlusion quite well. The propagated labels have only minor artifacts along the object borders which can be remedied by boundary label relaxation. Next, we show quantitative comparison between learned motion vectors and FlowNet2. As we can see in <ref type="figure" target="#fig_5">Fig.  5b</ref>, the learned motion vectors (blue) perform significantly better than FlowNet2 (red) at all propagation lengths. As we propagate longer, the gap between them becomes larger, which indicates the low quality of the FlowNet2 augmented samples. Note that when the propagation length is Â±1, Â±4 and Â±5, the performance of FlowNet2 is even lower than the baseline. Comparison to State-of-the-Art As shown in <ref type="table" target="#tab_2">Table 3</ref> top, our proposed video reconstruction-based data synthesis together with joint propagation improves by 1.0% mIoU over the baseline. Incorporating label relaxation brings another 0.9% mIoU improvement. We observe that the largest improvements come from small/thin object classes, such as pole, street light/sign, person, rider and bicycle. This can be explained by the fact that our augmented samples result in more variation for these classes and helps with model generalization. We show several visual comparisons in <ref type="figure">Fig. 6</ref>.</p><p>For test submission, we train our model using the best recipe suggested above, and replace the network backbone with WideResNet38 <ref type="bibr" target="#b37">[38]</ref>. We adopt a multi-scale strategy <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b13">14]</ref> to perform inference on multi-scaled (0.5, 1.0 and 2.0), left-right flipped and overlapping-tiled images, and compute the final class probabilities after averaging logits per inference. More details can be found in the supplementary materials. As shown in <ref type="table" target="#tab_2">Table 3</ref> bottom, we achieve an mIoU of 83.5%, outperforming all prior methods. We get the highest IoU on 18 out of the 20 classes except for wall and truck. In addition, we show several visual examples in <ref type="figure" target="#fig_6">Fig. 7</ref>. We demonstrate that our model can handle situations <ref type="figure">Figure 8</ref>: Visual comparison between our results and those of the winning entry <ref type="bibr" target="#b9">[10]</ref> of ROB challenge 2018 on KITTI. From left to right: image, prediction from <ref type="bibr" target="#b9">[10]</ref> and ours. Boxes indicate regions in which we perform better than <ref type="bibr" target="#b9">[10]</ref>. Our model can predict semantic objects as a whole (bus), detect thin objects (poles and person) and distinguish confusing classes (sidewalk and road, building and sky). with multiple cars (row 1), dense crowds (row 2) and thin objects (row 3). We also show two interesting failure cases in <ref type="figure" target="#fig_6">Fig. 7</ref>. Our model mis-classifies a reflection in the mirror (row 4) and a model inside the building (row 5) as person (red boxes). However, in terms of appearance without reasoning about context, our predictions are correct. More visual examples can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">CamVid</head><p>CamVid is one of the first datasets focusing on semantic segmentation for driving scenarios. It is composed of 701 densely annotated images with size 720 Ã— 960 from five video sequences. We follow the standard protocol proposed in <ref type="bibr" target="#b2">[3]</ref> to split the dataset into 367 training, 101 validation and 233 test images. A total of 32 classes are provided. However, most literature only focuses on 11 due to the rare occurrence of the remaining classes. To create the augmented samples, we directly use the video reconstruction model trained on Cityscapes without fine tuning on CamVid. The training strategy is similar to Cityscapes. We compare our method to recent literature in <ref type="table" target="#tab_3">Table 4</ref>. For fair comparison, we only report single-scale evaluation scores. As can be seen in <ref type="table" target="#tab_3">Table 4</ref>, we achieve an mIoU of 81.7%, outperforming all prior methods by a large margin. Furthermore, our multi-scale evaluation score is 82.9%. Per-class breakdown can be seen in the supplementary materials.</p><p>One may argue that our encoder is more powerful than prior methods. To demonstrate the effectiveness of our proposed techniques, we perform training under the same set- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">KITTI</head><p>The KITTI Vision Benchmark Suite <ref type="bibr" target="#b16">[17]</ref> was introduced in 2012 but updated with semantic segmentation ground truth <ref type="bibr" target="#b0">[1]</ref> in 2018. The data format and metrics conform with Cityscapes, but with a different image resolution of 375 Ã— 1242. The dataset consists of 200 training and 200 test images. Since the dataset is quite small, we perform 10-split cross validation fine-tuning on the 200 training images. Eventually, we determine the best model in terms of mIoU on the whole training set because KITTI only allows one submission for each algorithm. For 200 test images, we run multi-scale inference by averaging over 3 scales (1.5, 2.0 and 2.5). We compare our method to recent literature in <ref type="table" target="#tab_4">Table 5</ref>. We achieve significantly better performance than prior methods on all four evaluation metrics. In terms of mIoU, we outperform previous state-of-the-art [10] by 3.3%. Note that <ref type="bibr" target="#b9">[10]</ref> is the winning entry to Robust Vision Challenge 2018, which is achieved by an ensemble of five models, we only use one. We show two visual comparisons between ours and <ref type="bibr" target="#b9">[10]</ref> in <ref type="figure">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose an effective video prediction-based data synthesis method to scale up training sets for semantic segmentation. We also introduce a joint propagation strategy to alleviate mis-alignments in synthesized samples. Furthermore, we present a novel boundary relaxation technique to mitigate label noise. The label relaxation strategy can also be used for human annotated labels and not just synthesized labels. We achieve state-of-the-art mIoUs of 83.5% on Cityscapes, 82.9% on CamVid, and 72.8% on KITTI. The superior performance demonstrates the effectiveness of our proposed methods.</p><p>We hope our approach inspires other ways to perform data augmentation, such as GANs <ref type="bibr" target="#b25">[26]</ref>, to enable cheap dataset collection and achieve improved accuracy in target tasks. For future work, we would like to explore soft label relaxation using the learned kernels in <ref type="bibr" target="#b33">[34]</ref> for better uncertainty reasoning. Our state-of-the-art implementation, will be made publicly available to the research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details of Our Video Prediction/Reconstruction Models</head><p>In this section, we first describe the network architecture of our video prediction model and then we illustrate the training details. The network architecture and training details of our video reconstruction model is similar, except the input is different.</p><p>Recalling equation <ref type="formula" target="#formula_0">(1)</ref> from the main submission, the future frame I t+1 is given by,</p><formula xml:id="formula_6">I t+1 = T G I 1:t , F 2:t , I t ,</formula><p>where G is a general CNN that predicts the motion vectors (u, v) conditioned on the input frames I 1:t and the estimated optical flow F i between successive input frames I i and I iâˆ’1 .</p><p>T is an operation that bilinearly samples from the most recent input I t using the predicted motion vectors (u, v).</p><p>In our implementation, we use the vector-based architecture as described in <ref type="bibr" target="#b33">[34]</ref>. G is a fully convolutional U-net architecture, complete with an encoder and decoder and skip connections between encoder/decoder layers of the same output dimensions. Each of the 10 encoder layers is composed of a convolution operation followed by a Leaky ReLU. The 6 decoder layers are composed of a deconvolution operation followed by a Leaky ReLU. The output of the decoder is fed into one last convolutional layer to generate the motion vector predictions. The input to G is I tâˆ’1 , I t and F t (8 channels), and the output is the predicted 2-channel motion vectors that can best warp I t to I t+1 . For the video reconstruction model, we simply add I t+1 and F t+1 to the input, and change the number of channels in the first convolutional layer to 13 instead of 8.</p><p>We train our video prediction model using frames extracted from short sequences in the Cityscapes dataset. We use the Adam optimizer with Î² 1 = 0.9, Î² 2 = 0.999, and a weight decay of 1Ã—10 âˆ’4 . The frames are randomly cropped to 256 Ã— 256 with no extra data augmentation. We set the batch size to 128 over 8 V100 GPUs. The initial learning rate is set to 1 Ã— 10 âˆ’4 and the number of epochs is 400. We refer interested readers to <ref type="bibr" target="#b33">[34]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Non-Accumulated and Accumulated Comparison</head><p>Recalling Sec. 4.1 from the main submission, we have two ways to augment the dataset. The first is the nonaccumulated case, where we simply use synthesized data from timesteps Â±k, excluding intermediate synthesized data from timesteps &lt; |k|. For the accumulated case, we include all the synthesized data from timesteps â‰¤ |k|, which makes the augmented dataset 2k + 1 times larger than the original training set. We showed that we achieved the best performance at Â±3, so we use k = 3 here. We compare three configurations: 1. Baseline: using the ground truth dataset only.</p><p>2. Non-accumulated case: using the union of the ground truth dataset and Â±3;</p><p>3. Accumulated case: using the union of the ground truth dataset, Â±3, Â±2 and Â±1.</p><p>For these experiments, we use boundary label relaxation and joint propagation. We report segmentation accuracy on the Cityscapes validation set. We have two observations from <ref type="table" target="#tab_5">Table 6</ref>. First, using the augmented dataset always improves segmentation quality as quantified by mIoU. Second, the non-accumulated case performs better than the accumulated case. We suspect this is because the cumulative case significantly decreases the probability of sampling a hand-annotated training example within each epoch, ultimately placing too much weight on the synthesized ones and their imperfections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cityscapes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. More Training Details</head><p>We perform 3-split cross-validation to evaluate our algorithms, in terms of cities. The three validation splits are {cv0: munster, lindau, frankfurt}, {cv1: darmstadt, dusseldorf, erfurt} and {cv2: monchengladbach, strasbourg, stuttgart}. The rest cities will be in the training set, respectively. cv0 is the standard validation split. We found that models trained on cv2 split leads to higher performance on the test set, so we adopt cv2 split for our final test submission. Using our best model, we perform multiscale inference on the 'stuttgart 00' sequence and generate a demo video. The video is composed of both video frames and predicted semantic labels, with a 0.5 alpha blending.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Failure Cases</head><p>We show several more failure cases in <ref type="figure" target="#fig_8">Fig. 9</ref>. First, we show four challenging scenarios of class confusion. From rows (a) to (d), our model has difficulty in segmenting: (a) car and truck. (b) person and rider. (c) wall and fence (d) terrain and vegetation.</p><p>Furthermore, we show three cases where it could be challenging even for a human to label. In <ref type="figure" target="#fig_8">Fig. 9 (e)</ref>, it is very hard to tell whether it is a bus or train when the object is far away. In <ref type="figure" target="#fig_8">Fig. 9 (f)</ref>, it is also hard to predict whether it is a car or bus under such strong occlusion (more than 95% of the object is occluded). In <ref type="figure" target="#fig_8">Fig. 9 (g)</ref>, there is a bicycle hanging on the back of a car. The model needs to know whether the bicycle is part of the car or a painting on the car, or whether they are two separate objects, in order to make the correct decision. Finally, we show two training samples where the annotation might be wrong. In <ref type="figure" target="#fig_8">Fig. 9 (h)</ref>, the rider should be on a motorcycle, not a bicycle. In <ref type="figure" target="#fig_8">Fig. 9</ref> (i), there should be a fence before the building. However, the whole region was labelled as building by a human annotator. In both cases, our model predicts the correct semantic labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. More Synthesized Training Samples</head><p>We show 15 synthesized training samples in the demo video to give readers a better understanding. Each is a 11frame video clip, in which only the 5th frame is the ground truth. The neighboring 10 frames are generated using the video reconstruction model. We also show the comparison to using the video prediction model and FlowNet2 <ref type="bibr" target="#b19">[20]</ref>. In general, the video reconstruction model gives us the best propagated frames/labels in terms of visualization. It also works the best in our experiments in terms of segmentation accuracy. Since the Cityscapes dataset is recorded at 17Hz <ref type="bibr" target="#b14">[15]</ref>, the motion between frames is very large. Hence, propagation artifacts can be clearly observed, especially at the image borders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CamVid</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Class Breakdown</head><p>We show the per-class mIoU results in <ref type="table" target="#tab_6">Table 7</ref>. Our model has the highest mIoU on 8 out of 11 classes (all classes but tree, sky and sidewalk). This is expected because our synthesized training samples help more on classes with small/thin structures. Overall, our method significantly outperforms previous state-of-the-art by 7.7% mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. More Synthesized Training Samples</head><p>For CamVid, we show two demo videos of synthesized training samples. One is on the validation sequence '006E15', which is manually annotated every other frame. The other is on the training sequence '0001TP', which has manually annotated labels for every 30th frame. For '006E15', we do one step of forward propagation to generate a label for the unlabeled intermediate frame. For '0001TP', we do 15 steps of forward propagation and 14 steps of backward propagation to label the 29 unlabeled frames in between. For both videos, the synthesized samples are generated using the video reconstruction model trained on Cityscapes, without fine-tuning on CamVid. This demonstrates the great generalization ability of our video reconstruction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Demo Video</head><p>We present all the video clips mentioned above at https://nv-adlr.github.io/publication/ 2018-Segmentation.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Motivation of joint image-label propagation. Row 1: original frames. Row 2: propagated labels. Row 3: propagated frames. The red and green boxes are two zoomed-in regions which demonstrate the mis-alignment problem. Note how the propagated frames align perfectly with propagated labels as compared to the original frames. The black areas in the labels represent a void class. (Image brightness has been adjusted for better visualization.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Motivation of boundary label relaxation. For the entropy image, the lighter pixel value, the larger the entropy. We find that object boundaries often have large entropy, due to ambiguous annotations or propagation distortions. The green boxes are zoomed-in figures showing such distortions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Boundary label relaxation leads to higher mIoU at all propagation lengths. The longer propagation, the bigger the gap between the solid (with label relaxation) and dashed (without relaxation) lines. The black dashed line represents our baseline (79.46%). x-axis equal to 0 indicates no augmented samples are used. For each experiment, we perform three runs and report the mean and sample standard deviation as the error bar<ref type="bibr" target="#b7">[8]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Our learned motion vectors from video reconstruction are better than optical flow (FlowNet2). 5a Qualitative result. The learned motion vectors are better in terms of occlusion handling. 5b Quantitative result. The learned motion vectors are better at all propagation lengths in terms of mIoU. tion model in the following experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Visual examples on Cityscapes. From left to right: image, GT, prediction and their differences. We demonstrate that our model can handle situations with multiple cars (row 1), dense crowds (row 2) and thin objects (row 3). The bottom two rows show failure cases. We mis-classify a reflection in the mirror (row 4) and a model inside the building (row 5) as person (red boxes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Failure cases (in yellow boxes). From left to right: image, ground truth, prediction and their difference. Green boxes are zoomed in regions for better visualization. Row (a) to (d) show class confusion problems. Our model has difficulty in segmenting: (a) car and truck. (b) person and rider. (c) wall and fence (d) terrain and vegetation. Row (e) to (f) show challenging cases when the object is far away, strongly occluded, or overlaps other objects. The last two rows show two training samples with wrong annotations: (h) mislabeled motorcycle to bicycle and (i) mislabeled fence to building.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effectiveness of Mapillary pre-training and class uniform sampling on both fine and coarse annotations.</figDesc><table><row><cell>Method</cell><cell>mIoU (%)</cell></row><row><cell>Baseline</cell><cell>76.60</cell></row><row><cell>+ Mapillary Pre-training</cell><cell>78.32</cell></row><row><cell>+ Class Uniform Sampling (Fine + Coarse)</cell><cell>79.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison between (1) label propagation (LP) and joint propagation (JP); (2) video prediction (VPred) and video reconstruction (VRec). Using the proposed video reconstruction and joint propagation techniques, we improve over the baseline by 1.08% mIoU (79.46% 80.54%). LP 79.46 79.79 79.77 79.71 79.55 79.42 VPred + JP 79.46 80.26 80.21 80.23 80.11 80.04 VRec + JP 79.46 80.54 80.47 80.51 80.34 80.18</figDesc><table><row><cell>0</cell><cell>Â±1</cell><cell>Â±2</cell><cell>Â±3</cell><cell>Â±4</cell><cell>Â±5</cell></row><row><cell>VPred +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Per-class mIoU results on Cityscapes. Top: our ablation improvements on the validation set. Bottom: comparison with topperforming models on the test set.</figDesc><table><row><cell>Method</cell><cell cols="8">split road swalk build. wall fence pole tlight tsign veg. terrain</cell><cell>sky</cell><cell cols="2">person rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train mcycle bicycle mIoU</cell></row><row><cell>Baseline</cell><cell>val</cell><cell>98.4</cell><cell>86.5</cell><cell>93.0</cell><cell>57.4</cell><cell>65.5</cell><cell>66.7 70.6 78.9 92.7</cell><cell>65.0</cell><cell>95.3</cell><cell>80.8</cell><cell cols="5">60.9 95.3 87.9 91.0 84.3</cell><cell>65.8</cell><cell>76.2</cell><cell>79.5</cell></row><row><cell>+ VRec with JP</cell><cell>val</cell><cell>98.0</cell><cell>86.5</cell><cell>94.7</cell><cell>47.6</cell><cell>67.1</cell><cell>69.6 71.8 80.4 92.2</cell><cell>58.4</cell><cell>95.6</cell><cell>88.3</cell><cell cols="5">71.1 95.6 76.8 84.7 90.3</cell><cell>79.6</cell><cell>80.3</cell><cell>80.5</cell></row><row><cell cols="2">+ Label Relaxation val</cell><cell>98.5</cell><cell>87.4</cell><cell>93.5</cell><cell>64.2</cell><cell>66.1</cell><cell>69.3 74.2 81.5 92.9</cell><cell>64.6</cell><cell>95.6</cell><cell>83.5</cell><cell cols="5">66.5 95.7 87.7 91.9 85.7</cell><cell>70.1</cell><cell>78.8</cell><cell>81.4</cell></row><row><cell>ResNet38 [38]</cell><cell cols="2">test 98.7</cell><cell>86.9</cell><cell>93.3</cell><cell>60.4</cell><cell>62.9</cell><cell>67.6 75.0 78.7 93.7</cell><cell>73.7</cell><cell>95.5</cell><cell>86.8</cell><cell cols="5">71.1 96.1 75.2 87.6 81.9</cell><cell>69.8</cell><cell>76.7</cell><cell>80.6</cell></row><row><cell>PSPNet [44]</cell><cell cols="2">test 98.7</cell><cell>86.9</cell><cell>93.5</cell><cell>58.4</cell><cell>63.7</cell><cell>67.7 76.1 80.5 93.6</cell><cell>72.2</cell><cell>95.3</cell><cell>86.8</cell><cell cols="5">71.9 96.2 77.7 91.5 83.6</cell><cell>70.8</cell><cell>77.5</cell><cell>81.2</cell></row><row><cell>InPlaceABN [10]</cell><cell cols="2">test 98.4</cell><cell>85.0</cell><cell>93.6</cell><cell>61.7</cell><cell>63.9</cell><cell>67.7 77.4 80.8 93.7</cell><cell>71.9</cell><cell>95.6</cell><cell>86.7</cell><cell cols="5">72.8 95.7 79.9 93.1 89.7</cell><cell>72.6</cell><cell>78.2</cell><cell>82.0</cell></row><row><cell>DeepLabV3+ [14]</cell><cell cols="2">test 98.7</cell><cell>87.0</cell><cell>93.9</cell><cell>59.5</cell><cell>63.7</cell><cell>71.4 78.2 82.2 94.0</cell><cell>73.0</cell><cell>95.8</cell><cell>88.0</cell><cell cols="5">73.0 96.4 78.0 90.9 83.9</cell><cell>73.8</cell><cell>78.9</cell><cell>82.1</cell></row><row><cell>DRN-CRL [46]</cell><cell cols="2">test 98.8</cell><cell>87.7</cell><cell cols="3">94.0 65.1 64.2</cell><cell>70.1 77.4 81.6 93.9</cell><cell>73.5</cell><cell>95.8</cell><cell>88.0</cell><cell cols="5">74.9 96.5 80.8 92.1 88.5</cell><cell>72.1</cell><cell>78.8</cell><cell>82.8</cell></row><row><cell>Ours</cell><cell cols="3">test 98.8 87.8</cell><cell cols="4">94.2 64.1 65.0 72.4 79.0 82.8 94.2</cell><cell>74.0</cell><cell>96.1</cell><cell>88.2</cell><cell cols="5">75.4 96.5 78.8 94.0 91.6</cell><cell>73.8</cell><cell>79.0</cell><cell>83.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on the CamVid test set. Pre-train indicates the source dataset on which the model is trained.</figDesc><table><row><cell>Method</cell><cell>Pre-train</cell><cell>Encoder</cell><cell>mIoU (%)</cell></row><row><cell>SegNet [3]</cell><cell>ImageNet</cell><cell>VGG16</cell><cell>60.1</cell></row><row><cell>RTA [19]</cell><cell>ImageNet</cell><cell>VGG16</cell><cell>62.5</cell></row><row><cell>Dilate8 [42]</cell><cell>ImageNet</cell><cell>Dilate</cell><cell>65.3</cell></row><row><cell>BiSeNet [41]</cell><cell>ImageNet</cell><cell>ResNet18</cell><cell>68.7</cell></row><row><cell>PSPNet [44]</cell><cell>ImageNet</cell><cell>ResNet50</cell><cell>69.1</cell></row><row><cell cols="2">DenseDecoder [6] ImageNet</cell><cell>ResNeXt101</cell><cell>70.9</cell></row><row><cell cols="2">VideoGCRF [11] Cityscapes</cell><cell>ResNet101</cell><cell>75.2</cell></row><row><cell>Ours (baseline)</cell><cell cols="2">Cityscapes WideResNet38</cell><cell>79.8</cell></row><row><cell>Ours</cell><cell cols="2">Cityscapes WideResNet38</cell><cell>81.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on KITTI test set. The performance of this configuration on the test set is 79.8%, a significant IoU drop of 1.9%.</figDesc><table><row><cell>Method</cell><cell cols="4">IoU class iIoU class IoU category iIoU category</cell></row><row><cell>APMoE seg [23]</cell><cell>47.96</cell><cell>17.86</cell><cell>78.11</cell><cell>49.17</cell></row><row><cell>SegStereo [40]</cell><cell>59.10</cell><cell>28.00</cell><cell>81.31</cell><cell>60.26</cell></row><row><cell>AHiSS [30]</cell><cell>61.24</cell><cell>26.94</cell><cell>81.54</cell><cell>53.42</cell></row><row><cell>LDN2 [24]</cell><cell>63.51</cell><cell>28.31</cell><cell>85.34</cell><cell>59.07</cell></row><row><cell>MapillaryAI [10]</cell><cell>69.56</cell><cell>43.17</cell><cell>86.52</cell><cell>68.89</cell></row><row><cell>Ours</cell><cell>72.83</cell><cell>48.68</cell><cell>88.99</cell><cell>75.26</cell></row><row><cell cols="5">tings without using the augmented samples and boundary</cell></row><row><cell>label relaxation.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Accumulated and non-accumulated comparison. The numbers in brackets are the sample standard deviations.</figDesc><table><row><cell>Method</cell><cell>Baseline</cell><cell cols="2">Non-accumulated Accumulated</cell></row><row><cell cols="2">mIoU (%) 80.85 (Â±0.04)</cell><cell>81.35 (Â±0.03)</cell><cell>81.12 (Â±0.02)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Per-class mIoU results on CamVid. Comparison with recent top-performing models on the test set. 'SS' indicates singlescale inference, 'MS' indicates multi-sclae inference. Our model achieves the highest mIoU on 8 out of 11 classes (all classes but tree, sky and sidewalk). This is expected because our synthesized training samples help more on classes with small/thin structures.</figDesc><table><row><cell>Method</cell><cell cols="2">Build. Tree</cell><cell>Sky</cell><cell>Car</cell><cell cols="5">Sign Road Pedes. Fence Pole Swalk Cyclist mIoU</cell></row><row><cell>RTA [19]</cell><cell>88.4</cell><cell cols="4">89.3 94.9 88.9 48.7 95.4</cell><cell>73.0</cell><cell>45.6</cell><cell>41.4</cell><cell>94.0</cell><cell>51.6</cell><cell>62.5</cell></row><row><cell>Dilate8 [42]</cell><cell>82.6</cell><cell cols="4">76.2 89.0 84.0 46.9 92.2</cell><cell>56.3</cell><cell>35.8</cell><cell>23.4</cell><cell>75.3</cell><cell>55.5</cell><cell>65.3</cell></row><row><cell>BiSeNet [41]</cell><cell>83.0</cell><cell cols="4">75.8 92.0 83.7 46.5 94.6</cell><cell>58.8</cell><cell>53.6</cell><cell>31.9</cell><cell>81.4</cell><cell>54.0</cell><cell>68.7</cell></row><row><cell>VideoGCRF [11]</cell><cell>86.1</cell><cell cols="4">78.3 91.2 92.2 63.7 96.4</cell><cell>67.3</cell><cell>63.0</cell><cell>34.4</cell><cell>87.8</cell><cell>66.4</cell><cell>75.2</cell></row><row><cell>Ours (SS)</cell><cell>90.9</cell><cell cols="4">82.9 92.8 94.2 69.9 97.7</cell><cell>76.2</cell><cell>74.7</cell><cell>51.0</cell><cell>91.1</cell><cell>78.0</cell><cell>81.7</cell></row><row><cell>Ours (MS)</cell><cell>91.2</cell><cell cols="4">83.4 93.1 93.9 71.5 97.7</cell><cell>79.2</cell><cell cols="2">76.8 54.7</cell><cell>91.3</cell><cell>79.7</cell><cell>82.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Saad Godil, Matthieu Le, Ming-Yu Liu and Guilin Liu for suggestions and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Augmented Reality Meets Computer Vision: Efficient Data Generation for Urban Driving Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Label Propagation in Video Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic Segmentation with Boundary Neural Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional Random Walk Networks for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation and Recognition Using Structure from Motion Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Standard Deviation, Standard Error: Which &apos;Standard&apos; Should We Use?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Diseases of Children</title>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large Scale Labelled Video Data Augmentation for Semantic Segmentation in Driving Scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Breen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV) Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">In-Place Activated BatchNorm for Memory-Optimized Training of DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Spatio-Temporal Random Fields for Efficient Video Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<title level="m">Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic Video CNNs Through Representation Warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-Consistent Adversarial Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient Uncertainty Estimation for Semantic Segmentation in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive Affinity Fields for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pixel-wise Attentional Gating for Parsimonious Pixel Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ladder-style DenseNets for Semantic Segmentation of Large Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Egvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via Deep Layer Cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pixel Level Data Augmentation for Semantic Image Segmentation using Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00174</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ParseNet: Looking Wider to See Better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting Deeper into the Future of Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Training of Convolutional Networks on Multiple Heterogeneous Datasets for Street Scene Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05675</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Can Ground Truth Label Propagation from Video help Semantic Segmentation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic Video Segmentation by Gated Recurrent Flow Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SDC-Net: Video Prediction using Spatially-Displaced Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Abd Arpit Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segmentation-based Multi-class Semantic Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Domenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Braquelaire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or Deeper: Revisiting the ResNet Model for Visual Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SegStereo: Exploiting Semantic Information for Disparity Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Context Encoding for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Penalizing Top Performers: Conservative Loss for Semantic Segmentation Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dense Relation Network: Learning Consistent and Context-Aware Representation For Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the Importance of Label Quality for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zlateski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jaroensri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
