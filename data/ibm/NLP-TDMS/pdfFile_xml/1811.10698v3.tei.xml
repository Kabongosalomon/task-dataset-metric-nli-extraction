<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LSTA: Long Short-Term Attention for Egocentric Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Bruno Kessler</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
							<email>sergio@maia.ub.es</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Vision Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Universitat de Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
							<email>lanz@fbk.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Bruno Kessler</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LSTA: Long Short-Term Attention for Egocentric Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Egocentric activity recognition is one of the most challenging tasks in video analysis. It requires a fine-grained discrimination of small objects and their manipulation. While some methods base on strong supervision and attention mechanisms, they are either annotation consuming or do not take spatio-temporal patterns into account. In this paper we propose LSTA as a mechanism to focus on features from relevant spatial parts while attention is being tracked smoothly across the video sequence. We demonstrate the effectiveness of LSTA on egocentric activity recognition with an end-to-end trainable two-stream architecture, achieving state-of-the-art performance on four standard benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognizing human actions from videos is a widely studied problem in computer vision. Most research is devoted to the analysis of videos captured from distant, thirdperson views. Egocentric (first-person) video analysis is an important and relatively less explored branch with potential applications in robotics, indexing and retrieval, humancomputer interaction, or human assistance, just to mention a few. Recent advances in deep learning highly benefited problems such as image classification <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">39]</ref> and object detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11]</ref>. However, the performance of deep learning action recognition from videos is still not comparable to the advances made in object recognition from still images <ref type="bibr" target="#b11">[12]</ref>. One of the main difficulties in action recognition is the huge variations present in the data caused by the highly articulated nature of the human body. Human kinesics, being highly flexible in nature, results in high intra-subject and low inter-subject variabilities. This is further challenged by the variations introduced by the unconstrained nature of the environment where the video is captured. Since videos are composed of image frames, this introduces an additional dimension to the data, making it more difficult to define a model that properly focuses on the regions of interest that better discriminate particular action classes. In order to mitigate these problems, one approach could be the design of a large scale dataset with fine-grain annotations covering the space of spatio-temporal variabilities defined by the problem domain, which would be unfeasible in practice.</p><p>Here, we consider the problem of identifying finegrained egocentric activities from trimmed videos. This is a comparatively difficult task considered to action recognition since the activity class depends on the action and the object on to which the action is applied to. This requires the development of a method that can simultaneously recognize the action as well as the object. In addition, the presence of strong ego-motion caused by the sharp movements of the camera wearer introduces noise to the video that complicates the encoding of motion in the video frame. While incorporating object detection can help the task of egocentric action recognition, still this would require finegrain frame level annotations, becoming costly and impractical in a large scale setup.</p><p>Attention in deep learning was recently proposed to guide networks to focus on regions of interest relevant for a particular recognition task. This prunes the network search space and avoids computing features from irrelevant image regions, resulting in a better generalization. Existing works explore both bottom-up <ref type="bibr" target="#b41">[41]</ref> and top-down attention mechanisms <ref type="bibr" target="#b32">[32]</ref>. Bottom-up attention relies on the salient features of the data and is trained to identify such visual patterns that distinguish one class from another. Top-down attention applies prior knowledge about the data for developing attention, e.g. the presence of certain objects which can be obtained from a network trained for a different task. Recently, attention mechanisms have been successfully applied to egocentric action recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">32]</ref>, surpassing the performance of non-attentive alternatives. Still, very few attempts have been done to track attention into spatiotemporal egocentric action recognition data. As a result, current models may lose a proper smooth tracking of attention regions in egocentric action videos. Furthermore, most current models base on separate pre-training with strong supervision, requiring complex annotation operations.</p><p>To address these limitations, in this work we investigate on the more general question of how a video CNN-RNN can learn to focus on the regions of interest to better discriminate the action classes. We analyze the shortcomings of LSTMs in this context and derive Long Short-Term Attention (LSTA), a new recurrent neural unit that augments LSTM with built-in spatial attention and a revised output gating. The first enables LSTA to attend the feature regions of interest while the second constraints it to expose a distilled view of internal memory. Our study confirms that it is effective to improve the output gating of recurrent unit since it does not only affect prediction overall but controls the recurrence, being responsible for a smooth and focused tracking of the latent memory state across the sequence. Our main contributions can be summarized as follows:</p><p>• We present Long Short-Term Attention (LSTA), a new recurrent unit that addresses shortcomings of LSTM when the discriminative information in the input sequence can be spatially localized; • We deploy LSTA into a two stream architecture with cross-modal fusion, a novel control of the bias parameter of one modality by using the other 1 ; • We report an ablation analysis of the model and evaluate it on egocentric activity recognition, providing state-of-the-art results in four public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We discuss the most relevant deep learning methods for addressing egocentric vision problems in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">First Person Action Recognition</head><p>The works of <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b43">43]</ref> train specialized CNN for hand segmentation and object localization related to the activities to be recognized. These methods base on specialized pre-training for hand segmentation and object detection networks, requiring high amounts of annotated data for that purpose. Additionally, they just base on single RGB images for encoding appearance without considering temporal information. In <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b40">40]</ref> features are extracted from a series of frames to perform temporal pooling with different operations, including max pooling, sum pooling, or histogram of gradients. Then, a temporal pyramid structure allows the encoding of both long term and short term characteristics. However, all these methods do not take into consideration the temporal order of the frames. Techniques that use a recurrent neural network such as Long Short-Term Memory (LSTM) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">36]</ref> and Convolutional Long Short-Term Memory (ConvLSTM) <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32]</ref> are proposed to encode the temporal order of features extracted from a sequence of frames. Sigurdsson et al. <ref type="bibr" target="#b28">[28]</ref> proposes a triplet network to develop a joint representation of paired third person and first person videos. Their method can be used for transferring knowledge from third person domain to first person domain thereby partially solving the problem of lack of large first person datasets. Tang et al. <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35]</ref> add an additional stream that accepts depth maps to the two stream networkenabling it to encode 3D information present in the scene. Li et al. <ref type="bibr" target="#b14">[15]</ref> propose a deep neural network to jointly predict the gaze and action from first person videos, which requires gaze information during training.</p><p>Majority of the state-of-the-art techniques rely on additional annotations such as hand segmentation, object bounding box or gaze information. This allows the network to concentrate on the relevant regions in the frame and helps in distinguishing each activity from one another better. However, manually annotating all the frames of a video with these information is impractical. For this reason, development of techniques that can identify the relevant regions of a frame without using additional annotations is crucial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention</head><p>Attention mechanism was proposed for focusing attention on features that are relevant for the task to be recognized. This includes <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">26]</ref> for first person action recognition, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">37]</ref> for image and video captioning and <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18]</ref> for visual question answering. The works of <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b14">15]</ref> use an attention mechanism for weighting spatial regions that are representative for a particular task. Sharma et al. <ref type="bibr" target="#b25">[25]</ref> and Zhang et al. <ref type="bibr" target="#b41">[41]</ref> generate attention masks implicitly by training the network with video labels. Authors of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b32">32]</ref> use top-down attention generated from the prior information encoded in a CNN pretrained for object recognition while <ref type="bibr" target="#b14">[15]</ref> uses gaze information for generating attention. The work of <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b26">26]</ref> uses attention for weighting relevant frames, thereby adding temporal attention. This is based on the idea that not all frames present in a video are equally important for understanding the action being carried out. In <ref type="bibr" target="#b23">[23]</ref> a series of temporal attention filters is learnt that weight frame level features depending on their relevance for identifying actions. <ref type="bibr" target="#b26">[26]</ref> uses change in gaze for generating the temporal attention. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b4">5]</ref> apply attention on both spatial and temporal dimensions to select relevant frames and the regions present in them.</p><p>Most existing techniques for generating spatial attention in videos consider each frame independently. Since video frame sequences have an absolute temporal consistency, per frame processing results in the loss of valuable information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Relation to state-of-the-art alternatives</head><p>The proposed LSTA method generates the spatial attention map in a top-down fashion utilizing prior information encoded in a CNN pre-trained for object recognition and another pre-trained for action recognition. <ref type="bibr" target="#b32">[32]</ref> proposes a similar top-down attention mechanism. However, they generate the attention map independently in each frame whereas in the proposed approach, the attention map is generated in a sequential manner. This is achieved by propagating the attention map generated from past frames across time by maintaining an internal state for attention. Our method uses attention on the motion stream followed by a cross-modal fusion of the appearance and motion streams, thereby enabling both streams to interact earlier in the layers to facilitate flow of information between them. <ref type="bibr" target="#b41">[41]</ref> proposes an attention mechanism that takes in to consideration the inputs from past frames. Their method is based on bottom-up attention and generates a single weight matrix which is trained with the video level label. However, the proposed method generates attention, based on the input, from a pool of attention maps which are learned using video level label alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analysis of LSTM</head><p>LSTM is the widely adopted neuron design for processing and/or predicting sequences. A latent memory state c t is tracked across a sequence with a forget-update mechanism</p><formula xml:id="formula_0">c t = f c t−1 + i c<label>(1)</label></formula><p>where (f, i) have a gating function on the previous state c t−1 and an innovation term c. (f, i, c) are parametric functions of input x t and a gated non-linear view of previous memory state o t−1 η(c t−1 )</p><formula xml:id="formula_1">(i, f, o t , c) = (σ, σ, σ, η)(W [x t , o t−1 η(c t−1 )]) (2)</formula><p>The latter, referred to as hidden state h t = o t η(c t ), is often exposed to realize a sequence prediction. For sequence classification instead, the final memory state can be used as a fixed-length descriptor of the input sequence. Two features of LSTM design explain its success. First, the memory update (Eq. 1) is flexibly controlled by (f, i): a state can, in a single iteration, be erased (0, 0), reset (0, 1), left unchanged (1, 0), or progressively memorize new input.</p><p>(1, 1) resembles residual learning <ref type="bibr" target="#b11">[12]</ref>, a key design pattern in very deep networks -depth here translates to sequence length. Indeed, LSTMs has strong gradient flow and learn long-term dependencies <ref type="bibr" target="#b12">[13]</ref>. Second, the gating functions (Eq. 2) are learnable neurons and their interaction in memory updating is transparent (Eq. 1). When applied to video classification, a few limitations are to be discussed: 1. Memory. Standard LSTMs use fully connected neuron gates and consequently, the memory state is unstructured. This may be desired e.g. for image captioning where one modality (vision) has to be translated into another (language). For video classification it might be advantageous to preserve the spatial layout of images and their convolutional features by propagating a memory tensor instead. ConvLSTM <ref type="bibr" target="#b27">[27]</ref> addresses this shortcoming through convolutional gates in the LSTM. 2. Attention. The discriminative information is often confined locally in the video frame. Thus, not all convolutional features are equally important for recognition. In LSTMs the filtering of irrelevant features (and memory) is deferred to the gating neurons, that is, to a linear transformation (or convolution) and a non-linearity. Attention neurons were introduced to suppress activations from irrelevant features ahead of gating. We augment LSTM with built-in attention that directly interacts with the memory tracking in Sec. 4.1. 3. Output gating. Output gating not only impacts sequence prediction but it critically affects memory tracking too, cf. Eq 2. We replace the output gating neuron of LSTM with a high-capacity neuron whose design is inspired by that of attention. There is indeed a relation among them, we make this explicit in Sec. 4.2. 4. External bias control. The neurons in Eq. 2 have a bias term that is learnt from data during training, and it is fixed at prediction time in standard LSTM. We leverage on adapting the biases based on the input video for each prediction. State-of-the-art video recognition is realized with two-stream architectures, we use flow stream to control appearance biases in Sec. 5.3.</p><formula xml:id="formula_2">c t−1 c t o t−1 o t a t , s t x t a t−1 , s t−1 η × × + × σ σ η × σ + × RNN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Long Short-Term Attention</head><p>We present a schematic view of LSTA in <ref type="figure" target="#fig_0">Fig. 1</ref>. LSTA extends LSTM <ref type="bibr" target="#b8">[9]</ref> with two newly designed components. The core operation is a pooling ς, that selects one out of a pool of specialized mappings to realize attention tracking (red part) and output gating (green part). The pooling ς on features x t returns a map ν a that is fed through a conventional RNN cell with memory a t and output gate s t . Its output state s t η(a t ) is added to the input ν a and softmax calibrated to obtain an attention map s. The map s is then applied to x t , that is, s x t is the attention filtered feature for updating memory state c t using conventional LSTM recurrence (black part). Our redesigned output gating uses a filtered view of the updated memory state, ν c c t , instead of x t . To obtain ν c through pooling we use s x t to control the bias of operator ς, hereby coupling attention tracking with output gating. This model is instantiated for action recognition from egocentric video in its convolutional version as</p><formula xml:id="formula_3">ν a = ς(x t , w a ) (3) (i a , f a , s t , a) = (σ, σ, σ, η)(W a * [ν a , s t−1 η(a t−1 )])(4) a t = f a a t−1 + i a a (5) s = softmax(ν a + s t η(a t )) (6) (i c , f c , c) = (σ, σ, η)(W c * [s x t , o t−1 η(c t−1 )]) (7) c t = f c c t−1 + i c c (8) ν c = ς(c t , w c + w o (s x t )) (9) o t = σ(W o * [ν c c t , o t−1 η(c t−1 )]) (10)</formula><p>Eqs. 3-6 implement our recurrent attention as detailed in Sec. 4.1, Eqs. 9-10 is our coupled output gating of Sec. 4.2. Bold symbols represent the recurrent variables: (a t , s t ) of shape N ×1, (c t , o t ) of shape N ×K. Trainable parameters are: (W a , W c ) are both K convolution kernels, (w a , w c ) have shape K × C, w o has shape C × C. N, K, C are introduced below. σ, η are sigmoid and tanh activation functions, * is convolution, is point-wise multiplication. ς, are from the pooling model presented next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Attention Pooling</head><p>Given a matrix view x ik of convolutional feature tensor x where i indexes one of N spatial locations and k indexes one of K feature planes, we aim at suppressing those activations x i that are uncorrelated with the recognition task. That is, we seek a ς(x, w) of shape 1 × N such that parameters w can be tuned in a way that ς(x, w) x are the discriminative features for recognition. For egocentric activity recognition these can be from objects, hands, or implicit patterns representing object-hand interactions during manipulation.</p><p>Our design of ς(x, w) is grounded on the assumption that there is a limited number of pattern categories that are relevant for an activity recognition task. Each category itself can, however, instantiate patterns with high variability during and across executions. We therefore want ς to select from a pool of category-specific mappings, based on the current input x. We want both the selector and the pool of mappings be learnable and self-consistent, and realized with fewer tunable parameters.</p><p>A selector with parameters w maps an image features x into a category-score space C from which the category c * ∈ C obtaining the highest score is returned. Our selector is of the form c * = arg max c π( (x), θ c ) where is a reduction and θ c ∈ w are the parameters for scoring x against category c. If π is chosen to be equivariant to reduction then π( (x), θ c ) = (π(x, θ c )) and we can use { ⊥ (π(·, θ c )), c ∈ C} as the pool of category-specific mappings associated to . Here ⊥ denotes the -orthogonal reduction, e.g. if is max-pooling along one dimension then ⊥ is max-pooling along the other dimensions. That is, our pooling model is determined by the triplet</p><formula xml:id="formula_4">(ς) = ( , π, {θ c }) , π is -equivariant<label>(11)</label></formula><p>and realized on a feature tensor x by</p><formula xml:id="formula_5">ς(x, {θ c }) = ⊥ (π(x, θ c * ))<label>(12)</label></formula><p>where c * = arg max</p><formula xml:id="formula_6">c π( (x), θ c )<label>(13)</label></formula><p>In our model we choose</p><formula xml:id="formula_7">(x) ← spatial average pooling π( , θ c ) ← linear mapping so ς(x, {θ c })</formula><p>is a differentiable spatial mapping, i.e., we can use ς as a trainable attention model for x. This is related to class activation mapping <ref type="bibr" target="#b42">[42]</ref> introduced for discriminative localization. Note however that, in contrast to <ref type="bibr" target="#b42">[42]</ref> that uses strong supervision to train the selector directly, we leverage video-level annotation to implicitly learn an attention mechanism for video classification. Our formulation is also a generalization: other choices are possible for the reduction , and the use of differentiable structured layers <ref type="bibr" target="#b13">[14]</ref> in this context are an interesting direction for future work. To inflate attention in LSTA, we introduce a new state tensor a t of shape N × 1. Its update rule is that of standard LSTM (Eq. 5) with gatings (f a , i a , s t ) and innovation a computed from the pooled ν a = ς(x t , w a ) as input (Eq. 4). We compute the attention tensor s using the hidden state s t η(a t ) as residual (Eq. 6), followed by a softmax calibration. Eqs. 7-10 implement the LSTA memory update based on the filtered input s x t , this is described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Output Pooling</head><p>If we analyze standard LSTM Eq. 2 with input s x t instead of x t , it becomes evident that o t−1 (output gating) has on c t−1 a same effect as s (attention) has on x t . Indeed, in Eq. 7 the gatings and innovation are all computed from [s x t , o t−1 η(c t−1 )]. We build upon this analogy to enhance the output gating capacity of LSTA and, consequently, its forget-update behavior of memory tracking.</p><p>We introduce attention pooling in the output gating update. Instead of computing o t as by Eq. 2 we replace s x t with ν c c t to obtain update Eqs. 9-10, that is</p><formula xml:id="formula_8">σ(W o * [s x t , o t−1 η(c t−1 )]) ← standard gating σ(W o * [ν c c t , o t−1 η(c t−1 )]) with ν c = ς(c t , w c + w o (s x t )) ← output pooling</formula><p>This choice is motivated as follows. We want to preserve the recursive nature of output gating, which is we keep right-concatenating o t−1 η(c t−1 ) to obtain the 2N × K-shaped tensor to convolve and tanh point-wise. Since the new memory state c t is available at this stage, which already integrates s x t , we can use this for leftconcatenating instead of the raw attention-pooled input tensor. This is similar to a peephole connection in the output gate <ref type="bibr" target="#b7">[8]</ref>. We can even produce a filtered version ν c c t of it if we introduce a second attention pooling neuron for localizing the actual discriminative memory component of c t , that is via ν c , Eq. 9. Note that c t integrates information from past memory updates by design, so localizing current activations is pretty much required here. Consequently, and in contrast to feature tensors x t , the memory activations might not be well localized spatially. We thus use a slightly different version of Eq. 12 for output pooling, we remove ⊥ to obtain a full-rank N × K-shaped attention tensor ν c .</p><p>To further enhance active memory localization, we use s x t to control the bias term of attention pooling, Eq. 9. We apply a reduction (s x t ) followed by a linear regression with learnable parameters w o to obtain the instancespecific bias w o (s x t ) for activation mapping. Note that is the reduction associated to ς so this is consistent. We will use a similar idea in Sec. 5.3 for cross-modal fusion in twostream architecture. Our ablation study in Sec. 6.4 confirms that this further coupling of c t with x t boosts the memory distillation in the LSTA recursion, and consequently its tracking capability, by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Two Stream Architecture</head><p>In this section, we explain our network architecture for egocentric activity recognition incorporating the LSTA module of Sec. 4. Like the majority of the deep learning methods proposed for action recognition, we also follow the two stream architecture; one stream for encoding appearance information from RGB frames and the second stream for encoding motion information from optical flow stacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Attention on Appearance Stream</head><p>The network consists of a ResNet-34 pre-trained on im-ageNet for image recognition. We use the output of the last convolution layer of block conv5_3 of ResNet-34 as the input of the LSTA module. From this frame level features, LSTA generates the attention map which is used to weight the input features. We select 512 as the depth of LSTA memory and all the gates use a kernel size of 3 × 3. We use the internal state (c t ) for classification.</p><p>We follow a two stage training. In the first stage, the classifier and the LSTA modules are trained while in the second stage, the convolutional layers in the final block (conv5_x) and the FC layer of ResNet-34 along with the layers trained in stage 1 are trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Attention on Motion Stream</head><p>We use a network trained on optical flow stacks for explicit motion encoding. For this, we use a ResNet-34 CNN. The network is first trained on action verbs (take, put, pour, open, etc.) using an optical flow stack of 5 frames. We average the weights in the input convolutional layer of an imagenet pre-trained network and replicate it 10 times to initialize the input layer. This is analogous to the ima-geNet pre-training done on the appearance stream. The network is then trained for activity recognition as follows. We use the action-pretrained ResNet-34 FC weights as the parameter initialization of attention pooling (Eqs. 12-13) on conv5_3 flow features. We use this attention map to weight the features for classification. Since the activities are temporally located in the videos and they are not sequential in nature, we take the optical flow corresponding to the five frames located in the temporal center of the videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Cross-modal Fusion</head><p>Majority of the existing methods with two stream architecture perform a simple late fusion by averaging for combining the outputs from the appearance and motion streams <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b38">38]</ref>. Feichtenhofer et al. <ref type="bibr" target="#b6">[7]</ref> propose a pooling strategy at the output of the final convolutional layer for improved fusion of the two streams. In <ref type="bibr" target="#b5">[6]</ref> the authors observe that adding a residual connection from the motion stream to the appearance stream enables the network to improve the joint modeling of the information flowing through the two streams. Inspired by the aforementioned observations, we propose a novel cross-modal fusion strategy in the earlier layers of the network in order to facilitate the flow of information across the two modalities.</p><p>In the proposed cross-modal fusion approach, each stream is used to control the biases of the other as follows.</p><p>To perform cross-modal fusion on the appearance stream, the flow feature from the conv5_3 of the motion stream CNN is applied as bias to the gates of the LSTA layer. To perform cross-modal fusion on the motion stream instead, the sequence of features from the conv5_3 of the RGB stream CNN are 3D convolved into a summary feature. We add a ConvLSTM cell of memory size 512 in the motion stream as an embedding layer and use the RGB summary feature to control the bias of the ConvLSTM gates.</p><p>In this way, each individual stream is made to influence the encoding of the other so that we have a flow of information between them deep inside the neural network. We then perform a late average fusion of the two individual streams' output to obtain the class scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets</head><p>We evaluate the proposed method on four standard first person activity recognition datasets namely, GTEA 61, GTEA 71, EGTEA Gaze+ and EPIC-KITCHENS. GTEA 61 and GTEA 71 are relatively small scale datasets with 61 and 71 activity classes respectively. EGTEA Gaze+ is a recently developed large scale dataset with approximately 10K samples having 106 activity classes. EPIC-KITCHENS dataset is the largest egocentric activities dataset available now. The dataset consists of more than 28K video samples with 125 verb and 352 noun classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental Settings</head><p>The appearance and motion networks are first trained separately followed by a combined training of the two stream cross-modal fusion network. We train the networks for minimizing the cross-entropy loss. The appearance stream is trained for 200 epochs in stage 1 with a learning rate of 0.001 which is decayed after 25, 75 and 150 epochs at a rate of 0.1. In the second stage, the network is trained with a learning rate of 0.0001 for 100 epochs. The learning rate is decayed by 0.1 after 25 and 75 epochs. We use ADAM as the optimization algorithm. 25 frames uniformly sampled from the videos are used as input. The number of classes used in the output pooling (w c in 4.2) is chosen as 100 for GTEA 61 and GTEA 71 datasets after empirical evaluation on the fixed split of GTEA 61. For EGTEA Gaze+ and EPIC-KITCHENS datasets, the value is scaled to 150 and 300 respectively, in accordance with the relative increase in the number of activity classes.</p><p>For the pre-training of the motion stream on action classification task, we use a learning rate of 0.01 which is reduced by 0.5 after 75, 150, 250 and 500 epochs and is trained for 700 epochs. In the activity classification stage, we train the network for 500 epochs with a learning rate of 0.01. The learning rate is decayed after 50 and 100 epochs by 0.5. SGD algorithm is used for optimizing the parameter updates of the network.</p><p>The two stream network is trained for 200 epochs for GTEA 61 and GTEA 71 datasets while EGTEA is trained till 100 epochs, with a learning rate of 0.01 using ADAM algorithm. Learning rate is reduced by 0.99 after each epoch. We use a batch size of 32 for all networks. We use random horizontal flipping and multi-scale corner cropping techniques proposed in <ref type="bibr" target="#b38">[38]</ref> during training and the center crop of the frame is used during inference.  <ref type="table">Table 1</ref>: Ablation analysis on GTEA 61 fixed split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation Study</head><p>An extensive ablation analysis 2 has been carried out, on the fixed split of GTEA 61 dataset, to determine the performance improvement obtained by each component of LSTA.</p><p>The results are shown in Tab. 1, which compares the performance of RGB and two stream networks on the top and bottom sections respectively. We choose a network with vanilla ConvLSTM as the baseline since LSTA without attention and output pooling converges to the standard ConvLSTM. The baseline model results in an accuracy of 51.72%. We then analyze the impact of each of the contributions explained in Sec 4. We first analyze the effect of output pooling on the baseline. By adding output pooling the performance is improved by 8%. We analyzed the classes that are improved by adding output pooling over the baseline model and observe that the major improvement is achieved by predicting the correct action classes. Output pooling enables the network to propagate a filtered a version of the memory which is localized on the most discriminative components.</p><p>Adding attention pooling to the baseline improves the performance by 14%. Attention pooling enables the network to identify the relevant regions in the input frame and to maintain a history of the relevant regions seen in the past frames. This enables the network to have a smoother tracking of attentive regions. Detailed analysis show that attention pooling enables the network to correctly classify activities with multiple objects. It should be noted that this is equivalent to a network with two ConvLSTMs, one for attention tracking and one for frame level feature tracking.</p><p>Incorporating both attention and output pooling to the baseline results in a gain of 16%. By analyzing the top improved classes, we found that the model has increased its capacity to correctly classify both actions and objects. By adding bias control, as explained in Sec. 4, we obtain the proposed LSTA model and gains an additional improvement of 6% in recognition accuracy.</p><p>Compared to the network with the vanilla ConvLSTM, LSTA achieves an improvement of 22%. From the previous analyses we have seen the importance of attention pooling and output pooling present in LSTA. This enables the network to focus on encoding the features more relevant for Method Accuracy (%) eleGAtt <ref type="bibr" target="#b41">[41]</ref> 59.48 ego-rnn <ref type="bibr" target="#b32">[32]</ref> 63.79 LSTA 74.14 ego-rnn two stream <ref type="bibr" target="#b32">[32]</ref> 77.59 LSTA two stream 79.31 <ref type="table">Table 2</ref>: Comparative analysis on GTEA 61 fixed split.</p><p>the concrete classification task. Detailed analysis shows ConvLSTM confuses with both activities involving same action with different objects as well as activities consisting of different action with same objects. With the attention mechanism, LSTA weights the most discriminant features, thereby allowing the network to distinguish between the different activity classes. We also evaluated the performance improvement achieved by applying attention to the motion stream. The baseline is a ResNet-34 pre-trained on actions followed by training for activities. We obtained an accuracy of 40.52% for the network with attention compared to the 36.21% of the baseline. <ref type="figure" target="#fig_1">Fig. 2</ref> (fourth row) visualizes the attention map generated by the network. For visualization, we overlay the resized attention map on the RGB frames corresponding to the optical flow stack used as input. From the figure, it can be seen that the network generates the attention map around/near the hands, where the discriminant motion is occurring, thereby enabling the network to recognize the activity undertaken by the user. It can also be seen that the attention maps generated by the appearance stream and the flow stream are complementary to each other; appearance stream focuses on the object regions while the motion stream focuses on hand regions. We also analyzed the classes where the network with attention performs better compared to the standard flow network and found that the network with attention is able to recognize actions better than the standard network. This is because the attention mechanism enables the network to focus on regions where motion is occurring in the frame.</p><p>Next we compare the performance of the cross-modal fusion technique explained in Sec. 5.3 over traditional late fusion two stream approach. The cross-modal fusion approach improves by 1% over late fusion. Analysis shows that the cross-modal fusion approach is able to correctly identify activities with same objects. The fifth and sixth rows of <ref type="figure" target="#fig_1">Fig. 2</ref> visualize the attention maps generated after cross-modal fusion training. It can be seen that the motion stream attention expands to regions containing objects. This validates the effect of cross-modal fusion where the two networks are made to interact deep inside the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Comparative Analysis</head><p>In this section, we compare the performance of LSTA over two closely related methods, namely, eleGAtt <ref type="bibr" target="#b41">[41]</ref> and ego-rnn <ref type="bibr" target="#b32">[32]</ref>. Results are shown in Tab. 2. EleGAtt is an attention mechanism which can be applied to any generic RNN using its hidden state for generating the attention map. We evaluated eleGAtt on LSTM, consisting of 512 hidden units, with the same training setting as LSTA for fair comparison. EleGAtt learns a single weight matrix for generating the attention map irrespective of the input whereas LSTA generates the attention map from a pool of weights which are selected in a top-down manner based on input. This enables the selection of a proper attention map for each input activity class. This leads to a performance gain of 13% over eleGAtt. Analyzing the classes with the highest improvement by LSTA compared to eleGAtt reveals that el-eGAtt fails in identifying the object while correctly classifying the action. Ego-rnn <ref type="bibr" target="#b32">[32]</ref> derives an attention map generated from class activation map to weight the discriminant regions in the image which are then applied to a ConvLSTM cell for temporal encoding. It generates a per frame attention map which has no dependency on the information present in the previous frames. This can result in selecting different objects in adjacent frames. On the contrary, LSTA uses an attention memory to track the previous attention maps enabling their smooth tracking. This results in a 10% improvement obtained by LSTA over ego-rnn. Detailed analysis on the classification results show that ego-rnn struggles to classify activities involving multiple objects. Since the attention map generated in each frame is independent of the previous frames, the network fails to track previously activated regions, thereby resulting in wrong predictions. This is further illustrated by visualizing the attention maps produced by ego-rnn and LSTA in <ref type="figure" target="#fig_1">Fig. 2</ref>. From the figure, one can see that ego-rnn (second row) fails to identify the relevant object in the case of close chocolate example and it failed to track the object in the final frames in the case of the scoop coffee example. LSTA with cross-modal fusion performs 2% better than ego-rnn two stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">State-of-the-art comparison</head><p>Our approach is compared against the state-of-the-art methods on Tab. 3. The methods listed in the first section of the table uses strong supervision signals such as gaze <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>, hand segmentation <ref type="bibr" target="#b21">[21]</ref> or object bounding boxes <ref type="bibr" target="#b21">[21]</ref> during the training stage. Two stream <ref type="bibr" target="#b29">[29]</ref>, I3D <ref type="bibr" target="#b2">[3]</ref> and TSN <ref type="bibr" target="#b38">[38]</ref> are methods proposed for action recognition from third person videos while all other methods except eleGAtt <ref type="bibr" target="#b41">[41]</ref> are proposed for first-person activity recognition. ele-GAtt <ref type="bibr" target="#b41">[41]</ref> is proposed as a generic method for incorporating attention mechanism to any RNN module. From the table, we can see that the proposed method outperforms all the existing methods for egocentric activity recognition.</p><p>In EPIC-KITCHENS dataset, the labels are provided in the form of verb and noun, which are combined to form an activity class. The fact that not all combinations of verbs   <ref type="table">Table 3</ref>: Comparison with state-of-the-art methods on popular egocentric datasets, we report recognition accuracy in %. ( * : fixed split; * * : trained with strong supervision). and nouns are feasible and that not all test classes might have a representative training sample make it a challenging problem. We train the network for multi-task classification with verb, noun and activity supervision. We use activity classifier activations to control the bias of verb and noun classifiers. The dataset provides two evaluation settings, seen kitchens (S1) and unseen kitchens (S2).</p><p>We obtained an accuracy of 30.16% (S1) and 15.88% (S2) using RGB frames. The best performing baseline is a two stream TSN that achieves 20.54% (S1) and 10.89% (S2) <ref type="bibr" target="#b3">[4]</ref>. Our model is particularly strong on verb prediction (58%)</p><p>where we gain +10% points over TSN. verb in this context is typically describing actions that develop into an activity over time, confirming once more LSTA efficiently learns encoding of sequences with localized patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We presented LSTA that extends LSTM with two core features: 1) attention pooling that spatially filters the input sequence and 2) output pooling that exposes a distilled view of the memory at each iteration. As shown in a detailed ablation study, both contributions are essential for a smooth and focused tracking of a latent representation of the video to achieve superior performance in classification tasks where the discriminative features can be localized spatially. We demonstrate its practical benefits for egocentric activity recognition with a two stream CNN-LSTA architecture featuring a novel cross-modal fusion and we achieve state-of-the-art accuracy on four standard benchmarks. For this, we compute the action recognition and object recognition performance of a network trained for activity recognition. There are some activity classes with multiple objects and these objects are combined to form a metaobject class for this analysis. It can be seen that adding output pooling to the ConvLSTM improves the network's capability in recognizing different actions with the same objects (take_water/pour_water,cup and close_water/take_water). This confirms our hypothesis that the output gating of LSTM affects memory tracking, replacing the output gating of LSTM with the proposed output pooling technique localizes the active memory component. This improves the tracking of relevant spatiotemporal patterns in the memory and consequently boosts recognition performance. A gain of 13.79% is achieved for action recognition as shown in Tab. 4.</p><p>In <ref type="figure" target="#fig_5">Fig. 4</ref>, we can see that the network with the attention pooling described in Sec. 4.1 improves the categories with different actions and same objects as well as activity classes with multiple objects (stir_spoon,cup/pour_sugar,spoon,cup; put_cheese,bread/take_bread; pour_coffee,spoon,cup/scoop_coffee,spoon, etc.). Attention helps the network to encode the features from the spatially relevant areas. This allows the network to keep a track of the active object regions and improves the performance. From Tab. 4, a gain of 20.69% is obtained for object recognition which gives further validation regarding the importance of attention.</p><p>Adding both attention pooling and output pooling further improves the network's capability in distinguishing between different actions with same objects and same actions with different objects. This is visible in <ref type="figure" target="#fig_6">Fig. 5</ref> and also from the 13.72% and 18.1% performance gain obtained for action and object recognition, respectively.</p><p>Incorporating bias control, introduced in Sec. 4.2, to the output pooling results in the proposed method, LSTA, which further improves the capacity of the network in recognizing activities <ref type="figure">(Fig. 6</ref>). This further verifies the hypothesis in Sec. 4.2 that bias control increases the active memory localization of the network. This is also evident from Tab. 4 where an increase of 22.41% is obtained for action recognition.</p><p>It is worth noting that output pooling boosts action recognition performance more (+13.79% action vs +12,07% object) while with attention pooling the object recognition performance receives a higher gain (+12,93% vs +16,38%). Coupling attention and output pooling through bias control finally boosts performance by a significant margin on both (+22.41% vs +21,55%). This provides further evidence that the two contributions are complementary and reflects the intuitions behind the design choices of LSTA, making the improvements explainable and the benefits of each of the contributions transparently confirmed by this analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Comparative Analysis</head><p>Figs. 7 -9 compares our method with state-of-theart alternatives discussed in Sec. 2.3, ego-rnn <ref type="bibr" target="#b32">[32]</ref> and eleGatt <ref type="bibr" target="#b41">[41]</ref>. Compared to ego-rnn, LSTA is capable of identifying activities involving multiple objects (pour_mustard,hotdog,bread/pour_mustard, cheese,bread; pour_honey,cup/pour_honey, bread;</p><p>put_hotdog,bread/spread_peanut, spoon,bread, etc.). This may be attributed to the attention mechanism with memory for tracking previously attended regions, helping the network attending to the same objects in subsequent frames. From <ref type="figure">Fig. 8</ref> it can be seen that eleGAtt-LSTM fails to identify the objects correctly (take_mustard/take_honey; take_bread/take_spoon; take_spoon/take_honey, etc.). This shows the attention map generated by LSTA selects more relevant regions compared to eleGAtt-LSTM .  <ref type="table">Table 4</ref>: Detailed ablation analysis on GTEA 61 fixed split. We compute the action and object recognition score by decomposing the action and objects from the predicted activity label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Confusion Matrix</head><p>Figs. 10 -12 show the confusion matrix of the LSTA (two stream cross-modal fusion) for all the datasets explained in Sec. 6.1 of the manuscript. We average the confusion matrices of each of the available train/test splits to generate a single confusion matrix representing the dataset under consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">EPIC-KITCHENS</head><p>We compare the recognition accuracies obtained for EPIC-KITCHENS dataset with the currently available baselines <ref type="bibr" target="#b3">[4]</ref> in Tab. 5. As explained in Sec. 6.6 in the paper, we train the network for predicting verb and noun and activity classes. Our two stream cross-modal fusion model obtains an activity recognition performance of 30.33% and 16.63% on S1 and S2 settings as opposed to the 20.54% and 10.89% obtained by TSN strongest baseline (two stream). It is also worth noting that our model is strong on predicting verb (+11.32% points on S1 setting over strongest baseline). This indicates LSTA accurately performs encoding of sequences, indeed verb in this context is typically describing actions that develop into an activity over time, and this is learned effectively with LSTA just using video-level supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Attention Map Visualization</head><p>Figs 13 -17 visualize the generated attention maps for different video sequences. In <ref type="figure" target="#fig_0">Figs. 13 -15</ref>, one can see that LSTA is able to successfully identify the relevant regions and track them across the sequences while ego-rnn misses the regions in some frames. This shows the ability of LSTA in identifying and tracking the discriminant regions that are relevant for classifying the activity category. However, in <ref type="figure" target="#fig_0">Figs. 16 and 17</ref>, the network fails to recognize the relevant regions. In both of these video sequences, the object is not present in the first few frames and the network attends to wrong regions, failing to move its attention towards the object when it appears. Since the proposed method maintains a memory of attention maps, occlusion of the relevant object in the initial frames results in the network attending to the wrong regions in the frame.               <ref type="figure" target="#fig_0">Figure 13</ref>: Attention maps generated by ego-rnn (second row) and LSTA (third) for scoop sugar,spoon video sequence. We show the 5 frames that are uniformly sampled from the 25 frames used as input to the corresponding networks. Fourth row shows the attention map generated by the motion stream. For flow, we visualize the attention map on the five frames corresponding to the optical flow stack given as input.</p><p>Input ego-rnn LSTA Flow <ref type="figure" target="#fig_0">Figure 14</ref>: Attention maps generated by ego-rnn (second row) and LSTA (third) for take water video sequence. We show the 5 frames that are uniformly sampled from the 25 frames used as input to the corresponding networks. Fourth row shows the attention map generated by the motion stream. For flow, we visualize the attention map on the five frames corresponding to the optical flow stack given as input.</p><p>Input ego-rnn LSTA Flow <ref type="figure" target="#fig_0">Figure 15</ref>: Attention maps generated by ego-rnn (second row) and LSTA (third) for shake tea,cup video sequence. We show the 5 frames that are uniformly sampled from the 25 frames used as input to the corresponding networks. Fourth row shows the attention map generated by the motion stream. For flow, we visualize the attention map on the five frames corresponding to the optical flow stack given as input.</p><p>Input ego-rnn LSTA Flow <ref type="figure" target="#fig_0">Figure 16</ref>: Attention maps generated by ego-rnn (second row) and LSTA (third) for take bread video sequence. We show the 5 frames that are uniformly sampled from the 25 frames used as input to the corresponding networks. Fourth row shows the attention map generated by the motion stream. For flow, we visualize the attention map on the five frames corresponding to the optical flow stack given as input.</p><p>Input ego-rnn LSTA Flow <ref type="figure" target="#fig_0">Figure 17</ref>: Attention maps generated by ego-rnn (second row) and LSTA (third) for take spoon video sequence. We show the 5 frames that are uniformly sampled from the 25 frames used as input to the corresponding networks. Fourth row shows the attention map generated by the motion stream. For flow, we visualize the attention map on the five frames corresponding to the optical flow stack given as input.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>LSTA extends LSTM with two novel components: recurrent attention and output pooling. The first (red part) tracks a weight map s to focus on relevant features, while the second (green part) introduces a high-capacity output gate. At the core of both is a pooling operation ς, that selects one out of a pool of specialized mappings to realize smooth attention tracking and flexible output gating. Circles indicate point-wise or concat operations, square blocks are linear/convolutional parametric nodes with non-linearities indicated by their symbols. Recurrent variables in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Attention maps generated by ego-rnn (second row) and LSTA (third) for two video sequences. We show the 5 frames that are uniformly sampled from the 25 frames used as input to the corresponding networks. Fourth row shows the attention map generated by the motion stream. Fifth and sixth rows show the attention map generated by the appearance and flow streams after two stream cross-modal training. For flow, we visualize the attention map on the five frames corresponding to the optical flow stack given as input. ( * : Attention map obtained after two stream cross-modal fusion training).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figs. 3 -</head><label>3</label><figDesc>6 show details of the classes which are improved by proposed LSTA variants over the baseline (ConvLSTM) and the difference of the confusion matrices. We show the top 25 improved classes in the comparison graphs and those with less number list all the improved classes. The difference of confusion matrices show the overall details of the classes which are improved. Ideally, the positive values should be in the diagonal and the negative values offdiagonal. Tab. 4 lists a breakdown of the recognition performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>compares the baseline (ConvLSTM) with a network having baseline+output pooling, as explained in Sec. 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>(a) Most improvement categories by adding output pooling to the baseline on GTEA 61 fixed split. X axis labels are in the format true label (baseline + output pooling)/predicted label (baseline). Y axis shows the number of corrected samples for each class. (b) shows the difference of confusion matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>(a) Most improvement categories by adding attention pooling to the baseline on GTEA 61 fixed split. X axis labels are in the format true label (baseline + attention pooling)/predicted label (baseline). Y axis shows the number of corrected samples for each class. (b) shows the difference of confusion matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Most improvement categories by adding both attention and output pooling to the baseline on GTEA 61 fixed split. X axis labels are in the format true label (baseline + pooling)/predicted label (baseline). Y axis shows the number of corrected samples for each class. (b) shows the difference of confusion matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :Figure 8 :</head><label>68</label><figDesc>Most improvement categories by adding attention and output pooling with bias control (full LSTA model) to the baseline on GTEA 61 fixed split. X axis labels are in the format true label (LSTA)/predicted label (baseline). Y axis shows the number of corrected samples for each class. (b) shows the difference of confusion matrices. (a) Most improvement categories by LSTA over eleGAtt-LSTM on GTEA 61 fixed split. X axis labels are in the format true label (LSTA)/predicted label (eleGAtt-LSTM). Y axis shows the number of corrected samples for each class. (b) shows the difference of confusion matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>(a) Most improvement categories by two stream cross-modal fusion over two stream on GTEA 61 fixed split. X axis labels are in the format true label (two stream cross-modal fusion)/predicted label (two stream late fusion). Y axis shows the number of corrected samples for each class. (b) shows the difference of confusion matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Confusion matrix of EGTEA Gaze+ averaged across the three train/test splits. Input ego-rnn LSTA Flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>36.80 19.86 85.56 64.19 41.89 61.64 34.32 11.02 23.81 31.62 9.76 TSN (two stream) 48.23 36.71 20.54 84.09 62.32 39.79 47.26 35.42 11.57 22.33 30.53 9.78 LSTA (RGB) 58.25 38.93 30.16 86.57 62.96 50.16 44.09 36.30 16.54 37.32 36.52 19.00 LSTA (two stream) 59.55 38.35 30.33 85.77 61.49 49.97 42.72 36.19 14.46 38.12 36.19 17.76 45.51 23.46 15.88 75.25 43.16 30.01 26.19 17.58 8.44 20.80 19.67 11.29 LSTA (two stream) 47.32 22.16 16.63 77.02 43.15 30.93 31.57 17.91 8.97 26.17 17.80 11.92</figDesc><table><row><cell>Method</cell><cell cols="3">Top-1 Accuracy (%)</cell><cell cols="2">Top-5 Accuracy (%)</cell><cell cols="2">Precision (%)</cell><cell>Recall (%)</cell></row><row><cell></cell><cell>Verb</cell><cell cols="3">Noun Action Verb</cell><cell>Noun Action Verb</cell><cell cols="2">Noun Action Verb</cell><cell>Noun Action</cell></row><row><cell>2SCNN (RGB)</cell><cell cols="6">40.44 30.46 13.67 83.04 57.05 33.25 34.74 28.23</cell><cell>6.66</cell><cell>15.90 23.23</cell><cell>5.47</cell></row><row><cell cols="7">2SCNN (two stream) 42.16 29.14 13.23 80.58 53.70 30.36 29.39 30.73 TSN (RGB) 2SCNN (RGB) 34.89 21.82 10.11 74.56 45.34 25.33 19.48 14.67 2SCNN (two stream) 36.16 18.03 7.31 71.97 38.41 19.49 18.11 15.31 45.68 S2 S1 TSN (RGB) 34.89 21.82 10.11 74.56 45.34 25.33 19.48 14.67</cell><cell>5.92 5.32 3.19 5.32</cell><cell>14.83 21.10 11.22 17.24 10.52 12.55 11.22 17.24</cell><cell>4.93 6.34 3.00 6.34</cell></row><row><cell>TSN (two stream)</cell><cell>39.4</cell><cell>22.7</cell><cell cols="4">10.89 74.29 45.72 25.26 22.54 15.33</cell><cell>6.21</cell><cell>13.06 17.52</cell><cell>6.49</cell></row><row><cell>LSTA (RGB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison of recognition accuracies with state-of-the-art in EPIC-KITCHENS dataset.</figDesc><table><row><cell></cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell></row><row><cell>True</cell><cell>Inspect/Read recipe Open fridge Take eating_utensil Cut tomato Turn on faucet Put eating_utensil Open cabinet Take condiment_container Cut cucumber Operate stove Close fridge Cut carrot Put condiment_container Cut onion Open drawer Take plate Take bowl Put bowl Put trash,trash_container Put plate Cut bell_pepper Put cooking_utensil Take paper_towel Move Around bacon Open condiment_container Wash eating_utensil Spread condiment,bread,eating_utensil Turn off faucet Put pan Take cooking_utensil Put lettuce Move Around patty Put pot Close cabinet Put bread Take bread Close condiment_container Open fridge_drawer Wash hand Put tomato Take seasoning_container Take cup Divide/Pull Apart lettuce Put cup Take pot Clean/Wipe counter Take bread_container Take tomato Take pan Move Around pan Wash cutting_board Put bread_container Take sponge Take lettuce Take onion Put sponge Put oil_container Close oil_container Open cheese_container Take egg Put grocery_bag Move Around pot Move Around bowl Wash strainer Close fridge_drawer Operate microwave Squeeze washing_liquid,sponge Open oil_container Take cutting_board Crack egg Put onion Put cheese_container Compress sandwich Put cucumber Put paper_towel Pour water,faucet,pot Mix egg Pour seasoning,seasoning_container,salad Take cheese Take grocery_bag Open bread_container Mix pasta Wash pot Pour condiment,condiment_container,salad Close drawer Cut olive Wash bowl Put tomato_container Cut lettuce Take pasta_container Pour oil,oil_container,pan Put bell_pepper Move Around eating_utensil Put cheese Take tomato_container Mix mixture,eating_utensil Put cutting_board Divide/Pull Apart onion Take bell_pepper Wash pan Take cucumber Put seasoning_container Take oil_container Take cheese_container Open dishwasher Divide/Pull Apart paper_towel</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Inspect/Read recipe Open fridge Take eating_utensil Cut tomato Turn on faucet Put eating_utensil Open cabinet Take condiment_container Cut cucumber Operate stove Close fridge Cut carrot Put condiment_container Cut onion Open drawer Take plate Take bowl Put bowl Put trash,trash_container Put plate Cut bell_pepper Put cooking_utensil Take paper_towel Move Around bacon Open condiment_container Wash eating_utensil Spread condiment,bread,eating_utensil Turn off faucet Put pan Take cooking_utensil Put lettuce Move Around patty Put pot Close cabinet Put bread Take bread Close condiment_container Open fridge_drawer Wash hand Put tomato Take seasoning_container Take cup Divide/Pull Apart lettuce Put cup Take pot Clean/Wipe counter Take bread_container Take tomato Take pan Move Around pan Wash cutting_board Put bread_container Take sponge Take lettuce Take onion Put sponge Divide/Pull Apart paper_towel Open dishwasher Take cheese_container Take oil_container Put seasoning_container Take cucumber Wash pan Take bell_pepper Divide/Pull Apart onion Put cutting_board Mix mixture,eating_utensil Take tomato_container Put cheese Move Around eating_utensil Put bell_pepper Pour oil,oil_container,pan Take pasta_container Cut lettuce Put tomato_container Wash bowl Cut olive Close drawer Pour condiment,condiment_container,salad Wash pot Mix pasta Open bread_container Take grocery_bag Take cheese Pour seasoning,seasoning_container,salad Mix egg Pour water,faucet,pot Put paper_towel Put cucumber Compress sandwich Put cheese_container Put onion Crack egg Take cutting_board Open oil_container Squeeze washing_liquid,sponge Operate microwave Close fridge_drawer Wash strainer Move Around bowl Move Around pot Put grocery_bag Take egg Open cheese_container Close oil_container Put oil_container</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Predicted</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at https://github.com/swathikirans/LSTA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Detailed analysis available in the supplementary document.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work has been partially supported by the Spanish project TIN2016-74946-P (MINECO/FEDER, UE), CERCA Programme / Generalitat de Catalunya and ICREA under the ICREA Academia programme. We gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Egocentric gesture recognition using recurrent 3d convolutional neural networks with spatiotemporal transformer modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent spatial-temporal attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1347" to="1360" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent nets that time and count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks(IJCNN)</title>
		<meeting>the IEEE-INNS-ENNS International Joint Conference on Neural Networks(IJCNN)</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to Forget: Continual Prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matrix Backpropagation for Deep Networks with Structured Layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">In the eye of beholder: Joint learning of gaze and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving into Egocentric Actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Videolstm convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focal visual-text attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attend and interact: Higher-order object interactions for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Going deeper into firstperson activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning latent sub-events in activity videos using temporal attention filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pooled motion features for first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLRW</title>
		<meeting>ICLRW</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Egocentric activity prediction via event modulated attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Actor and observer: Joint modeling of first and thirdperson videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-Stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">First person action recognition using deep learned descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional long short-term memory networks for recognizing first person interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCVW</title>
		<meeting>ICCVW</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all we need: Nailing down object-centric attention for egocentric activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Top-down attention recurrent vlad encoding for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Conference of the Italian Association for Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action recognition in rgb-d egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-stream deep neural networks for rgb-d egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Making third person techniques recognize first-person actions in egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bidirectional attentive fusion with context gating for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling sub-event dynamics in first-person action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F M</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adding attentiveness to the neurons in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapedriza</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cascaded interactional targeting network for egocentric video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
