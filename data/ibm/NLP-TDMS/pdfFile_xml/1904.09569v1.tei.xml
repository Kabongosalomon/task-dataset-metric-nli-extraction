<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple Pooling-Based Design for Real-Time Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">TKLNDST</orgName>
								<orgName type="department" key="dep2">College of CS</orgName>
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">TKLNDST</orgName>
								<orgName type="department" key="dep2">College of CS</orgName>
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">TKLNDST</orgName>
								<orgName type="department" key="dep2">College of CS</orgName>
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NUS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shenzhen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple Pooling-Based Design for Real-Time Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We solve the problem of salient object detection by investigating how to expand the role of pooling in convolutional neural networks. Based on the U-shape architecture, we first build a global guidance module (GGM) upon the bottom-up pathway, aiming at providing layers at different feature levels the location information of potential salient objects. We further design a feature aggregation module (FAM) to make the coarse-level semantic information well fused with the fine-level features from the top-down pathway. By adding FAMs after the fusion operations in the topdown pathway, coarse-level features from the GGM can be seamlessly merged with features at various scales. These two pooling-based modules allow the high-level semantic features to be progressively refined, yielding detail enriched saliency maps. Experiment results show that our proposed approach can more accurately locate the salient objects with sharpened details and hence substantially improve the performance compared to the previous state-of-the-arts. Our approach is fast as well and can run at a speed of more than 30 FPS when processing a 300 × 400 image. Code can be found at http://mmcheng.net/poolnet/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Benefiting from the capability of detecting the most visually distinctive objects from a given image, salient object detection plays an important role in many computer vision tasks, such as visual tracking <ref type="bibr" target="#b7">[8]</ref>, content-aware image editing <ref type="bibr" target="#b3">[4]</ref>, and robot navigation <ref type="bibr" target="#b4">[5]</ref>. Traditional methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3]</ref> mostly rely on hand-crafted features to capture local details and global context separately or simultaneously, but the lack of high-level semantic information restricts their ability to detect the integral salient objects in complex scenes. Luckily, convolutional neural networks (CNNs) greatly promote the development of salient object detection models because of their capability of extracting both high-level semantic information and low-level * Indicates equal contributions. † M.M. Cheng (cmm@nankai.edu.cn) is the corresponding author.</p><p>detail features in multiple scale space.</p><p>As pointed out in many previous approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44]</ref>, because of the pyramid-like structural characteristics of CNNs, shallower stages usually have larger spatial sizes and keep rich, detailed low-level information while deeper stages contain more high-level semantic knowledge and are better at locating the exact places of salient objects. Based on the aforementioned knowledge, a variety of new architectures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b9">10]</ref> for salient object detection have been designed. Among these approaches, U-shape based structures <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22]</ref> receive the most attentions due to their ability to construct enriched feature maps by building top-down pathways upon classification networks.</p><p>Despite the good performance achieved by this type of approaches, there is still a large room for improving it. First, in the U-shape structure, high-level semantic information is progressively transmitted to shallower layers, and hence the location information captured by deeper layers may be gradually diluted at the same time. Second, as pointed out in <ref type="bibr" target="#b46">[47]</ref>, the receptive field size of a CNN is not proportional to its layer depth. Existing methods solve the abovementioned problems by introducing attention mechanisms <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b23">24]</ref> into U-shape structures, refining feature maps in a recurrent way <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b35">36]</ref>, combining multi-scale feature information <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b9">10]</ref>, or add extra constraints to saliency maps like the boundary loss term in <ref type="bibr" target="#b27">[28]</ref>.</p><p>In this paper, different from the methods mentioned above, we investigate how to solve these problems by expanding the role of the pooling techniques in U-shape based architectures. In general, our model consists of two primary modules on the base of the feature pyramid networks (FPNs) <ref type="bibr" target="#b21">[22]</ref>: a global guidance module (GGM) and a feature aggregation module (FAM). As shown in <ref type="figure">Fig. 1</ref>, our GGM composes of a modified version of pyramid pooling module (PPM) and a series of global guiding flows (GGFs). Unlike <ref type="bibr" target="#b36">[37]</ref> which directly plugs PPM into the U-shape networks, our GGM is an individual module. More specifically, the PPM is placed on the top of the backbone to capture global guidance information (where the salient objects are). By introducing GGFs, high-level semantic information collected by PPM can be delivered to feature maps at 1 arXiv:1904.09569v1 [cs.CV] 21 Apr 2019  <ref type="figure">Figure 1</ref>. The overall pipeline of our proposed approach. For clarity, we also place a standard U-shape FPN structure <ref type="bibr" target="#b21">[22]</ref> at the top-left corner. The top part for edge detection is optional.</p><formula xml:id="formula_0">R R F F F F F F P P P F F R R R R F F F F F F</formula><p>all pyramid levels, remedying the drawback of U-shape networks that top-down signals are gradually diluted. Taking into account the fusion problem of the coarse-level feature maps from GGFs with the feature maps at different scales of the pyramid, we further propose a feature aggregation module (FAM), which takes the feature maps after fusion as input. This module first converts the fused feature maps into multiple feature spaces to capture local context information at different scales and then combines the information to weigh the compositions of the fused input feature maps better.</p><p>As both the above modules are based on the pooling techniques, we call our method PoolNet. To the best of our knowledge, this is the first paper that aims at studying how to design various pooling-based modules to assist in improving the performance for salient object detection. As an extension of this work, we also equip our architecture with an edge detection branch to further sharpen the details of salient objects by joint training our model with edge detection. To evaluate the performance of our proposed approach, we report results on multiple popular salient object detection benchmarks. Without bells and whistles, our PoolNet surpasses all previous state-of-the-art methods in a large margin. In addition, we conduct a series of ablation experiments to let readers better understand the impact of each component in our architecture on the performance and show how joint training with edge detection helps enhance the details of the predicted results.</p><p>Our network can run at a speed of more than 30 FPS on a single NVIDIA Titan Xp GPU for an input image with size 300 × 400. When the edge branch is not incorporated, training only takes less than 6 hours on a training set of 5,000 images, which is quite faster than most of the previous methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b8">9]</ref>. This is mainly due to the effective utilization of pooling techniques. PoolNet, therefore, can be viewed as a baseline to help ease future research in salient object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, benefiting from the powerful feature extraction capability of CNNs, most of the traditional saliency detection methods based on hand-crafted features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31]</ref> have been gradually surpassed. Li et al. <ref type="bibr" target="#b17">[18]</ref> used the multi-scale features extracted from a CNN to compute the saliency value for each super-pixel. Wang et al. <ref type="bibr" target="#b33">[34]</ref> adopted two CNNs, aiming at combining local super-pixel estimation and global proposal searching together, to produce saliency maps. Zhao et al. <ref type="bibr" target="#b47">[48]</ref> presented a multicontext deep learning framework which extracts both local and global context information by employing two independent CNNs. Lee et al. <ref type="bibr" target="#b5">[6]</ref> combined low-level heuristic features, such as color histogram and Gabor responses, with high-level features extracted from CNNs. All these methods take image patches as the inputs of CNNs and hence are time-consuming. Moreover, they ignore the essential spatial information of the whole input image.</p><p>To overcome the above problems, more research attentions are put on predicting pixel-wise saliency maps, inspired by the fully convolutional networks <ref type="bibr" target="#b26">[27]</ref>. Wang et al. <ref type="bibr" target="#b35">[36]</ref> generated saliency prior maps using low-level cues and further exploited it to guide the prediction of saliency recurrently. Liu et al. <ref type="bibr" target="#b22">[23]</ref> proposed a two-stage network which produces coarse saliency maps first and then integrates local context information to refine them recurrently and hierarchically. Hou et al. <ref type="bibr" target="#b8">[9]</ref> introduced short connections into multi-scale side outputs to capture fine details. Luo et al. <ref type="bibr" target="#b27">[28]</ref> and Zhang et al. <ref type="bibr" target="#b43">[44]</ref> both advanced the Ushape structures and utilized multiple levels of context in-formation for accurate detection of salient objects. Zhang et al. <ref type="bibr" target="#b45">[46]</ref> and Liu et al. <ref type="bibr" target="#b23">[24]</ref> combined attention mechanisms with U-shape models to guide the feature integration process. Wang et al. <ref type="bibr" target="#b37">[38]</ref> proposed a network to recurrently locate the salient object and then refine them with local context information. Zhang et al. <ref type="bibr" target="#b42">[43]</ref> used a bi-directional structure to pass messages between multi-level features extracted by CNNs for better predicting saliency maps. Xiao et al. <ref type="bibr" target="#b38">[39]</ref> adopted one network to tailor the distracting regions first and then used another network for saliency detection.</p><p>Our method is quiet different from the above approaches. Instead of exploring new network architectures, we investigate how to apply the simple pooling techniques to CNNs to simultaneously improve the performance and accelerate the running speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PoolNet</head><p>It has been pointed out in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> that high-level semantic features are helpful for discovering the specific locations of salient objects. At the meantime, low-and midlevel features are also essential for improving the features extracted from deep layers from coarse level to fine level. Based on the above knowledge, in this section, we propose two complementary modules that are capable of accurately capturing the exact positions of salient objects and meanwhile sharpening their details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Pipeline</head><p>We build our architecture based on the feature pyramid networks (FPNs) <ref type="bibr" target="#b21">[22]</ref> which are a type of classic U-shape architectures designed in a bottom-up and top-down manner as shown at the top-left corner of <ref type="figure">Fig. 1</ref>. Because of the strong ability to combine multi-level features from classification networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref>, this type of architectures has been widely adopted in many vision tasks, including salient object detection. As shown in <ref type="figure">Fig. 1</ref>, we introduce a global guidance module (GGM) which is built upon the top of the bottom-up pathway. By aggregating the high-level information extracted by GGM with into feature maps at each feature level, our goal is to explicitly notice the layers at different feature levels where salient objects are. After the guidance information from GGM is merged with the features at different levels, we further introduce a feature aggregation module (FAM) to ensure that feature maps at different scales can be merged seamlessly. In what follows, we describe the structures of the above mentioned two modules and explain their functions in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Global Guidance Module</head><p>FPNs provide a classic architecture for combining multilevel features from the classification backbone. However, because the top-down pathway is built upon the bottom-up backbone, one of the problems to this type of U-shape architectures is that the high-level features will be gradually diluted when they are transmitted to lower layers. It is shown in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b46">47]</ref> that the empirical receptive fields of CNNs are much smaller than the ones in theory especially for deeper layers, so the receptive fields of the whole networks are not large enough to capture the global information of the input images. The immediate effect on this is that only parts of the salient objects can be discovered as shown in <ref type="figure" target="#fig_0">Fig. 2c</ref>. Regarding the lack of high-level semantic information for fine-level feature maps in the top-down pathway, we introduce a global guidance module which contains a modified version of pyramid pooling module (PPM) <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b36">37]</ref> and a series of global guiding flows (GGFs) to explicitly make feature maps at each level be aware of the locations of the salient objects.</p><formula xml:id="formula_1">(a) (b) (c) (d) (e) (f) (g)</formula><p>To be more specific, the PPM in our GGM consists of four sub-branches to capture the context information of the input images. The first and last sub-branches are respectively an identity mapping layer and a global average pooling layer. For the two middle sub-branches, we adopt the adaptive average pooling layer 1 to ensure the output feature maps of them are with spatial sizes 3 × 3 and 5 × 5, respectively. Given the PPM, what we need to do now is how to guarantee that the guidance information produced by PPM can be reasonably fused with the feature maps at different levels in the top-down pathway.</p><p>Quite different from the previous work <ref type="bibr" target="#b36">[37]</ref> which simply views the PPM as a part of the U-shape structure, our GGM is independent of the U-shape structure. By introducing a series of global guiding flows (identity mappings), the high-level semantic information can be easily delivered to feature maps at various levels (see the green arrows in <ref type="figure">Fig. 1</ref>). In this way, we explicitly increase the weight of the global guidance information in each part of the top-down pathway to make sure that the location information will not be diluted when building FPNs.</p><p>To better demonstrate the effectiveness of our GGM, we  show some visual comparisons. As depicted in <ref type="figure" target="#fig_0">Fig. 2c</ref>, we show some saliency maps produced by a VGGNet version of FPNs 2 . It can be easily found that with only the FPN backbone, it is difficult to locate salient objects for some complex scenes. There are also some results in which only parts of the salient object are detected. However, when our GGM is incorporated, the quality of the resulting saliency maps are greatly improved. As shown in <ref type="figure" target="#fig_0">Fig. 2f</ref>, salient objects can be precisely discovered, which demonstrates the importance of GGM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Aggregation Module</head><p>The utilization of our GGM allows the global guidance information to be delivered to feature maps at different pyramid levels. However, a new question that deserves asking is how to make the coarse-level feature maps from GGM seamlessly merged with the feature maps at different scales of the pyramid. Taking the VGGNet version of FPNs as an example, feature maps corresponding to C = {C 2 , C 3 , C 4 , C 5 } in the pyramid have downsampling rates of {2, 4, 8, 16} compared to the size of the input image, respectively. In the original top-down pathway of FPNs, feature maps with coarser resolutions are upsampled by a factor of 2. Therefore, adding a convolutional layer with kernel size 3 × 3 after the merging operation can effectively reduce the aliasing effect of upsampling. However, our GGFs need larger upsampling rates (e.g. , <ref type="bibr" target="#b7">8)</ref>. It is essential to bridge the big gaps between GGFs and the feature maps of different scales effectively and efficiently.</p><p>To this end, we propose a series of feature aggregation modules, each of which contains four sub-branches as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. In the forward pass, the input feature map is first converted to different scale spaces by feeding it into <ref type="bibr" target="#b1">2</ref> Similarly to <ref type="bibr" target="#b21">[22]</ref>, we use the feature maps outputted by conv2, conv3, conv4, conv5 which are denoted by {C 2 , C 3 , C 4 , C 5 } to build the feature pyramid upon the VGGNet <ref type="bibr" target="#b32">[33]</ref>. The channel numbers corresponding to {C 2 , C 3 , C 4 , C 5 } are set to {128, 256, 512, 512}, respectively. average pooling layers with varying downsampling rates. The upsampled feature maps from different sub-branches are then merged together, followed by a 3 × 3 convolutional layer. Generally speaking, our FAM has two advantages. First, it assists our model in reducing the aliasing effect of upsampling, especially when the upsampling rate is large (e.g. , <ref type="bibr" target="#b7">8</ref>). In addition, it allows each spatial location to view the local context at different scale spaces, further enlarging the receptive field of the whole network. To the best of our knowledge, this is the first work revealing that FAMs are helpful for reducing the aliasing effect of upsampling.</p><formula xml:id="formula_2">F F F GGFs FAM FAM F F F GGFs (a) (b) (c) (d) conv conv a c d b A A A A</formula><p>To verify the effectiveness of our proposed FAMs, we visualize the feature maps near the FAMs in <ref type="figure" target="#fig_3">Fig. 4</ref>. By comparing the left part (w/ FAMs) with the right part (w/o FAMs), feature maps after FAMs (Column a) can better capture the salient objects than those without FAMs (Column c). In addition to visualizing the intermediate feature maps, we also show some saliency maps produced by models with different settings in <ref type="figure" target="#fig_0">Fig. 2</ref>. By comparing the results in Column f (w/o FAMs) and Column g (w/ FAMs), it can be easily found that introducing FAM multiple times allows our network to better sharpen the details of the salient objects. This phenomenon is especially clear by observing the sec- (e-f) Edge maps and saliency maps by joint training with the edge dataset <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref>. By comparing the results in Column d and Column f, we can easily observe that joint training with high-quality edge datasets substantially improves the details of the detected salient objects.</p><p>ond row of <ref type="figure" target="#fig_0">Fig. 2</ref>. All the aforementioned discussions verify the significant effect of our FAMs on better fusing feature maps at different scales. In our experiment section, we will give more numerical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Joint Training with Edge Detection</head><p>The architecture described in Sec. 3 has already surpassed all previous state-of-the-art single-model results on multiple popular salient object detection benchmarks. Despite so, by observing the resulting saliency maps produced by our model, we find out that many inaccurate (incomplete or over-predicted) predictions are caused by unclear object boundaries.</p><p>At first, we attempt to solve this problem by adding an extra prediction branch built upon the architecture presented in Sec. 3 to estimate the boundaries of the salient objects. The detailed structure can be found on the top side of <ref type="figure">Fig. 1</ref>. We add three residual blocks <ref type="bibr" target="#b6">[7]</ref> after the FAMs at three feature levels in the top-down pathway, which are used for information transformation. These residual blocks are similar to the design in <ref type="bibr" target="#b6">[7]</ref> and have channel numbers of {128, 256, 512} from the fine level to the coarse level. As done in <ref type="bibr" target="#b25">[26]</ref>, each residual block is then followed by a 16channel 3 × 3 convolutional layer for feature compression plus a one-channel 1×1 convolutional layer for edge prediction. We also concatenate these three 16-channel 3 × 3 convolutional layers and feed them to three consecutive 3 × 3 convolutional layers with 48 channels to transmit the captured edge information to the salient object detection branch for detail enhancement.</p><p>Similar to <ref type="bibr" target="#b16">[17]</ref>, during the training phase, we use the boundaries of the salient objects as our ground truths for joint training. However, this procedure does not bring us any performance gain, and some results are still short of de-tail information of the object boundaries. For example, as demoed in Column c of <ref type="figure" target="#fig_4">Fig. 5</ref>, the resulting saliency maps and boundary maps are still ambiguous for scenes with low contrast between the foreground and background. The reason for this might be that the ground-truth edge maps derived from salient objects still lack most of the detailed information of salient objects. They just tell us where the outermost boundaries of salient objects are, especially for cases where there are overlaps between salient objects.</p><p>Taking the aforementioned argument into account, we attempt to perform joint training with the edge detection task using the same edge detection dataset <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref> as in <ref type="bibr" target="#b25">[26]</ref>. During training, images from the salient object detection dataset and the edge detection dataset are inputted alternatively. As can be seen in <ref type="figure" target="#fig_4">Fig. 5</ref>, joint training with the edge detection task greatly improves the details of the detected salient objects. We will provide more quantitative analysis in our experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section, we first describe the experiment setups, including the implementation details, the used datasets and the evaluation metrics. We then conduct a series of ablation studies to demonstrate the impact of each component of our proposed approach on the performance. At last, we report the performance of our approach and compare it with previous state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Setup</head><p>Implementation Details. The proposed framework is implemented based on the PyTorch repository 3 . All the experiments are performed using the Adam <ref type="bibr" target="#b12">[13]</ref> optimizer with a weight decay of 5e-4 and an initial learning rate of 5e-5 which is divided by 10 after 15 epochs. Our network is trained for 24 epochs in total. The backbone parameters of our network (e.g. , VGG-16 <ref type="bibr" target="#b32">[33]</ref> and ResNet-50 <ref type="bibr" target="#b6">[7]</ref>) are initialized with the corresponding models pretrained on the ImageNet dataset <ref type="bibr" target="#b15">[16]</ref> and the rest ones are randomly initialized. By default, our ablation experiments are performed based on the VGG-16 backbone and the union set of MSRA-B <ref type="bibr" target="#b24">[25]</ref> and HKU-IS <ref type="bibr" target="#b17">[18]</ref> datasets as done in <ref type="bibr" target="#b16">[17]</ref> unless special explanations. We only use the simple random horizontal flipping for data augmentation. In both training and testing, the sizes of the input images are kept unchanged as done in <ref type="bibr" target="#b8">[9]</ref>.</p><p>Datasets &amp; Loss Functions. To evaluate the performance of our proposed framework, we conduct experiments on 6 commonly used datasets, including ECSSD <ref type="bibr" target="#b40">[41]</ref>, PASCAL-S <ref type="bibr" target="#b20">[21]</ref>, DUT-OMRON <ref type="bibr" target="#b41">[42]</ref>, HKU-IS <ref type="bibr" target="#b17">[18]</ref>, SOD <ref type="bibr" target="#b29">[30]</ref> and DUTS <ref type="bibr" target="#b34">[35]</ref>. Sometimes, for convenience, we use the initials of the datasets as their abbreviations if there is no ex-3 https://pytorch.org</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head><p>Ours PiCANet <ref type="bibr" target="#b23">[24]</ref> DGRL <ref type="bibr" target="#b37">[38]</ref> PAGR <ref type="bibr" target="#b45">[46]</ref> SRM <ref type="bibr" target="#b36">[37]</ref> Amulet <ref type="bibr" target="#b43">[44]</ref> DSS <ref type="bibr" target="#b8">[9]</ref> MSR <ref type="bibr" target="#b16">[17]</ref> DCL <ref type="bibr" target="#b18">[19]</ref>  <ref type="figure">Figure 6</ref>. Qualitative comparisons to previous state-of-the-art methods. Obviously, compared to other methods, our approach is capable of not only locating the integral salient objects but also refining the details of the detected salient objects. This makes our resulting saliency map very close to the ground-truth annotations. plicit conflict. We use standard binary cross entropy loss for salient object detection and balanced binary cross entropy loss <ref type="bibr" target="#b39">[40]</ref> for edge detection.</p><p>Evaluation Criteria. We evaluate the performance of our approach and other methods using three widely-used metrics: precision-recall (PR) curves, F-measure score, and mean absolute error (MAE). F-measure, denoted as F β , is an overall performance measurement and is computed by the weighted harmonic mean of the precision and recall:</p><formula xml:id="formula_3">F β = (1 + β 2 ) × P recision × Recall β 2 × P recision + Recall<label>(1)</label></formula><p>where β 2 is set to 0.3 as done in previous work to weight precision more than recall. The MAE score indicates how similar a saliency map S is compared to the ground truth G:</p><formula xml:id="formula_4">M AE = 1 W × H W x=1 H y=1 |S(x, y) − G(x, y)| (2)</formula><p>where W and H denote the width and height of S, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies</head><p>In this subsection, we investigate the effectiveness of our proposed GGM and FAMs first. Then, we conduct more experiments on the configurations of our GGM and FAMs. Finally, we show the effect of joint training with edge detection on the performance. GGM and FAMs, all other configurations are the same. <ref type="table">Table 1</ref> shows the performance on two challenging datasets: DUT-O and SOD. The corresponding visual comparisons can be found in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of GGM and</head><p>• GGM Only. The addition of GGM (the 4th row in <ref type="table">Table 1</ref>) gives performance gains in terms of both Fmeasure and MAE on the two datasets over the FPN baseline. The global guidance information produced by GGM allows our network to focus more on the integrity of salient objects, greatly improving the quality of the resulting saliency maps. Therefore, the details of the salient objects can be sharpened, which might be wrongly estimated as background for models with limited receptive fields (e.g. , the last row in <ref type="figure" target="#fig_0">Fig. 2</ref>).</p><p>• FAMs Only. Simply embedding FAMs (the 5th row of <ref type="table">Table 1)</ref>  also helps improve the performance on both F-measure and MAE scores on the same two datasets. This might be because the pooling operations inside FAMs also enlarge the receptive field of the whole network compared to the baseline, and the FPN baseline still needs to merge feature maps from different levels, which indicates the effectiveness of our FAMs for solving the aliasing effect of upsampling.</p><p>• GGM &amp; FAMs. By introducing both GGM and FAMs into the baseline (the last row of <ref type="table">Table 1</ref>), the performance compared to the above two cases can be further enhanced on both F-measure and MAE scores. This phenomenon demonstrates that our GGM and FAM are two complementary modules. The utilization of them allows our approach to possess the strong capability of accurately discovering the salient objects and refining the details as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. More qualitative results can be found in <ref type="figure">Fig. 6</ref> as well.</p><p>Configuration of GGM. To have a better understanding of the constitution of our proposed GGM, we perform two ablation experiments, which correspond to the 2nd and 3rd rows of <ref type="table">Table 1</ref>, respectively. We alternatively remove one of the PPM and GGFs while keeping the other one unchanged. As can be seen, both operations make the performance decline compared to the results with both of them considered (the 4th row). These numerical results indicate that both PPM and GGFs play an important role in our GGM. The absence of any one of them is harmful for the performance of our approach.</p><p>The Impact of Joint Training. To further improve the quality of saliency maps produced by our approach, we attempt to combine edge detection with salient object detection in a joint training manner. In <ref type="table">Table 2</ref>, we list the results when two kinds of boundary information are considered.</p><p>As can be seen, using the boundaries of salient objects as supervision results in no improvement while using standard boundaries can greatly boost the performance on all three  <ref type="table">Table 2</ref>. Ablation analysis of our approach when different kinds of boundaries are used. The baseline here refers to the VGG-16 version of FPN plus GGM + FAMs. We also use the combination of MSRA-B <ref type="bibr" target="#b24">[25]</ref> and HKU-IS <ref type="bibr" target="#b17">[18]</ref> as the training set. 'SalEdge' refers to the boundaries of salient objects and 'StdEdge' refers to the standard datasets for edge detection, which include BSDS500 <ref type="bibr" target="#b0">[1]</ref> and PASCAL VOC Context <ref type="bibr" target="#b28">[29]</ref> as done in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>datasets especially on the MAE metric. This indicates that involving detailed edge information is helpful for salient object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparisons to the State-of-the-Arts</head><p>In this section, we compare our proposed PoolNet with 13 previous state-of-the-art methods, including DCL <ref type="bibr" target="#b18">[19]</ref>, RFCN <ref type="bibr" target="#b35">[36]</ref>, DHS <ref type="bibr" target="#b22">[23]</ref>, MSR <ref type="bibr" target="#b16">[17]</ref>, DSS <ref type="bibr" target="#b8">[9]</ref>, NLDF <ref type="bibr" target="#b27">[28]</ref>, UCF <ref type="bibr" target="#b44">[45]</ref>, Amulet <ref type="bibr" target="#b43">[44]</ref>, GearNet <ref type="bibr" target="#b9">[10]</ref>, PAGR <ref type="bibr" target="#b45">[46]</ref>, Pi-CANet <ref type="bibr" target="#b23">[24]</ref>, SRM <ref type="bibr" target="#b36">[37]</ref>, and DGRL <ref type="bibr" target="#b37">[38]</ref>. For fair comparisons, the saliency maps of these methods are generated by the original code released by the authors or directly provided by them. Moreover, all results are directly from single-model test without relying on any post-processing tools and all the predicted saliency maps are evaluated with the same evaluation code.</p><p>Quantitative Comparisons. Quantitative results are listed in <ref type="table" target="#tab_3">Table 3</ref>. We consider both VGG-16 <ref type="bibr" target="#b32">[33]</ref> and ResNet-50 <ref type="bibr" target="#b6">[7]</ref> as our backbones and show results on both of them. Additionally, we also conduct experiments on different training sets to eliminate the potential performance fluctuation. From   <ref type="table">Table 4</ref>. Average speed (FPS) comparisons between our approach (ResNet-50, w/ edge) and the previous state-of-the-art methods. environment) are also reported in <ref type="table">Table 4</ref>. Obviously, our approach runs in real time and faster than other methods.</p><p>PR Curves. Other than numerical results, we also show the PR curves on three datasets as shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. As can be seen, the PR curves by our approach (red ones) are especially outstanding compared to all other previous approaches. As the recall score approaches 1, our precision score is much higher than other methods. This phenomenon reveals that the false positives in our saliency map are low.</p><p>Visual Comparisons. To further explain the advantages of our approach, we show some qualitative results in <ref type="figure">Fig. 6</ref>. From top to bottom, the images correspond to scenes with transparent objects, small objects, large objects, complex texture, and low contrast between foreground and background, respectively. It can be easily seen that our approach can not only highlight the right salient objects but also main-tain their sharp boundaries in almost all circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we explore the potentials of pooling on salient object detection by designing two simple poolingbased modules: global guidance module (GGM) and feature aggregation module (FAM). By plugging them into the FPN architecture, we show that our proposed PoolNet can surpass all previous state-of-the-art approaches on six widelyused salient object detection benchmarks. Furthermore, we also reveal that joint training our network with the standard edge detection task in an end-to-end learning manner can greatly enhance the details of the detected salient objects. Our modules are independent of network architectures and hence can be flexibly applied to any pyramid-based models. These directions also provide promising ways to improve the quality of saliency maps.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Visual comparisons for salient object detection with different combinations of our proposed GGM and FAMs. (a) Source image; (b) Ground truth; (c) Results of FPN baseline; (d) Results of FPN + FAMs; (e) Results of FPN + PPM; (f) Results of FPN + GGM; (g) Results of FPN + GGM + FAMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Detailed illustration of our feature aggregation module (FAM). It comprises four sub-branches, each of which works in an individual scale space. After upsampling, all sub-branches are combined and then fed into a convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualizing feature maps around FAMs. Feature maps shown on the left are from models with FAMs, while feature maps displayed on the right are from models replacing FAMs with two convolution layers. The last row are source images and the corresponding ground-truth annotations. (a-d) are visualizations of feature maps at different places. As can be seen, when our FAMs are used, feature maps after FAMs can more precisely capture the location and detail information of salient objects (Column a), compared to those after two convolution layers (Column c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visual results by joint training with edge detection. (a) Source image; (b) Ground truth; (c-d) Edge maps and saliency maps using the boundaries of salient objects as ground truths of the edge branch;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>into the FPN baseline as shown inFig. 1Precision (vertical axis) recall (horizontal axis) curves on three popular salient object datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>FAMs. To demonstrate the effectiveness of our proposed GGM and FAMs, we conduct ablation experiments based on the FPN baseline with the VGG-16 backbone. Except for different combinations of</figDesc><table><row><cell>No.</cell><cell>GGM + FAMs</cell><cell cols="2">DUT-O [42]</cell><cell cols="2">SOD [30]</cell></row><row><cell></cell><cell cols="5">PPM GGFs FAMs MaxF ↑ MAE ↓ MaxF ↑ MAE ↓</cell></row><row><cell>1</cell><cell></cell><cell>0.770</cell><cell>0.076</cell><cell>0.838</cell><cell>0.124</cell></row><row><cell>2</cell><cell></cell><cell>0.783</cell><cell>0.071</cell><cell>0.847</cell><cell>0.125</cell></row><row><cell>3</cell><cell></cell><cell>0.772</cell><cell>0.076</cell><cell>0.843</cell><cell>0.121</cell></row><row><cell>4</cell><cell></cell><cell>0.790</cell><cell>0.069</cell><cell>0.855</cell><cell>0.120</cell></row><row><cell>5</cell><cell></cell><cell>0.798</cell><cell>0.065</cell><cell>0.852</cell><cell>0.118</cell></row><row><cell>6</cell><cell></cell><cell>0.806</cell><cell>0.063</cell><cell>0.861</cell><cell>0.117</cell></row><row><cell cols="6">Table 1. Ablation analysis for the proposed architecture on two</cell></row><row><cell cols="6">popular datasets. All experiments are based on the VGG-16 back-</cell></row><row><cell cols="6">bone and trained on the union set of MSRA-B [25] and HKU-IS</cell></row><row><cell cols="6">[18]. By default, our baseline is the VGG-16 version of FPN [22].</cell></row><row><cell cols="6">As can be observed, each component in our architecture plays an</cell></row><row><cell cols="6">important role and contributes to the performance. Best result in</cell></row><row><cell cols="3">each column are highlighted in red.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 ,</head><label>3</label><figDesc>we can observe that our PoolNet surpasses almost all previous state-of-the-art results on all datasets with the same backbone and training set. Average speed (FPS) comparisons among different methods (tested in the sameMAE  ↓ MaxF ↑ MAE ↓ MaxF ↑ MAE ↓ MaxF ↑ MAE ↓ MaxF ↑ MAE ↓ MaxF ↑ MAE ↓</figDesc><table><row><cell></cell><cell cols="2">Training</cell><cell cols="2">ECSSD [41]</cell><cell cols="2">PASCAL-S [21]</cell><cell cols="2">DUT-O [42]</cell><cell cols="2">HKU-IS [18]</cell><cell cols="2">SOD [30]</cell><cell cols="2">DUTS-TE [35]</cell></row><row><cell cols="4">Model #Images Dataset MaxF ↑ VGG-16 backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DCL [19]</cell><cell>2,500</cell><cell>MB</cell><cell>0.896</cell><cell>0.080</cell><cell>0.805</cell><cell>0.115</cell><cell>0.733</cell><cell>0.094</cell><cell>0.893</cell><cell>0.063</cell><cell>0.831</cell><cell>0.131</cell><cell>0.786</cell><cell>0.081</cell></row><row><cell>RFCN [36]</cell><cell>10,000</cell><cell>MK</cell><cell>0.898</cell><cell>0.097</cell><cell>0.827</cell><cell>0.118</cell><cell>0.747</cell><cell>0.094</cell><cell>0.895</cell><cell>0.079</cell><cell>0.805</cell><cell>0.161</cell><cell>0.786</cell><cell>0.090</cell></row><row><cell>DHS [23]</cell><cell cols="3">9,500 MK+DTO 0.905</cell><cell>0.062</cell><cell>0.825</cell><cell>0.092</cell><cell>-</cell><cell>-</cell><cell>0.892</cell><cell>0.052</cell><cell>0.823</cell><cell>0.128</cell><cell>0.815</cell><cell>0.065</cell></row><row><cell>MSR [17]</cell><cell>5,000</cell><cell>MB + H</cell><cell>0.903</cell><cell>0.059</cell><cell>0.839</cell><cell>0.083</cell><cell>0.790</cell><cell>0.073</cell><cell>0.907</cell><cell>0.043</cell><cell>0.841</cell><cell>0.111</cell><cell>0.824</cell><cell>0.062</cell></row><row><cell>DSS [9]</cell><cell>2,500</cell><cell>MB</cell><cell>0.906</cell><cell>0.064</cell><cell>0.821</cell><cell>0.101</cell><cell>0.760</cell><cell>0.074</cell><cell>0.900</cell><cell>0.050</cell><cell>0.834</cell><cell>0.125</cell><cell>0.813</cell><cell>0.065</cell></row><row><cell>NLDF [28]</cell><cell>3,000</cell><cell>MB</cell><cell>0.903</cell><cell>0.065</cell><cell>0.822</cell><cell>0.098</cell><cell>0.753</cell><cell>0.079</cell><cell>0.902</cell><cell>0.048</cell><cell>0.837</cell><cell>0.123</cell><cell>0.816</cell><cell>0.065</cell></row><row><cell>UCF [45]</cell><cell>10,000</cell><cell>MK</cell><cell>0.908</cell><cell>0.080</cell><cell>0.820</cell><cell>0.127</cell><cell>0.735</cell><cell>0.131</cell><cell>0.888</cell><cell>0.073</cell><cell>0.798</cell><cell>0.164</cell><cell>0.771</cell><cell>0.116</cell></row><row><cell>Amulet [44]</cell><cell>10,000</cell><cell>MK</cell><cell>0.911</cell><cell>0.062</cell><cell>0.826</cell><cell>0.092</cell><cell>0.737</cell><cell>0.083</cell><cell>0.889</cell><cell>0.052</cell><cell>0.799</cell><cell>0.146</cell><cell>0.773</cell><cell>0.075</cell></row><row><cell>GearNet[10]</cell><cell>5,000</cell><cell>MB + H</cell><cell>0.923</cell><cell>0.055</cell><cell>-</cell><cell>-</cell><cell>0.790</cell><cell>0.068</cell><cell>0.934</cell><cell>0.034</cell><cell>0.853</cell><cell>0.117</cell><cell>-</cell><cell>-</cell></row><row><cell>PAGR [46]</cell><cell>10,553</cell><cell>DTS</cell><cell>0.924</cell><cell>0.064</cell><cell>0.847</cell><cell>0.089</cell><cell>0.771</cell><cell>0.071</cell><cell>0.919</cell><cell>0.047</cell><cell>-</cell><cell>-</cell><cell>0.854</cell><cell>0.055</cell></row><row><cell>PiCANet [24]</cell><cell>10,553</cell><cell>DTS</cell><cell>0.930</cell><cell>0.049</cell><cell>0.858</cell><cell>0.078</cell><cell>0.815</cell><cell>0.067</cell><cell>0.921</cell><cell>0.042</cell><cell>0.863</cell><cell>0.102</cell><cell>0.855</cell><cell>0.053</cell></row><row><cell>PoolNet (Ours)</cell><cell>2,500</cell><cell>MB</cell><cell>0.918</cell><cell>0.057</cell><cell>0.828</cell><cell>0.098</cell><cell>0.783</cell><cell>0.065</cell><cell>0.908</cell><cell>0.044</cell><cell>0.846</cell><cell>0.124</cell><cell>0.819</cell><cell>0.062</cell></row><row><cell>PoolNet (Ours)</cell><cell>5,000</cell><cell>MB + H</cell><cell>0.930</cell><cell>0.053</cell><cell>0.838</cell><cell>0.093</cell><cell>0.806</cell><cell>0.063</cell><cell>0.936</cell><cell>0.032</cell><cell>0.861</cell><cell>0.118</cell><cell>0.855</cell><cell>0.053</cell></row><row><cell>PoolNet (Ours)</cell><cell>10,553</cell><cell>DTS</cell><cell>0.936</cell><cell>0.047</cell><cell>0.857</cell><cell>0.078</cell><cell>0.817</cell><cell>0.058</cell><cell>0.928</cell><cell>0.035</cell><cell>0.859</cell><cell>0.115</cell><cell>0.876</cell><cell>0.043</cell></row><row><cell cols="2">PoolNet  † (Ours) 10,553</cell><cell>DTS</cell><cell>0.937</cell><cell>0.044</cell><cell>0.865</cell><cell>0.072</cell><cell>0.821</cell><cell>0.056</cell><cell>0.931</cell><cell>0.033</cell><cell>0.866</cell><cell>0.105</cell><cell>0.880</cell><cell>0.041</cell></row><row><cell cols="2">ResNet-50 backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SRM [37]</cell><cell>10,553</cell><cell>DTS</cell><cell>0.916</cell><cell>0.056</cell><cell>0.838</cell><cell>0.084</cell><cell>0.769</cell><cell>0.069</cell><cell>0.906</cell><cell>0.046</cell><cell>0.840</cell><cell>0.126</cell><cell>0.826</cell><cell>0.058</cell></row><row><cell>DGRL [38]</cell><cell>10,553</cell><cell>DTS</cell><cell>0.921</cell><cell>0.043</cell><cell>0.844</cell><cell>0.072</cell><cell>0.774</cell><cell>0.062</cell><cell>0.910</cell><cell>0.036</cell><cell>0.843</cell><cell>0.103</cell><cell>0.828</cell><cell>0.049</cell></row><row><cell>PiCANet [24]</cell><cell>10,553</cell><cell>DTS</cell><cell>0.932</cell><cell>0.048</cell><cell>0.864</cell><cell>0.075</cell><cell>0.820</cell><cell>0.064</cell><cell>0.920</cell><cell>0.044</cell><cell>0.861</cell><cell>0.103</cell><cell>0.863</cell><cell>0.050</cell></row><row><cell>PoolNet (Ours)</cell><cell>10,553</cell><cell>DTS</cell><cell>0.940</cell><cell>0.042</cell><cell>0.863</cell><cell>0.075</cell><cell>0.830</cell><cell>0.055</cell><cell>0.934</cell><cell>0.032</cell><cell>0.867</cell><cell>0.100</cell><cell>0.886</cell><cell>0.040</cell></row><row><cell cols="2">PoolNet  † (Ours) 10,553</cell><cell>DTS</cell><cell>0.945</cell><cell>0.038</cell><cell>0.880</cell><cell>0.065</cell><cell>0.833</cell><cell>0.053</cell><cell>0.935</cell><cell>0.030</cell><cell>0.882</cell><cell>0.102</cell><cell>0.892</cell><cell>0.036</cell></row></table><note>MB: MSRA-B [25], MK: MSRA10K [3], DTO: DUT-OMRON [42], H: HKU-IS [18], DTS: DUTS-TR [35].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Quantitative salient object detection results on 6 widely used datasets. The best results with different backbones are highlighted in blue and red, respectively. † : joint training with edge detection. As can be seen, our approach achieves the best results on nearly all datasets in terms of F-measure and MAE.</figDesc><table><row><cell></cell><cell>Ours</cell><cell cols="4">PiCANet [24] DGRL [38] SRM [37] Amulet [44]</cell></row><row><cell cols="6">Size 400 × 300 224 × 224 384 × 384 353 × 353 256 × 256</cell></row><row><cell>FPS</cell><cell>32</cell><cell>7</cell><cell>8</cell><cell>14</cell><cell>16</cell></row><row><cell></cell><cell cols="2">UCF [45] NLDF [28]</cell><cell>DSS [9]</cell><cell cols="2">MSR [17] DHS [23]</cell></row><row><cell cols="6">Size 224 × 224 400 × 300 400 × 300 400 × 300 224 × 224</cell></row><row><cell>FPS</cell><cell>23</cell><cell>12</cell><cell>12</cell><cell>2</cell><cell>23</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://pytorch.org/docs/stable/nn.html# adaptiveavgpool2d</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research was supported by NSFC (61620106008, 61572264), the national youth talent support program, Tianjin Natural Science Foundation (17JCJQJC43700, 18ZXZNGX00110) and the Fundamental Research Funds for the Central Universities (Nankai University, NO. 63191501).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting local and global patch rarities for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xumin</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Repfinder: finding approximately repeated scene elements for image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Lue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Environment exploration for object-based visual saliency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celine</forename><surname>Craye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Filliat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-François</forename><surname>Goudou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2303" to="2309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Gayoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Yu-Wing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Junmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tackgeun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Three birds one stone: A unified framework for salient object segmentation, edge detection and skeleton extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09860</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2083" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Center-surround divergence of feature statistics for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dominik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frintrop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07386</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Instancelevel salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2976" to="2983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonlocal deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshaya</forename><surname>Kumar Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">A</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Marc</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Design and perceptual validation of performance measures for salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vida</forename><surname>Movahedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linzhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4019" to="4028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3127" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep salient object detection with dense connections and distraction diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maojun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">You He, and Gang Wang. A bi-directional message passing model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning uncertain convolutional features for accurate saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
