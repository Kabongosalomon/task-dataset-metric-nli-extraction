<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks Samy Bengio</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-04">Hsieh. 2019. August 4-8, 2019,</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
							<email>xqliu@cs.ucla.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
							<email>liyang@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
							<email>chohsieh@cs.ucla.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
							<email>bengio@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Anchorage</settlement>
									<region>AK</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks Samy Bengio</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-04">Hsieh. 2019. August 4-8, 2019,</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3292500.3330925</idno>
					<note>ACM Reference Format: Cluster-GCN: An Efficient Algorithm for Training Deep and * This work was done during the first and the second author&apos;s internship at Google Research. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD &apos;19,</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional network (GCN) has been successfully applied to many graph-based applications; however, training a large-scale GCN remains challenging. Current SGD-based algorithms suffer from either a high computational cost that exponentially grows with number of GCN layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer GCN on this data, Cluster-GCN is faster than the previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our algorithm can finish in around 36 minutes while all the existing GCN training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-GCN allows us to train much deeper GCN without much time and memory overhead, which leads to improved prediction accuracy-using a 5-layer Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by <ref type="bibr" target="#b15">[16]</ref>. Our codes are publicly available at https://github.com/google-research/google-research/ tree/master/cluster_gcn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph convolutional network (GCN) <ref type="bibr" target="#b8">[9]</ref> has become increasingly popular in addressing many graph-based applications, including semi-supervised node classification <ref type="bibr" target="#b8">[9]</ref>, link prediction <ref type="bibr" target="#b16">[17]</ref> and recommender systems <ref type="bibr" target="#b14">[15]</ref>. Given a graph, GCN uses a graph convolution operation to obtain node embeddings layer by layer-at each layer, the embedding of a node is obtained by gathering the embeddings of its neighbors, followed by one or a few layers of linear transformations and nonlinear activations. The final layer embedding is then used for some end tasks. For instance, in node classification problems, the final layer embedding is passed to a classifier to predict node labels, and thus the parameters of GCN can be trained in an end-to-end manner.</p><p>Since the graph convolution operator in GCN needs to propagate embeddings using the interaction between nodes in the graph, this makes training quite challenging. Unlike other neural networks that the training loss can be perfectly decomposed into individual terms on each sample, the loss term in GCN (e.g., classification loss on a single node) depends on a huge number of other nodes, especially when GCN goes deep. Due to the node dependence, GCN's training is very slow and requires lots of memory -backpropagation needs to store all the embeddings in the computation graph in GPU memory.</p><p>Previous GCN Training Algorithms: To demonstrate the need of developing a scalable GCN training algorithm, we first discuss the pros and cons of existing approaches, in terms of 1) memory requirement 1 , 2) time per epoch 2 and 3) convergence speed (loss reduction) per epoch. These three factors are crucial for evaluating a training algorithm. Note that memory requirement directly restricts the scalability of algorithm, and the later two factors combined together will determine the training speed. In the following discussion we denote N to be the number of nodes in the graph, F the embedding dimension, and L the number of layers to analyze classic GCN training algorithms.</p><p>• Full-batch gradient descent is proposed in the first GCN paper <ref type="bibr" target="#b8">[9]</ref>. To compute the full gradient, it requires storing all the intermediate embeddings, leading to O(N F L) memory requirement, which is not scalable. Furthermore, although the time per epoch is efficient, the convergence of gradient descent is slow since the parameters are updated only once per epoch.</p><p>[memory: bad; time per epoch: good; convergence: bad] • Mini-batch SGD is proposed in <ref type="bibr" target="#b4">[5]</ref>. Since each update is only based on a mini-batch gradient, it can reduce the memory requirement and conduct many updates per epoch, leading to a faster convergence. However, mini-batch SGD introduces a significant computational overhead due to the neighborhood expansion problem-to compute the loss on a single node at layer L, it requires that node's neighbor nodes' embeddings at layer L − 1, which again requires their neighbors' embeddings at layer L − 2 and recursive ones in the downstream layers. This leads to time complexity exponential to the GCN depth. Graph-SAGE <ref type="bibr" target="#b4">[5]</ref> proposed to use a fixed size of neighborhood samples during back-propagation through layers and FastGCN <ref type="bibr" target="#b0">[1]</ref> proposed importance sampling, but the overhead of these methods is still large and will become worse when GCN goes deep. In this paper, we propose a novel GCN training algorithm by exploiting the graph clustering structure. We find that the efficiency of a mini-batch algorithm can be characterized by the notion of "embedding utilization", which is proportional to the number of links between nodes in one batch or within-batch links. This finding motivates us to design the batches using graph clustering algorithms that aims to construct partitions of nodes so that there are more graph links between nodes in the same partition than nodes in different partitions. Based on the graph clustering idea, we proposed Cluster-GCN, an algorithm to design the batches based on efficient graph clustering algorithms (e.g., METIS <ref type="bibr" target="#b7">[8]</ref>). We take this idea further by proposing a stochastic multi-clustering framework to improve the convergence of Cluster-GCN. Our strategy leads to huge memory and computational benefits. In terms of memory, we only need to store the node embeddings within the current batch, which is O(bF L) with the batch size b. This is significantly better than VR-GCN and full gradient decent, and slightly better than other SGD-based approaches. In terms of computational complexity, our algorithm achieves the same time cost per epoch with gradient descent and is much faster than neighborhood searching approaches. In terms of the convergence speed, our algorithm is competitive with other SGD-based approaches. Finally, our algorithm is simple to implement since we only compute matrix multiplication and no neighborhood sampling is needed. Therefore for Cluster-GCN, we have [memory: good; time per epoch: good; convergence: good].</p><p>We conducted comprehensive experiments on several large-scale graph datasets and made the following contributions:</p><p>• Cluster-GCN achieves the best memory usage on large-scale graphs, especially on deep GCN. For example, Cluster-GCN uses 5x less memory than VRGCN in a 3-layer GCN model on Amazon2M. Amazon2M is a new graph dataset that we construct to demonstrate the scalablity of the GCN algorithms. This dataset contains a amazon product co-purchase graph with more than 2 millions nodes and 61 millions edges. • Cluster-GCN achieves a similar training speed with VR-GCN for shallow networks (e.g., 2 layers) but can be faster than VR-GCN when the network goes deeper (e.g., 4 layers), since our complexity is linear to the number of layers L while VR-GCN's complexity is exponential to L. • Cluster-GCN is able to train a very deep network that has a large embedding size. Although several previous works show that deep GCN does not give better performance, we found that with proper optimization, deeper GCN could help the accuracy. For example, with a 5-layer GCN, we obtain a new benchmark accuracy 99.36 for PPI dataset, comparing with the highest reported one 98.71 by <ref type="bibr" target="#b15">[16]</ref>.</p><p>Implementation of our proposed method is publicly available. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Suppose we are given a graph G = (V, E, A), which consists of N = |V | vertices and |E | edges such that an edge between any two vertices i and j represents their similarity. The corresponding adjacency matrix A is an N × N sparse matrix with (i, j) entry equaling to 1 if there is an edge between i and j and 0 otherwise. Also, each node is associated with an F -dimensional feature vector and X ∈ R N ×F denotes the feature matrix for all N nodes. An L-layer GCN <ref type="bibr" target="#b8">[9]</ref> consists of L graph convolution layers and each of them constructs embeddings for each node by mixing the embeddings of the node's neighbors in the graph from the previous layer:</p><formula xml:id="formula_0">Z (l +1) = A ′ X (l ) W (l ) , X (l +1) = σ (Z (l +1) ),<label>(1)</label></formula><p>where X (l ) ∈ R N ×F l is the embedding at the l-th layer for all the N nodes and X (0) = X ; A ′ is the normalized and regularized adjacency matrix and W (l ) ∈ R F l ×F l +1 is the feature transformation matrix which will be learnt for the downstream tasks. Note that for simplicity we assume the feature dimensions are the same for all layers (F 1 = · · · = F L = F ). The activation function σ (·) is usually set to be the element-wise ReLU. Semi-supervised node classification is a popular application of GCN. When using GCN for this application, the goal is to learn weight matrices in (1) by minimizing the loss function:</p><formula xml:id="formula_1">L = 1 |Y L | i ∈Y L loss(y i , z L i ),<label>(2)</label></formula><p>where Y L contains all the labels for the labeled nodes; z (L) i is the i-th row of Z (L) with the ground-truth label to be y i , indicating the final layer prediction of node i. In practice, a cross-entropy loss is commonly used for node classification in multi-class or multi-label problems. <ref type="table">Table 1</ref>: Time and space complexity of GCN training algorithms. L is number of layers, N is number of nodes, ∥A∥ 0 is number of nonzeros in the adjacency matrix, and F is number of features. For simplicity we assume number of features is fixed for all layers. For SGD-based approaches, b is the batch size and r is the number of sampled neighbors per node. Note that due to the variance reduction technique, VR-GCN can work with a smaller r than GraphSAGE and FastGCN. For memory complexity, LF 2 is for storing {W (l ) } L l =1 and the other term is for storing embeddings. For simplicity we omit the memory for storing the graph (GCN) or sub-graphs (other approaches) since they are fixed and usually not the main bottleneck. <ref type="bibr">GCN [9]</ref> Vanilla SGD GraphSAGE <ref type="bibr" target="#b4">[5]</ref> FastGCN <ref type="bibr" target="#b0">[1]</ref> VR-GCN <ref type="bibr" target="#b1">[2]</ref> Cluster-GCN Time complexity</p><formula xml:id="formula_2">O(L∥A∥ 0 F + LN F 2 ) O(d L N F 2 ) O(r L N F 2 ) O(rLN F 2 ) O(L∥A∥ 0 F + LN F 2 + r L N F 2 ) O(L∥A∥ 0 F + LN F 2 ) Memory complexity O(LN F + LF 2 ) O(bd L F + LF 2 ) O(br L F + LF 2 ) O(brLF + LF 2 ) O(LN F + LF 2 ) O(bLF + LF 2 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED ALGORITHM</head><p>We first discuss the bottleneck of previous training methods to motivate the proposed algorithm.</p><p>In the original paper <ref type="bibr" target="#b8">[9]</ref>, full gradient descent is used for training GCN, but it suffers from high computational and memory cost. In terms of memory, computing the full gradient of (2) by backpropagation requires storing all the embedding matrices {Z (l ) } L l =1 which needs O(N F L) space. In terms of convergence speed, since the model is only updated once per epoch, the training requires more epochs to converge.</p><p>It has been shown that mini-batch SGD can improve the training speed and memory requirement of GCN in some recent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5]</ref>. Instead of computing the full gradient, SGD only needs to calculate the gradient based on a mini-batch for each update. In this paper, we use B ⊆ [N ] with size b = |B| to denote a batch of node indices, and each SGD step will compute the gradient estimation</p><formula xml:id="formula_3">1 |B| i ∈B ∇loss(y i , z (L) i )<label>(3)</label></formula><p>to perform an update. Despite faster convergence in terms of epochs, SGD will introduce another computational overhead on GCN training (as explained in the following), which makes it having much slower per-epoch time compared with full gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why does vanilla mini-batch SGD have slow per-epoch time?</head><p>We consider the computation of the gradient associated with one node i : ∇loss(y i , z (L) i ). Clearly, this requires the embedding of node i, which depends on its neighbors' embeddings in the previous layer. To fetch each node i's neighbor nodes' embeddings, we need to further aggregate each neighbor node's neighbor nodes' embeddings as well. Suppose a GCN has L + 1 layers and each node has an average degree of d, to get the gradient for node i, we need to aggregate features from O(d L ) nodes in the graph for one node. That is, we need to fetch information for a node's hop-k (k = 1, · · · , L) neighbors in the graph to perform one update. Computing each embedding requires O(F 2 ) time due to the multiplication with W (l ) , so in average computing the gradient associated with one node requires O(d L F 2 ) time.</p><p>Embedding utilization can reflect computational efficiency. If a batch has more than one node, the time complexity is less straightforward since different nodes can have overlapped hopk neighbors, and the number of embedding computation can be less than the worst case O(bd L ). To reflect the computational efficiency of mini-batch SGD, we define the concept of "embedding utilization" to characterize the computational efficiency. During the algorithm, if the node i's embedding at l-th layer z (l ) i is computed and is reused u times for the embedding computations at layer l + 1, then we say the embedding utilization of z (l ) i is u. For mini-batch SGD with random sampling, u is very small since the graph is usually large and sparse. Assume u is a small constant (almost no overlaps between hop-k neighbors), then mini-batch SGD needs to compute O(bd L ) embeddings per batch, which leads to O(bd L F 2 ) time per update and O(Nd L F 2 ) time per epoch.</p><p>We illustrate the neighborhood expansion problem in the left panel of <ref type="figure">Fig. 1</ref>. In contrary, full-batch gradient descent has the maximal embedding utilization-each embedding will be reused d (average degree) times in the upper layer. As a consequence, the original full gradient descent <ref type="bibr" target="#b8">[9]</ref> only needs to compute O(N L) embeddings per epoch, which means on average only O(L) embedding computation is needed to acquire the gradient of one node.</p><p>To make mini-batch SGD work, previous approaches try to restrict the neighborhood expansion size, which however do not improve embedding utilization. GraphSAGE <ref type="bibr" target="#b4">[5]</ref> uniformly samples a fixed-size set of neighbors, instead of using a full-neighborhood set. We denote the sample size as r . This leads to O(r L ) embedding computations for each loss term but also makes gradient estimation less accurate. FastGCN <ref type="bibr" target="#b0">[1]</ref> proposed an important sampling strategy to improve the gradient estimation. VR-GCN <ref type="bibr" target="#b1">[2]</ref> proposed a strategy to store the previous computed embeddings for all the N nodes and L layers and reuse them for unsampled neighbors.</p><p>Despite the high memory usage for storing all the N L embeddings, we find their strategy very useful and in practice, even for a small r (e.g., 2) can lead to good convergence.</p><p>We summarize the time and space complexity in <ref type="table">Table 1</ref>. Clearly, all the SGD-based algorithms suffer from exponential complexity with respect to the number of layers, and for VR-GCN, even though r can be small, they incur huge space complexity that could go beyond a GPU's memory capacity. In the following, we introduce our Cluster-GCN algorithm, which achieves the best of two worldsthe same time complexity per epoch with full gradient descent and the same memory complexity with vanilla SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vanilla Cluster-GCN</head><p>Our Cluster-GCN technique is motivated by the following question: In mini-batch SGD updates, can we design a batch and the corresponding computation subgraph to maximize the embedding utilization? We answer this affirmative by connecting the concept of embedding utilization to a clustering objective.</p><p>Consider the case that in each batch we compute the embeddings for a set of nodes B from layer 1 to L. Since the same subgraph A B, B (links within B) is used for each layer of computation, we can <ref type="figure">Figure 1</ref>: The neighborhood expansion difference between traditional graph convolution and our proposed cluster approach. The red node is the starting node for neighborhood nodes expansion. Traditional graph convolution suffers from exponential neighborhood expansion, while our method can avoid expensive neighborhood expansion. then see that embedding utilization is the number of edges within this batch ∥A B, B ∥ 0 . Therefore, to maximize embedding utilization, we should design a batch B to maximize the within-batch edges, by which we connect the efficiency of SGD updates with graph clustering algorithms. Now we formally introduce Cluster-GCN. For a graph G, we partition its nodes into c groups:</p><formula xml:id="formula_4">V = [V 1 , · · · V c ]</formula><p>where V t consists of the nodes in the t-th partition. Thus we have c subgraphs as</p><formula xml:id="formula_5">G = [G 1 , · · · , G c ] = [{V 1 , E 1 }, · · · , {V c , E c }],</formula><p>where each E t only consists of the links between nodes in V t . After reorganizing nodes, the adjacency matrix is partitioned into c 2 submatrices as</p><formula xml:id="formula_6">A =Ā + ∆ =        A 11 · · · A 1c . . . . . . . . . A c1 · · · A cc        (4) andĀ =        A 11 · · · 0 . . . . . . . . . 0 · · · A cc        , ∆ =        0 · · · A 1c . . . . . . . . . A c1 · · · 0        ,<label>(5)</label></formula><p>where each diagonal block A t t is a |V t | × |V t | adjacency matrix containing the links within G t .Ā is the adjacency matrix for graph G; A st contains the links between two partitions V s and V t ; ∆ is the matrix consisting of all off-diagonal blocks of A. Similarly, we can partition the feature matrix X and training labels Y according to the partition</p><formula xml:id="formula_7">[V 1 , · · · , V c ] as [X 1 , · · · , X c ] and [Y 1 , · · · , Y c ]</formula><p>where X t and Y t consist of the features and labels for the nodes in V t respectively.</p><p>The benefit of this block-diagonal approximationḠ is that the objective function of GCN becomes decomposible into different batches (clusters). LetĀ ′ denotes the normalized version ofĀ, the final embedding matrix becomes</p><formula xml:id="formula_8">Z (L) =Ā ′ σ (Ā ′ σ (· · · σ (Ā ′ XW (0) )W (1) ) · · · )W (L−1) (6) =       Ā ′ 11 σ (Ā ′ 11 σ (· · · σ (Ā ′ 11 X 1 W (0) )W (1) ) · · · )W (L−1) . . . A ′ cc σ (Ā ′ cc σ (· · · σ (Ā ′ cc X c W (0) )W (1) ) · · · )W (L−1)        due to the block-diagonal form ofĀ (note thatĀ ′ t t is the correspond- ing diagonal block ofĀ ′ ).</formula><p>The loss function can also be decomposed into</p><formula xml:id="formula_9">LĀ′ = t |V t | N LĀ′ t t and LĀ′ t t = 1 |V t | i ∈V t loss(y i , z (L) i ). (7)</formula><p>The Cluster-GCN is then based on the decomposition form in (6) and <ref type="bibr" target="#b6">(7)</ref>. At each step, we sample a cluster V t and then conduct SGD to update based on the gradient of LĀ′ t t , and this only requires the sub-graph A t t , the X t , Y t on the current batch and the models {W (l ) } L l =1 . The implementation only requires forward and backward propagation of matrix products (one block of <ref type="formula">(6)</ref>) that is much easier to implement than the neighborhood search procedure used in previous SGD-based training methods.</p><p>We use graph clustering algorithms to partition the graph. Graph clustering methods such as Metis <ref type="bibr" target="#b7">[8]</ref> and Graclus <ref type="bibr" target="#b3">[4]</ref> aim to construct the partitions over the vertices in the graph such that withinclusters links are much more than between-cluster links to better capture the clustering and community structure of the graph. These are exactly what we need because: 1) As mentioned before, the embedding utilization is equivalent to the within-cluster links for each batch. Intuitively, each node and its neighbors are usually located in the same cluster, therefore after a few hops, neighborhood nodes with a high chance are still in the same cluster. 2) Since we replace A by its block diagonal approximationĀ and the error is proportional to between-cluster links ∆, we need to find a partition to minimize number of between-cluster links.</p><p>In <ref type="figure">Figure 1</ref>, we illustrate the neighborhood expansion with full graph G and the graph with clustering partitionḠ. We can see that cluster-GCN can avoid heavy neighborhood search and focus on the neighbors within each cluster. In <ref type="table" target="#tab_1">Table 2</ref>, we show two different node partition strategies: random partition versus clustering partition. We partition the graph into 10 parts by using random partition and METIS. Then use one partition as a batch to perform a SGD update. We can see that with the same number of epochs, using clustering partition can achieve higher accuracy. This shows using graph clustering is important and partitions should not be formed randomly.</p><p>Time and space complexity. Since each node in V t only links to nodes inside V t , each node does not need to perform neighborhoods searching outside A t t . The computation for each batch will purely be matrix productsĀ ′ t t X (l ) t W (l ) and some element-wise operations, so the overall time complexity per batch is O(∥A t t ∥ 0 F + bF 2 ). Thus the overall time complexity per epoch becomes O(∥A∥ 0 F + N F 2 ). In average, each batch only requires computing O(bL) embeddings, which is linear instead of exponential to L. In terms of space complexity, in each batch, we only need to load b samples and store their embeddings on each layer, resulting in O(bLF ) memory for storing embeddings. Therefore our algorithm is also more memory  Here we present within each batch using random partition versus clustering partition. Most clustering partitioned batches have low label entropy, indicating skewed label distribution within each batch. In comparison, random partition will lead to larger label entropy within a batch although it is less efficient as discussed earlier. We partition the Reddit dataset with 300 clusters in this example.</p><p>efficient than all the previous algorithms. Moreover, our algorithm only requires loading a subgraph into GPU memory instead of the full graph (though graph is usually not the memory bottleneck). The detailed time and memory complexity are summarized in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stochastic Multiple Partitions</head><p>Although vanilla Cluster-GCN achieves good computational and memory complexity, there are still two potential issues:</p><p>• After the graph is partitioned, some links (the ∆ part in Eq. (4)) are removed. Thus the performance could be affected. • Graph clustering algorithms tend to bring similar nodes together.</p><p>Hence the distribution of a cluster could be different from the original data set, leading to a biased estimation of the full gradient while performing SGD updates.</p><p>In <ref type="figure" target="#fig_0">Figure 2</ref>, we demonstrate an example of unbalanced label distribution by using the Reddit data with clusters formed by Metis. We calculate the entropy value of each cluster based on its label distribution. Comparing with random partitioning, we clearly see that entropy of most clusters are smaller, indicating that the label distributions of clusters are biased towards some specific labels. This increases the variance across different batches and may affect the convergence of SGD.  To address the above issues, we propose a stochastic multiple clustering approach to incorporate between-cluster links and reduce variance across batches. We first partition the graph into p clusters V 1 , · · · , V p with a relatively large p. When constructing a batch B for an SGD update, instead of considering only one cluster, we randomly choose q clusters, denoted as t 1 , . . . , t q and include their nodes {V t 1 ∪ · · · ∪ V t q } into the batch. Furthermore, the links between the chosen clusters,</p><formula xml:id="formula_10">{A i j | i, j ∈ t 1 , . . . , t q },</formula><p>are added back. In this way, those between-cluster links are reincorporated and the combinations of clusters make the variance across batches smaller. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates our algorithm-for each epochs, different combinations of clusters are chosen as a batch. We conduct an experiment on Reddit to demonstrate the effectiveness of the proposed approach. In <ref type="figure" target="#fig_2">Figure 4</ref>, we can observe that using multiple clusters as one batch could improve the convergence. Our final Cluster-GCN algorithm is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Issues of training deeper GCNs</head><p>Previous attempts of training deeper GCNs <ref type="bibr" target="#b8">[9]</ref> seem to suggest that adding more layers is not helpful. However, the datasets used in the experiments may be too small to make a proper justification. For example, <ref type="bibr" target="#b8">[9]</ref> considered a graph with only a few hundreds of training nodes for which overfitting can be an issue. Moreover, we observe that the optimization of deep GCN models becomes difficult as it may impede the information from the first few layers being passed through. In <ref type="bibr" target="#b8">[9]</ref>, they adopt a technique similar to residual Algorithm 1: Cluster GCN Input: Graph A, feature X , label Y ; Output: Node representationX 1 Partition graph nodes into c clusters V 1 , V 2 , · · · , V c by METIS; 2 for iter = 1, · · · , max_iter do <ref type="bibr" target="#b2">3</ref> Randomly choose q clusters, t 1 , · · · , t q from V without replacement; <ref type="bibr" target="#b3">4</ref> Form the subgraphḠ with nodesV = [V t 1 , V t 2 , · · · , V t q ] and links AV ,V ; <ref type="bibr" target="#b4">5</ref> Compute д ← ∇L AV ,V (loss on the subgraph AV ,V ) ; <ref type="bibr" target="#b5">6</ref> Conduct Adam update using gradient estimator д</p><formula xml:id="formula_11">7 Output: {W l } L l =1</formula><p>connections <ref type="bibr" target="#b5">[6]</ref> to enable the model to carry the information from a previous layer to a next layer. Specifically, they modify (1) to add the hidden representations of layer l into the next layer.</p><formula xml:id="formula_12">X (l +1) = σ (A ′ X (l ) W (l ) ) + X (l )<label>(8)</label></formula><p>Here we propose another simple technique to improve the training of deep GCNs. In the original GCN settings, each node aggregates the representation of its neighbors from the previous layer. However, under the setting of deep GCNs, the strategy may not be suitable as it does not take the number of layers into account. Intuitively, neighbors nearby should contribute more than distant nodes. We thus propose a technique to better address this issue. The idea is to amplify the diagonal parts of the adjacency matrix A used in each GCN layer. In this way, we are putting more weights on the representation from the previous layer in the aggregation of each GCN layer. An example is to add an identity toĀ as follows.</p><formula xml:id="formula_13">X (l +1) = σ ((A ′ + I )X (l ) W (l ) )<label>(9)</label></formula><p>While (9) seems to be reasonable, using the same weight for all the nodes regardless of their numbers of neighbors may not be suitable. Moreover, it may suffer from numerical instability as values can grow exponentially when more layers are used. Hence we propose a modified version of (9) to better maintain the neighborhoods information and numerical ranges. We first add an identity to the original A and perform the normalizatioñ</p><formula xml:id="formula_14">A = (D + I ) −1 (A + I ),<label>(10)</label></formula><p>and then consider X (l +1) = σ ((Ã + λdiag(Ã))X (l ) W (l ) ).</p><p>Experimental results of adopting the "diagonal enhancement" techniques are presented in Section 4.3 where we show that this new normalization strategy can help to build deep GCN and achieve SOTA performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate our proposed method for training GCN on two tasks: multi-label and multi-class classification on four public datasets. The statistic of the data sets are shown in <ref type="table" target="#tab_2">Table 3</ref>. Note that the Reddit dataset is the largest public dataset we have seen so far for  GCN, and the Amazon2M dataset is collected by ourselves and is much larger than Reddit (see more details in Section 4.2). We include the following state-of-the-art GCN training algorithms in our comparisons:</p><p>• Cluster-GCN (Our proposed algorithm): the proposed fast GCN training method. • VRGCN 4 <ref type="bibr" target="#b1">[2]</ref>: It maintains the historical embedding of all the nodes in the graph and expands to only a few neighbors to speedup training. The number of sampled neighbors is set to be 2 as suggested in [2] 5 . • GraphSAGE 6 <ref type="bibr" target="#b4">[5]</ref>: It samples a fixed number of neighbors per node. We use the default settings of sampled sizes for each layer (S 1 = 25, S 2 = 10) in GraphSAGE.</p><p>We implement our method in PyTorch <ref type="bibr" target="#b12">[13]</ref>. For the other methods, we use all the original papers' code from their github pages. Since <ref type="bibr" target="#b8">[9]</ref> has difficulty to scale to large graphs, we do not compare with it here. Also as shown in <ref type="bibr" target="#b1">[2]</ref> that VRGCN is faster than FastGCN, so we do not compare with FastGCN here. For all the methods we use the Adam optimizer with learning rate as 0.01, dropout rate as 20%, weight decay as zero. The mean aggregator proposed by <ref type="bibr" target="#b4">[5]</ref> is adopted and the number of hidden units is the same for all methods.</p><p>Note that techniques such as <ref type="formula" target="#formula_0">(11)</ref> is not considered here. In each experiment, we consider the same GCN architecture for all methods. For VRGCN and GraphSAGE, we follow the settings provided by the original papers and set the batch sizes as 512. For Cluster-GCN, the number of partitions and clusters per batch for each dataset are listed in <ref type="table" target="#tab_3">Table 4</ref>. Note that clustering is seen as a preprocessing step and its running time is not taken into account in training. In Section 6, we show that graph clustering only takes a small portion of preprocessing time. All the experiments are conducted on a machine with a NVIDIA Tesla V100 GPU (16 GB memory), 20-core Intel Xeon CPU (2.20 GHz), and 192 GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Performance for median size datasets</head><p>Training Time vs Accuracy: First we compare our proposed method with other methods in terms of training speed. In <ref type="figure" target="#fig_4">Figure 6</ref>, the x-axis shows the training time in seconds, and y-axis shows the accuracy (F1 score) on the validation sets. We plot the training time  versus accuracy for three datasets with 2,3,4 layers of GCN. Since GraphSAGE is slower than VRGCN and our method, the curves for GraphSAGE only appear for PPI and Reddit datasets. We can see that our method is the fastest for both PPI and Reddit datasets for GCNs with different numbers of layers. For Amazon data, since nodes' features are not available, an identity matrix is used as the feature matrix X . Under this setting, the shape of parameter matrix W (0) becomes 334863x128. Therefore, the computation is dominated by sparse matrix operations such as AW (0) . Our method is still faster than VRGCN for 3-layer case, but slower for 2-layer and 4-layer ones. The reason may come from the speed of sparse matrix operations from different frameworks. VRGCN is implemented in TensorFlow, while Cluster-GCN is implemented in PyTorch whose sparse tensor support are still in its very early stage. In <ref type="table" target="#tab_5">Table 6</ref>, we show the time for TensorFlow and PyTorch to do forward/backward operations on Amazon data, and a simple two-layer network are used for benchmarking both frameworks. We can clearly see that TensorFlow is faster than PyTorch. The difference is more significant when the number of hidden units increases. This may explain why Cluster-GCN has longer training time in Amazon dataset.</p><p>Memory usage comparison: For training large-scale GCNs, besides training time, memory usage needed for training is often more important and will directly restrict the scalability. The memory usage includes the memory needed for training the GCN for many epochs. As discussed in Section 3, to speedup training, VRGCN needs to save historical embeddings during training, so it needs much more memory for training than Cluster-GCN. Graph-SAGE also has higher memory requirement than Cluster-GCN due to the exponential neighborhood growing problem. In <ref type="table" target="#tab_4">Table 5</ref>, we compare our memory usage with VRGCN's memory usage for GCN with different layers. When increasing the number of layers, Cluster-GCN's memory usage does not increase a lot. The reason is that when increasing one layer, the extra variable introduced is the weight matrix W (L) , which is relatively small comparing to the sub-graph and node features. While VRGCN needs to save each layer's history embeddings, and the embeddings are usually dense and will soon dominate the memory usage. We can see from <ref type="table" target="#tab_4">Table 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results on Amazon2M</head><p>A new GCN dataset: Amazon2M. By far the largest public data for testing GCN is Reddit dataset with the statistics shown in <ref type="table" target="#tab_2">Table  3</ref>, which contains about 200K nodes. As shown in <ref type="figure" target="#fig_4">Figure 6</ref> GCN training on this data can be finished within a few hundreds seconds.</p><p>To test the scalability of GCN training algorithms, we constructed a much larger graph with over 2 millions of nodes and 61 million edges based on Amazon co-purchasing networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. The raw co-purchase data is from Amazon-3M 7 . In the graph, each node is a product, and the graph link represents whether two products are purchased together. Each node feature is generated by extracting bag-of-word features from the product descriptions followed by Principal Component Analysis <ref type="bibr" target="#b6">[7]</ref> to reduce the dimension to be 100. In addition, we use the top-level categories as the labels for that product/node (see <ref type="table" target="#tab_6">Table 7</ref> for the most common categories). The detailed statistics of the data set are listed in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>In <ref type="table" target="#tab_7">Table 8</ref>, we compare with VRGCN for GCNs with a different number of layers in terms of training time, memory usage, and test accuracy (F1 score). As can be seen from the table that 1) VRGCN is faster than Cluster-GCN with 2-layer GCN but slower than Cluster-GCN when increasing one layer while achieving similar accuracy. 2) In terms of memory usage, VRGCN is using much more memory than Cluster-GCN (5 times more for 3-layer case), and it is running out of memory when training 4-layer GCN, while Cluster-GCN does not need much additional memory when increasing the number of layers, and achieves the best accuracy for this data when training a 4-layer GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Deeper GCN</head><p>In this section we consider GCNs with more layers. We first show the timing comparisons of Cluster-GCN and VRGCN in <ref type="table" target="#tab_8">Table 9</ref>. PPI is used for benchmarking and we run 200 epochs for both methods. We observe that the running time of VRGCN grows exponentially  We present numbers of epochs (x-axis) versus validation accuracy (yaxis). All methods except for the one using (11) fail to converge. because of its expensive neighborhood finding, while the running time of Cluster-GCN only grows linearly. Next we investigate whether using deeper GCNs obtains better accuracy. In Section 4.3, we discuss different strategies of modifying the adjacency matrix A to facilitate the training of deep GCNs. We apply the diagonal enhancement techniques to deep GCNs and run experiments on PPI. Results are shown in <ref type="table" target="#tab_10">Table 11</ref>. For the case of 2 to 5 layers, the accuracy of all methods increases with more layers added, suggesting that deeper GCNs may be useful. However, when 7 or 8 GCN layers are used, the first three methods fail to converge within 200 epochs and get a dramatic loss of accuracy. A possible reason is that the optimization for deeper GCNs becomes more difficult. We show a detailed convergence of a 8-layer GCN in <ref type="figure" target="#fig_3">Figure 5</ref>. With the proposed diagonal enhancement technique <ref type="bibr" target="#b10">(11)</ref>, the convergence can be improved significantly and similar accuracy can be achieved.</p><p>State-of-the-art results by training deeper GCNs. With the design of Cluster-GCN and the proposed normalization approach, we now have the ability for training much deeper GCNs to achieve better accuracy (F1 score). We compare the testing accuracy with other existing methods in <ref type="table" target="#tab_9">Table 10</ref>. For PPI, Cluster-GCN can achieve the state-of-art result by training a 5-layer GCN with 2048 hidden units. For Reddit, a 4-layer GCN with 128 hidden units is used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We present ClusterGCN, a new GCN training algorithm that is fast and memory efficient. Experimental results show that this method can train very deep GCN on large-scale graph, for instance on a graph with over 2 million nodes, the training time is less than an hour using around 2G memory and achieves accuracy of 90.41 (F1 score). Using the proposed approach, we are able to successfully train much deeper GCNs, which achieve state-of-the-art test F1 score on PPI and Reddit datasets.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">MORE DETAILS ABOUT THE EXPERIMENTS</head><p>In this section we describe more detailed settings about the experiments to help in reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets and software versions</head><p>We describe more details about the datasets in <ref type="table" target="#tab_1">Table 12</ref>. We download the datasets PPI, Reddit from the website <ref type="bibr" target="#b7">8</ref> and Amazon from the website 9 . Note that for Amazon, we consider GCN in an inductive setting, meaning that the model only learns from training data. In <ref type="bibr" target="#b2">[3]</ref> they consider a transductive setting. Regarding software versions, we install CUDA 10.0 and cuDNN 7.0. TensorFlow 1.12.0 and PyTorch 1.0.0 are used. We download METIS 5.1.0 via the offcial website <ref type="bibr" target="#b9">10</ref> and use a Python wrapper 11 for METIS library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation details</head><p>Previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> propose to pre-compute the multiplication of AX in the first GCN layer. We also adopt this strategy in our implementation. By precomputing AX , we are essentially using the exact 1-hop neighborhood for each node and the expensive neighbors searching in the first layer can be saved.</p><p>Another implementation detail is about the technique mentioned in Section 3.2 When multiple clusters are selected, some betweencluster links are added back. Thus the new combined adjacency matrix should be re-normalized to maintain numerical ranges of the resulting embedding matrix. From experiments we find the renormalization is helpful.</p><p>As for the inductive setting, the testing nodes are not visible during the training process. Thus we construct an adjacency matrix containing only training nodes and another one containing all nodes. Graph partitioning are applied to the former one and the partitioned adjacency matrix is then re-normalized. Note that feature normalization is also conducted. To calculate the memory usage, we consider tf.contrib.memory_stats.BytesInUse() for Ten-sorFlow and torch.cuda.memory_allocated() for PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">The running time of graph clustering algorithm and data preprocessing</head><p>The experiments of comparing different GCN training methods in Section 4 consider running time for training. The preprocessing time for each method is not presented in the tables and figures. <ref type="bibr" target="#b7">8</ref> http://snap.stanford.edu/graphsage/ 9 https://github.com/Hanjun-Dai/steady_state_embedding 10 http://glaros.dtc.umn.edu/gkhome/metis/metis/download 11 https://metis.readthedocs.io/en/latest/ While some of these preprocessing steps such as data loading or parsing are shared across different methods, some steps are algorithm specific. For instance, our method needs to run graph clustering algorithm during the preprocessing stage.</p><p>In <ref type="table" target="#tab_2">Table 13</ref>, we present more details about preprocessing time of Cluster-GCN on the four GCN datasets. For graph clustering, we adopt Metis, which is a fast and scalable graph clustering library. We observe that the graph clustering algorithm only takes a small portion of preprocessing time, showing a small extra cost while applying such algorithms and its scalability on large data sets. In addition, graph clustering only needs to be conducted once to form the node partitions, which can be re-used for later training processes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Histograms of entropy values based on the label distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The proposed stochastic multiple partitions scheme. In each epoch, we randomly sample q clusters (q = 2 is used in this example) and their between-cluster links to form a new batch. Same color blocks are in the same batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparisons of choosing one cluster versus multiple clusters. The former uses 300 partitions. The latter uses 1500 and randomly select 5 to form one batch. We present epoch (x-axis) versus F1 score (y-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Convergence figure on a 8-layer GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparisons of different GCN training methods. We present the relation between training time in seconds (x-axis) and the validation F1 score (y-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>[memory: good; time per epoch: bad; convergence: good] • VR-GCN [2] proposes to use a variance reduction technique to reduce the size of neighborhood sampling nodes. Despite successfully reducing the size of samplings (in our experiments VR-GCN with only 2 samples per node works quite well), it requires</figDesc><table /><note>storing all the intermediate embeddings of all the nodes in memory, leading to O(N F L) memory requirement. If the num- ber of nodes in the graph increases to millions, the memory requirement for VR-GCN may be too high to fit into GPU. [memory: bad; time per epoch: good; convergence: good.]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Random partition versus clustering partition of the graph (trained on mini-batch SGD). Clustering partition leads to better performance (in terms of test F1 score) since it removes less between-partition links. These three datasetes are all public GCN datasets. We will explain PPI data in the experiment part. Cora has 2,708 nodes and 13,264 edges, and Pubmed has 19,717 nodes and 108,365 edges.</figDesc><table><row><cell cols="3">Dataset random partition clustering partition</cell></row><row><cell>Cora</cell><cell>78.4</cell><cell>82.5</cell></row><row><cell>Pubmed</cell><cell>78.9</cell><cell>79.9</cell></row><row><cell>PPI</cell><cell>68.1</cell><cell>92.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Data statistics</figDesc><table><row><cell>Datasets</cell><cell>Task</cell><cell>#Nodes</cell><cell cols="3">#Edges #Labels #Features</cell></row><row><cell>PPI</cell><cell>multi-label</cell><cell>56,944</cell><cell>818,716</cell><cell>121</cell><cell>50</cell></row><row><cell>Reddit</cell><cell>multi-class</cell><cell cols="2">232,965 11,606,919</cell><cell>41</cell><cell>602</cell></row><row><cell>Amazon</cell><cell>multi-label</cell><cell>334,863</cell><cell>925,872</cell><cell>58</cell><cell>N/A</cell></row><row><cell cols="4">Amazon2M multi-class 2,449,029 61,859,140</cell><cell>47</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The parameters used in the experiments.</figDesc><table><row><cell>Datasets</cell><cell cols="3">#hidden units # partitions #clusters per batch</cell></row><row><cell>PPI</cell><cell>512</cell><cell>50</cell><cell>1</cell></row><row><cell>Reddit</cell><cell>128</cell><cell>1500</cell><cell>20</cell></row><row><cell>Amazon</cell><cell>128</cell><cell>200</cell><cell>1</cell></row><row><cell>Amazon2M</cell><cell>400</cell><cell>15000</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of memory usages on different datasets. Numbers in the brackets indicate the size of hidden units used in the model.</figDesc><table><row><cell></cell><cell></cell><cell>2-layer</cell><cell></cell><cell></cell><cell>3-layer</cell><cell></cell><cell></cell><cell>4-layer</cell><cell></cell></row><row><cell></cell><cell cols="9">VRGCN Cluster-GCN GraphSAGE VRGCN Cluster-GCN GraphSAGE VRGCN Cluster-GCN GraphSAGE</cell></row><row><cell>PPI (512)</cell><cell>258 MB</cell><cell>39 MB</cell><cell>51 MB</cell><cell>373 MB</cell><cell>46 MB</cell><cell>71 MB</cell><cell>522 MB</cell><cell>55 MB</cell><cell>85 MB</cell></row><row><cell>Reddit (128)</cell><cell>259 MB</cell><cell>284 MB</cell><cell>1074 MB</cell><cell>372 MB</cell><cell>285 MB</cell><cell>1075 MB</cell><cell>515 MB</cell><cell>285 MB</cell><cell>1076 MB</cell></row><row><cell>Reddit (512)</cell><cell>1031 MB</cell><cell>292 MB</cell><cell cols="2">1099 MB 1491 MB</cell><cell>300 MB</cell><cell cols="2">1115 MB 2064 MB</cell><cell>308 MB</cell><cell>1131 MB</cell></row><row><cell cols="2">Amazon (128) 1188 MB</cell><cell>703 MB</cell><cell cols="2">N/A 1351 MB</cell><cell>704 MB</cell><cell cols="2">N/A 1515 MB</cell><cell>705 MB</cell><cell>N/A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Benchmarking on the Sparse Tensor operations in PyTorch and TensorFlow. A network with two linear layers is used and the timing includes forward and backward operations. Numbers in the brackets indicate the size of hidden units in the first layer. Amazon data is used.</figDesc><table><row><cell></cell><cell cols="2">PyTorch TensorFlow</cell></row><row><cell>Avg. time per epoch (128)</cell><cell>8.81s</cell><cell>2.53s</cell></row><row><cell>Avg. time per epoch (512)</cell><cell>45.08s</cell><cell>7.13s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>The most common categories in Amazon2M.</figDesc><table><row><cell>Categories</cell><cell>number of products</cell></row><row><cell>Books</cell><cell>668,950</cell></row><row><cell>CDs &amp; Vinyl</cell><cell>172,199</cell></row><row><cell>Toys &amp; Games</cell><cell>158,771</cell></row><row><cell cols="2">that Cluster-GCN is much more memory efficient than VRGCN. For</cell></row><row><cell cols="2">instance, on Reddit data to train a 4-layer GCN with hidden dimen-</cell></row><row><cell cols="2">sion to be 512, VRGCN needs 2064MB memory, while Cluster-GCN</cell></row><row><cell>only uses 308MB memory.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparisons of running time, memory and testing accuracy (F1 score) for Amazon2M.</figDesc><table><row><cell></cell><cell>Time</cell><cell></cell><cell cols="2">Memory</cell><cell>Test F1 score</cell><cell></cell></row><row><cell></cell><cell cols="2">VRGCN Cluster-GCN</cell><cell cols="4">VRGCN Cluster-GCN VRGCN Cluster-GCN</cell></row><row><cell>Amazon2M (2-layer)</cell><cell>337s</cell><cell>1223s</cell><cell>7476 MB</cell><cell>2228 MB</cell><cell>89.03</cell><cell>89.00</cell></row><row><cell>Amazon2M (3-layer)</cell><cell>1961s</cell><cell cols="2">1523s 11218 MB</cell><cell>2235 MB</cell><cell>90.21</cell><cell>90.21</cell></row><row><cell>Amazon2M (4-layer)</cell><cell>N/A</cell><cell>2289s</cell><cell>OOM</cell><cell>2241 MB</cell><cell>N/A</cell><cell>90.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Comparisons of running time when using different numbers of GCN layers. We use PPI and run both methods for 200 epochs.</figDesc><table><row><cell></cell><cell cols="4">2-layer 3-layer 4-layer 5-layer 6-layer</cell></row><row><cell>Cluster-GCN</cell><cell>52.9s</cell><cell cols="3">82.5s 109.4s 137.8s 157.3s</cell></row><row><cell>VRGCN</cell><cell cols="2">103.6s 229.0s 521.2s</cell><cell>1054s</cell><cell>1956s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>State-of-the-art performance of testing accuracy reported in recent papers.</figDesc><table><row><cell></cell><cell>PPI</cell><cell>Reddit</cell></row><row><cell>FastGCN [1]</cell><cell>N/A</cell><cell>93.7</cell></row><row><cell cols="2">GraphSAGE [5] 61.2</cell><cell>95.4</cell></row><row><cell>VR-GCN [2]</cell><cell>97.8</cell><cell>96.3</cell></row><row><cell>GaAN [16]</cell><cell cols="2">98.71 96.36</cell></row><row><cell>GAT [14]</cell><cell>97.3</cell><cell>N/A</cell></row><row><cell cols="2">GeniePath [10] 98.5</cell><cell>N/A</cell></row><row><cell>Cluster-GCN</cell><cell cols="2">99.36 96.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Comparisons of using different diagonal enhancement techniques. For all methods, we present the best validation accuracy achieved in 200 epochs. PPI is used and dropout rate is 0.1 in this experiment. Other settings are the same as in Section 4.1. The numbers marked red indicate poor convergence.</figDesc><table><row><cell></cell><cell cols="7">2-layer 3-layer 4-layer 5-layer 6-layer 7-layer 8-layer</cell></row><row><cell>Cluster-GCN with (1)</cell><cell>90.3</cell><cell>97.6</cell><cell>98.2</cell><cell>98.3</cell><cell>94.1</cell><cell>65.4</cell><cell>43.1</cell></row><row><cell>Cluster-GCN with (10)</cell><cell>90.2</cell><cell>97.7</cell><cell>98.1</cell><cell>98.4</cell><cell>42.4</cell><cell>42.4</cell><cell>42.4</cell></row><row><cell>Cluster-GCN with (10) + (9)</cell><cell>84.9</cell><cell>96.0</cell><cell>97.1</cell><cell>97.6</cell><cell>97.3</cell><cell>43.9</cell><cell>43.8</cell></row><row><cell>Cluster-GCN with (10) + (11), λ = 1</cell><cell>89.6</cell><cell>97.5</cell><cell>98.2</cell><cell>98.3</cell><cell>98.0</cell><cell>97.4</cell><cell>96.2</cell></row><row><cell>(a) PPI (2 layers)</cell><cell></cell><cell>(b) PPI (3 layers)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c) PPI (4 layers)</cell></row><row><cell>(d) Reddit (2 layers)</cell><cell cols="2">(e) Reddit (3 layers)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(f) Reddit (4 layers)</cell></row><row><cell>(g) Amazon (2 layers)</cell><cell cols="2">(h) Amazon (3 layers)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(i) Amazon (4 layers)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>The training, validation, and test splits used in the experiments. Note that for the two amazon datasets we only split into training and test sets.</figDesc><table><row><cell>Datasets</cell><cell cols="2">Task Data splits (Tr./Val./Te.)</cell></row><row><cell>PPI</cell><cell>Inductive</cell><cell>44906/6514/5524</cell></row><row><cell>Reddit</cell><cell>Inductive</cell><cell>153932/23699/55334</cell></row><row><cell>Amazon</cell><cell>Inductive</cell><cell>91973/242890</cell></row><row><cell cols="2">Amazon2M Inductive</cell><cell>1709997/739032</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>The running time of graph clustering algorithm (METIS) and data preprocessing before the training of GCN.</figDesc><table><row><cell>Datasets</cell><cell cols="3">#Partitions Clustering Preprocessing</cell></row><row><cell>PPI</cell><cell>50</cell><cell>1.6s</cell><cell>20.3s</cell></row><row><cell>Reddit</cell><cell>1500</cell><cell>33s</cell><cell>286s</cell></row><row><cell>Amazon</cell><cell>200</cell><cell>0.3s</cell><cell>67.5s</cell></row><row><cell>Amazon2M</cell><cell>15000</cell><cell>148s</cell><cell>2160s</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Here we consider the memory for storing node embeddings, which is dense and usually dominates the overall memory usage for deep GCN.<ref type="bibr" target="#b1">2</ref> An epoch means a complete data pass.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/google-research/google-research/tree/master/cluster_gcn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">GitHub link: https://github.com/thu-ml/stochastic_gcn 5 Note that we also tried the default sample size 20 in VRGCN package but it performs much worse than sample size= 2.<ref type="bibr" target="#b5">6</ref> GitHub link: https://github.com/williamleif/GraphSAGE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">http://manikvarma.org/downloads/XC/XMLRepository.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement CJH acknowledges the support of NSF via IIS-1719097, Intel faculty award, Google Cloud and Nvidia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stochastic Training of Graph Convolutional Networks with Variance Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning Steady-States of Iterative Algorithms over Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1114" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weighted Graph Cuts Without Eigenvectors A Multilevel Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="770" to="778" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="417" to="441" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GeniePath: Graph Neural Networks with Adaptive Receptive Paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inferring Networks of Substitutable and Complementary Products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-Based Recommendations on Styles and Substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Qinfeng Shi, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph Attention Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Link Prediction Based on Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
