<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Universal Representation for Unseen Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Open Lab</orgName>
								<orgName type="institution">Newcastle University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Guan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Open Lab</orgName>
								<orgName type="institution">Newcastle University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI)</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Universal Representation for Unseen Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unseen Action Recognition (UAR) aims to recognise novel action categories without training examples. While previous methods focus on inner-dataset seen/unseen splits, this paper proposes a pipeline using a large-scale training source to achieve a Universal Representation (UR) that can generalise to a more realistic Cross-Dataset UAR (CD-UAR) scenario. We first address UAR as a Generalised Multiple-Instance Learning (GMIL) problem and discover 'building-blocks' from the large-scale ActivityNet dataset using distribution kernels. Essential visual and semantic components are preserved in a shared space to achieve the UR that can efficiently generalise to new datasets. Predicted UR exemplars can be improved by a simple semantic adaptation, and then an unseen action can be directly recognised using UR during the test. Without further training, extensive experiments manifest significant improvements over the UCF101 and HMDB51 benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The field of human action recognition has advanced rapidly over the past few years. We have moved from manually designed features <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b7">8]</ref> to learned convolutional neural network (CNN) features <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b14">15]</ref>; from encoding appearance information to encoding motion information <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36]</ref>; and from learning local features to learning global video features <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref>. The performance has continued to soar higher as we incorporate more of the steps into an end-to-end learning framework <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b54">55]</ref>. However, such robust and accurate action classifiers often rely on large-scale training video datasets using deep neural networks, which require large numbers of expensive annotated samples per action class. Although several large-scale video datasets have been proposed like Sports-1M <ref type="bibr" target="#b14">[15]</ref>, Activi- * Yang Long contributed equally to this work. tyNet <ref type="bibr" target="#b12">[13]</ref>, YouTube-8M <ref type="bibr" target="#b0">[1]</ref> and Kinetics <ref type="bibr" target="#b15">[16]</ref>, it is practically infeasible and extremely costly to annotate action videos with the ever-growing need of new categories.</p><p>Zero-shot action recognition has recently drawn considerable attention because of its ability to recognize unseen action categories without any labelled examples. The key idea is to make a trained model that can generalise to unseen categories with a shared semantic representation. The most popular side information being used are attributes, word vectors and visual-semantic embeddings. Such zero-shot learning frameworks effectively bypass the data collection limitations of traditional supervised learning approaches, which makes them more promising paradigms for UAR.</p><p>Extensive work on zero-shot action recognition has been done in the past five years. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24]</ref> considered using attributes for classifications. These attribute-based methods are easy to understand and implement, but hard to define and scale up to a large-scale scenario. Semantic representations like word vectors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b24">25]</ref> are thus preferred since only category names are required for constructing the label embeddings. There also has been much recent work on using visual-semantic embeddings extracted from pre-trained deep networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b27">28]</ref> due to their superior performance over single view word vectors or attributes.</p><p>However, whichever side information we adopt, the generalisation capability of these approaches is not promising, which is referred to as the domain shift problem. Most previous work thus still focuses on inner-dataset seen/unseen splits. This is not very practical since each new dataset or each category will require re-training. Motivated by such a fact we propose to utilise a large-scale training source to achieve a Universal Representation (UR) that can automatically generalise to a more realistic Cross-Dataset UAR (CD-UAR) scenario. Unseen actions from new datasets can be directly recognised via the UR without further training or fine-tuning on the target dataset.</p><p>The proposed pipeline is illustrated in <ref type="figure">Fig. 1</ref>. We first leverage the power of deep neural networks to extract vi- <ref type="figure">Figure 1</ref>. The proposed CD-UAR pipeline: 1) Extract deep features for each frame and summarise the video by essential components that are kernelised by GMIL; 2) Preserve shared components with the label embedding to achieve UR using NMF with JSD; 3) New concepts can be represented by UR and adjusted by domain adaptation. Test (green line): unseen actions are encoded by GMIL using the same essential components in ActivityNet to achieve a matching using UR. sual features, which results in a Generative Multiple Instance Learning (GMIL) problem. Namely, all the visual features (instances) in a video share the label while only a small portion is determinative. Compared to conventional global summaries of visual features using Bag-of-Visual-Word or Fisher Vector encoding, GMIL aims to discover those essential "building-blocks" to represent actions in the source and target domains and suppress the ambiguous instances. We then introduce our novel Universal Representation Learning (URL) algorithm composed of Non-negative Matrix Factorisation (NMF) with a Jensen-Shannon Divergence (JSD) constraint. The non-negativity property of NMF allows us to learn a part-based representation, which serves as the key bases between the visual and semantic modalities. JSD is a symmetrised and bounded version of the Kullback-Leibler divergence, which can make balanced generalisation to new distributions of both visual and semantic features. A representation that can generalise to both visual and semantic views, and both source and target domains, is referred to as the UR. More insighs of NMF, JSD, and UR will be discussed in the experiments. Our main contributions can be summarised as follows:</p><p>• This paper extends conventional UAR tasks to more realistic CD-UAR scenarios. Unseen actions in new datasets can be directly recognised via the UR without further training or fine-tuning on the target dataset.</p><p>• We propose a CD-UAR pipeline that incorporates deep feature extraction, Generative Multiple Instance Learning, Universal Representation Learning, and semantic domain adaptation.</p><p>• Our novel URL algorithm unifies NMF with a JSD constraint. The resultant UR can substantially preserve both the shared and generative bases of visual semantic features so as to withstand the challenging CD-UAR scenario.</p><p>• Extensive experiments manifest that the UR can effectively generalise across different datasets and outperform state-of-the-art approaches in inductive UAR scenarios using either low-level or deep features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Zero-shot human action recognition has advanced rapidly due to its importance and necessity as aforementioned. The common practice of zero-shot learning is to transfer action knowledge through a semantic embedding space, such as attributes, word vectors or visual features.</p><p>Initial work <ref type="bibr" target="#b21">[22]</ref> has considered a set of manually defined attributes to describe the spatial-temporal evolution of the action in a video. Gan et al. <ref type="bibr" target="#b11">[12]</ref> investigated the problem of how to accurately and robustly detect attributes from images or videos, and the learned high-quality attribute detectors are shown to generalize well across different categories. However, attribute-based methods suffer from several drawbacks: (1) Actions are complex compositions including various human motions and human-object interaction. It is extremely hard (e.g., subjective, labor-intensive, lack of domain knowledge) to determine a set of attributes for describing all actions; (2) Attribute-based approaches are not applicable for large-scale settings since they always require re-training of the model when adding new attributes;</p><p>(3) Despite the fact that the attributes can be data-driven learned or semi-automatically defined <ref type="bibr" target="#b9">[10]</ref>, their semantic meanings may be unknown or inappropriate.</p><p>Hence, word vectors have been preferred for zero-shot action recognition, since only category names are required for constructing the label embeddings. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45]</ref> are among the first works to adopt semantic word vector spaces as the intermediate-level embedding for zero-shot action recognition. Following <ref type="bibr" target="#b44">[45]</ref>, Alexiou et al. <ref type="bibr" target="#b2">[3]</ref> proposed to explore broader semantic contextual information (e.g., synonyms) in the text domain to enrich the word vector representation of action classes. However, word vectors alone are deficient for discriminating various classes because of the semantic gap between visual and textual information.</p><p>Thus, a large number of recent works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b42">43</ref>] exploit large object/scene recognition datasets to map object/scene scores in videos to actions. This makes sense since objects and scenes could serve as the basis to construct arbitrary action videos and the semantic representation can alleviate such visual gaps. The motivation can also be ascribed to the success of CNNs <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b47">48]</ref>. With the help of off-the-shelf object detectors, such methods <ref type="bibr" target="#b27">[28]</ref> could even perform zero-shot spatio-temporal action localization.</p><p>There are also other alternatives to solve zero-shot action recognition. Gan et al. <ref type="bibr" target="#b10">[11]</ref> leveraged the semantic inter-class relationships between the known and unknown actions followed by label transfer learning. Such similarity mapping doesn't require attributes. Qin et al. <ref type="bibr" target="#b31">[32]</ref> formulated zero-shot learning as designing error-correcting output codes, which bypass the drawbacks of using attributes or word vectors. Due to the domain shift problem, several works have extended the methods above using either transductive learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref> or domain adaptation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>However, all previous methods focus on inner-dataset seen/unseen splits while we extend the problem to CD-UAR. This scenario is more realistic and practical; for example, we can directly recognise unseen categories from new datasets without further training or fine-tuning. Though promising, CD-UAR is much more challenging compared to conventional UAR. We contend that when both CD and UAR are considered, the severe domain shift exceeds the generalization capability of existing approaches. Hence, we propose the URL algorithm to obtain a more robust universal representation. Our novel CD-UAR pipeline dramatically outperforms both conventional benchmarks and stateof-the-art approaches, which are in inductive UAR scenarios using low-level features and CD-UAR using deep features, respectively. One related work also applies NMF to zero-shot image classification <ref type="bibr" target="#b49">[50]</ref>. Despite the fact that promising generalisation is reported, which supports our insights, it still focuses on inner-class splits without considering CD-UAR. Also, their sparsity constrained NMF has completely different goals to our methods with JSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we first formalise the problem and clarify each step as below. We then introduce our CD-UAR pipeline in detail, which includes Genearalised Multiple-Instance Learning, Universal Representation Learning and semantic adaptation. Training Let (x 1 , y 1 ), · · · , (x Ns , y Ns ) ⊆ X s × Y s denote the training actions and their class labels in pairs in the source domain D s , where N s is the training sample size; each action</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution: Feature Learning+ Ambiguity Removal</head><formula xml:id="formula_0">x i has L i frames in a D-dimensional visual fea- ture space [x i ] = (x 1 i , ..., x Li i ) ∈ R D×Li ; y i ∈ {1, · · · , C} consists of C discrete labels of training classes.</formula><p>Inference Given a new dataset in the target domain D t with C u unseen action classes that are novel and distinct,</p><formula xml:id="formula_1">i.e. Y u = {C + 1, ..., C + C u } and Y u ∩ Y s = ∅,</formula><p>the key solution to UAR needs to associate these novel concepts to D s by human teaching. To avoid expensive annotations, we adopt Word2vec semantic (S) label embedding</p><formula xml:id="formula_2">(ŝ 1 ,ŷ 1 ), · · · , (ŝ Cu ,ŷ Cu ) ⊆ S u × Y u .</formula><p>Hat and subscript u denote information about unseen classes. Inference then can be achieved by learning a visual-semantic compatibility function min L(Φ(X s ), Ψ(S s )) that can generalise to S u . Test Using the learned L, an unseen actionx can be recognised by f :</p><formula xml:id="formula_3">Φ(x) → Ψ(S u ) × Y u .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Genearalised Multiple-Instance Learning</head><p>Conventional summary of x i can be achieved by Bagof-Visual-Words or Fisher Vectors <ref type="bibr" target="#b30">[31]</ref>. In GMIL, it is assumed that instances in the same class can be drawn from different distributions. Let P (·) denote the space of Borel probability measures over its argument, which is known as a bag. Conventionally, it is assumed that some instances are attractive P + (x) while others are repulsive P − (x). This paper argues that many instances may exist in neutral bags. In <ref type="figure" target="#fig_0">Fig. 2</ref>, we show an example of visual feature distributions of 'long-jump' and 'triple-jump'. Each point denotes a frame. While most frames fall in the neutral bags (red thumb), only a few frames (green thumb) are attractive to one class and repulsive to others. The neutral bags may contain many basic action bases shared by classes or just background noise. Conventional Maximum Mean Discrepancy <ref type="bibr" target="#b6">[7]</ref> may not well represent such distributions. Instead, this paper adopts the odds ratio embedding, which aims to discover the most attractive bases to each class c and suppress the neutral ones. This can be simply implemented by the pooled Naive Bayes Nearest Neighbor (NBNN) kernel <ref type="bibr" target="#b32">[33]</ref> at the 'bag-level'. We conduct k-means on each class to cluster them into H bags. The associated kernel function is:</p><formula xml:id="formula_4">k(x, x ) = φ(x) T φ(x ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_5">Φ(X s ) = [φ(x i )] = [φ 1 (x i ), ..., φ CH (x i )]</formula><p>T is the kernelised representation with odds ratio <ref type="bibr" target="#b26">[27]</ref> applies to each kernel embedding:</p><formula xml:id="formula_6">φ ci (x) = Li l=1 log p(c|x l ) p(c|x l )</formula><p>. Specific implementation details can be found in the supplementary material. In this way, we discover C × H bases as 'building-blocks' to represent any actions in both the source and target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Universal Representation Learning</head><p>For clarity, we use A = Φ(X s ) and B = S s to define the visual and semantic embeddings in the source domain D s : A × B. Towards universal representation, we aim to find a shared space that can: 1) well preserve the key bases between visual and semantic modalities; 2) generalise to new distributions of unseen datasets. For the former, let</p><formula xml:id="formula_7">A = [a 1 , · · · , a Ns ] ∈ R M1×Ns ≥0 and B = [b 1 , · · · , b Ns ] ∈ R M2×Ns ≥0 ; M 1 = C × H and M 2 = L. NMF is employed to find two nonnegative matrices from A: U ∈ R M1×D1 ≥0 and V 1 ∈ R D1×Ns ≥0</formula><p>and two nonnegative matrices from B:</p><formula xml:id="formula_8">W ∈ R M2×D2 ≥0 and V 2 ∈ R D2×Ns ≥0</formula><p>with full rank whose product can approximately represent the original matrix A and B, i.e., A ≈ U V 1 and B ≈ W V 2 . In practice, we set D 1 &lt; min(M 1 , N s ) and D 2 &lt; min(M 2 , N s ). We constrain the shared coefficient matrix:</p><formula xml:id="formula_9">V 1 = V 2 = V ∈ R D×Ns ≥0 .</formula><p>For the latter aim, we introduce JSD to preserve the generative components from the GMIL and use these essential 'building-blocks' to generalise to unseen datasets. Hence, the overall objective function is given as:</p><formula xml:id="formula_10">L = min U,W,V A − U V 2 F + B − W V 2 F +η JSD, s.t. U, W, V ≥ 0,<label>(2)</label></formula><p>where · F is the Frobenius norm; η is a smoothness parameter; JSD is short for the following equation:</p><formula xml:id="formula_11">JSD(P A ||P B ) = 1 2 KL(P A ||Q) + 1 2 KL(P B ||Q) = 1 2 i j p ij A log p ij A − p ij A log q ij + 1 2 i j p ij B log p ij B − p ij B log q ij<label>(3)</label></formula><p>where P A and P B are probability distributions in space A and B. We aim to find the joint probability distribution Q in the shared space V that is generalised to by P A and P B and their shifted distributions in the target domain. Specifically, JSD can be estimated pairwise as:</p><formula xml:id="formula_12">                   p ij A = g(a i , a j ) k =l g(a k , a l ) p ij B = g(b i , b j ) k =l g(b k , b l ) q ij = (1 + v i − v j 2 ) −1 k =l (1 + v k − v l 2 ) −1 .<label>(4)</label></formula><p>Without loss of generality, this paper use the crossentropy distance to implement g(·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Optimization</head><p>Let the Lagrangian of Eq. 2 be:</p><formula xml:id="formula_13">L = A − U V 2 + B − W V 2 + η JSD + tr(ΦU T ) + tr(ΘW T ) + tr(ΨV T ),<label>(5)</label></formula><p>where Φ, Θ and Ψ are three Lagrangian multiplier matrices. tr(·) denotes the trace of a matrix. For clarity, JSD in Eq. 3 is simply denoted as G. We define two auxiliary variables d ij and Z as follows:</p><formula xml:id="formula_14">d ij = v i − v j and Z = k =l (1 + d 2 kl ) −1 .<label>(6)</label></formula><p>Note that if v i changes, the only pairwise distances that change are d ij and d ji . Therefore, the gradient of function G with respect to v i is given by:</p><formula xml:id="formula_15">∂G ∂v i = 2 N j=1 ∂G ∂d ij (v i − v j ).<label>(7)</label></formula><p>Then ∂G ∂dij can be calculated by JS divergence in Eq. <ref type="formula" target="#formula_11">(3)</ref>:</p><formula xml:id="formula_16">∂G ∂d ij =− η 2 k =l (p kl A +p kl B ) 1 q kl Z ∂((1+d 2 kl ) −1 ) ∂d ij − 1 Z ∂Z ∂d ij .<label>(8)</label></formula><p>Since</p><formula xml:id="formula_17">∂((1+d 2 kl ) −1 ) ∂dij</formula><p>is nonzero if and only if k = i and l = j, and k =l p kl = 1, it can be simplified as:</p><formula xml:id="formula_18">∂G ∂d ij = η(p ij A + p ij B − 2q ij )(1 + d 2 ij ) −1 .<label>(9)</label></formula><p>Substituting Eq. (9) into Eq. <ref type="formula" target="#formula_15">(7)</ref>, we have the gradient of the JS divergence as:</p><formula xml:id="formula_19">∂G ∂v i =2η N j=1 (p ij A +p ij B −2q ij )(v i −v j )(1+ v i −v j 2 ) −1 .<label>(10)</label></formula><p>Let the gradients of L be zeros to minimize O f :</p><formula xml:id="formula_20">∂L ∂V =2(−U T A+U T U V−W T B+W T W V )+ ∂G ∂V +Ψ=0, (11) ∂L ∂U = 2(−AV T + U V V T ) + Φ = 0,<label>(12)</label></formula><formula xml:id="formula_21">∂L ∂W = 2(−BW T + W V V T ) + Θ = 0.<label>(13)</label></formula><p>In addition, we also have KKT conditions: Φ ij U ij = 0, Θ ij W ij = 0 and Ψ ij V ij = 0, ∀i, j. Then multiplying V ij , U ij and W ij in the corresponding positions on both sides of Eqs. (11), <ref type="bibr" target="#b11">(12)</ref> and <ref type="formula" target="#formula_4">(13)</ref> respectively, we obtain:</p><formula xml:id="formula_22">2(−U T A+U T U V −W T B+W T W V )+ ∂G ∂vi ij V ij =0, (14) 2(−AV T + U V V T ) ij U ij = 0,<label>(15)</label></formula><formula xml:id="formula_23">2(−BV T + W V V T ) ij W ij = 0.<label>(16)</label></formula><p>Note that</p><formula xml:id="formula_24">∂G ∂v j i = 2η N k=1 (p jk A + p jk B − 2q jk )(v j − v k ) 1 + v j − v k 2 i = 2η N k=1 (p jk A + p jk B − 2q jk )(V ij − V ik ) 1 + v j − v k 2 .</formula><p>The multiplicative update rules of the bases of both W and U for any i and j are obtained as:</p><formula xml:id="formula_25">U ij ← (AV T ) ij (U V V T ) ij U ij ,<label>(17)</label></formula><formula xml:id="formula_26">W ij ← (BV T ) ij (W V V T ) ij W ij .<label>(18)</label></formula><p>The update rule of the shared space preserving the coefficient matrix V between the visual and semantic data spaces is:</p><formula xml:id="formula_27">V ij ← (U T A) ij +(W T B) ij +Υ (U T U V ) ij +(W T W V ) ij +Γ V ij ,<label>(19)</label></formula><p>where for simplicity, we let Υ = η N k=1</p><formula xml:id="formula_28">(p jk A +p jk B )V ik +2q jk Vij 1+ vj−v k 2 , Γ = η N k=1 (p jk A +p jk B )Vij+2q jk V ik 1+ vj−v k 2 .</formula><p>All the elements in U , W and V can be guaranteed to be nonnegative from the allocation. <ref type="bibr" target="#b19">[20]</ref> proves that the objective function is monotonically non-increasing after each update of U , W or V . The proof of convergence about U , W and V is similar to that in <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b4">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Orthogonal Projection</head><p>After U , W and V have converged, we need two projection matrices P A and P B to project A and B into V . However, since our algorithm is NMF-based, a direct projection to the shared space does not exist. Inspired by <ref type="bibr" target="#b3">[4]</ref>, we learn two rotations to protect the data originality while projecting it into the universal space, which is known as the Orthogonal Procrustes problem <ref type="bibr" target="#b34">[35]</ref>:</p><formula xml:id="formula_29">   min P A P A A − V , s.t. P T A P A = I, min P B P B B − V , s.t. P T B P B = I,<label>(20)</label></formula><p>where I is an identity matrix. According to <ref type="bibr" target="#b51">[52]</ref>, orthogonal projection has the following advantages: 1) It can preserve the data structure; 2) It can redistribute the variance more evenly, which maximally decorrelates dimensions. The optimisation is simple. We first use the singular value decomposition (SVD) algorithm to decompose the matrix: </p><formula xml:id="formula_30">A T V = QΣ S T . Then P A = SΛQ T ,</formula><p>wherev Bu ∈V B . The overall Universal Representation Learning (URL) is summarised in Algorithm 1. </p><formula xml:id="formula_32">P A = S A Ω Q T A ; P B = S B Ω Q T B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Computational Complexity Analysis</head><p>The UAR test can be achieved by efficient NN search among a small number of prototypes. The training con-sists of three parts. For NMF optimisation, each iteration takes O(max{M 1 N D, M 2 N D}). In comparison, the basic NMF algorithm in <ref type="bibr" target="#b19">[20]</ref> applied to A and B separately will have complexity of O(M 1 N D) and O(M 2 N D) respectively. In other words, our algorithm is no more complex than the basic NMF. The second regression requires SVD decomposition which has complexity O <ref type="figure" target="#fig_0">(2N 2 D)</ref>. Therefore, the total computational complexity is: O(max{M 1 N D, M 2 N D}t+2N 2 D), w.r.t. the number of iterations t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Semantic Adaptation</head><p>Since we aim to make the UR generalise to new datasets, the domain shift between D s and D u is unknown. For improved performance, we can use the semantic information of the target domain to approximate the shift. The key insight is to measure new unseen class labels using our discovered 'building blocks'. Because the learnt UR can reliably associate visual and semantic modalities, i.e. V A ∼V B we could approximate the seen-unseen discrep-</p><formula xml:id="formula_33">ancy V A →V A by V A →V B .</formula><p>To this end, we employ Transfer Joint Matching (TJM) <ref type="bibr" target="#b22">[23]</ref>, which achieves feature matching and instance reweighing in a unified framework. We first mix the projected semantic embeddings of unseen classes with our training samples in the UR space by</p><formula xml:id="formula_34">[V A ,V B ] ∈ R D×(Ns+Cu) , where V A = P A A.</formula><p>TJM can provide an adaptive matrix A and a kernel matrix K:</p><formula xml:id="formula_35">L T JM (V A ,V B ) → (A, K),<label>(22)</label></formula><p>through which we can achieve the adapted unseen class</p><formula xml:id="formula_36">prototypesV B in the UR space via Z = A T K = [V A ,V B ].</formula><p>Unseen Action Recognition Given a test actionx, we first convert it into a kernelised representation using the trained GMIL kernel embedding in Eq. 1:â = [φ 1 (x), ..., φ CH (x)] T . Similar to Eq. 21, we can now make a prediction using the adapted unseen prototypes:</p><formula xml:id="formula_37">y = arg max C+1 u C+Cu P Aâ −v Bu 2 2 .<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We perform the URL on the large-scale ActivityNet <ref type="bibr" target="#b12">[13]</ref> dataset. Cross-dataset UAR experiments are conducted on two widely-used benchmarks, UCF101 <ref type="bibr" target="#b37">[38]</ref> and HMDB51 <ref type="bibr" target="#b17">[18]</ref>. UCF101 and HMDB51 contain trimmed videos while ActivityNet contains untrimmed ones. We first compare our approach to state-of-the-art methods using either low-level or deep features. To understand the contribution of each component of our method, we also provide detailed analysis of possible alternative baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Datasets ActivityNet 1 consists of 10024 training, 4926 validation, and 5044 test videos from 200 activity classes. Each class has at least 100 videos. Since the videos are untrimmed, a large proportion of videos have a duration between 5 and 10 minutes. UCF101 is composed of realistic action videos from YouTube. It contains 13320 video clips distributed among 101 action classes. Each class has at least 100 video clips and each clip lasts an average duration of 7.2s. HMDB51 includes 6766 videos of 51 action classes extracted from a wide range of sources, such as web videos and movies. Each class has at least 101 video clips and each clip lasts an average duration of 4.3s. Visual and Semantic Representation For all three datasets, we use a single CNN model to obtain the video features. The model is a ResNet-200 initially trained on ImageNet and fine-tuned on ActivityNet dataset. Overlapping classes between ActivityNet and UCF101 are not used during fine-tuning. We adopt the good practices from temporal segment networks (TSN) <ref type="bibr" target="#b41">[42]</ref>, which is one of the state-of-the-art action classification frameworks. We extract feature from the last average pooling layer (2048-d) as our frame-level representation. Note that we only use features extracted from a single RGB frame. We believe better performance could be achieved by considering motion information, e.g. features extracted from multiple RGB frames <ref type="bibr" target="#b38">[39]</ref> or consecutive optical flow <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b53">54]</ref>. However, our primary aim is to demonstrate the ability of universal representations. Without loss of generality, we use the widelyused skip-gram neural network model <ref type="bibr" target="#b28">[29]</ref> that is trained on Google News dataset and represent each category name by an L2-normalized 300-d word vector. For multi-word names, we use accumulated word vectors <ref type="bibr" target="#b29">[30]</ref>. Implementation Details For GMIL, we estimate the pooled local NBNN kernel <ref type="bibr" target="#b32">[33]</ref> using k nn = 200 to estimate the odds-ratio in <ref type="bibr" target="#b26">[27]</ref>. The best hyper-parameter η for URL and that in TJM are achieved through cross-validation. In order to enhance the robustness, we propose a leave-onehop-away cross validation. Specifically, the training set of ActivityNet is evenly divided into 5 hops according to the ontological structure. In each iteration, we use 1 hop for validation while the other furthest 3 hops are used for training. Except for feature extraction, the whole experiment is conducted on a PC with an Intel quad-core 3.4GHz CPU and 32GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art Methods</head><p>Comparison Using Low-level Features Since most existing methods are based on low-level features, we observe a significant performance gap. For fair comparison, we first follow <ref type="bibr" target="#b31">[32]</ref> and conduct experiments in a conventional inductive scenario. The seen/unseen splits for HMDB51 and UCF101 are 27/26 and 51/50, respectively. Visual features are 50688-d Fisher Vectors of improved dense trajectory <ref type="bibr" target="#b40">[41]</ref>, which are provided by <ref type="bibr" target="#b46">[47]</ref>. Semantic features use the same Word2vec model. Without local features for each frame, our training starts from the URL. Note some methods <ref type="bibr" target="#b44">[45]</ref> are also based on a transductive assumption. Our method can simply address such a scenario by incorporatinĝ V A into the TJM domain adaptation. We report our results in <ref type="table" target="#tab_1">Table 1</ref>. The accuracy is averaged over 10 random splits.</p><p>Our method outperforms all of the compared state-ofthe-art methods in the same inductive scenario. Although the transductive setting to some extent violates the 'unseen' action recognition constraint, the TJM domain adaptation method shows significant improvements. However, none of the compared methods are competitive to the proposed pipeline even though it is completely inductive plus crossdataset challenge. Comparison Using Deep Features In <ref type="table">Table 2</ref>, we follow recent work <ref type="bibr" target="#b27">[28]</ref> which provides the most comparisons to related zero-shot approaches. Due to many different data splits and evaluation metrics, the comparison is divided into the three most common settings, i.e. using the standard supervised test splits; using 50 randomly selected actions for testing; and using 20 actions randomly for testing.</p><p>The highlights of the comparison are summarised as fol- lows. First, <ref type="bibr" target="#b27">[28]</ref> is also a deep-feature based approach, which employs a GoogLeNet network, pre-trained on a 12,988-category shuffle of ImageNet. In addition, it adopts the Faster R-CNN pre-trained on the MS-COCO dataset. Secondly, it also does not need training or fine-tuning on the test datasets. In other words, <ref type="bibr" target="#b27">[28]</ref> shares the same spirit to our cross-dataset scenario, but from an object detection perspective. By contrast, our CD-UAR is achieved by pure representation learning. Overall, this is a fair comparison and worthy of a thorough discussion.</p><p>Our method consistently outperforms all of the compared approaches, with minimum margins of 1.4%, 2.1%, and 2.6% over <ref type="bibr" target="#b27">[28]</ref>, respectively. Note that, other than <ref type="bibr" target="#b13">[14]</ref> which is also deep-model-based, there are no other competitive results. Such a finding suggests future UAR research should focus on deep features instead. Besides visual features, we use the similar skip-gram model of Word2vec for label embeddings.Therefore, the credit of performance improvements should be given to the method itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">In-depth Analysis</head><p>Since our method outperforms all of the compared benchmarks, to further understand the success of the method, we conduct 5 baselines as alternatives to our main approach. The results are summarised in <ref type="table" target="#tab_3">Table 3</ref>. Convergence Analysis Before analysing baselines, we first show examples of convergence curves in <ref type="figure" target="#fig_2">Fig. 3</ref> during our URL optimisation. It can be seen the overall loss reliably converges after approximately 400 iterations. The JSD constraint in (2) gradually resolves while the decomposition losses (3) and (4) tend to be competing to each other. This can be ascribed to the difference of ranks between A and B. While A is instance-level kernelised features, B is classlevel Word2vec that has much lower rank than that of A. The alternation in each iteration reweighs A and B once in turn, despite the overall converged loss. GMIL vs FV As we stated earlier, the frame-based action features can be viewed as the GMIL problem. Therefore, we change the encoding to conventional FV and keep the rest of the pipeline. It can be seen that the average performance drop is 2% with as high as 6.9% in transductive scenario on UCF101. Separated Contribution Our URL algorithm is arguably the main contribution in this paper. To see our progress over conventional NMF, we set η = 0 to remove the JSD constraint. As shown in <ref type="table" target="#tab_3">Table 3</ref>, the performance is severely degraded. This is because NMF can only find the shared bases regardless of the data structural change. GNMF <ref type="bibr" target="#b4">[5]</ref> may not address this problem as well (not proved) because we need to preserve the distributions of those generative bases rather than data structures. While generative bases are 'building blocks' for new actions, the data structure may completely change in new datasets. However, NMF is better at preserving bases than canonical correlation analysis (CCA) which is purely based on mutual-information maximisation. Therefore, a significant performance gap can be observed between the results of CCA and NMF. Without Domain Adaptation In our pipeline, TJM is used to adjust the inferred unseen prototypes from Word2vec.</p><p>The key insight is to align the inferred bases to that of GMIL in the source domain that is also used to represent unseen actions. In this way, visual and semantic UR is connected byV B ∼ V A ∼V A . Without such a scheme, however, we observe marginal performance degradation in the CD-UAR scenario (roughly 3%). This is probably because Activi-tyNet is rich and the concepts of HMDB51 and UCF101 are not very distinctive. We further investigate the CD transductive scenario, which assumesV A can be observed for TJM. As a result, the benefit from domain adaptation is large (roughly 5% on HMDB51 and 1% on UCF101 between 'Ours' and 'No TJM'). Basis Space Size We propose two sets of size according to the original sizes of A and B (recall section 3.2), namely the high one D high = 1 2 (M 1 + M 2 ) and the low one D low = 1 4 (M 1 + M 2 ). As shown in <ref type="table" target="#tab_3">Table 3</ref>, the higher dimension gives better results in most cases. Note that the performance difference is not significant. We can thus conclude that our method is not sensitive to the basis space size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper studied a challenging Cross-Dataset Unseen Action Recognition problem. We proposed a pipeline consisting of deep feature extraction, Generative Multiple-Instance Learning, Universal Representation Learning, and Domain Adaptation. A novel URL algorithm was proposed to incorporate Non-negative Matrix Factorisation with a Jensen-Shannon Divergence constraint. NMF was shown to be advantageous for finding shared bases between visual and semantic spaces, while the remarkable improvement of JSD was empirically demonstrated in distributive basis preserving for unseen dataset generalisation. The resulting Universal Representation effectively generalises to unseen actions without further training or fine-tuning on the new dataset. Our experimental results exceeded that of stateof-the-art methods using both conventional and deep features. Detailed evaluation manifests that most of contribution should be credited to the URL approach.</p><p>We leave several interesting open questions. For methodology, we have not examined other variations of NMF or divergences. The GMIL problem is proposed without indepth discussion, although a simple trial using pooled local-NBNN kernel showed promising progress. In addition, the improvement of TJM was not significant in inductive CD-UAR. A unified framework for GMIL, URL and domain adaptation could be a better solution in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Visualisation of feature distributions of action 'longjump' and 'triple-jump' in the ActivityNet dataset using tSNE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where Λ is a connection matrix as Λ = [I, 0] ∈ R D×M and 0 indicates all zeros in the matrix. P B is achieved in the same way. Given a new dataset D t , semantic embeddings B u = S u can be projected into V as class-level UR prototypes in an unseen action galleryV B = P B B u . A test exampleâ can be simply predicted by nearest neighbour search:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Convergence analysis w.r.t. # iterations. (1) is the overall loss in Eq. 2. (2) is the JSD loss. (3) and (4) show decomposition losses of A and B, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The basis matrices U , W , orthogonal projections P A and P B . 1: Initialize U , W and V with uniformly distributed random values between 0 and 1. 2: repeat 3: Compute the basis matrices U and W and UR matrix V via Eqs. (17), (18) and (19), respectively; 4: until convergence 5: SVD decomposes the matrices A T V and B T V to obtain Q A Σ S T A and Q B Σ S T</figDesc><table /><note>Algorithm 1 Universal Representation Learning (URL) Require: Source domain Ds : A ∈ R M1×N and B ∈ R M2×N ; number of bases D; hyper-parameter η; Ensure:B 6:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art methods using standard low-level features. Last two sets of results are just for reference. T: transductive; I: inductive; Results are in %.</figDesc><table><row><cell>Method</cell><cell>Feature</cell><cell cols="3">Setting HMDB51 UCF101</cell></row><row><cell>ST [45]</cell><cell>BoW</cell><cell>T</cell><cell cols="2">15.0±3.0 15.8±2.3</cell></row><row><cell>ESZSL [34]</cell><cell>FV</cell><cell>I</cell><cell cols="2">18.5±2.0 15.0±1.3</cell></row><row><cell>SJE [2]</cell><cell>FV</cell><cell>I</cell><cell>13.3±2.4</cell><cell>9.9±1.4</cell></row><row><cell>MTE [47]</cell><cell>FV</cell><cell>I</cell><cell cols="2">19.7±1.6 15.8±1.3</cell></row><row><cell>ZSECOC [32]</cell><cell>FV</cell><cell>I</cell><cell cols="2">22.6±1.2 15.1±1.7</cell></row><row><cell>Ours</cell><cell>FV</cell><cell>I</cell><cell cols="2">24.4±1.6 17.5±1.6</cell></row><row><cell>Ours</cell><cell>FV</cell><cell>T</cell><cell cols="2">28.9±1.2 20.1±1.4</cell></row><row><cell>Ours</cell><cell>GMIL-D</cell><cell>CD</cell><cell cols="2">51.8±0.7 42.5±0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>In-depth analysis with baseline approaches. 'Ours' refers to the complete pipeline with deep features, GMIL kernel embedding, URL with NMF and JSD, and TJM. (Results are in %).</figDesc><table><row><cell>Dataset</cell><cell cols="2">HMDB51</cell><cell></cell><cell cols="2">UCF101</cell></row><row><cell>Setting</cell><cell cols="5">Cross-Dataset Transductive Cross-Dataset Transductive</cell></row><row><cell>GMIL+ESZSL[34]</cell><cell>25.7</cell><cell>30.2</cell><cell></cell><cell>19.8</cell><cell>24.9</cell></row><row><cell cols="2">UR Dimensionality Low High</cell><cell cols="3">Low High Low High</cell><cell>Low High</cell></row><row><cell>Fisher Vector</cell><cell>47.7 48.6</cell><cell>53.9 54.6</cell><cell cols="2">35.8 39.7</cell><cell>42.2 43.0</cell></row><row><cell>NMF (no JSD)</cell><cell>17.2 18.0</cell><cell>19.2 20.4</cell><cell cols="2">15.5 17.4</cell><cell>18.2 19.8</cell></row><row><cell>CCA</cell><cell>13.8 12.2</cell><cell>18.2 17.1</cell><cell>8.2</cell><cell>9.6</cell><cell>12.9 13.6</cell></row><row><cell>No TJM</cell><cell>48.9 50.5</cell><cell>51.8 53.9</cell><cell cols="2">32.5 36.6</cell><cell>38.1 38.6</cell></row><row><cell>Ours</cell><cell>49.6 51.8</cell><cell>57.8 58.2</cell><cell cols="2">36.1 42.5</cell><cell>47.4 49.9</cell></row><row><cell cols="3">Pipeline Validation Due to the power of deep features</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">demonstrated by the above comparison, an intuitive as-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">sumption is that the CD-UAR can be easily resolved by</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">deep features. We thus use the same GMIL features fol-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">lowed by a state-of-the-art ESZSL [34] using RBF ker-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">nels. The performance in Table 1 (15.0%) is improved to</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(19.8%), which is marginal to our surprise. Such a results</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">shows the difficulty of CD-UAR while confirms the contri-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>bution of the proposed pipeline.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the latest release 1.3 of ActivityNet for our experiments</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported in part by a NSF CAREER grant, No. IIS-1150115. We gratefully acknowledge the support of NVIDIA Corporation through the donation of the Titan X GPUs used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">YouTube-8M: A Large-Scale Video Classification Benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of Output Embeddings for Fine-Grained Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring Synonyms as Context in Zero-Shot Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alexiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral Regression for Efficient Regularized Subspace Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph Regularized Nonnegative Matrix Factorization for Data Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Temporal Linear Encoding Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Unifying Framework for Learning Bag Labels from Generalized Multiple-Instance Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tuytelaars. Modeling Video Evolution for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transductive Multi-view Embedding for Zero-Shot Recognition and Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Multi-modal Latent Attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring Semantic Inter-Class Relationships (SIR) for Zero-Shot Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Attributes Equals Multi-Source Domain Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Objects2action: Classifying and Localizing Actions without Any Video Example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The Kinetics Human Action Video Dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation for Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">HMDB: A Large Video Database for Human Motion Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Local Video Feature for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Algorithms for Non-Negative Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing Unseen Actions in a Domain-Adapted Embedding Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing Human Actions by Attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfer Joint Matching for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-shot learning using synthesised unseen visual data with diffusion regularisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards affordable semantic searching: Zero-shot retrieval via dominant attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to recognise unseen classes by a few similes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Local Naive Bayes Nearest Neighbor for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Spatial-Aware Object Embeddings for Zero-Shot Localization and Classification of Actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and Their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evaluating Neural Word Representations in Tensor-Based Compositional Settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Milajevs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving the Fisher Kernel for Large-Scale Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-Shot Action Recognition with Error-Correcting Output Codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Pooled NBNN Kernel: Beyond Image-to-Class and Image-to-Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An Embarrassingly Simple Approach to Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Generalized Solution of the Orthogonal Procrustes Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Schönemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Asynchronous Temporal Fields for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-Stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Long-term Temporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action Recognition with Improved Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Alternative Semantic Representations for Zero-Shot Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Harnessing Object and Scene Semantics for Large-Scale Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semantic Embedding Space for Zero-Shot Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Transductive Zero-Shot Action Recognition by Word-Vector Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-Task Zero-Shot Action Recognition with Prioritised Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep Texture Manifold for Ground Terrain Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Differential Angular Imaging for Material Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zero-shot classification with discriminative semantic representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05957</idno>
		<title level="m">Image De-raining Using a Conditional Generative Adversarial Network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast Orthogonal Projection based on Kronecker Product</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction with Category Information Fusion and Non-Negative Matrix Factorization for Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AICI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02295</idno>
		<title level="m">Guided Optical Flow Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Hidden Two-Stream Convolutional Networks for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00389</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Depth2Action: Exploring Embedded Depth for Large-Scale Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">DenseNet for Dense Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
