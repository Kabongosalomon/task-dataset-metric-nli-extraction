<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rotation Equivariant CNNs for Digital Pathology</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastiaan</forename><forename type="middle">S</forename><surname>Veeling</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Linmans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Winkens</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rotation Equivariant CNNs for Digital Pathology</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new model for digital pathology segmentation, based on the observation that histopathology images are inherently symmetric under rotation and reflection. Utilizing recent findings on rotation equivariant CNNs, the proposed model leverages these symmetries in a principled manner. We present a visual analysis showing improved stability on predictions, and demonstrate that exploiting rotation equivariance significantly improves tumor detection performance on a challenging lymph node metastases dataset. We further present a novel derived dataset to enable principled comparison of machine learning models, in combination with an initial benchmark. Through this dataset, the task of histopathology diagnosis becomes accessible as a challenging benchmark for fundamental machine learning research. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of digital pathology is developing rapidly, following recent advancements in microscopic imaging hardware that allow digitizing glass slides into whole-slide images (WSIs). This digitization has facilitated image analysis algorithms to assist and automate diagnostic tasks. A proven approach is to use convolutional neural networks (CNNs), a type of deep learning model, trained on patches extracted from whole-slide images. The aggregate of these patchbased predictions serves as a slide-level representation used by models to identify metastases, stage cancer or diagnose complications. This approach has been shown to outperform pathologists in a variety of tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>This performance is achieved using off-the-shelf CNN architectures originally designed for natural images <ref type="bibr" target="#b1">[2]</ref>. The effectiveness of these models can be largely attributed to the efficient sharing of parameters in convolutional layers. As a result, local patterns are encoded independently of their spatial location, and shifting the input leads to a predictable shift in the output. This property, known as translational equivariance, effectively exploits the translational symmetry inherent in natural images leading to strong generalization.</p><p>In contrast to natural images, WSIs exhibit not only translational symmetry but rotation and reflection symmetry as well. CNNs do not exploit these symmetries, and as a result are found empirically to spend a large part of their parameter budget on multiple rotated and reflected copies of filters <ref type="bibr" target="#b3">[4]</ref>. Additionally, we find that CNNs trained on histopathology data exhibit erratic fluctuations in predictions under input rotation and reflection. Enforcing equivariance in the model under these transformations is expected to reduce such instabilities, and lower the risk of overfitting by improving parameter sharing.</p><p>To encode these symmetries, we leverage recent findings in rotation equivariant CNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, a current topic of interest in the machine learning community. These methods show strong generalization under limited dataset size and are more robust under adversarial perturbations in rotation, translation and local geometric distortions <ref type="bibr" target="#b7">[8]</ref>. We propose a fully-convolutional patch-classification model that is equivariant to 90 • rotations and reflection, using the method proposed by <ref type="bibr" target="#b4">[5]</ref>. We evaluate the model on the Camelyon16 benchmark <ref type="bibr" target="#b8">[9]</ref>, showing significant improvement over a comparable CNN on slide level classification and tumor localization tasks.</p><p>As slide-level metrics potentially obscure the relative performance of patchlevel models, we further validate on a patch-level task. In this regime, there is currently no benchmark that harbors the high volume, quality and variety of Camelyon16. Thus, we present PatchCamelyon(PCam), a large-scale patch-level dataset derived from Camelyon16 data. Through this dataset, we demonstrate that the proposed model is more accurate and more robust under input rotation and reflection, compared to an equivalent standard CNN.</p><p>The contributions of this work are as follows: (1) we propose a novel deep learning model that utilizes symmetries inherent to histopathology 1 , (2) demonstrate that rotation equivariance improves model reliability and (3) present a new large-scale histopathology dataset that enables precise model evaluation.</p><p>Related Work A common approach to improve orientation robustness is to train CNNs using extensive data augmentation, perturbing data with random transformations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Although this may improve generalization, it fails to capture local symmetries and does not guarantee equivariance at every layer. As CNNs have to learn rotation equivariance from data, they require a larger model capacity to hold copies of identical filters. Even if rotation equivariance is achieved on training data, there is no guarantee that this generalizes to a test set. Orthogonally, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> propose a test-time augmentation strategy that averages the predictions of 90 • -rotated and mirrored versions to improve robustness to orientation-induced instability. As a downside, this comes at 8 times the computational cost and does not provide guarantees on equivariance <ref type="bibr" target="#b10">[11]</ref>.</p><p>Methods that enable equivariance under rotations and other transformations include Harmonic Networks <ref type="bibr" target="#b5">[6]</ref>, which constrain the set of filters to circular harmonics, allowing for full 360 • -equivariance. <ref type="bibr" target="#b6">[7]</ref> employs steerable filters and evenly samples a small number of rotations. In this work, we focus on the straight-forward G-CNN method from <ref type="bibr" target="#b4">[5]</ref> applied on discrete rotation/reflection groups. Although these groups do not cover the full continuous rotational symmetry inherent in WSIs, the empirical evidence gathered so far shows that 90 • rotation equivariance improves performance significantly <ref type="bibr" target="#b6">[7]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>In the mathematical model of CNNs and G-CNNs introduced in <ref type="bibr" target="#b4">[5]</ref>, input images and output segmentation masks are considered to be functions f : Z 2 → R K , where K denotes the number of channels, and f is implicitly assumed to be zero outside of some rectangular domain.</p><p>A standard convolution 2 (denoted * ) of an input f with filter ψ is defined as:</p><formula xml:id="formula_0">[f * ψ](x) = y∈Z 2 K k=1 f k (y)ψ k (x − y).<label>(1)</label></formula><p>G-CNNs are a generalization of CNNs that are equivariant under more general symmetry groups, such as the group G = p4 of 90 • rotations, or G = p4m which additionally includes reflection. In a G-CNN, the feature maps are thought of as functions on this group. For p4 and p4m, this simply means that feature channels come in groups of 4 or 8, corresponding to the 4 pure rotations in p4 or the 8 roto-reflections in p4m. In the first layer, these are produced using the (Z 2 → G)-convolution:  where g = (r, t) is a roto-translation (in case G = p4) or roto-reflection-translation (in case G = p4m).</p><formula xml:id="formula_1">[f * ψ](g) = y∈Z 2 K k=1 f k (y)ψ k (g −1 y),<label>(2)</label></formula><p>In further layers, both feature maps and filters are functions on G, and these are combined using the (G → G)-convolution:</p><formula xml:id="formula_2">[f * ψ](g) = h∈G K k=1 f k (h)ψ k (g −1 h).<label>(3)</label></formula><p>In the final layer, a group-pooling layer is used to ensure that the output is either invariant (for classification tasks) or equivariant as a function on the plane (for segmentation tasks, where the output is supposed to transform together with the input). In <ref type="figure" target="#fig_0">Fig. 1</ref> we demonstrate how equivariance is achieved through this process. Non-linear activations and pooling operations are equivariant in p4m <ref type="bibr" target="#b4">[5]</ref>, allowing layers to be freely stacked to enable deep architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">G-CNN DenseNet architecture</head><p>The proposed patch-classification model is shown in <ref type="figure" target="#fig_1">Fig. 2</ref> for p4 (the p4mvariant is a trivial extension). The architecture is based on the densely connected convolutional network (DenseNet) <ref type="bibr" target="#b11">[12]</ref>, which consist of dense blocks with layers that use the stack of all previous layers as input, alternated with transition blocks consisting of a 1×1 convolutional layer and 2×2 strided average pooling. We use one layer per dense block due to the limited receptive field of the model, with 5 dense-block/transition-block pairs. The model spatially-pools the input by a factor of 2 5 , the output of which resembles the segmentation resolution used in <ref type="bibr" target="#b0">[1]</ref>.</p><p>Full-model group equivariance is achieved by replacing all convolution layers with group-equivariant versions <ref type="bibr" target="#b4">[5]</ref>. Batch normalization layers <ref type="bibr" target="#b12">[13]</ref> are made group-equivariant by aggregating moments per group feature map rather than spatial feature map (as proposed by <ref type="bibr" target="#b4">[5]</ref>). Zero-padding is removed to prevent boundary-effects. The final layer consists of a group-pooling layer followed by a sigmoid activation, resulting in tumor-probability output on the plane Z 2 . As the model is fully convolutional, efficient inference can be achieved at test time by reusing computation of neighbouring patches, reducing segmentation time of a full WSI from hours to ∼ 2 minutes on a NVIDIA Titan XP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation</head><p>To evaluate the proposed model, we use Camelyon16 <ref type="bibr" target="#b8">[9]</ref> and PCam. Additional testing is performed on BreakHis <ref type="bibr" target="#b13">[14]</ref>. (1) The Camelyon16 dataset contains 400 H&amp;E stained WSIs of sentinel lymph node sections split into 270 slides with pixel-level annotations for training and 130 unlabeled slides for testing. The slides were acquired and digitized at 2 different centers using a 40× objective (resultant pixel resolution of 0.243 microns). In the Camelyon16 challenge, model performance is evaluated using the FROC curve for tumor localization. (2) The PCam dataset contains 327,680 patches extracted from Camelyon16 at a size of 96 × 96 pixels @ 10× magnification, with a 75/12.5/12.5% train/validate/test split, selected using a hard-negative mining regime 1 . (3) The BreakHis dataset contains 7909 H&amp;E stained microscopy images at a size of 700 × 460 pixels. The task is to classify the images into benign or malignant cases for multiple magnification factors. We limit our evaluation to the images at 4× magnification, for which previous approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> have reported the highest accuracy.</p><p>For the evaluation on the WSI-level Camelyon16 benchmarks, we largely follow the pipeline proposed in <ref type="bibr" target="#b0">[1]</ref>, uniformly sampling WSIs and drawing tumor/nontumor patches with equal probability. To prevent overrepresentation of background and non-tissue patches, slides are converted to HSV, blurred, and rejected if the max. pixel saturation lies below 0.07 (range [0,1]) and value above 0.1. This was empirically verified to not drop tissue patches. For computing the FROC score, tumor location candidates are selected with an efficient square nonmaximum suppression window rather than radial. The window-size is tuned per model on the validation set. FROC score confidence bounds are computed using 2000 bootstrap samples <ref type="bibr" target="#b0">[1]</ref>. Train and validation splits are created by dividing the available WSIs randomly, maintaining equal tumor/normal ratio. We focus on the WSI data at 10× magnification (4 times smaller than the original dataset, at 0.972 microns per pixel) to fit the compute budget available for this work. Following <ref type="bibr" target="#b0">[1]</ref>, we focus on the more-granular tumor-detection FROC metric in favor of slide-level AUC.</p><p>Training Details: Models are optimized using Adam <ref type="bibr" target="#b15">[16]</ref> with batch size 64 and initial learning rate 1e−3 (halved after 20 epochs of no improvement in validation loss). Epochs consists of 312 batches with a batch size of 64. Validation loss is computed using 40.000 sampled patches. Weights with lowest validation loss are selected for test evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model reliability</head><p>We evaluate stability of predictions under rotation of the input. We present a visual analysis in <ref type="figure" target="#fig_2">Fig. 3</ref>. For a comparable baseline we use an equivalent model with standard convolutions. For a fair model comparison, we keep the number of parameters consistent by multiplying the growth rate of the baseline model by the square root of the group size <ref type="bibr" target="#b4">[5]</ref>. Bar the expected fluctuation around the tumor boundary (that arises due to the sub-sampled segmentation), the p4m-model is more robust to transformations even outside the group (sub-90 • rotations). In addition, we observe a higher confidence for predictions inside the tumor regions for P4M-DenseNet as compared to the baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">P4M-DenseNet Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camelyon16</head><p>We evaluate our patch-based model on the slide-level tumor localization task of the Camelyon16 challenge. <ref type="figure">Fig. 4</ref> reports the performance on the FROC score, next to those of a pathologist <ref type="bibr" target="#b8">[9]</ref> and the state-of-the-art approaches reported on this dataset, including <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>. For the baseline DenseNet, the training data is augmented with 90 • rotations and reflection. We experiment with multiple data regimes, where the number of WSIs in the training set is incrementally reduced by a factor of two.</p><p>The results indicate that the proposed method performs consistently better than all compared methods in terms of the FROC metric. Comparing to the baseline DenseNet results, we see that the superiority of our proposed architecture is predominantly due to the increased parameter sharing by the p4m-equivariance, which frees up model capacity and reduces the redundancy of detecting the same histological patterns in different orientations.</p><p>We also observe that the performance gap between our model and the baseline increases when we limit the dataset size by removing WSIs. This seems to indicate that the performance in the small-data regime benefits significantly from the sample efficiency of P4M-DenseNet, with diminishing returns when the amount of data is sufficient for the baseline network to achieve (approximate) rotation equivariance. This performance gap remains for the full data set.</p><p>BreakHis As an additional evaluation method, we assess the performance of the proposed model on the binary classification task of BreakHis as described in Section 3.1. As training the model from scratch is impractical given the small dataset, we pre-train on Camelyon16 at a similar pixel resolution. Similar to <ref type="bibr" target="#b13">[14]</ref>, we predict the malignancy of a test image by using the maximum activation of 1000 random crops. We obtain an accuracy of 96.1 ± 3.2 and 93.5 ± 4.7 for P4M-Densenet and the baseline respectively, outperforming previous approaches <ref type="bibr" target="#b13">[14]</ref>[15].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We present a novel histopathology patch-classification model that outperforms a competitive traditional CNN by enforcing rotation and reflection equivariance. A derived patch-level dataset is presented, allowing straightforward and precise evaluation on a challenging histopathology task. We demonstrate that rotation equivariance improves reliability of the model, motivating the application and further research of rotation equivariant models in the medical image analysis domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Given a canonical input and a rotated duplicate, we demonstrate how a 2layer G-CNN is equivariant in p4. Feature maps of one kernel per layer are shown, and the dashed blue arrows indicate how (intermediate) representations of the two inputs correspond. The Z 2 → p4 convolution correlates the input with 4 rotated versions of the same kernel. The p4 → p4 convolution correlates the resulting feature map with the p4-kernel, cyclically-shifting and rotating the kernel for each orientation. The final layer demonstrates how average-pooling over the orientations produces a representation that is locally invariant and globally equivariant to rotation. Global average pooling over p4 would result in a representation globally invariant to both translation and rotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The proposed equivariant DenseNet architecture for the p4 group, consisting of 5 Dense Blocks (D.B.) alternated with Transition Blocks (T.B.). The final layer of the model is a p4 → Z 2 group pooling layer followed by a sigmoid activation. The four orientations in p4 are illustrated through primary colors. A Z 2 → p4 kernel (left), p4 → p4 kernel (middle) and p4 → Z 2 kernel (right) illustrate how equivariance arises in the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>(a) shows a large input region spanning multiple patches, with the tumor ground truth overlayed in green. The region is predicted under 32 evenly spaced sub-90 • rotations, and prediction maps rotated back to original orientation. (b) shows the mean prediction and (c) shows the standard deviation of the predictions across all rotations, using DenseNet (left) and P4M-DenseNet (right). Both networks are trained on the 12.5% data regime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :Table 2 :</head><label>12</label><figDesc>Performance maps, #W number of weights, K number of Z 2 maps per layer. We assess the performance of our main contribution, the P4M-DenseNet architecture, on the PCam dataset.Table 1reports the performance. P4M-DenseNet outperforms other models, closely followed by the P4-DenseNet, indicating that both rotation and reflection are useful inductive biases, that can not be learned by data augmentation alone. Keeping the number of Z 2 maps fixed in the baseline degrades performance further, demonstrating the sample-efficiency of the P4M model. Performance on the Camelyon16 test set. The confidence bounds are obtained using a 2000-fold bootstrap regime. *Challenge winner<ref type="bibr" target="#b16">[17]</ref> uses 40× resolution and is not directly comparable.</figDesc><table><row><cell>on PCam, measured by negative log-likelihood, accuracy and AUC. Experiments with additional data augmentation with 90 • rotations and reflections are marked by +. M indicates matching number of Z 2 Network P4M-DenseNet P4M-DenseNet M 24 19K 0.273 89.3 95.8 K #W NLL Acc AUC 64 119K 0.260 89.8 96.3 P4-DenseNet 48 125K 0.329 89.0 94.5 DenseNet+ 24 128K 0.306 88.1 95.1 DenseNet+ M 64 902K 0.365 87.2 94.6 DenseNet 24 128K 0.315 87.6 95.5</cell></row><row><cell>PatchCamelyon (PCam)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">PCam details and data at https://github.com/basveeling/pcam. Implementations of equivariant layers available at https://github.com/basveeling/keras_gcnn.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Technically, this is a cross-correlation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Geert Litjens, Jakub Tomczak, Dimitrios Mavroeidis and the anonymous reviewers especially for their insightful comments. This research was supported by Philips Research, the SURFSara Lisa cluster and the NVIDIA GPU Grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Detecting cancer metastases on gigapixel pathology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stain specific standardization of Whole-Slide histopathological images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="404" to="415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m">Group equivariant convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<idno>openaccess.thecvf.com</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning steerable filters for rotation equivariant CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Robustness of Rotation-Equivariant networks to adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dumont</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2199" to="2210" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="411" to="418" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding image representations by measuring their equivariance and equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="991" to="999" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A dataset for breast cancer histopathological image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Spanhol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1455" to="1462" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised intra-embedding of fisher vectors for histopathology image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep learning for identifying metastatic breast cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
