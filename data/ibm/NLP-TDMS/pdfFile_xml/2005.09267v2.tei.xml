<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ITERATIVE PSEUDO-LABELING FOR SPEECH RECOGNITION A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-28">August 28, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
							<email>qiantong@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park &amp; New York</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park &amp; New York</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
							<email>jacobkahn@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park &amp; New York</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park &amp; New York</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park &amp; New York</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park &amp; New York</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ITERATIVE PSEUDO-LABELING FOR SPEECH RECOGNITION A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-28">August 28, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: speech recognition</term>
					<term>language modeling</term>
					<term>pseudo-labeling</term>
					<term>semi-supervised learning</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pseudo-labeling has recently shown promise in end-to-end automatic speech recognition (ASR). We study Iterative Pseudo-Labeling (IPL), a semi-supervised algorithm which efficiently performs multiple iterations of pseudo-labeling on unlabeled data as the acoustic model evolves. In particular, IPL fine tunes an existing model at each iteration using both labeled data and a subset of unlabeled data. We study the main components of IPL: decoding with a language model and data augmentation. We then demonstrate the effectiveness of IPL by achieving state-of-the-art word-error rate on the LIBRISPEECH test sets in both standard and low-resource setting. We also study the effect of language models trained on different corpora to show IPL can effectively utilize additional text. Finally, we release a new large in-domain text corpus which does not overlap with the LIBRISPEECH training transcriptions to foster research in low-resource, semi-supervised ASR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances in end-to-end speech recognition are largely due to acoustic model (AM) architecture improvements. Some of the most promising are from the Transformer family <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, which give state-of-the-art results on many ASR benchmarks and close the gap between end-to-end and hybrid systems. Given the performance gain from new architectures, research has shifted focus towards leveraging self-and semi-supervised techniques to better utilize unlabeled data. For example, pseudo-labeling successfully boosts the performance on LIBRISPEECH <ref type="bibr" target="#b5">[6]</ref> baselines by a large margin <ref type="bibr" target="#b0">[1]</ref>. Many algorithms exist which incorporate unlabelled data to improve ASR in the low-resource setting, including representation learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, pseudo-labeling <ref type="bibr" target="#b9">[10]</ref>, local prior matching <ref type="bibr" target="#b10">[11]</ref>, pseudo-label augmentation <ref type="bibr" target="#b11">[12]</ref>, adversarial training <ref type="bibr" target="#b12">[13]</ref> and back translation <ref type="bibr" target="#b13">[14]</ref>. While many of these methods outperform a supervised baseline with limited resources, a large gap to fully-supervised training remains. Furthermore, not all approaches scale easily to large amounts of data, such as that recently used in the LIBRILIGHT benchmark <ref type="bibr" target="#b6">[7]</ref>.</p><p>In this work, we study iterative pseudo-labeling (IPL), a straightforward method that can easily scale to large unlabeled datasets and further boost the performance in both standard-and low-resource settings. IPL is motivated by the simplicity and effectiveness of pseudo-labeling (PL) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1]</ref>. A simple extension to <ref type="bibr" target="#b0">[1]</ref> involves conducting more iterations of PL as the model trains so as to continuously refine and improve the quality of generated pseudo-labels. That said, training a model from scratch after each round of pseudo-labeling and relabeling a large collection of unlabeled data is expensive. IPL mitigates these challenges by 1) labeling only a subset of the unlabeled data in each iteration, and 2) fine tuning the existing model on this subset, rather than training from scratch. An intuitive motivation for this is shown in <ref type="figure">Figure 1</ref>, where the same acoustic model is trained to convergence with a fixed learning rate; both settings reach a similar word-error rate (WER). Training from scratch with PL is also shown to be roughly equivalent to iterating in a machine translation <ref type="bibr" target="#b14">[15]</ref> and a image classification <ref type="bibr" target="#b15">[16]</ref>   <ref type="figure">Figure 1</ref>: WER on dev-other for two different strategies of using the unlabeled data. In fine tuning, the model is first trained on train-clean-100 only for 160 epochs and then used to generate pseudo-labels on train-clean-360. Both train-clean-100 and train-clean-360 are then used to fine tune the same model. In "from scratch training", same pseudo-labels on train-clean-360 together with the true labels on train-clean-100 are used to train a new model with the same architecture from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semi-supervision in ASR is well-studied <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>; our work builds primarily on recent work with end-to-end systems, especially PL <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1]</ref>. In <ref type="bibr" target="#b9">[10]</ref>, PL is shown to be effective with only 100h of labeled audio. The model uses a sequence-to-sequence loss and requires additional pseudo-label filtering to achieve the best results. To mitigate the instability found in sequence-to-sequence decoding, as in <ref type="bibr" target="#b0">[1]</ref> all pseudo-labels are generated with models trained with Connectionist Temporal Classification (CTC) loss <ref type="bibr" target="#b22">[23]</ref>. Our work extends <ref type="bibr" target="#b0">[1]</ref> by conducting more rounds of PL and fine tuning the existing model and demonstrates the effectiveness of the IPL approach in settings with both 960h and 100h of labeled audio. Still other work <ref type="bibr" target="#b7">[8]</ref> learns discrete audio feature representations directly from the waveform and works quite well with limited data, even in settings with under 100h of labeled audio. In this setting, learned acoustic features are presumably more competitive than an acoustic model trained end-to-end with MFCC or log-mel filterbank features. Other work including <ref type="bibr" target="#b10">[11]</ref>, those with CPC baselines <ref type="bibr" target="#b6">[7]</ref>, and those using adversarial training <ref type="bibr" target="#b12">[13]</ref> and back-translation-style techniques <ref type="bibr" target="#b13">[14]</ref> also provide promising end-to-end semi-supervised approaches, but results are not comparable as newer end-to-end approaches outperform these works' baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first introduce the iterative pseudo-labeling algorithm (IPL). We then give theoretical justifications why IPL facilitates effective training. Finally, we perform analysis and experiments on a small-scale labeled dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Iterative Pseudo-Labeling</head><p>Algorithm 1: Iterative pseudo-labeling</p><formula xml:id="formula_0">Data: Labeled data L = {x i , y i } l i=1 , Unlabeled data U = {x j } u j=1</formula><p>Result: Acoustic model p θ Initialize p θ by training on only labeled data L; repeat 1. Draw a subset of unpaired dataŨ ∈ U ; 2. Apply p θ and decoding with LM to the subsetŨ to generateÛ = {(x,ŷ)|x ∈Ũ }; 3. Fine tune p θ on L ∪Û with data augmentation; until convergence or maximum iterations are reached;</p><p>As listed in Algorithm 1, IPL utilizes both labeled and unlabeled data as in the conventional semi-supervised learning. The model minimizes the following loss function:</p><formula xml:id="formula_1">L = L L + λL U<label>(1)</label></formula><p>where L L and L U denote the parts of the loss function on labeled and unlabeled data accordingly:  Note that in ASR, instead of sampling from p θ (y|x), the transcriptions as well as the pseudo-labels are usually selected from the greedy path:ŷ = argmax y p θ (y|x).</p><formula xml:id="formula_2">L L = −E x,y∼p(x,y) log(p θ (y|x)) (2) L U = −E x∼p(x) Eŷ ∼p θ (y|x) log(p θ (ŷ|x)).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Avoidance of Local Minima</head><p>As discussed in <ref type="bibr" target="#b14">[15]</ref>, one bane of loss (1) optimization in fine tuning is that it tends to get stuck at a local minima after each round of training with existing pseudo-labels; the conditional log likelihood (3) is already maximized when p θ (y|x) matches the underlying data distribution p θ * (y|x), so that ∇ θ L | θ=θ * = 0. The IPL algorithm has two distinct components: one with respect to the target (y) and the other with respect to the data (x), that we found to be effective to overcome this behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">External Language Model</head><p>In modern ASR systems, in addition to the acoustic model p θ , a decoding procedure (typically either WFST-based <ref type="bibr" target="#b23">[24]</ref> or beam-search-based (BS) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> as well as lattice/beam rescoring) is always used to integrate an external language model (LM). Thus, instead of using the greedy path (4) as transcriptions, we consider:</p><formula xml:id="formula_4">y = argmax y log p θ (y|x) + α log p LM (y) + β|y|,<label>(5)</label></formula><p>where α and β are hyper-parameters <ref type="bibr" target="#b0">[1]</ref> usually optimized on validation set. This differsŷ fromŷ by introducing extra LM knowledge into transcriptions so that the learned weights θ are no longer optimal given the new labelsŷ , and the model will keep training with ∇ θ L | θ =θ * = 0. This is also observed in machine translation <ref type="bibr" target="#b14">[15]</ref>, where the gain from using greedy-path decoding is limited in self-training with PL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Data Augmentation</head><p>With respect to data, when data augmentation is introduced, the log likelihood the model optimizes also changes. We can rewrite (3) as</p><formula xml:id="formula_5">L U = −E x∼p(x),x ∼q(x |x) Eŷ ∼p θ (y|x) log(p θ (ŷ|x )),<label>(6)</label></formula><p>where q(·) is the data augmentation function, which is SpecAugment <ref type="bibr" target="#b26">[27]</ref> in our experiments. The model weights optimized before could be no longer optimal given the new augmented input; the model keeps updating with ∇ θ L | θ =θ * = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Empirical Study</head><p>We use train-clean-100 and train-clean-360 in LIBRISPEECH as labeled and unlabeled training data and dev-other as a validation set. The AMs detailed in Section 4.5, are first trained on labeled data for 100 epochs (≈ 10 4 hours) and then continue with IPL. Pseudo-labeling is performed every 10 epochs with all unlabeled data without down-sampling. The learning rate is fixed throughout training for a fair comparison. As shown in <ref type="figure">Figure 7</ref>, if both data augmentation and LM decoding are used, there is a dramatic WER drop when unlabeled data is first in use, and the WER keeps decreasing as IPL progresses. If either data augmentation or LM decoding is removed, convergence degrades noticeably. Further, if both are removed, adding unlabeled data provides no benefit to IPL, which is in consistent with hitting local minima. One should note that the model is not fully converged at epoch 100; the WER continues decreasing. The contribution of the beam search alone is also limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Distribution Approximation</head><p>Usually, unlabeled dataset U is much larger than the labeled one L; it is very time-consuming to label the entire U in each round of PL. If only insufficient data is selected in each round, however, the p(x) in (3) will be poorly estimated, which will increase the chance of model overfitting. To balance the trade-off between the accuracy of p(x) approximation and the PL efficiency, with the same setup as in Section 3.2, we conduct an empirical ablation where the PL is performed only on a randomly sampled subset of U . Note that we treat samples from L and U equally following <ref type="bibr" target="#b0">[1]</ref>, so that the λ in (1) is implicitly set to |U |/|L|, the ratio between the number of samples in the two sets.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, even though there is an up to 5-time gap in λ, using 20% to 40% of U can reach the same WER as using 100%. This motivates us to 1) pay less attention to λ tuning and (2) down sample U in PL, so as to perform more rounds of PL in total to better utilize large unlabeled set. On the other hand, using only 10% from U significantly hurts the convergence, which shows the importance of p(x) approximation and sets up a lower bound of down-sample rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Audio Data</head><p>Audio data for our experiments comes from two sources: LIBRISPEECH, containing 960h of audio and paired transcriptions, and audio from LIBRIVOX (54K hours of audio) extracted following <ref type="bibr" target="#b6">[7]</ref>. Three setups of labeled data are used: 1) full LIBRISPEECH (960h), 2) train-clean-100 subset (100h) from LIBRISPEECH and 3) the 10-hour training subset from LIBRISPEECH prepared in <ref type="bibr" target="#b6">[7]</ref>, LibriLight-10. We use the standard development (for all hyper-parameters optimization) and test (for final evaluation only) sets from LIBRISPEECH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Gutenberg Text Corpus</head><p>As discussed in <ref type="bibr" target="#b9">[10]</ref>, it is important to remove components of the LM training corpus that overlap with unlabeled audio to ensure the LM has no information about ground truth transcriptions from the unlabeled audio. We study the contribution of the LM to IPL and conduct rigorous experiments when a subset of LIBRISPEECH is used as unlabeled audio. For LM training, we prepare a larger in-domain text corpus using books from Project Gutenberg <ref type="bibr" target="#b27">[28]</ref>. To prepare the corpus, we first start with a large subset of English books from Project Gutenberg (which includes some of the 14.5k books present in the LIBRISPEECH LM corpus <ref type="bibr" target="#b5">[6]</ref> with 0.8B words) and filter out all books present in LIBRIVOX audio data. We perform the same procedure as in <ref type="bibr" target="#b0">[1]</ref> along with a manual matching step to find exact or similar titles (after normalization) in LIBRIVOX (α, β are the same as in <ref type="bibr" target="#b0">[1]</ref>), filtering out the resulting books. Similarly, we remove from the corpus books present in the LIBRISPEECH validation and test sets. The resulting filtered corpus is normalized in the same way as in <ref type="bibr" target="#b0">[1]</ref> which mimics the normalization in the LIBRISPEECH corpus, but has additional mappings of some abbreviations and does not split text mid-sentence. We denote this final corpus as GB \ LV (2.16B words from 34k books). Further, with the same procedure, we filter out books containing LIBRISPEECH training transcriptions (960h) and form a new corpus denoted as GB \ LV \ LS (2.11B words from 33.4k books).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Acoustic Model</head><p>We use the best-performing Transformer architecture on LIBRISPEECH and LIBRIVOX with 322M parameters from <ref type="bibr" target="#b0">[1]</ref> in our experiments. In particular, there is a convolutional front-end containing 6 layers of 1-D convolutions with kernel-width 3 followed by 36 4-head Transformer blocks <ref type="bibr" target="#b28">[29]</ref> with self-attention dimension D tr = 768. The 2nd, 4th and the final convolutions in the front-end have stride 2, so the overall sub-sampling rate of the model is 8. The AMs take 80-channel log-mel filterbanks as input and are trained end-to-end with CTC loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Language Model</head><p>For fair comparison with existing works, IPL experiments in <ref type="table" target="#tab_3">Table 2</ref> use BS decoding with the 4-gram LM (200k top words) used in <ref type="bibr" target="#b0">[1]</ref>, which is trained on the official LIBRISPEECH LM corpus with transcriptions in LIBRIVOX excluded, denoted as LS \ LV . The same LM is used for the final beam-search decoding of trained models. Also we are using Transfomer LM from <ref type="bibr" target="#b0">[1]</ref> for the final beams rescoring. For IPL experiments, where beam rescoring is used for PL generation in addition to the n-gram BS decoding, transfomer LM (with the same architecture as transformer LM from <ref type="bibr" target="#b0">[1]</ref>) is trained on LS \ LV . As an ablation study, we train 5-gram LMs on GB \ LV \ LS and GB \ LV with top 200k words in each and without pruning using the KenLM toolkit <ref type="bibr" target="#b29">[30]</ref>. The LMs perplexities are listed in <ref type="table" target="#tab_2">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Training</head><p>We use word pieces (WP) <ref type="bibr" target="#b30">[31]</ref> as modeling units in our experiments. Following <ref type="bibr" target="#b0">[1]</ref>, we use the same 10k WP estimated from the training transcriptions, if full LIBRISPEECH training set is used. If train-clean-100 is in use, we switch to the 5k WP estimated on train-clean-100 transcriptions as in <ref type="bibr" target="#b10">[11]</ref>. We use a lexicon, including words only in training and validation sets, to limit the search space of the BS decoding in IPL. For LibriLight-10 setup, the same units and lexicon as train-clean-100 are used.</p><p>Dropout <ref type="bibr" target="#b31">[32]</ref> and layer drop <ref type="bibr" target="#b32">[33]</ref> are tuned and used to regularize each model. For models that either use LibriLight-10 as labeled data or trained without LIBRIVOX, we set both dropout and layer drop to 0.3; for models trained on LIBRIVOX, layer drop is set to 0.2 while dropout is 0.2 and 0.15 for models using 100 and 960 hours labeled data, respectively. All models are trained on 64 GPUs with a batch size of 4 per GPU if using LIBRIVOX and 6 otherwise. We use the Adagrad <ref type="bibr" target="#b33">[34]</ref> optimizer; the learning rate is initialized to 0.03 and is never decreased for models trained on LIBRIVOX but is halved once at epoch 800 for LIBRISPEECH-only models.</p><p>In terms of IPL training, we implemented the automated pipeline in wav2letter++ <ref type="bibr" target="#b24">[25]</ref>. For models trained with LIBRIVOX data, we use only BS with n-grams LM in decoding; while for models trained on LIBRISPEECH only, we apply two stage decoding in PL generating: 1) BS decoding with n-grams LM and 2) beam rescoring with Transformer LM. We use random search with 256 jobs to optimize the hyper-parameters in decoding <ref type="bibr" target="#b25">[26]</ref> on dev-other and use the optimal values in the subsequent pseudo-labeling. As mentioned in Section 3.3, we only select 20% to 40% of the data in each round of PL if LIBRIVOX is the unlabeled dataset. Otherwise, if the rest of LIBRISPEECH is the unlabeled set, the entire unlabeled set is pseudo-labeled. Pseudo-labels are regenerated every 10 epochs. Note that there is no filtering applied to the pseudo-labels generated, i.e. the whole unlabeled set will be used in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>In this section we compare our results with other recent work in semi-supervised learning. All results are listed in <ref type="table" target="#tab_3">Table 2</ref>. Given LibriLight-10 as labeled data, our method reaches 26.02% and 19.92% on test-other with the full LIBRISPEECH or LIBRIVOX as unlabeled data, respectively; while with train-clean-100, we further get 8.95% We conduct 3 rounds of pseudo-labeling on the entire unlabeled set and retraining a new AM from scratch. All the models in this section are trained with n-grams BS decoding in pseudo-labeling. As shown in <ref type="table" target="#tab_4">Table 3</ref>, WER on dev-other decreases with better PL generated, but the marginal gain diminishes as iterations continue. IPL, however, clearly outperforms the 3 rounds PL baseline, indicating it is effective to accumulate gains through training with more (up to 80) rounds of PL updates. Given the same amount of time for 3 rounds of PL training in <ref type="table" target="#tab_4">Table 3</ref> to finish, which is 4, 11 and 17 days from top to bottom, IPL achieves WER 10.69%, 8.50% and 4.14%, respectively. To achieve the same WER after round 3, however, IPL takes only 0.7, 3.3 and 8 days. This efficiency derives directly from the two proposed changes in IPL: 1) fine tuning the existing model with new labels to save computation in re-bootstrapping and 2) down sampling the unlabeled set to shorten the PL time, i.e. 20% down-sample rate leads to 5 time speed up in labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3">LM Study</head><p>Comparison of IPL with different LMs is shown in <ref type="table" target="#tab_5">Table 4</ref> with LMs perplexity in <ref type="table" target="#tab_2">Table 1</ref>. All the models in this section are trained with n-grams BS decoding in pseudo-labeling. Given a better LM, IPL better transfers LM knowledge into AM and achieves better performance. IPL can thus effectively leverage large amount of unpaired text, in addition to unpaired audio. However, although the difference in perplexity between the GB \ LV and GB \ LV \ LS LMs is small, there is still a large gap in WER. This is because the LM implicitly leaks the labels of unlabeled audio. Fortunately, comparing WER between LS \ LV and GB \ LV \ LS, when the transcription leaking is completely removed, it is still possible to reach similar (or even better) WER by utilizing more in-domain text. As shown in <ref type="table" target="#tab_5">Table 4</ref>, there is still an observable improvement in WER across greedy path and the BS decoding with n-grams LM. One inhibitor to transferring LM knowledge into the AM is a mismatch in decoding parameters: the parameters optimized on dev-other may not be optimal for decoding unlabeled audio. Thus, in the final stage of IPL training, the marginal WER improvement on dev-other is not reflected similarly on the one on unlabeled audio, so as to prevent the AM from improving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have shown that iterative pseudo-labeling can give superior results in both standard and low-resource settings and provides an efficient algorithm with which to train as compared to conventional pseudo-labeling approaches. Iterative pseudo-labeling benefits from beam-search decoding with a language model and data augmentation along with dataset sub-sampling which also improves efficiency. With our Transformer acoustic model, IPL achieves the state-of-the art results on LIBRISPEECH test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Learning Curves</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Typical IPL curves</head><p>We provide training curves for three scenarios with different amount of labeled data. There is an obvious inflection point when IPL starts and unlabeled data is in use. In the first several rounds of IPL, WER decreases dramatically each time when new data and their PLs are generated and consumed. IPL is also able to keep accumulating marginal gains until the end of training. Surprisingly, IPL is able to bootstrap from an immature model, even with about 80% WER. Key to the success here is the LM and lexicon that limits the search space of words.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 IPL v.s. Retraining</head><p>We show in Section 1 that fine tuning and training from scratch converge to the same ball park in the early stage of IPL. This experiment is designed to verify if this still holds when the IPL model is converged. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, if we train a new AM from scratch using the pseudo-labels generated from a fully converged IPL model, it will not outperform the IPL model. Even with beam search decoding, both models can still reach similar results. For the model trained on train-clean-100, WER are 8.83 and 8.87 for IPL and retraining models, respectively; while for models trained on LibriLight-10, WERs are 25.69 and 25.62. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Decoding Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 LM weights for different LMs</head><p>We conduct grid search over LM weight used in beam-search decoding and rescoring for an IPL model. Specifically, we use a 4-gram LM in the beam-search decoding to generate a beam of candidates and rescore the beam using an NN-LM. The final WER is not sensitive to the LM weight used in the beam-search decoding, which means as long as the LM weight is reasonable (e.g. between 0 to 2) the good candidates can always be preserved in the beam and later selected out by rescoring. This enables us to pay less attention to the beam-search decoder parameter tuning and focus on rescoring.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Comparison With The Latest Methods</head><p>In <ref type="table" target="#tab_7">Table 5</ref>, we mainly compare with two latest works: one with multi-rounds of pseudo-labeling <ref type="bibr" target="#b34">[35]</ref> and the other with unsupervised pre-training <ref type="bibr" target="#b35">[36]</ref>. Both works achieve better results than us but with different approach. <ref type="bibr" target="#b34">[35]</ref> uses the same pseudo-labeling, but is equipped with 1) Listen, Attend and Spell (LAS) network as an acoustic model reaching better WER on labeled data only and 2) filtering mechanism on the pseudo-labels. <ref type="bibr" target="#b35">[36]</ref> learns pre-trained speech features on large scale clean speech dataset and further fine tunes the model on labeled transcriptions only. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>WER on dev-other with different IPL training strategies. Beam-search (BS) decoding is performed with an 4-gram LM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>WER on dev-other with different amount of unlabeled data used in each iteration of IPL training (data augmentation and BS decoding with 4-gram LM are applied).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Typical IPL training curves on dev-other. The labeled training data used in each sub-plot is (left) full LIBRISPEECH, (middle) train-clean-100, (right) LibriLight-10. LIBRISPEECH and LIBRIVOX are used as the unlabeled data. Each epoch may contain different amount of data depending on the subset size selected from unlabeled data.A.2 IPL v.s. Training From ScratchIf the AM is trained from scratch each time as the PL evolves, we can see the gain is diminishing. In addition, both training AM from scratch and label the whole 54K hours dataset are time consuming. Ignoring the labeling time and given the same amount of training time for an AM to converge from scratch, IPL can always outperform and reach better WER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Comparison between IPL and training from scratch with pseudo-labels. Label and unlabeled training data used in these experiments are train-clean-100 and LIBRIVOX plus the rest of LIBRISPEECH, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Retraining a final model from scratch with the pseudo-labels generated from a fully converged IPL model. Labeled training data used in the subplots are (left) train-clean-100 and (right) LibriLight-10, while the rest of LIBRISPEECH is used as unlabeled data in both.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of the WER heatmap of rescoring LM weight on development and training set. Left: decoding a checkpoint of an AM that is not fully converged, with (a) WER on dev-other and (b) WER on train-clean-100 and train-other-500. Right: decoding a fully converged AM, with (c) WER on dev-other and (d) WER on train-clean-100 and train-other-500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>setting. arXiv:2005.09267v2 [cs.CL] 27 Aug 2020</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A PREPRINT -AUGUST 28, 2020</cell></row><row><cell>Word Error Rate</cell><cell>20 35 50</cell><cell>0</cell><cell>2</cell><cell>4 training from scratch 6 8 fine tuning</cell><cell>·10 4</cell></row><row><cell></cell><cell></cell><cell cols="3">Training Data Consumed (hours)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Perplexities of language models.</figDesc><table><row><cell>dev-clean</cell><cell>161.7</cell><cell>101.6</cell><cell>99.7</cell><cell>58.3</cell><cell>48.2</cell></row><row><cell>dev-other</cell><cell>152.5</cell><cell>112.9</cell><cell>110.5</cell><cell>59.3</cell><cell>50.2</cell></row></table><note>Data LS \ LV GB \ LV \ LS GB \ LV Transf. LS \ LV Transf.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of WER with other semi-supervised methods on LIBRISPEECH (LS) and LIBRIVOX (LV) data. The beam-search decoding with 4-gram LS \ LV LM is used in IPL training for pseudo-labels generation (for models used LIBRISPEECH as unlabeled data beam rescoring with transformer LS \ LV LM is applied for pseudo-labels generation). LM column refers either to the greedy path ("-") or to the final decoding and rescoring (" * ") with external LMs. 1.85 3.26 2.10 4.01 and 7.11% on test-other with the rest of LIBRISPEECH or LIBRIVOX as unlabeled data, respectively. If all of LIBRISPEECH is used as labeled data, we achieve 4.01% on test-other. Our result achieves a clear state-of-the-art in all the three semi-supervised learning setups.<ref type="bibr" target="#b0">1</ref> The final decoding and rescoring strategies are the same as in<ref type="bibr" target="#b0">[1]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">Data (hours)</cell><cell>LM</cell><cell cols="2">Dev WER</cell><cell cols="2">Test WER</cell></row><row><cell></cell><cell cols="2">Labeled Unlabeled</cell><cell></cell><cell cols="4">clean other clean other</cell></row><row><cell>Semi-supervision with PL [10]</cell><cell>LS-100</cell><cell>LS-360</cell><cell>GCNN</cell><cell cols="4">5.37 22.13 5.93 24.07</cell></row><row><cell>Local Prior Matching [11]</cell><cell>LS-100</cell><cell>LS-860</cell><cell>GCNN</cell><cell cols="4">4.87 13.84 4.88 15.28</cell></row><row><cell>DeCoAR [9]</cell><cell>LS-100</cell><cell>LS-860</cell><cell>4-gram</cell><cell>-</cell><cell>-</cell><cell cols="2">4.74 12.20</cell></row><row><cell>vq-wav2vec + BERT [8]</cell><cell>LS-100</cell><cell>LS-860</cell><cell>4-gram</cell><cell>4.0</cell><cell>10.9</cell><cell>4.5</cell><cell>12.1</cell></row><row><cell cols="2">Semi-supervision with PL, CTC [1] LS-960 Semi-supervision with PL, S2S [1]</cell><cell>LV-54K</cell><cell cols="5">GCNN + Transf.  *  GCNN WP + Transf.  *  2.00 3.65 2.09 4.11 2.01 3.95 2.31 4.54</cell></row><row><cell></cell><cell>LL-10</cell><cell>LS-960</cell><cell>-4-gram + Transf.  *</cell><cell cols="4">23.84 25.70 24.58 26.44 23.51 25.48 24.37 26.02</cell></row><row><cell></cell><cell></cell><cell>LV-54K</cell><cell>-4-gram + Transf.  *</cell><cell cols="4">19.76 21.67 20.63 22.38 17.00 19.34 18.03 19.92</cell></row><row><cell>Ours, IPL</cell><cell>LS-100</cell><cell>LS-860</cell><cell>-4-gram + Transf.  *</cell><cell cols="4">5.41 9.32 5.95 10.28 4.98 7.97 5.59 8.95</cell></row><row><cell></cell><cell></cell><cell>LV-54K</cell><cell>-4-gram + Transf.  *</cell><cell cols="4">4.35 7.90 5.07 8.84 3.19 6.14 3.72 7.11</cell></row><row><cell cols="4">LS-960 4-gram + Transf.  4.6 Analysis -LV-54K</cell><cell cols="4">2.05 4.12 2.21 4.71</cell></row><row><cell>4.6.1 Effectiveness</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>WER of greedy path on dev-other for IPL and training from scratch for multiple rounds. 4-gram LS \ LV LM is used for pseudo-labels generation.</figDesc><table><row><cell></cell><cell>Data</cell><cell></cell><cell cols="2"># Rounds of PL</cell><cell>IPL</cell></row><row><cell cols="2">Labeled Unlabeled</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>LS-100</cell><cell>LS-860</cell><cell cols="4">27.76 17.1 15.8 15.09 10.69</cell></row><row><cell>LS-100</cell><cell>LS + LV</cell><cell cols="4">27.76 16.3 12.9 10.95 7.90</cell></row><row><cell>LS-960</cell><cell>LV-54K</cell><cell cols="4">7.31 5.00 4.69 4.57</cell><cell>4.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>IPL training with different LMs in BS decoding. WER on dev-other (test-other) is reported for the greedy path (top), an extra BS decoding with the same n-grams LM used in IPL training (middle) and Transformer LM rescoring (bottom).</figDesc><table><row><cell>Decoding</cell><cell></cell><cell>Data</cell><cell></cell><cell>Language Model</cell><cell></cell></row><row><cell></cell><cell cols="2">Labeled Unlabeled</cell><cell>LS\LV</cell><cell>GB\LV \LS</cell><cell>GB\LV</cell></row><row><cell></cell><cell>LS-100</cell><cell>LS-860</cell><cell cols="3">10.69 (11.48) 10.80 (11.61) 10.19 (11.09)</cell></row><row><cell>None</cell><cell>LS-100</cell><cell>LS + LV</cell><cell>7.90 (8.84)</cell><cell>7.21 (8.28)</cell><cell>6.82 (7.89)</cell></row><row><cell></cell><cell>LS-960</cell><cell>LV-54K</cell><cell>4.12 (4.71)</cell><cell>-</cell><cell>4.02 (4.42)</cell></row><row><cell></cell><cell>LS-100</cell><cell>LS-860</cell><cell cols="2">10.05 (10.90) 9.82 (10.49)</cell><cell>9.09 (9.82)</cell></row><row><cell>4-gram</cell><cell>LS-100</cell><cell>LS + LV</cell><cell>7.21 (8.19)</cell><cell>6.70 (7.74)</cell><cell>6.15 (7.35)</cell></row><row><cell></cell><cell>LS-960</cell><cell>LV-54K</cell><cell>3.67 (4.33)</cell><cell>-</cell><cell>3.65 (4.10)</cell></row><row><cell></cell><cell>LS-100</cell><cell>LS-860</cell><cell>8.72 (9.51)</cell><cell>8.90 (9.67)</cell><cell>8.25 (9.11)</cell></row><row><cell>Trans.</cell><cell>LS-100</cell><cell>LS + LV</cell><cell>6.14 (7.11)</cell><cell>5.96 (6.99)</cell><cell>5.56 (6.71)</cell></row><row><cell></cell><cell>LS-960</cell><cell>LV-54K</cell><cell>3.26 (4.01)</cell><cell>-</cell><cell>3.42 (3.83)</cell></row><row><cell cols="2">4.6.4 Decoding Parameters Mismatch</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of WER with lastest semi-supervised methods on LIBRISPEECH (LS) and LIBRIVOX (LV) data. 23.51 25.48 24.37 26.02 LV-54K -19.76 21.67 20.63 22.38 4-gram + Transf. * 17.00 19.34 18.03 19.92 3.19 6.14 3.72 7.11 1.85 3.26 2.10 4.01</figDesc><table><row><cell>Method</cell><cell cols="2">Data (hours)</cell><cell>LM</cell><cell cols="2">Dev WER</cell><cell cols="2">Test WER</cell></row><row><cell></cell><cell cols="2">Labeled Unlabeled</cell><cell></cell><cell cols="4">clean other clean other</cell></row><row><cell>Improved T/S [35]</cell><cell>LS-100 LS-960</cell><cell>LS-860 LV-54K</cell><cell>LSTM LSTM</cell><cell>3.9 1.6</cell><cell>8.8 3.4</cell><cell>4.2 1.7</cell><cell>8.6 3.4</cell></row><row><cell></cell><cell>LL-10</cell><cell>LS-860</cell><cell>Transf.</cell><cell>2.9</cell><cell>5.7</cell><cell>3.2</cell><cell>6.1</cell></row><row><cell></cell><cell>LL-10</cell><cell>LV-54K</cell><cell>Transf.</cell><cell>2.5</cell><cell>5.2</cell><cell>2.6</cell><cell>5.2</cell></row><row><cell>wav2vec 2.0 [36]</cell><cell>LS-100</cell><cell>LS-860</cell><cell>Transf.</cell><cell>2.1</cell><cell>4.8</cell><cell>2.3</cell><cell>5.0</cell></row><row><cell></cell><cell>LS-100</cell><cell>LV-54K</cell><cell>Transf.</cell><cell>2.0</cell><cell>4.1</cell><cell>2.1</cell><cell>4.4</cell></row><row><cell></cell><cell>LS-960</cell><cell>LV-54K</cell><cell>Transf.</cell><cell>1.6</cell><cell>3.2</cell><cell>1.9</cell><cell>3.5</cell></row><row><cell>Ours, IPL</cell><cell cols="7">LL-10 4-gram + Transf.  LS-100 -LS-960 -LS-860 4-gram + Transf.  *  4.98 7.97 5.59 8.95 23.84 25.70 24.58 26.44 5.48 9.32 5.95 10.31</cell></row><row><cell></cell><cell cols="3">-4-gram + Transf.  LS-960 LV-54K -LV-54K 4-gram + Transf.</cell><cell cols="4">4.35 7.90 5.07 8.84 2.05 4.12 2.21 4.71</cell></row></table><note>***</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The results of this work were first published in May 2020 and the works that we are comparing to here are selected by then. Comparison with the latest methods is shown in Appendix.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">End-to-end asr: from supervised to semi-supervised learning with modern architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08460</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11660</idno>
		<title level="m">Transformers with convolutional context for asr</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparative study on transformer vs rnn in speech applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Someki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition using multi-stream self-attention with dilated 1d convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="54" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformer-based acoustic modeling for hybrid speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6874" to="6878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Libri-light: A benchmark for asr with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7669" to="7673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Effectiveness of self-supervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03912</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep contextualized acoustic representations for semi-supervised speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6429" to="6433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-training for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7084" to="7088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised speech recognition via local prior matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10336</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised asr by end-to-end self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09128</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial training of end-to-end speech recognition using a criticizing language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6176" to="6180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Self-supervised sequence-tosequence asr using unpaired speech and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01152</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting self-training for neural sequence generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJgdnAVKDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised training of acoustic models for large vocabulary continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lightly supervised and unsupervised acoustic model training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised versus supervised training of acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised training and directed manual transcription for lvcsr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="652" to="663" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised maximum mutual information training of deep neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large scale deep neural network acoustic modeling with semi-supervised training data for youtube video transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="368" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducers in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">wav2letter++: The fastest open-source speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07625</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fully convolutional speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06864</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Project gutenberg</title>
		<ptr target="https://www.gutenberg.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kenlm: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth workshop on statistical machine translation</title>
		<meeting>the sixth workshop on statistical machine translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SylO2yStDr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improved noisy student training for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09629</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Figure 7: WER with different LM weights used in beam-search decoding and rescoring</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">2 LM weights on different dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">We conduct parameter sweep on both development and training set to see if there is a mismatch between the optimal decoding/rescoring parameters on them. As shown in Figure 8, the optimal LM weight is aligned between development and train for both immature and converged model</title>
		<imprint/>
	</monogr>
	<note>This shows that the development set can be used as a good proxy of decoder parameter tuning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
