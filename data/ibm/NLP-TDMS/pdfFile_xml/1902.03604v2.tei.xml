<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MOTS: Multi-Object Tracking and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
							<email>voigtlaender@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
							<email>osep@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
							<email>luiten@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berin</forename><surname>Balachandar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gnana</forename><surname>Sekar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">MPI for Intelligent Systems</orgName>
								<orgName type="institution">University of TÃ¼bingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<email>leibe@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MOTS: Multi-Object Tracking and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper extends the popular task of multi-object tracking to multi-object tracking and segmentation (MOTS). Towards this goal, we create dense pixel-level annotations for two existing tracking datasets using a semi-automatic annotation procedure. Our new annotations comprise 65,213 pixel masks for 977 distinct objects (cars and pedestrians) in 10,870 video frames. For evaluation, we extend existing multi-object tracking metrics to this new task. Moreover, we propose a new baseline method which jointly addresses detection, tracking, and segmentation with a single convolutional network. We demonstrate the value of our datasets by achieving improvements in performance when training on MOTS annotations. We believe that our datasets, metrics and baseline will become a valuable resource towards developing multi-object tracking approaches that go beyond 2D bounding boxes. We make our annotations, code, and models available at https: //www.vision.rwth-aachen.de/page/mots.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the computer vision community has made significant advances in increasingly difficult tasks. Deep learning techniques now demonstrate impressive performance in object detection as well as image and instance segmentation. Tracking, on the other hand, remains challenging, especially when multiple objects are involved. In particular, results of recent tracking evaluations <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref> show that bounding box level tracking performance is saturating. Further improvements will only be possible when moving to the pixel level. We thus propose to think of all three tasks -detection, segmentation and tracking -as interconnected problems that need to be considered together.</p><p>Datasets that can be used to train and evaluate models for instance segmentation usually do not provide annotations on video data or even information on object identities across different images. Common datasets for multiobject tracking, on the other hand, provide only bounding box annotations of objects. These can be too coarse, e.g., when objects are partially occluded such that their bounding box contains more information from other objects than from themselves, see <ref type="figure" target="#fig_0">Fig. 1</ref>. In these cases, pixel-wise segmentation of the objects results in a more natural description of the scene and may provide additional information for subsequent processing steps. For segmentation masks there is a well-defined ground truth, whereas many different (non-tight) boxes might roughly fit an object. Similarly, tracks with overlapping bounding boxes create ambiguities when compared to ground truth that usually need to be resolved at evaluation time by heuristic matching procedures. Segmentation based tracking results, on the other hand, are by definition non-overlapping and can thus be compared to ground truth in a straightforward manner.</p><p>In this paper, we therefore propose to extend the wellknown multi-object tracking task to instance segmentation tracking. We call this new task "Multi-Object Tracking and Segmentation (MOTS)". To the best of our knowledge, there exist no datasets for this task to date. While there are many methods for bounding box tracking in the literature, MOTS requires combining temporal and mask cues for success. We thus propose TrackR-CNN as a baseline method which addresses all aspects of the MOTS task. TrackR-CNN extends Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> with 3D convolutions to incorporate temporal information and by an association head which is used to link object identities over time.</p><p>In summary, this paper makes the following contributions: <ref type="bibr" target="#b0">(1)</ref> We provide two new datasets with temporally consistent object instance segmentations based on the popular KITTI <ref type="bibr" target="#b12">[13]</ref> and MOTChallenge <ref type="bibr" target="#b37">[38]</ref> datasets for training and evaluating methods that tackle the MOTS task. <ref type="bibr" target="#b1">(2)</ref> We propose the new soft Multi-Object Tracking and Segmentation Accuracy (sMOTSA) measure that can be used to simultaneously evaluate all aspects of the new task. <ref type="bibr" target="#b2">(3)</ref> We present TrackR-CNN as a baseline method which addresses detection, tracking, and segmentation jointly and we compare it to existing work. <ref type="bibr" target="#b3">(4)</ref> We demonstrate the usefulness of the new datasets for end-to-end training of pixel-level multi-object trackers. In particular, we show that with our datasets, joint training of segmentation and tracking procedures becomes possible and yields improvements over training only for instance segmentation or bounding box tracking, which was possible previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-Object Tracking Datasets. In the multi-object tracking (MOT) task, an initially unknown number of targets from a known set of classes must be tracked as bounding boxes in a video. In particular, targets may enter and leave the scene at any time and must be recovered after long-time occlusion and under appearance changes. Many MOT datasets focus on street scenarios, for example the KITTI tracking dataset <ref type="bibr" target="#b12">[13]</ref>, which features video from a vehicle-mounted camera; or the MOTChallenge datasets <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> that show pedestrians from a variety of different viewpoints. UA-DETRAC <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b35">36]</ref> also features street scenes but contains annotations for vehicles only. Another MOT dataset is PathTrack <ref type="bibr" target="#b36">[37]</ref>, which provides annotations of human trajectories in diverse scenes. PoseTrack <ref type="bibr" target="#b1">[2]</ref> contains annotations of joint positions for multiple persons in videos. None of these datasets provide segmentation masks for the annotated objects and thus do not describe complex interactions like in <ref type="figure" target="#fig_0">Fig. 1</ref> in sufficient detail. Video Object Segmentation Datasets. In the video object segmentation (VOS) task, instance segmentations for one or multiple generic objects are provided in the first frame of a video and must be segmented with pixel accuracy in all subsequent frames. Existing VOS datasets contain only few objects which are also present in most frames. In addition, the common evaluation metrics for this task (region Jaccard in-dex and boundary F-measure) do not take error cases like id switches into account that can occur when tracking multiple objects. In contrast, MOTS focuses on a set of pre-defined classes and considers crowded scenes with many interacting objects. MOTS also adds the difficulty of discovering and tracking a varying number of new objects as they appear and disappear in a scene. Datasets for the VOS task include the DAVIS 2016 dataset <ref type="bibr" target="#b43">[44]</ref>, which focuses on single-object VOS, and the DAVIS 2017 <ref type="bibr" target="#b45">[46]</ref> dataset, which extends the task for multiobject VOS. Furthermore, the YouTube-VOS dataset <ref type="bibr" target="#b59">[60]</ref> is available and orders of magnitude larger than DAVIS. In addition, the Segtrackv2 <ref type="bibr" target="#b28">[29]</ref> dataset, FBMS <ref type="bibr" target="#b40">[41]</ref> and an annotated subset of the YouTube-Objects dataset <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b19">20]</ref> can be used to evaluate this task. Video Instance Segmentation Datasets. Cityscapes <ref type="bibr" target="#b11">[12]</ref>, BDD <ref type="bibr" target="#b61">[62]</ref>, and ApolloScape <ref type="bibr" target="#b18">[19]</ref> provide video data for an automotive scenario. Instance annotations, however, are only provided for a small subset of non-adjacent frames or, in the case of ApolloScape, for each frame but without object identities over time. Thus, they cannot be used for endto-end training of pixel-level tracking approaches. Methods. While a comprehensive review of methods proposed for the MOT or VOS tasks is outside the scope of this paper (for the former, see e.g. <ref type="bibr" target="#b27">[28]</ref>), we will review some works that have tackled (subsets of) the MOTS task or are in other ways related to TrackR-CNN.</p><p>Seguin et al. <ref type="bibr" target="#b51">[52]</ref> derive instance segmentations from given bounding box tracks using clustering on a superpixel level, but they do not address the detection or tracking problem. Milan et al. <ref type="bibr" target="#b38">[39]</ref> consider tracking and segmentation jointly in a CRF utilizing superpixel information and given object detections. In contrast to both methods, our proposed baseline operates on pixel rather than superpixel level. CAMOT <ref type="bibr" target="#b42">[43]</ref> performs mask-based tracking of generic objects on the KITTI dataset using stereo information, which limits its accuracy for distant objects. CDTS <ref type="bibr" target="#b24">[25]</ref> performs unsupervised VOS, i.e., without using first-frame information. It considers only short video clips with few object appearances and disappearances. In MOTS, however, many objects frequently enter or leave a crowded scene. While the above mentioned methods are able to produce tracking outputs with segmentation masks, their performance could not be evaluated comprehensively, since no dataset with MOTS annotations existed.</p><p>Lu et al. <ref type="bibr" target="#b33">[34]</ref> tackle tracking by aggregating location and appearance features per frame and combining these across time using LSTMs. Sadeghian et al. <ref type="bibr" target="#b50">[51]</ref> also combine appearance features obtained by cropped detections with velocity and interaction information using a combination of LSTMs. In both cases, the combined features are input into a traditional Hungarian matching procedure. For our baseline model, we directly enrich detections using temporal in-formation and learn association features jointly with the detector rather than only "post-processing" given detections. Semi-Automatic Annotation. There are many methods for semi-automatic instance segmentation, e.g. generating segmentation masks from scribbles <ref type="bibr" target="#b49">[50]</ref>, or clicks <ref type="bibr" target="#b58">[59]</ref>. These methods require user input for every object to be segmented, while our annotation procedure can segment many objects fully-automatically, letting annotators focus on improving results for difficult cases. While this is somewhat similar to an active learning setting <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b56">57]</ref>, we leave the decision which objects to annotate with our human annotators to guarantee that all annotations reach the quality necessary for a long-term benchmark dataset (c.f . <ref type="bibr" target="#b32">[33]</ref>).</p><p>Other semi-automatic annotation techniques include Polygon-RNN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b0">1]</ref>, which automatically predicts a segmentation in form of a polygon from which vertices can be corrected by the annotator. Fluid Annotation <ref type="bibr" target="#b2">[3]</ref> allows the annotator to manipulate segments predicted by Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> in order to annotate full images. While speeding up the creation of segmentation masks of objects in isolated frames, these methods do not operate on a track level, do not make use of existing bounding box annotations, and do not exploit segmentation masks which have been annotated for the same object in other video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets</head><p>Annotating pixel masks for every frame of every object in a video is an extremely time-consuming task. Hence, the availability of such data is very limited. We are not aware of any existing datasets for the MOTS task. However, there are some datasets with MOT annotations, i.e., tracks annotated at the bounding box level. For the MOTS task, these datasets lack segmentation masks. Our annotation procedure therefore adds segmentation masks for the bounding boxes in two MOT datasets. In total, we annotated 65,213 segmentation masks. This size makes our datasets viable for training and evaluating modern learning-based techniques. Semi-automatic Annotation Procedure.</p><p>In order to keep the annotation effort manageable, we propose a semiautomatic method to extend bounding box level annotations by segmentation masks. We use a convolutional network to automatically produce segmentation masks from bounding boxes, followed by a correction step using manual polygon annotations. Per track, we fine-tune the initial network using the manual annotations as additional training data, similarly to <ref type="bibr" target="#b5">[6]</ref>. We iterate the process of generating and correcting masks until pixel-level accuracy for all annotation masks has been reached.</p><p>For converting bounding boxes into segmentation masks, we use a fully-convolutional refinement network <ref type="bibr" target="#b34">[35]</ref> based on DeepLabv3+ <ref type="bibr" target="#b9">[10]</ref> which takes as input a crop of the input image specified by the bounding box with a small context region added, together with an additional input channel that encodes the bounding box as a mask. Based on these cues, the refinement network predicts a segmentation mask for the given box. The refinement network is pre-trained on COCO <ref type="bibr" target="#b29">[30]</ref> and Mapillary <ref type="bibr" target="#b39">[40]</ref>, and then trained on manually created segmentation masks for the target dataset.</p><p>In the beginning, we annotate (as polygons) two segmentation masks per object in the considered dataset. <ref type="bibr" target="#b0">1</ref> The refinement network is first trained on all manually created masks and afterwards fine-tuned individually for each object. These fine-tuned variants of the network are then used to generate segmentation masks for all bounding boxes of the respective object in the dataset. This way the network adapts to the appearance and context of each individual object. Using two manually annotated segmentation masks per object for fine-tuning the refinement network already produces relatively good masks for the object's appearances in the other frames, but often small errors remain. Hence, we manually correct some of the flawed generated masks and re-run the training procedure in an iterative process. Our annotators also corrected imprecise or wrong bounding box annotations in the original MOT datasets. KITTI MOTS. We performed the aforementioned annotation procedure on the bounding box level annotations from the KITTI tracking dataset <ref type="bibr" target="#b12">[13]</ref>. A sample of the annotations is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. To facilitate training and evaluation, we divided the 21 training sequences of the KITTI tracking dataset 2 into a training and validation set, respectively <ref type="bibr" target="#b2">3</ref> . Our split balances the number of occurrences of each class -cars and pedestrians -roughly equally across training and validation set. Statistics are given in <ref type="table" target="#tab_1">Table 1</ref>  The relatively high number of manual annotations required demonstrates that existing single-image instance segmentation techniques still perform poorly on this task. This is a major motivation for our proposed MOTS dataset which allows for incorporating temporal reasoning into instance segmentation models.</p><p>MOTSChallenge. We further annotated 4 of 7 sequences of the MOTChallenge 2017 <ref type="bibr" target="#b37">[38]</ref> training dataset 4 and obtained the MOTSChallenge dataset. MOTSChallenge focuses on pedestrians in crowded scenes and is very challenging due to many occlusion cases, for which a pixel-wise description is especially beneficial. A sample of the annotations is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, statistics are given in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Measures</head><p>As evaluation measures we adapt the well-established CLEAR MOT metrics for multi-object tracking [4] to our task. For the MOTS task, the segmentation masks per object need to be accommodated in the evaluation metric. Inspired by the Panoptic Segmentation task <ref type="bibr" target="#b23">[24]</ref>, we require that both the ground truth masks of objects and the masks produced by a MOTS method are non-overlapping, i.e., each pixel can be assigned to at most one object. We now introduce our evaluation measures for MOTS.</p><p>Formally, the ground truth of a video with T time frames, height h, and width w consists of a set of N non-empty ground truth pixel masks M = {m 1 , . . . , m N } with m i â {0, 1} hÃw , each of which belongs to a corresponding time frame t m â {1, . . . , T } and is assigned a ground truth track id id m â N. The output of a MOTS method is a set of K non-empty hypothesis masks H = {h 1 , . . . , h K } with h i â {0, 1} hÃw , each of which is assigned a hypothesized track id id h â N and a time frame t h â {1, . . . , T }. Establishing Correspondences. An important step for <ref type="bibr" target="#b3">4</ref> Sequences 2, 5, 9 and 11 were annotated. the CLEAR MOT metrics <ref type="bibr" target="#b3">[4]</ref> is to establish correspondences between ground truth objects and tracker hypotheses. In the bounding box-based setup, establishing correspondences is non-trivial and performed by bipartite matching, since ground truth boxes may overlap and multiple hypothesized boxes can fit well to a given ground truth box. In the case of MOTS, establishing correspondences is greatly simplified since we require that each pixel is uniquely assigned to at most one object in the ground truth and the hypotheses respectively. Thus, at most one predicted mask can have an Intersection-over-Union (IoU) of more than 0.5 with a given ground truth mask <ref type="bibr" target="#b23">[24]</ref>. Hence, the mapping c : H â M âª {â} from hypothesis masks to ground truth masks can simply be defined using mask-based IoU as</p><formula xml:id="formula_0">c(h) = arg max mâM IoU(h, m), if max mâM IoU(h, m) &gt; 0.5 â,</formula><p>otherwise.</p><p>(1)</p><p>The set of true positives TP = {h â H | c(h) = â} is comprised of hypothesized masks which are mapped to a ground truth mask. Similarly, false positives are hypothesized masks that are not mapped to any ground truth mask, i.e. FP = {h â H | c(h) = â}. Finally, the set FN = {m â M | c â1 (m) = â} of false negatives contains the ground truth masks which are not covered by any hypothesized mask.</p><p>In the following, let pred : M â M âª {â} denote the latest tracked predecessor of a ground truth mask, or â if no tracked predecessor exists. So q = pred (p) is the mask q with the same id (id q = id p ) and the largest t q &lt; t p such that c â1 (q) = â 5 . The set IDS of id switches is then defined as the set of ground truth masks whose predecessor was tracked with a different id. Formally,</p><formula xml:id="formula_1">IDS = {m â M | c â1 (m) = â â§ pred (m) = â â§ id c â1 (m) = id c â1 (pred(m)) }.<label>(2)</label></formula><p>Mask-based Evaluation Measures. Additionally, we define a soft version TP of the number of true positives by</p><formula xml:id="formula_2">TP = hâT P IoU(h, c(h)).<label>(3)</label></formula><p>Given the previous definitions, we define mask-based variants of the original CLEAR MOT metrics <ref type="bibr" target="#b3">[4]</ref>. We propose the multi-object tracking and segmentation accuracy (MOTSA) as a mask IoU based version of the box-based MOTA metric, i.e.</p><formula xml:id="formula_3">MOTSA = 1 â |F N | + |F P | + |IDS| |M | = |T P | â |F P | â |IDS| |M | ,<label>(4)</label></formula><p>and the mask-based multi-object tracking and segmentation precision (MOTSP) as  </p><formula xml:id="formula_4">MOTSP = T P |T P | .<label>(5)</label></formula><p>Finally, we introduce the soft multi-object tracking and segmentation accuracy (sMOTSA)</p><formula xml:id="formula_5">sMOTSA = T P â |F P | â |IDS| |M | ,<label>(6)</label></formula><p>which accumulates the soft number TP of true positives instead of counting how many masks reach an IoU of more than 0.5. sMOTSA therefore measures segmentation as well as detection and tracking quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Method</head><p>In order to tackle detection, tracking, and segmentation, i.e. the MOTS task, jointly with a neural network, we build upon the popular Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> architecture, which extends the Faster R-CNN <ref type="bibr" target="#b48">[49]</ref> detector with a mask head. We propose TrackR-CNN (see <ref type="figure" target="#fig_2">Fig. 3</ref>) which in turn extends Mask R-CNN by an association head and two 3D convolutional layers to be able to associate detections over time and deal with temporal dynamics. TrackR-CNN provides mask-based detections together with association features. Both are input to a tracking algorithm that decides which detections to select and how to link them over time.</p><p>Integrating temporal context. In order to exploit the temporal context of the input video <ref type="bibr" target="#b7">[8]</ref>, we integrate 3D convolutions (where the additional third dimension is time) into Mask R-CNN on top of a ResNet-101 <ref type="bibr" target="#b15">[16]</ref> backbone. The 3D convolutions are applied to the backbone features in order to augment them with temporal context. These augmented features are then used by the region proposal network (RPN). As an alternative we also consider convolutional LSTM <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b30">31]</ref> layers. Convolutional LSTM retains the spatial structure of the input by calculating its activations using convolutions instead of matrix products.</p><p>Association Head. In order to be able to associate detections over time, we extend Mask R-CNN by an association head which is a fully connected layer that gets region proposals as inputs and predicts an association vector for each proposal. The association head is inspired by the embedding vectors used in person re-identification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b62">63]</ref>. Each association vector represents the identity of a car or a person. They are trained in a way that vectors belonging to the same instance are close to each other and vectors belonging to different instances are far away from each other. We define the distance d(v, w) between two association vectors v and w as their Euclidean distance, i.e. d(v, w) := v â w .</p><p>We train the association head using the batch hard triplet loss proposed by Hermans et al. <ref type="bibr" target="#b17">[18]</ref> adapted for video sequences. This loss samples hard positives and hard negatives for each detection. Formally, let D denote the set of detections for a video. Each detection d â D consists of a mask mask d and an association vector a d , which come from time frame t d , and is assigned a ground truth track id id d determined by its overlap with the ground truth objects. For a video sequence of T time steps, the association loss in the batch-hard formulation with margin Î± is then given by  Mask-based IoU together with optical flow warping is a strong cue for associating pixel masks over time <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b34">35]</ref>. Hence, we also experiment with mask warping as an alternative cue to association vector similarities. For a detection d â D at time t â 1 with mask mask d and a detection e â D with mask mask e at time t, we define the mask propagation score <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43]</ref> as maskprop(mask d , maske) = IoU(W(mask d ), maske), <ref type="bibr" target="#b8">(9)</ref> where W(m) denotes warping mask m forward by the optical flow between frames t â 1 and t. Tracking. In order to produce the final result, we still need to decide which detections to report and how to link them into tracks over time. For this, we extend existing tracks with new detections based on their association vector similarity to the most recent detection in that track.</p><p>More precisely, for each class and each frame t, we link together detections at the current frame that have detector confidence larger than a threshold Î³ with detections selected in the previous frames using the association vector distances from Eq. 7. We only choose the most recent detection for tracks from up to a threshold of Î² frames in the past. Matching is done with the Hungarian algorithm, while only allowing pairs of detections with a distance smaller than a threshold Î´. Finally, all unassigned high confidence detections start new tracks.</p><p>The resulting tracks can contain overlapping masks which we do not allow for the MOTS task (c.f . Section 4). In such a case, pixels belonging to detections with a higher confidence (given by the classification head of our network) take precedence over detections with lower confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Experimental Setup. For Mask R-CNN we use a ResNet-101 backbone <ref type="bibr" target="#b15">[16]</ref> and pre-train it on COCO <ref type="bibr" target="#b29">[30]</ref> and Mapillary <ref type="bibr" target="#b39">[40]</ref>. Afterwards, we construct TrackR-CNN by adding the association head and integrating two depthwise separable 3D convolution layers with 3 Ã 3 Ã 3 filter kernels each (two dimensions are spatial and the third is over time), ReLU activation, and 1024 feature maps between the backbone and the region proposal network. The 3D convolutions are initialized to an identity function after which the ReLU is applied. When using convolutional LSTM, weights are initialized randomly and a skip connection is added to preserve activations for the pretrained weights of subsequent layers during the initial steps of training. TrackR-CNN is then trained on the target dataset, i.e. KITTI MOTS or MOTSChallenge, for 40 epochs with a learning rate of 5 Â· 10 â7 using the Adam <ref type="bibr" target="#b22">[23]</ref>   obtained in one batch. We choose a margin of Î± = 0.2, which proved useful in <ref type="bibr" target="#b17">[18]</ref>. For the mask propagation experiments, we compute optical flow between all pairs of adjacent frames using PWC-Net <ref type="bibr" target="#b54">[55]</ref>. Our whole tracker achieves a speed of around 2 frames per second at test time. When using convolutional LSTM, it runs online and when using 3D convolutions in near-online fashion due to the two frames look-ahead of the 3D convolutions. We tune the thresholds for our tracking system (Î´, Î², Î³) for each class separately on the target training set with random search using 1000 iterations per experiment.</p><p>Main Results. <ref type="table" target="#tab_4">Table 2</ref> shows our results on the KITTI MOTS validation set. We achieve competitive results, beating several baselines. Mask R-CNN + maskprop denotes a simple baseline for which we fine-tuned the COCO and Mapillary pre-trained Mask R-CNN on the frames of the KITTI MOTS training set. We then evaluated it on the validation set and linked the mask-based detections over time using mask propagation scores (c.f . Section 5). Compared to this baseline, TrackR-CNN achieves higher sMOTSA and MOTSA scores, implying that the 3D convolution layers and the association head help with identifying objects in video. MOTSP scores remain similar.</p><p>TrackR-CNN (box orig) denotes a version of our model trained without mask head on the original bounding box annotations of KITTI. We then tuned for MOTA scores according to the original KITTI tracking annotations on our training split. We evaluate this baseline in our MOTS setting by adding segmentation masks as a post-processing step (denoted by +MG) with the mask head of the KITTI fine-tuned Mask R-CNN. sMOTSA and MOTSA scores for this setup are worse than for our method and the previous baseline, especially when considering pedestrians, adding to our observation that non-tight bounding boxes are not an ideal cue for tracking and that simply using an instance segmentation method on top of bounding box predictions is not sufficient to solve the MOTS task. We show qualitative results for this baseline in <ref type="figure" target="#fig_5">Figure 4</ref>. The box-based model often confuses similar occluding objects for one another, leading to missed masks and id switches. In contrast, our model hypothesizes consistent masks.</p><p>To show that adding segmentation masks as done above does not give an unfair (dis)advantage, we also use the Mask R-CNN mask head to replace the masks generated by our method (TrackR-CNN (ours) + MG). The results stay roughly similar, so no major (dis)advantage incurs.</p><p>In combination, our baseline experiments show that training on temporally consistent instance segmentation data for video gives advantages both over training on instance segmentation data without temporal information and over training just on bounding box tracking data. Joint training on both was not possible before, which underlines the usefulness of our proposed MOTS datasets.</p><p>CAMOT <ref type="bibr" target="#b42">[43]</ref> is a mask-based tracker which can track both objects from pre-defined classes and generic objects using 3D information from the stereo setup in KITTI. In the original version, CAMOT takes as input generic object proposals from SharpMask <ref type="bibr" target="#b44">[45]</ref>. For better comparability, we used the detections from our TrackR-CNN (obtained by running it as a normal detector without association) as inputs instead. Note that CAMOT can only track regions for  which depth from stereo is available which limits its recall. The results show that our proposed tracking method performs significantly better than CAMOT when using the same set of input detections. Since there are not many mask-based trackers with source code available, we also considered the bounding box-based tracking methods CIWT <ref type="bibr" target="#b41">[42]</ref> and BeyondPixels <ref type="bibr" target="#b52">[53]</ref> and again converted their results to segmentation masks using the KITTI fine-tuned Mask R-CNN mask head. Note that these methods were tuned to perform well on the original bounding box based task.</p><p>CIWT <ref type="bibr" target="#b41">[42]</ref> combines image-based information with 3D information from stereo for tracking jointly in image and world space. Once more, detections from our TrackR-CNN were used for comparability. Our proposed tracking system which tackles tracking and mask generation jointly performs better than CIWT when generating masks post-hoc.</p><p>BeyondPixels <ref type="bibr" target="#b52">[53]</ref> is one of the strongest tracking methods for cars on the original KITTI tracking dataset. It combines appearance information with 3D cues. We were not able to run their method with our detections since their code for extracting appearance features is not available. Instead we used their original detections which are obtained from RRC <ref type="bibr" target="#b47">[48]</ref>, a very strong detector. RRC achieves precise localization on KITTI in particular, while the more conventional Mask R-CNN detector was designed for general object detection. The resulting sMOTSA and MOTSA scores are higher than for our method, but still show that there are limits to state-of-the-art bounding box tracking methods on MOTS when simply segmenting boxes using Mask R-CNN. MOTS Using Ground Truth Boxes. For comparison, we derived segmentation results based on bounding box ground truth and evaluated it on our new annotations. Here, we consider two variants of the ground truth: the original bounding boxes from KITTI (orig), which are amodal, i.e. if only the upper body of a person is visible, the box will still extend to the ground, and tight bounding boxes (tight) derived from our segmentation masks. Again, we generated masks using the KITTI MOTS fine-tuned Mask R-CNN. Our results show that even with perfect track hypotheses generating accurate masks remains challenging, especially for pedestrians. This is even more the case when using amodal  boxes, which often contain large regions that do not show the object. This further validates our claim that MOT tasks can benefit from pixel-wise evaluation. Further baselines, where we fill the ground truth boxes with rectangles or ellipses can be found in the supplemental material. Temporal Component.</p><p>In <ref type="table" target="#tab_6">Table 3</ref>, we compare different variants of temporal components for TrackR-CNN. 1xConv3D and 2xConv3D means using either one or stacking two depthwise separable 3D convolutional layers between backbone and region proposal network, each with 1024 dimensions. Similarly, 1xConvLSTM and 2xConvL-STM denotes one or two stacked convolutional LSTM layers at the same stage with 128 feature channels each. The number of parameters per feature channel in a convolutional LSTM is higher due to gating. Using more feature channels did not seem to be helpful during initial experiments. Finally, None denotes adding no additional layers as temporal component. Compared to the None baseline, adding two 3D convolutions significantly improves sMOTSA and MOTSA scores for pedestrians, while performance for cars remains comparable. Surprisingly, using convolutional LSTM does not yield any significant gains over the baseline. Association Mechanism. In <ref type="table" target="#tab_8">Table 4</ref>, we compare different mechanism used for association between detections. Each line follows the proposed tracking system explained in Section 5, but different scores are used for the Hungarian matching step. When using the association head, association vectors may match with detections up to Î² frames in the past. For the remaining association mechanisms, only matching between adjacent frames is sensible.</p><p>For Mask IoU we only use mask propagation scores from Eq. 9, which degrades sMOTSA and MOTSA scores. This underlines the usefulness of our association head which can outperform an optical flow based cue using embeddings provided by a single neural network. Here, we also try training without the association loss (Mask IoU (train w/o assoc.)), which degrades MOTSA scores even more. Therefore, the association loss also has a positive effect on the detector itself. Surprisingly, using bounding box IoU (where the boxes were warped with the median of the optical flow values inside the box, Bbox IoU) performs almost the same as mask IoU. Finally, using only distances of bounding box  MOTSChallenge. <ref type="table" target="#tab_10">Table 5</ref> shows our results on the MOTSChallenge dataset. Since MOTSChallenge only has four video sequences, we trained our method (TrackR-CNN (ours)) in a leaving-one-out fashion (evaluating each sequence with a model trained and tuned on the three others).</p><p>For comparison, we took pre-computed results of four methods that perform well on the MOT17 benchmark and generated masks using a Mask R-CNN fine-tuned on MOTSChallenge (in a leaving-one-out fashion) to evaluate them on our data. We note that all four sets of results use the strongest set of public detections generated with SDP <ref type="bibr" target="#b60">[61]</ref>, while TrackR-CNN generates its own detections. It is also unclear how much these methods were trained to perform well on the MOTChallenge training set, on which MOTSChallenge is based. Despite these odds, TrackR-CNN outperforms all other methods. The last line demonstrates that even with the tight ground truth bounding boxes including track information over time, segmenting all pedestrians accurately remains difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Until now there has been no benchmark or dataset to evaluate the task of multi-object tracking and segmentation and to directly train methods using such temporally consistent mask-based tracking information. To alleviate this problem, we introduce two new datasets based on existing MOT datasets which we annotate using a semi-automatic annotation procedure. We further introduce the MOTSA and sMOTSA metrics, based on the commonly used MOTA metric, but adapted to evaluate all aspects of mask-based tracking. We finally develop a baseline model that was designed to take advantage of this data. We show that through training on our data, the method is able to outperform comparable methods which are only trained with bounding box tracks and single image instance segmentation masks. Our new datasets now make such joint training possible, which opens up many opportunities for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Losses for the Association Head</head><p>TrackR-CNN uses association scores based on vectors predicted by an association head to identify the same object across time. In our baseline model, we train this head using a batch hard triplet loss proposed by Hermans et al. <ref type="bibr" target="#b17">[18]</ref>, which we state again here: Let D denote the set of detections for a video. Each detection d â D has a corresponding association vector a d and is assigned a ground truth track id id d determined by its overlap with the ground truth objects (we only consider detections which sufficiently overlap with a ground truth object here). For a video sequence of T time steps, the association loss in the batch-hard formulation with margin Î± is then given by</p><formula xml:id="formula_7">L batch hard = 1 |D| dâD max max eâD: ide=id d a e â a d â min eâD: ide =id d a e â a d + Î±, 0 .<label>(10)</label></formula><p>Intuitively, each detection d is selected as an anchor and then the most dissimilar detection with the same id is selected as a hard positive example and the most similar detection with a different id is selected as a hard negative example for this anchor. The margin Î± and maximum operation ensure that the distance of the anchor to the hard positive is smaller than its distance to the hard negative example by at least Î±.</p><p>In order to justify our choice of the batch-hard loss, we also report results using two alternative loss formulations, namely the batch all loss <ref type="bibr" target="#b17">[18]</ref> which considers all pairs of detections, i.e.</p><formula xml:id="formula_8">L batch all = 1 |D| 2 dâD eâD max a e â a d â a e â a d + Î±, 0<label>(11)</label></formula><p>and the contrastive loss <ref type="bibr" target="#b13">[14]</ref>  <ref type="table" target="#tab_12">Table 6</ref> compares the performance of these different variants of the loss function on the KITTI MOTS validation set. It can be seen that the batch hard triplet loss performs better than just considering all pairs of detections (Batch All Triplet), or using the conventional contrastive loss (Contrastive). Especially for pedestrians performance using the contrastive loss is low.   </p><formula xml:id="formula_9">L contrastive = 1 |D| 2 dâD eâD ide=id d a e â a d 2 + dâD eâD ide =id d max(Î± â a e â a d , 0) 2 .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of the Annotation Procedure</head><p>We noticed that wrong segmentation results often stem from imprecise or wrong bounding box annotations of the original MOT datasets. For example, the annotated bounding boxes for the KITTI tracking dataset <ref type="bibr" target="#b12">[13]</ref> are amodal, i.e., they extend to the ground even if only the upper body of a person is visible. In these cases, our annotators corrected these bounding boxes instead of adding additional polygon annotations. We also corrected the bounding box level tracking annotations in cases where they contained errors or missed objects. Finally, we retained ignore regions that were labeled in the source datasets, i.e., image regions that contain unlabeled objects from nearby classes (like vans and buses) or target objects that were to small to be labeled. Hypothesized masks that are mapped to ignore regions are neither counted as true nor as false positives in our evaluation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ground Truth Experiments</head><p>We performed additional experiments to demonstrate the difficulty of generating accurate segmentation masks even when the ground truth bounding boxes are given (see <ref type="table" target="#tab_13">Table 7</ref>). As in the main paper, we consider two variants of the ground truth: the original bounding boxes from KITTI (orig), which are amodal, i.e. if only the upper body of a person is visible, the box will still extend to the ground, and tight bounding boxes (tight) derived from our segmentation masks. We created masks for the boxes by simply filling the full box (+Filling), by inserting an ellipse (+Ellipse), and by generating masks using the KITTI MOTS fine-tuned Mask R-CNN (+MG). In each case, instance ids are retained from the corresponding boxes.</p><p>Our results show that rectangles and ellipses are not sufficient to accurately localize objects when mask-based matching is used, even with perfect track hypotheses. The problem is amplified when using amodal boxes, which often contain large regions that do not show the object. This further validates our claim that MOT tasks can benefit from pixel-wise evaluation. The relatively low scores for pedestrians also imply a limit to post-hoc masks generation using the KITTI fine-tuned Mask R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization of Association Vectors</head><p>We present a visualization of the association vectors produced by our TrackR-CNN model on a sequence of the KITTI MOTS validation set in <ref type="figure" target="#fig_6">Figure 5</ref>. Here, all association vectors for detections produced by TrackR-CNN on sequence 18 were used for principal component analysis and then projected onto the two components explaining most of their variance. The resulting two dimensional vectors were used to arrange the crops for the corresponding detections in 2D. The visualization was created using the TensorBoard embedding projector. It can be seen that crops belonging to the same car are in most cases close to each other in the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative results</head><p>We present further qualitative results of our baseline TrackR-CNN model on the KITTI MOTS and MOTSChallenge validation sets including some illustrative failure cases. See <ref type="figure" target="#fig_4">Figures 6, 7, 8</ref> and 9 on the following pages.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Segmentations vs. Bounding Boxes. When objects pass each other, large parts of an object's bounding box may belong to another instance, while per-pixel segmentation masks locate objects precisely. The shown annotations are crops from our KITTI MOTS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sample Images of our Annotations. KITTI MOTS (top) and MOTSChallenge (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>TrackR-CNN Overview. We extend Mask R-CNN by 3D convolutions to incorporate temporal context and by an association head that produces association vectors for each detection. The Euclidean distances between association vectors are used to associate detections over time into tracks. Differences to Mask R-CNN are highlighted in yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>e âa d â min eâD: ide =id d a e âa d +Î±, 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 8 )</head><label>8</label><figDesc>Mask Propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative Results on KITTI MOTS. (a)+(c) Our TrackR-CNN model evaluated on validation sequences of KITTI MOTS. (b)+(d) TrackR-CNN (box orig) + MG evaluated on the same sequences. Training with masks on our data avoids confusion between similar near-by objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Visualization using PCA on the association vectors of detections generated by TrackR-CNN on sequence 18 of KITTI MOTS. Detections with similar appearance are grouped together by minimizing the association loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative Results on MOTSChallenge. While complex scenes with many occluding objects often work well, there can still be missing detections and id switches during difficult occlusions, as in this example (highlighted by red ellipses).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative Results on KITTI MOTS. In simpler scenes, the model is able to continue a track with the same ID after a missing detection (highlighted by red ellipses).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative Results on KITTI MOTS. In a rare failure case, pylons are confused for pedestrians (highlighted by red ellipses). In most cases, detections correspond to real instances of the class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative Results on KITTI MOTS. In less crowded scenes, distinguishing objects works well but some erroneous detections (highlighted by red ellipses) might still happen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell cols="3">KITTI MOTS MOTSChallenge</cell></row><row><cell></cell><cell>train</cell><cell>val</cell><cell></cell></row><row><cell># Sequences</cell><cell>12</cell><cell>9</cell><cell>4</cell></row><row><cell># Frames</cell><cell cols="2">5,027 2,981</cell><cell>2,862</cell></row><row><cell># Tracks Pedestrian</cell><cell>99</cell><cell>68</cell><cell>228</cell></row><row><cell># Masks Pedestrian</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell cols="2">8,073 3,347</cell><cell>26,894</cell></row><row><cell cols="2">Manually annotated 1,312</cell><cell>647</cell><cell>3,930</cell></row><row><cell># Tracks Car</cell><cell>431</cell><cell>151</cell><cell>-</cell></row><row><cell># Masks Car</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell cols="2">18,831 8,068</cell><cell>-</cell></row><row><cell cols="2">Manually annotated 1,509</cell><cell>593</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the Introduced KITTI MOTS and MOTSChallenge Datasets. We consider pedestrians for both datasets and also cars for KITTI MOTS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>optimizer. During training, mini-batches which consist of 8 adjacent frames of a single video are used, where 8 was the maximum possible number of frames to fit into memory with a Titan X (Pascal) graphics card. At batch boundaries, the input to the 3D convolution layer is zero padded in time. When using convolutional LSTM, gradients are backpropagated through all 8 frames during training and at test time the recurrent state is propagated over the whole sequence. The vectors produced by the association head have 128 dimensions and the association loss defined in Eq. 8 is computed over the detections Mask R-CNN + maskprop 75.1 45.0 86.6 63.5 87.1 75.6 TrackR-CNN (box orig) + MG 75.0 41.2 87.0 57.9 86.8 76.3</figDesc><table><row><cell></cell><cell cols="2">sMOTSA</cell><cell cols="2">MOTSA</cell><cell cols="2">MOTSP</cell></row><row><cell></cell><cell>Car</cell><cell>Ped</cell><cell>Car</cell><cell>Ped</cell><cell>Car</cell><cell>Ped</cell></row><row><cell>TrackR-CNN (ours)</cell><cell cols="6">76.2 46.8 87.8 65.1 87.2 75.7</cell></row><row><cell>TrackR-CNN (ours) + MG</cell><cell cols="6">76.2 47.1 87.8 65.5 87.2 75.7</cell></row><row><cell>CAMOT [43] (our det)</cell><cell cols="6">67.4 39.5 78.6 57.6 86.5 73.1</cell></row><row><cell>CIWT [42] (our det) + MG</cell><cell cols="6">68.1 42.9 79.4 61.0 86.7 75.7</cell></row><row><cell>BeyondPixels [53] + MG</cell><cell>76.9</cell><cell>-</cell><cell>89.7</cell><cell>-</cell><cell>86.5</cell><cell>-</cell></row><row><cell>GT Boxes (orig) + MG</cell><cell cols="6">77.3 36.5 90.4 55.7 86.3 75.3</cell></row><row><cell>GT Boxes (tight) + MG</cell><cell cols="6">82.5 50.0 95.3 71.1 86.9 75.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results on KITTI MOTS. +MG denotes mask generation with a KITTI MOTS fine-tuned Mask R-CNN.</figDesc><table /><note>BeyondPixels is a state-of-the-art MOT method for cars and uses a different detector than the other methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>46.3 87.8 64.5 87.1 75.7 2xConv3D 76.2 46.8 87.8 65.1 87.2 75.7 1xConvLSTM 75.7 45.0 87.3 63.4 87.2 75.6 2xConvLSTM 76.1 44.8 87.9 63.3 87.0 75.2 None 76.4 44.8 87.9 63.2 87.3 75.5</figDesc><table><row><cell>Temporal component</cell><cell>sMOTSA Car Ped</cell><cell>MOTSA Car Ped</cell><cell>MOTSP Car Ped</cell></row><row><cell>1xConv3D</cell><cell>76.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Different Temporal Components for TrackR-CNN. Comparison of results on KITTI MOTS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Mask IoU 75.5 46.1 87.1 64.4 87.2 75.7 Mask IoU (train w/o assoc.) 74.9 44.9 86.5 63.3 87.1 75.6</figDesc><table><row><cell>Association Mechanism</cell><cell>sMOTSA Car Ped Car Ped Car Ped MOTSA MOTSP</cell></row><row><cell>Association head</cell><cell>76.2 46.8 87.8 65.1 87.2 75.7</cell></row><row><cell>Bbox IoU</cell><cell>75.4 45.9 87.0 64.3 87.2 75.7</cell></row><row><cell>Bbox Center</cell><cell>74.3 43.3 86.0 61.7 87.2 75.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Different Association Mechanisms for TrackR-CNN. Comparison of results on KITTI MOTS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Results on MOTSChallenge.</figDesc><table><row><cell>+MG denotes mask</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Different Association Losses for TrackR-CNN. Comparison of results on the KITTI MOTS validation set.</figDesc><table><row><cell></cell><cell cols="2">sMOTSA</cell><cell cols="2">MOTSA</cell><cell cols="2">MOTSP</cell></row><row><cell></cell><cell>Car</cell><cell>Ped</cell><cell>Car</cell><cell>Ped</cell><cell>Car</cell><cell>Ped</cell></row><row><cell cols="7">GT Boxes (orig) + Filling 33.7 -66.1 55.5 -57.7 71.8 54.6</cell></row><row><cell cols="7">GT Boxes (orig) + Ellipse 52.3 -31.9 74.0 -14.5 74.9 57.4</cell></row><row><cell>GT Boxes (orig) + MG</cell><cell cols="6">77.3 36.5 90.4 55.7 86.3 75.3</cell></row><row><cell cols="7">GT Boxes (tight) + Filling 61.3 -1.7 83.9 22.0 75.4 60.5</cell></row><row><cell cols="7">GT Boxes (tight) + Ellipse 70.9 17.2 91.8 42.4 78.1 64.2</cell></row><row><cell>GT Boxes (tight) + MG</cell><cell cols="6">82.5 50.0 95.3 71.1 86.9 75.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Ground Truth Results on KITTI MOTS. +MG denotes mask generation with a KITTI MOTS fine-tuned Mask R-CNN..</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The two frames annotated per object are chosen by the annotator based on diversity.<ref type="bibr" target="#b1">2</ref> We are currently applying our annotation procedure to the KITTI test set with the goal of creating a publicly accessible MOTS benchmark.<ref type="bibr" target="#b2">3</ref> Sequences 2, 6, 7, 8, 10, 13, 14, 16 and 18 were chosen for the validation set, the remaining sequences for the training set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">This definition corresponds to the one used by MOTChallenge. Note that the original KITTI tracking benchmark does not count id switches if the target was lost by the tracker.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This project has been funded, in parts, by ERC Consolidator Grant DeeViSe (ERC-2017-COG-773161). The experiments were performed with computing resources granted by RWTH Aachen University under project rwth0373. We would like to thank our annotators.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient interactive annotation of segmentation datasets with polygon-rnn++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PoseTrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ensafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fluid annotation: a human-machine collaboration interface for full image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07527</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: The clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Image and Video Processing</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards a principled integration of multi-camera re-identification and tracking through optimal bayes filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Breuers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-TaixÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<title level="m">The 2018 DAVIS challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Annotating object instances with a polygon-rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards scalable dataset construction: An active learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fusion of head and full-body detectors for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPRW</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06184</idno>
		<title level="m">The apolloscape dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Motion segmentation &amp; multiple object tracking by correlation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Panoptic segmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CDTS: Collaborative detection, tracking, and segmentation for online multiple object segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A novel performance evaluation methodology for single-target trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Äehovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-TaixÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Motchallenge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<title level="m">Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Tracking the trackers: an analysis of the state of the art in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-TaixÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02781</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mobile video object detection with temporally-aware feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time multiple people tracking with deeply learned candidate selection and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haizhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zijie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">How transferable are the datasets collected by active learners?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04801</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online video object detection using association LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PReMVOS: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">UA-DETRAC 2017: Report of AVSS2017 &amp; IWT4S challenge on advanced traffic monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Del</forename><surname>Coco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<publisher>AVSS</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pathtrack: Fast trajectory annotation with path supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-TaixÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">MOT16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint tracking and segmentation of multiple targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-TaixÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Mapillary Vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Combined image-and world-space tracking in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>OÅ¡ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Track, then decide: Category-agnostic vision-based multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>OÅ¡ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>ArbelÃ¡ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 DAVIS challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">GrabCut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Instance-level video segmentation from object tracks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seguin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Beyond pixels: Leveraging geometry and shape cues for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi people tracking with lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Video annotation and tracking with active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04136</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">YouTube-VOS: Sequence-tosequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<title level="m">BDD100K: A diverse driving video database with scalable annotation tooling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
