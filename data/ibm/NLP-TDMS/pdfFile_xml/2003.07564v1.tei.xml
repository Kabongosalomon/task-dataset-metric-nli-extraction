<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feedback Graph Convolutional Network for Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-17">17 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<email>yanghao1@nuctech.com</email>
							<affiliation key="aff0">
								<orgName type="department">R&amp;D Center of Artificial Intelligent</orgName>
								<orgName type="institution">NUCTECH Company Limited</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Yan</surname></persName>
							<email>yandan@nuctech.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">R&amp;D Center of Artificial Intelligent</orgName>
								<orgName type="institution">NUCTECH Company Limited</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering Physics</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
							<email>li.dong@nuctech.com</email>
							<affiliation key="aff0">
								<orgName type="department">R&amp;D Center of Artificial Intelligent</orgName>
								<orgName type="institution">NUCTECH Company Limited</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunda</forename><surname>Sun</surname></persName>
							<email>sunyunda@nuctech.com</email>
							<affiliation key="aff0">
								<orgName type="department">R&amp;D Center of Artificial Intelligent</orgName>
								<orgName type="institution">NUCTECH Company Limited</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering Physics</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
							<email>s.you@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">R&amp;D Center of Artificial Intelligent</orgName>
								<orgName type="institution">NUCTECH Company Limited</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
							<email>sjmaybank@dcs.bbk.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering Physics</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Information Systems</orgName>
								<address>
									<addrLine>Birkbeck College</addrLine>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
						</author>
						<title level="a" type="main">Feedback Graph Convolutional Network for Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-17">17 Mar 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feedback</term>
					<term>Graph Convolutional Network</term>
					<term>Skeleton</term>
					<term>Action Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skeleton-based action recognition has attracted considerable attention in computer vision since skeleton data is more robust to the dynamic circumstance and complicated background than other modalities. Recently, many researchers have used the Graph Convolutional Network (GCN) to model spatial-temporal features of skeleton sequences by an end-to-end optimization. However, conventional GCNs are feedforward networks which are impossible for low-level layers to access semantic information in the high-level layers. In this paper, we propose a novel network, named Feedback Graph Convolutional Network (FGCN). This is the first work that introduces the feedback mechanism into GCNs and action recognition. Compared with conventional GCNs, FGCN has the following advantages: (1) a multi-stage temporal sampling strategy is designed to extract spatial-temporal features for action recognition in a coarse-to-fine progressive process; (2) A dense connections based Feedback Graph Convolutional Block (FGCB) is proposed to introduce feedback connections into the GCNs. It transmits the high-level semantic features to the low-level layers and flows temporal information stage by stage to progressively model global spatial-temporal features for action recognition; (3) The FGCN model provides early predictions. In the early stages, the model receives partial information about actions. Naturally, its predictions are relatively coarse. The coarse predictions are treated as the prior to guide the feature learning of later stages for a accurate prediction. Extensive experiments on the datasets, NTU-RGB+D, NTU-RGB+D120 and Northwestern-UCLA, demonstrate that the proposed FGCN is effective for action recognition. It achieves the state-of-the-art performance on the three datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the quantity of videos uploaded from various terminals has exploded. This has driven imperious demands for human action analysis automatically based on the content of videos. In particular, human action recognition using skeleton has attracted many computer vision researchers because of its strong adaptability to the effects of dynamic circumstance and complicated background, as compared with other modalities such as RGB <ref type="bibr" target="#b15">[16]</ref> and optical flow <ref type="bibr" target="#b46">[47]</ref>. Early deep learning methods using skeletons for action recognition usually represent the skeleton data as a sequence of joint-coordinate vectors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b23">24]</ref> or a pseudo-image <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27]</ref> which is then modeled by a RNN or CNN respectively. However, these methods do not explicitly exploit the spatial dependencies among correlated joints, even though the spatial dependencies are informative for understanding human actions. More recently, some methods <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b22">23]</ref> construct spatial temporal graphs based on the natural connections of joints and temporal edges of consecutive frames. They then exploit a GCN to model spatial-temporal features. However, the conventional GCNs are all single-pass feedforward networks that are fed with the entire skeleton sequence. It is difficult for these methods to extract effective spatial-temporal features, because the useful information is usually buried in the motion-irrelevant or undiscriminating clips when they are fed with entire skeleton sequence. For example, in the action "kicking something", most clips are "standing upright", and in the action "wear a shoe", most clips are a subject sitting on a chair. Then the singlepass feedforward networks can not access the high-level semantic information for the low-level layers. Meanwhile, inputting the entire skeleton sequence increases computational complexity of the model.</p><p>Motivated by this, we propose a novel neural network, named Feedback Graph Convolutional Network (FGCN), to extract effective spatial-temporal features from skeleton data in a coarse-to-fine progressive process for action recognition. The FGCN is the first work that introduces feedback mechanism into GCNs and action recognition. Compared with conventional GCNs, the FGCN has a multi-stage temporal sampling strategy which divides input skeleton sequences into multiple stages in the temporal domain and sparsely samples input skeleton clips from temporal stages to avoid feeding with the entire skeleton sequence. Each sampled clip is input into graph convolutional layers to extract local spatial-temporal features for each stage. A Feedback Graph Convolutional Block (FGCB) is proposed to model global spatial-temporal features by fusing the local features. The FGCB is a local dense graph convolutional network with lateral connections from each stage to the next stage and it introduces feedback connections into conventional GCNs. From a semantic point of view it works in a top down manner, which makes it possible for low-level convolutional layers to access semantic information in the high-level layers at each stage. From the temporal domain, the feedback mechanism in FGCB works with a sequence of cause-and-effect and the output of the previous stage flows into the next stage to modulate its input.</p><p>Another advantage of the FGCN is that it provides early predictions of the output in a fraction of the total inference time. This is valuable in many applications such as robotics or autonomous driving, in which latency time is very crucial. The early predictions are a result of the proposed multi-stage coarseto-fine progressively optimization. In the early stages, FGCN is only fed with a part of skeleton sequence and the information about the action is limited, so the inferences of it are relatively coarse. These inferences are treated as a prior to guide the feature learning in later stages. In later stages, the model receives more complete information about the action and the guider of former inferences, thus it outputs more accurate inferences. Several temporal fusion strategies are proposed to fuse the local predictions in temporal stages for a video-level prediction. The strategies enable the network to be optimized in a progressive process.</p><p>The main contributions of this paper are summarized as follows:</p><p>-We propose a novel Feedback Graph Convolutional Network (FGCN) for action recognition from skeleton sequences. It models spatial-temporal features by a multi-stage progressive process. To our knowledge, this is the first work that introduces the feedback mechanism into GCNs and action recognition. -We propose a dense connections based Feedback Graph Convolutional Block (FGCB) which is a local network with lateral connections between two temporal stages. Functionally, it transmits high-level semantic features as priors to module its features in low-level layers. -The FGCN model provides early predictions, which benefits from the multistage coarse-to-fine progressive optimization. The proposed model is extensively evaluated on three datasets, NTU-RGB+D, NTU-RGB+D120 and Northwestern-UCLA, and it achieves state-of-the-art performance on the three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Skeleton based Action Recognition</head><p>As the depth sensor technologies (i.e. kinect <ref type="bibr" target="#b58">[59]</ref>) and pose estimation algorithms <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b3">4]</ref> matured, it becomes possible to capture skeleton data in real time by locating the key joints. The skeleton data is robust to illumination change, scene variation, and complex background. These facilitate the data-driven method's development of skeleton-based action recognition. Conventional action recognition methods usually extract hand-crafted features from skeleton sequences. Some traditional methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref> design several view-invariant features of actions. Examples of these features are body part-based skeletal quads <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b52">53]</ref>, group sparsity based class-specific dictionary coding <ref type="bibr" target="#b32">[33]</ref>, and canonical view transformed features <ref type="bibr" target="#b39">[40]</ref>. Other traditional methods integrate the information from different modalities that are always available in 3D action datasets. Some works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b53">54]</ref> combine the depth information with the skeleton to improve performance. The depth information is represented by HOG features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref> and Fourier Temporal Pyramids <ref type="bibr" target="#b53">[54]</ref>, or it is modeled by random decision forests <ref type="bibr" target="#b38">[39]</ref>. The recent success of deep learning has led to a surge of deep network based skeleton modeling methods. The widely used models are RNNs and CNNs. RNN-based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b23">24]</ref> usually concatenate all of the joint-coordinates (2D or 3D) in each frame as a vector and then model the features of actions by a RNN fed with a sequence of the coordinate vectors. CNN-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27]</ref> stack the sequence of coordinate vectors to obtain a pseudo-image, and then reduce the action recognition using skeleton sequences to an image classification task. The two-stream based model <ref type="bibr" target="#b59">[60]</ref> combines RNN and CNN, operating on coordinate vectors of skeletons and RGB images respectively, to improve performance from a single network. However, these methods do not explicitly model the spatial dependence between correlated joints which is crucial for understanding human actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GCN based Action Recognition</head><p>The Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref> generalize the convolutional operation to deal with the data with graph construction. There are two main ways of constructing GCNs: spatial perspective and spectral perspective. Spatial perspective methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref> directly perform the convolution filters on the graph vertexes and their neighbors. In contrast, spectral perspective methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref> consider the graph convolution as a form of spectral analysis by utilizing the eigenvalues and eigenvectors of the graph Laplacian matrices. This work follows the spatial perspective based methods <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b22">23]</ref>. The ST-GCN model <ref type="bibr" target="#b55">[56]</ref> is proposed to move beyond the limitations of hand-crafted parts and traversal rules used in previous methods. It operates on a spatial temporal graph to model the structured information about the joints along both the spatial and temporal dimensions. Based on ST-GCN, the 2s-AGCN model <ref type="bibr" target="#b43">[44]</ref> proposes a two-stream adaptive graph convolutional network, which exploits the second-order information of the skeleton to improve the performance of action recognition. The DGNN model <ref type="bibr" target="#b42">[43]</ref> represents the skeleton data as a directed acyclic graph based on the kinematic dependency between the joints and bones.</p><p>The AS-GCN model <ref type="bibr" target="#b22">[23]</ref> proposes an actional-structural graph convolution network by generating the skeleton graph with actional links and structural links. However, conventional GCNs are all feedforward networks in which it is impossible for low-level layers to access the semantic information in high-level layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feedback Network</head><p>Feedback mechanism exists in the human visual cortex <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9]</ref>, and it has been a focus of research in psychology <ref type="bibr" target="#b0">[1]</ref> and control theory <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref>. In recent years, feedback mechanism has been introduced into deep neural networks in computer vision <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5]</ref>, because it allows the network to carry the information of output to correct previous states. In object recognition, the dasNet model <ref type="bibr" target="#b48">[49]</ref> exploits the feedback structure by dynamically altering its convolutional filter sensitivities during classification and iteratively focusing its internal attention on some of its convolutional filters. Feedback Network <ref type="bibr" target="#b56">[57]</ref> firstly introduces the feedback mechanism into the convolutional recurrent neural network, which transfers the hidden state with high-level information to the input layer. In super resolution, several efforts <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref> are made to take advantage of the feedback mechanism. The DBPN model <ref type="bibr" target="#b10">[11]</ref> proposes a deep back-projection network which exploits iterative up-projection and down-projection units to achieve error feedback. The DSRN model <ref type="bibr" target="#b9">[10]</ref> proposes a dual-state RNN and transmits the information between two recurrent states via a delayed feedback. The SRFBN model <ref type="bibr" target="#b25">[26]</ref> designs a feedback block to handle the feedback connections and refines low-level representations with high-level information. In human pose estimation, <ref type="bibr" target="#b4">[5]</ref> proposes an iterative error feedback (IEF) by iteratively estimating and applying a self-correction to the current estimation.</p><p>3 The Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Convolutional Network</head><p>GCNs generalize the convolution operation to learn effective representations from graph structured data. In action recognition, the skeleton of a body is defined as an undirected graph in which each joint of the skeleton is defined as a vertex of the graph and the natural connections in the human body are defined as edges of the graph. In this paper, the skeleton in the frame t is denoted as a</p><formula xml:id="formula_0">graph G t = {V t , E t },</formula><p>where V t is the set of joints in the frame and E t is the set of bones in the skeleton. For 3D skeleton data, the joint set is denoted as</p><formula xml:id="formula_1">V t = {v ti } N i=1 , where v ti = (x ti , y ti , z ti ).</formula><p>Given two joints v ti = (x ti , y ti , z ti ) and v tj = (x tj , y tj , z tj ), a bone of the skeleton is defined as a vector e vti,vtj = (x tj − x ti , y tj − y ti , z tj − z ti ), (i, j) ∈ Q, where Q is the set of naturally connected human body joints. The skeleton sequence with len frames is denoted as S = {G 1 , G 2 , . . . , G len }.</p><p>The graph convolution is defined operating on each vertex and its neighbors. For a vertex v ti in the graph, its neighbor set is denoted as</p><formula xml:id="formula_2">N (v ti ) = {v tj |d(v ti , v tj ) ≤ D}, where d(v ti , v tj )</formula><p>is the length of the shortest path from v tj to v ti . We set D = 1 for the 1-distance neighbor set in this paper. The graph convolution operating on the neighbor set of vertex v ti is formulated as:</p><formula xml:id="formula_3">f out (v ti ) = vtj ∈N (vti) 1 Z[l(v tj )] f in (v tj )W [l(v tj )],<label>(1)</label></formula><p>where f in and f out denote the input and output feature maps of this convolutional layer. l(v tj ) is the label function which allocates a label from 1 to K for the vertex in N (v ti ). In our experiments, we set K = 3 empirically to divide N (v ti ) into 3 subsets. W (·) is the weighting function which provides a weight vector according to the label l(v tj ). Similarly, Z[l(v tj )] denotes the number of vertexes corresponding to the subset of l(v tj ).</p><p>In implementation, the connections of a graph are recorded in an N × N adjacency matrix A k . With the adjacency matrix, Eqn. 1 can be formulated as: where denotes the dot product and Λ ii k = j A ij k is a diagonal matrix. W k is the weight vector of the convolution operation, which corresponds to the weighting function W (·) in Eqn. 1. In practice, A k is allocated with a learnable weight matrix M k which is an N × N attention map that indicates the importance of each vertex. It is initialized as an all-one matrix.</p><formula xml:id="formula_4">f out = K k=1 W k (Λ − 1 2 k A k Λ − 1 2 k f in ) (M k ),<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feedback Graph Convolutional Network</head><p>Traditional action recognition methods <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b22">23]</ref> based on GCNs are all fed with the entire skeleton sequence in a feedforward network. However, the useful information is usually buried in the motion-irrelevant and undiscriminating clips when fed with entire skeleton sequence. And single-pass feedforward networks can not access semantic information at low-level layers. To tackle these problems, we propose a Feedback Graph Convolutional Network (FGCN) which extracts spatial-temporal features by a multi-stage progressive process, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Specifically, in the FGCN a multi-stage temporal sampling strategy is designed to sparsely sample a sequence of input clips from the skeleton data, instead of operating on the entire skeleton sequence directly. These clips are first fed into graph convolutional layers to extract the local spatial-temporal features. Then, a Feedback Graph Convolutional Block (FGCB) is proposed to fuse the local spatial-temporal features from multiple temporal stages by transmitting the high-level information in the previous stage to the next stage to modulate its input. Finally, several temporal fusion strategies are proposed to fuse the local predictions from all temporal stages to give a video-level prediction.</p><p>Formally, given a skeleton sequence S, the multi-stage temporal sampling strategy first divides it into T temporal stages with equal time interval, denoted as S = {s 1 , s 2 , . . . , s T }. In each temporal stage, a skeleton clip is sampled randomly as an input of the deep model, denoted as {c 1 , c 2 , . . . , c T }, where c t is the input clip sampled from the corresponding stage s t . Each sampled clip c t is input to the stacked multiple graph convolutional layers to extract the local spatial-temporal features in the corresponding temporal stage, formulated as:</p><formula xml:id="formula_5">F t = f GConvs (c t ),<label>(3)</label></formula><p>where t = 1, 2, . . . , T , and F t is the local spatial-temporal features extracted by graph convolutional layers which are denoted as GConvs in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>The local features extracted from all temporal stages flow into the feedback block FGCB to learn global spatial-temporal features for action recognition. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, FGCB receives two inputs at the stage t: one is the hidden state from the previous stage t − 1, denoted as H t−1 ; the other is the local features from the current stage, denoted as F t . Particularly, the input feature at the first stage F 1 is regarded as the initial hidden state H 0 . Based on these two inputs, the feedback process of FGCB is formulated as:</p><formula xml:id="formula_6">H t = f F GCB (H t−1 , F t ),<label>(4)</label></formula><p>where H t is the output of FGCB at stage t, and the function f F GCB (·) represents the operations of the feedback block FGCB. More details about FGCB can be found in Section 3.3. Following the FGCB, a fully connected layer and a softmax loss layer are used at each stage to predict actions. The prediction process from the output H t of FGCB is formulated as:</p><formula xml:id="formula_7">P t = f pred (H t ),<label>(5)</label></formula><p>where P t ∈ R C denotes the local prediction at stage t and C is the number of actions. The function f pred (·) represents the operations of the fully connected layer and the softmax layer. After operating on T temporal stages, we will obtain totally T local predictions {P 1 , P 2 , . . . , P T }. Several temporal fusion strategies are proposed to fuse these local predictions corresponding to multiple stages for a video-level prediction P S which is computed as:</p><formula xml:id="formula_8">P S = f tf (P 1 , P 2 , . . . , P T ),<label>(6)</label></formula><p>where f tf is the operations of a temporal fusion strategy. In this paper, we propose three temporal fusion strategies, i.e. last-win-all fusion, average fusion and weighting fusion. The FGCN model is trained end-to-end with the crossentropy loss as follows:</p><formula xml:id="formula_9">L(y, P S ) = − C i=1 y i log(P i S ),<label>(7)</label></formula><p>where y is the action label of the skeleton S, if y = i, y i is set as 1, otherwise it is set as 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gconv GConv Gconv</head><p>Input: </p><formula xml:id="formula_10">Output −1 1 2 −1 Gconv … , ,<label>, , , , 2 , , 2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feedback Graph Convolutional Block</head><p>The feedback block FGCB is the core component of the FGCN model. On the one hand, the FGCB transmits the high-level semantic information back to lowlevel layers to refine their encoded features. On the other hand, the output at the previous stage flows into the next stage to modulate its input. To enable the FGCB to effectively transmit information from high-level to low-level and from the previous stage to the next stage, we propose a dense connected local graph convolutional network which adds shortcut connections from each layer to all subsequent layers. At a temporal stage t, the FGCB receives the high-level information from the output H t−1 of the previous stage to modulate the lowlevel input F t of the current stage. In our model, the FGCB consists of L spatial temporal graph convolutional layers. The spatial temporal graph convolutional layer is denoted as GConv(k s , k t , m) in <ref type="figure" target="#fig_1">Fig. 2</ref>, where k s and k t are the kernel size in the spatial and temporal domains respectively, and m denotes output channels of the graph convolutional layer. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the first convolutional layer in FGCB receives two inputs F t and H t−1 . It compresses and fuses the features from the concatenation of the two inputs [F t , H t−1 ]. The output of this layer is formulated as:</p><formula xml:id="formula_11">h 1 t = f 1 F GCB ([F t , H t−1 ]),<label>(8)</label></formula><p>where f 1 F GCB (·) denotes the operations in the first convolutional graph layer of FGCB, and h 1 t denotes the output feature maps of the first layer. Following the first layer, the l th layer receives the output feature maps from all preceding layers, h 1 t , h 2 t , . . . , h l−1 t , as input:</p><formula xml:id="formula_12">h l t = f l F GCB ([h 1 t , h 2 t , . . . , h l−1 t ]),<label>(9)</label></formula><p>where l = 1, 2, . . . , L and [h 1 t , h 2 t , . . . , h l−1 t ] refers to the concatenated feature maps in preceding layers. Similar to the first layer, the final layer in FGCB compresses and fuses the feature maps from the outputs of all preceding layers to produce the output of FGCB:</p><formula xml:id="formula_13">H t = h L t = f L F GCB ([h 1 t , h 2 t , . . . , h L−1 t ]),<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Two-stream Framework of FGCN</head><p>The joints and bones of a skeleton only contain spatial information of actions, However, many actions are difficult to recognize from the spatial information ＋ prediction FGCN-motion FGCN-spatial <ref type="figure">Fig. 3</ref>. The prediction scores of FGCN-spatial and FGCN-motion are fused for final action prediction.</p><p>alone, for example "wear a shoe" versus "take off a shoe", "wear on glasses" versus "take off glasses" and etc. Inspired by <ref type="bibr" target="#b42">[43]</ref>, we model the spatial-temporal features not only exploiting spatial information but the temporal movement information of skeleton sequences. As in Section 3.1, the joint and bone of skeleton are denoted as a vector of coordinates. The movement of a joint or bone is defined as the difference of the vectors for the same joint or bone in consecutive frames along the temporal dimension. Given the joints and bones from two consecutive frames, denoted as v ti , v (t+1)i and e vti,vtj , e v (t+1)i ,v (t+1)j respectively, the movement of joints is defined as mv ti = v (t+1)i − v ti . Similarly, the movement of bones is defined as me ij t = e v (t+1)i ,v (t+1)j − e vti,vtj . As the spatial information modeling, the motion information is formulated as a sequence of graphs</p><formula xml:id="formula_14">S m = {G m 1 , G m 2 , . . . , G m len }, where G m t = {V m t , E m t }, V m t = {mv ti } N i=1 and E m t = {me ij t } (i,j)∈Q .</formula><p>In this paper, the spatial graph S and the motion graph S m are fed into two separate FGCN models to predict action labels. The model fed with spatial graphs S is denoted as FGCN-spatial, the other fed with temporal graphs S m is denoted as FGCN-motion. The two models are finally fused by weighting the output scores of the softmax layers, as shown in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the proposed FGCN method by conducting extensive experiments on three 3D skeleton action datasets, NTU-RGB+D, NTU-RGB+D120, and Northwestern-UCLA.  20 subjects, and have 40,320 and 16,560 sequences respectively. The cross-view setup divides the data according to camera views. The training set has 37,920 sequences captured from the front and two side views, while the test set has 18,960 sequences captured from left and right 45 degree views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTU-RGB+D</head><p>NTU-RGB+D120 <ref type="bibr" target="#b27">[28]</ref> is currently the largest in-door captured 3D skeleton dataset. It is an extension of NTU-RGB+D with 120 action classes and more than 114,000 video samples. The newly added action classes make the action recognition more challenging. For example, different actions may have similar body motions but different subjects. There may be fine-grained hand or finger motions and so on. The dataset has 106 subjects and 32 setup IDs. Crosssubject and cross-setup benchmarks are defined. For cross-subject, 53 subjects constitute the training set, and the remaining 53 subjects constitute the test set. Analogously, the 32 setup IDs are also divided equally into two parts for training and testing in cross-setup.</p><p>Northwestern-UCLA [55] is a multi-view 3D event dataset captured simultaneously by three Kinect cameras from different viewpoints. This dataset includes 1494 video sequences covering 10 action categories performed by 10 subjects from 1 to 6 times. It provides 3D spatial coordinates of 20 major body joints. As reported in <ref type="bibr" target="#b54">[55]</ref>, we pick all samples from the first two cameras for training. The samples from the remaining cameras are for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>All experiments are implemented with PyTorch deep learning framework. A stochastic gradient descent (SGD) optimizer is used during training with the batch size as 32, the momentum as 0.9, and the initial learning rate as 0.1. The learning rate is divided by 10 at the 40th and 60th epoch. The training process ends at the 80th epoch. In our experiments, the input video is divided into five stages temporally and 64 consecutive frames are sampled randomly from each stage to form an input clip. Ten graph convolutional layers are stacked at the front of the feedback block FGCB and these layers have the same configuration as the graph convolutional layers in ST-GCN <ref type="bibr" target="#b55">[56]</ref>. The FGCB has four graph convolutional layers (i.e. L = 4). The spatial temporal kernel sizes and output channels of them are set as k s = 3, k t = 3 and m = 256 respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we design four ablation experiments to evaluate the influence of different hyper-parameters, architecture and inputs on the performance of our FGCN model. These ablation experiments are all conducted on the challenging skeleton dataset NTU-RGB+D.</p><p>In the first experiment, we evaluate the influence of two key hyper-parameters on the performance of our FGCN model, i.e., the number of stages and the length of the input clip in each stage. In <ref type="figure" target="#fig_2">Fig. 4(a)</ref>, the performances of FGCN with different numbers of temporal stages are reported. The FGCN model achieves the best performance when the input video is divided into 6 stages with equal duration. In the subsequent experiments, we set the number of temporal stages at 5, to balance performance against computational cost. Similar performances are obtained with 6 temporal stages. In <ref type="figure" target="#fig_2">Fig. 4(b)</ref>, we evaluate the performance of FGCN fed with different numbers of frames at each stage. Based on the similar model selection strategy in the last experiment, we set the frame length as 64 in the subsequent experiments to balance performance against computational cost.  In the second experiment, we evaluate the effectiveness of different temporal fusion strategies in the FGCN model, i.e. last-win-all fusion, average fusion, and weight fusion. The experiment results are listed in Tab. 1. Among these three fusion strategies, the average fusion strategy achieves the best performance. Based on the results, we use the average fusion strategy to fuse the local predictions for the video-level prediction in the subsequent experiments.</p><p>In the third experiment, we evaluate the effectiveness of the proposed FGCN model fed with joints and bones. We first compare the proposed FGCN model with its baseline ST-GCN model. The two models have the same architecture and configuration of convolutional layers. As shown in the upper part of Tab. 2, the FGCN model fed with joint sequences of skeletons (FGCN-joint) outperforms the baseline model ST-GCN-joint by 5.54% and 5.27% on the cross-subject and cross-view benchmarks respectively. The confusion matrices for the former 30 classes are shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, and the complete confusion matrices are shown in the supplementary materials. The improvements indicate that introducing feedback mechanism into GCNs is very effective for action recognition. Moreover, we fuse the softmax scores of two FGCN models, where one model is FGCN-joint, the other is FGCN-bone which is fed with the bone sequences. The fusion model FGCN-joint+FGCN-bone achieves a clear improvement, compared with FGCNjoint and FGCN-bone.</p><p>In the fourth experiment, we evaluate the effectiveness of FGCN model fed with the spatial information and the motion information, i.e. FGCN-spatial and FGCN-motion, on the NTU-RGB+D dataset. The experiment results of these two models and their fusion are reported in the under part of Tab. 2. Firstly, the FGCN-spatial model fed with spatial information (joints and bones) achieves 88.32% on cross-subject and 94.82% on cross-view. It is comparable with the performance of the FGCN-joint+FGCN-bone model that fuses the softmax scores of two models. Then, the FGCN-motion fed with the movement of joints and bones achieves 85.96% on cross-subject and 93.57% on cross-view. Finally, we fuse the softmax scores of FGCN-spatial and FGCN-motion. The FGCN-spatial+FGCN-motion achieves 90.22% on cross-subject and 96.25% on cross-view, and it achieves a clear improvement from both of FGCN-spatial and FGCN-motion. <ref type="table">Table 3</ref>. Comparisons with the state-of-the-art methods on NTU-RGB+D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Cross-subject(%) Cross-view(%) ResNet152-3S (ICMEW 2017) <ref type="bibr" target="#b21">[22]</ref> 85.0 92.3 ST-GCN (AAAI 2018) <ref type="bibr" target="#b55">[56]</ref> 81.5 88.3 DPRL+GCNN (CVPR 2018) <ref type="bibr" target="#b49">[50]</ref> 83.5 89.8 SR-TSL (ECCV 2018) <ref type="bibr" target="#b45">[46]</ref> 84.8 92.4 PB-GCN (BMVC 2018) <ref type="bibr" target="#b50">[51]</ref> 87.5 93.2 Bayesian GC-LSTM (ICCV 2019) <ref type="bibr" target="#b60">[61]</ref> 81.8 89.0 AS-GCN (CVPR 2019) <ref type="bibr" target="#b22">[23]</ref> 86.8 94.2 AGC-LSTM (CVPR 2019) <ref type="bibr" target="#b44">[45]</ref> 89.2 95.0 2s-AGCN (CVPR 2019) <ref type="bibr" target="#b43">[44]</ref> 88.5 95.1 DGNN (CVPR 2019) <ref type="bibr" target="#b42">[43]</ref> 89.9 96.1 FGCN (ours) 90.2 96.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-art</head><p>In this section, we compare the performance of the FGCN model with the recent state-of-the-art methods on the NTU-RGB+D dataset, the NTU-RGB+D120 dataset, and the Northwestern-UCLA dataset. For the NTU-RGB+D dataset, we display the accuracy of skeleton based action recognition methods, such as CNN-based methods <ref type="bibr" target="#b21">[22]</ref>, RNN-based methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b60">61]</ref> and GCN based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56]</ref>. As shown in Tab. 3, the proposed FGCN model achieves 8.7% and 8.0% improvements on the cross-subject and cross-view benchmarks respectively over the most comparable method ST-GCN <ref type="bibr" target="#b55">[56]</ref>. These improvements show the effectiveness of the proposed feedback framework in action recognition. Moreover, the FGCN model outperforms other recent state-of-the-art methods, such as AS-GCN <ref type="bibr" target="#b44">[45]</ref>, 2s-AGCN <ref type="bibr" target="#b43">[44]</ref>, and DGNN <ref type="bibr" target="#b42">[43]</ref>. Our FGCN model achieves state-of-the-art performance on both cross-subject and cross-view benchmarks of the NTU-RGB+D dataset. <ref type="table">Table 4</ref>. Comparisons with the state-of-the-art methods on NTU-RGB+D120.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Cross-subject(%) Cross-setup(%) Internal Feature Fusion (T-PAMI 2017) <ref type="bibr" target="#b29">[30]</ref> 58.  <ref type="bibr" target="#b28">[29]</ref> 59.9 62.4 TSRJI (SIBGRAPI 2019) <ref type="bibr" target="#b2">[3]</ref> 67.9 62.8 LSTM-IRN (arXiv 2019) <ref type="bibr" target="#b37">[38]</ref> 77.7 79.6 GVFE + AS-GCN (arXiv 2019) <ref type="bibr" target="#b35">[36]</ref> 78.3 79.8 FGCN (ours) 85.4 87.4 <ref type="table">Table 5</ref>. Comparisons with the state-of-the-art methods on Northwestern-UCLA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Accuracy(%) Actionlet ensemble (T-PAMI 2013) <ref type="bibr" target="#b53">[54]</ref> 76.0 Lie group (CVPR 2014 ) <ref type="bibr" target="#b52">[53]</ref> 74.2 HBRNN-L(CVPR 2015) <ref type="bibr" target="#b5">[6]</ref> 78.5 Skeleton Visualization (PR 2017) <ref type="bibr" target="#b31">[32]</ref> 86.1 Ensemble TS-LSTM (ICCV 2017) <ref type="bibr" target="#b20">[21]</ref> 89.2 AGC-LSTM (CVPR 2019) <ref type="bibr" target="#b44">[45]</ref> 93.3 JS+JM+BS+BM (ICME 2019) <ref type="bibr" target="#b24">[25]</ref> 91.3 HiGCN (ICIG 2019) <ref type="bibr" target="#b13">[14]</ref> 88.9 MSNN (CSVT 2020) <ref type="bibr" target="#b41">[42]</ref> 89.4 FGCN (ours) 95.3</p><p>For the UTU-RGB+D120 dataset, the results on cross-subject and crosssetup benchmarks of the recent state-of-the-art methods are listed in Tab. 4. The proposed FGCN model achieves 85.4% on cross-subject and 87.4% on crosssetup and it outperforms the most comparable ST-GCN model <ref type="bibr" target="#b55">[56]</ref> by 13.0% and 16.1% on the cross-subject and cross-setup benchmarks respectively. The FGCN model outperforms other state-of-the-art methods with much lager margins. For example, the FGCN model outperforms Two-Stream Attention LSTM <ref type="bibr" target="#b30">[31]</ref> by over 24% on both cross-subject and cross-setup benchmarks, and outperforms the most recent work FSNet <ref type="bibr" target="#b28">[29]</ref> by over 25% on both of cross-subject and cross-setup benchmarks.</p><p>For the typical 3D action recognition dataset Northwestern-UCLA, we compare the proposed FGCN model with the state-of-the-art methods in recent years. The results of these models are reported in Tab. 5. The FGCN model outperforms the part-based hierarchical recurrent neural network HBRNN-L [6] by 16.8%. The recent method AGC-LSTM proposes an attention enhanced graph convolutional LSTM network to capture discriminative features from the cooccurrence relationship between spatial configuration and temporal dynamics. The FGCN model outperforms it by 2%. Moreover, the FGCN model outperforms the most recent methods, such as JS+JM+BS+BM <ref type="bibr" target="#b24">[25]</ref>, HiGCN <ref type="bibr" target="#b13">[14]</ref> and MSNN <ref type="bibr" target="#b41">[42]</ref>. The proposed FGCN model achieves state-of-the-art performance on the Northwestern-UCLA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel FGCN model to extract effective spatialtemporal features of actions in a coarse-to-fine progressive process. Firstly, we propose a multi-stage temporal sampling strategy to sample sparse skeleton clips in multiple temporal stages and exploit graph convolutional layers to extract local spatial-temporal features for each stage. Then, we introduce the feedback mechanism into conventional GCNs by proposing the FGCB which is a local graph convolutional dense network. The FGCB transmits the semantic information from high-level layers to low-level layers and from the former stages to the later stages. Moreover, the FGCN provides early predictions which help agents in many applications to make timely decisions on-the-fly. The proposed FGCN model is extensively evaluated on the NTU-RGB+D, NTU-RGB+D120 and Northwestern-UCLA datasets, indicating that the FGCN is effective for action recognition. It has achieved state-of-the-art performance on the three datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Comparison of the conventional GCNs (left) and the proposed FGCN (right). Red arrows represent the feedback connections of the feedback block (FGCB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The detailed architecture of the proposed FGCB local network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Evaluating the influence of two key factors on NTU-RGB+D, (a) influence of the number of stages, (b) influence of frame length in each stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>The confusion matrices of ST-GCN-joint and FGCN-joint on NTU-RGB+D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>93.11 93.20 93.57 93.62 93.19</head><label></label><figDesc><ref type="bibr" target="#b40">[41]</ref> is a widely used dataset for skeleton-based action recognition. The dataset contains more than 56,000 skeleton sequences categorized into 60 action classes. It provides 25 major body joints with 3D coordinates for every human in each frame. Two benchmark evaluations are recommended: crosssubject and cross-view. For cross-subject, both training and test sets consist of</figDesc><table><row><cell></cell><cell>93.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>94.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93.77</cell></row><row><cell></cell><cell>93.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93.5</cell><cell></cell><cell></cell><cell>93.57</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93.0</cell><cell></cell><cell cols="2">93.30</cell><cell></cell></row><row><cell></cell><cell>93.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92.5</cell><cell></cell><cell></cell><cell></cell><cell cols="2">92.87</cell></row><row><cell>Accuracy(%)</cell><cell>93.3 93.4</cell><cell></cell><cell></cell><cell></cell><cell>Accuracy(%)</cell><cell>91.0 91.5 92.0</cell><cell>92.25</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>93.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>93.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>93.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89.0</cell><cell>89.08</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>16</cell><cell>32</cell><cell>48</cell><cell>64</cell><cell>80</cell><cell>96</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell cols="2">Number of stages</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell cols="2">Number of frames</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Evaluating different temporal fusion strategies on NTU-RGB+D.</figDesc><table><row><cell>Temporal Fusion Strategies</cell><cell>w1</cell><cell>w2</cell><cell>Weights w3</cell><cell>w4</cell><cell>w5</cell><cell>Cross-view(%)</cell></row><row><cell>Last-win-all fusion</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>89.88</cell></row><row><cell>Weight fusion-1</cell><cell cols="3">0.05 0.05 0.1</cell><cell>0.2</cell><cell>0.6</cell><cell>93.09</cell></row><row><cell>Weight fusion-2</cell><cell>0.1</cell><cell cols="4">0.15 0.2 0.25 0.3</cell><cell>93.05</cell></row><row><cell>Average fusion</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell><cell>93.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Evaluating the effectiveness of the FGCN model fed with different inputs on the NTU-RGB+D dataset.</figDesc><table><row><cell>Models</cell><cell cols="2">Cross-subject(%) Cross-view(%)</cell></row><row><cell>ST-GCN-joint [56]</cell><cell>81.5</cell><cell>88.3</cell></row><row><cell>FGCN-joint</cell><cell>87.04</cell><cell>93.57</cell></row><row><cell>FGCN-bone</cell><cell>86.96</cell><cell>93.22</cell></row><row><cell>FGCN-joint+FGCN-bone</cell><cell>89.24</cell><cell>95.28</cell></row><row><cell>FGCN-spatial</cell><cell>88.32</cell><cell>94.82</cell></row><row><cell>FGCN-motion</cell><cell>85.96</cell><cell>93.57</cell></row><row><cell>FGCN-spatial+FGCN-motion</cell><cell>90.22</cell><cell>96.25</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feedback as an individual resource: Personal strategies of creating information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ashford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Cummings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Performance</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="370" to="398" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skeleton image representation for 3D action recognition based on tree structure and reference joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Skeletal quads: Human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4513" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Brain states: top-down influences in sensory processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="677" to="696" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image superresolution via dual-state recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1654" to="1663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep convolutional networks on graph-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for RGB-D activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer International Conference on Image and Graphics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cortical feedback improves discrimination between figure and background by v1, v2 and v3 neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hupé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lomber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bullier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">394</biblScope>
			<biblScope unit="issue">6695</biblScope>
			<biblScope unit="page" from="784" to="787" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2842" to="2855" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Foundations of optimal control theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Markus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
		<respStmt>
			<orgName>Minnesota Univ Minneapolis Center For Control Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning shape-motion representations from geometric algebra spatio-temporal model for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1066" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3867" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Two-stream 3D convolutional neural network for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chichung</surname></persName>
		</author>
		<title level="m">NTU RGB+D 120: A large-scale benchmark for 3D human activity understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Skeleton-based online action prediction using scale selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition using spatio-temporal LSTM network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3007" to="3021" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Skeleton-based human action recognition with global context-aware attention LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Group sparsity and geometry constrained dictionary learning for action recognition from depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1809" to="1816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint angles similarities and HOG2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Vertex feature encoding and hierarchical temporal modeling in a spatial-temporal graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ghorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ottersten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09745</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Application of the recurrent multilayer perceptron in modeling complex process dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Parlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="266" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Interaction relational network for mutual action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04963</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real time action recognition using histograms of depth gradients and random decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="626" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for crossview action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2458" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning representations from skeletal self-similarities for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional LSTM network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep networks with internal selective attention through feedback connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3545" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Part-based graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3D skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3D human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="914" to="927" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2649" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Feedback networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1308" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Two-stream RNN/CNN for action recognition in 3D videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4260" to="4267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bayesian graph convolution LSTM for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6882" to="6892" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
