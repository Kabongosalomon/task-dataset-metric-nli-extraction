<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WIDER FACE: A Face Detection Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Change</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loy</forename><forename type="middle">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk@edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WIDER FACE: A Face Detection Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion, as shown in <ref type="figure">Fig. 1</ref>. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated. Dataset can be downloaded at: mmlab.ie.cuhk.edu.hk/projects/WIDERFace</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face detection is a critical step to all facial analysis algorithms, including face alignment <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref>, face recognition <ref type="bibr" target="#b26">[27]</ref>, face verification <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref>, and face parsing <ref type="bibr" target="#b19">[20]</ref>. Given an arbitrary image, the goal of face detection is to determine whether or not there are any faces in the image and, if present, return the image location and extent of each face <ref type="bibr" target="#b34">[35]</ref>. While this appears as an effortless task for human, it is a very difficult task for computers. The challenges associated with face detection can be attributed to variations in pose, scale, facial expression, occlusion, and lighting condition, as shown in <ref type="figure">Fig. 1</ref>. Face detection has made significant progress after the seminal work by Viola and Jones <ref type="bibr" target="#b28">[29]</ref>. Modern face detectors can easily detect near frontal faces and are widely used in real world applications, such as digital camera and electronic photo album. Recent research <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> in this area focuses on the unconstrained scenario, where a number of intricate factors <ref type="bibr">Figure 1</ref>. We propose a WIDER FACE dataset for face detection, which has a high degree of variability in scale, pose, occlusion, expression, appearance and illumination. We show example images (cropped) and annotations. The annotated face bounding box is denoted in green color. The WIDER FACE dataset consists of 393, 703 labeled face bounding boxes in 32, 203 images (Best view in color). such as extreme pose, exaggerated expressions, and large portion of occlusion can lead to large visual variations in face appearance.</p><p>Publicly available benchmarks such as FDDB <ref type="bibr" target="#b12">[13]</ref>, AFW <ref type="bibr" target="#b38">[39]</ref>, PASCAL FACE <ref type="bibr" target="#b31">[32]</ref>, have contributed to spurring interest and progress in face detection research. However, as algorithm performance improves, more chal-lenging datasets are needed to trigger progress and to inspire novel ideas. Current face detection datasets typically contain a few thousand faces, with limited variations in pose, scale, facial expression, occlusion, and background clutters, making it difficult to assess for real world performance. As we will demonstrate, the limitations of datasets have partially contributed to the failure of some algorithms in coping with heavy occlusion, small scale, and atypical pose.</p><p>In this work, we make three contributions. <ref type="bibr" target="#b0">(1)</ref> We introduce a large-scale face detection dataset called WIDER FACE. It consists of 32, 203 images with 393, 703 labeled faces, which is 10 times larger than the current largest face detection dataset <ref type="bibr" target="#b13">[14]</ref>. The faces vary largely in appearance, pose, and scale, as shown in <ref type="figure">Fig. 1</ref>. In order to quantify different types of errors, we annotate multiple attributes: occlusion, pose, and event categories, which allows in depth analysis of existing algorithms. <ref type="bibr" target="#b1">(2)</ref> We show an example of using WIDER FACE through proposing a multi-scale two-stage cascade framework, which uses divide and conquer strategy to deal with large scale variations. Within this framework, a set of convolutional networks with various size of input are trained to deal with faces with a specific range of scale. <ref type="bibr" target="#b2">(3)</ref> We benchmark four representative algorithms <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>, either obtained directly from the original authors or reimplemented using open-source codes. We evaluate these algorithms on different settings and analyze conditions in which existing methods fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Brief review of recent face detection methods: Face detection has been studied for decades in the computer vision literature. Modern face detection algorithms can be categorized into four categories: cascade based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>, part based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39]</ref>, channel feature based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>, and neural network based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>. Here we highlight a few notable studies. A detailed survey can be found in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref>. The seminal work by Viola and Jones <ref type="bibr" target="#b28">[29]</ref> introduces integral image to compute Haar-like features in constant time. These features are then used to learn AdaBoost classifier with cascade structure for face detection. Various later studies follow a similar pipeline. Among those variants, SURF cascade <ref type="bibr" target="#b17">[18]</ref> achieves competitive performance. Chen et al. <ref type="bibr" target="#b2">[3]</ref> learns face detection and alignment jointly in the same cascade framework and obtains promising detection performance.</p><p>One of the well-known part based methods is deformable part models (DPM) <ref type="bibr" target="#b7">[8]</ref>. Deformable part models define face as a collection of parts and model the connections of parts through Latent Support Vector Machine. The part based methods are more robust to occlusion compared with cascade-based methods. A recent study <ref type="bibr" target="#b20">[21]</ref> demonstrates state-of-the art performance with just a vanilla DPM, </p><formula xml:id="formula_0">AFW [39] - - 0.2k 0.47k 12% 70% 18% - - ! FDDB [13] - - 2.8k 5.1k 8% 86% 6% - - - PASCAL FACE [32] - - 0.85k 1.3k 41% 57% 2% - - - IJB-A [14] 16k 33k 8.3k 17k 13% 69% 18% - - - MALF [34] - - 5.25k 11.9k N/A N/A N/A ! -! WIDER FACE 16k 199k 16k 194k 50% 43% 7% ! ! !</formula><p>achieving better results than more sophisticated DPM variants <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39]</ref>. Aggregated channel feature (ACF) is first proposed by Dollar et al. <ref type="bibr" target="#b3">[4]</ref> to solve pedestrian detection. Later on, Yang et al. <ref type="bibr" target="#b32">[33]</ref> applied this idea on face detection. In particular, features such as gradient histogram, integral histogram, and color channels are combined and used to learn boosting classifier with cascade structure. Recent studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36]</ref> show that face detection can be further improved by using deep learning, leveraging the high capacity of deep convolutional networks. We anticipate that the new WIDER FACE data can benefit deep convolutional network that typically requires large amount of data for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Existing datasets:</head><p>We summarize some of the well-known face detection datasets in <ref type="table" target="#tab_0">Table 1</ref>. AFW <ref type="bibr" target="#b38">[39]</ref>, FDDB <ref type="bibr" target="#b12">[13]</ref>, and PASCAL FACE <ref type="bibr" target="#b31">[32]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WIDER FACE Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>To our knowledge, WIDER FACE dataset is currently the largest face detection dataset, of which images are selected from the publicly available WIDER dataset <ref type="bibr" target="#b30">[31]</ref>. We choose 32, 203 images and label 393, 703 faces with a high degree of variability in scale, pose and occlusion as depicted in <ref type="figure">Fig. 1</ref>. WIDER FACE dataset is organized based on 60 event classes. For each event class, we randomly select 40%/10%/50% data as training, validation and testing sets. Here, we specify two training/testing scenarios:</p><p>• Scenario-Ext: A face detector is trained using any external data, and tested on the WIDER FACE test partition.</p><p>• Scenario-Int: A face detector is trained using WIDER FACE training/validation partitions, and tested on WIDER FACE test partition.</p><p>We adopt the same evaluation metric employed in the PAS-CAL VOC dataset <ref type="bibr" target="#b5">[6]</ref>. Similar to MALF <ref type="bibr" target="#b33">[34]</ref> and Caltech <ref type="bibr" target="#b4">[5]</ref> datasets, we do not release bounding box ground truth for the test images. Users are required to submit final prediction files, which we shall proceed to evaluate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Collection</head><p>Collection methodology. WIDER FACE dataset is a subset of the WIDER dataset <ref type="bibr" target="#b30">[31]</ref>. The images in WIDER were collected in the following three steps: 1) Event categories were defined and chosen following the Large Scale Ontology for Multimedia (LSCOM) <ref type="bibr" target="#b21">[22]</ref>, which provides around 1, 000 concepts relevant to video event analysis. 2) Images are retrieved using search engines like Google and Bing. For each category, 1, 000-3, 000 images were collected.</p><p>3) The data were cleaned by manually examining all the images and filtering out images without human face. Then, similar images in each event category were removed to ensure large diversity in face appearance. A total of 32, 203 images are eventually included in the WIDER FACE dataset. Annotation policy. We label the bounding boxes for all the recognizable faces in the WIDER FACE dataset. The bounding box is required to tightly contain the forehead, chin, and cheek, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. If a face is occluded, we still label it with a bounding box but with an estimation on the scale of occlusion. Similar to the PASCAL VOC dataset <ref type="bibr" target="#b5">[6]</ref>, we assign an 'Ignore' flag to the face which is very difficult to be recognized due to low resolution and small scale (10 pixels or less). After annotating the face bounding boxes, we further annotate the following attributes: pose (typical, atypical) and occlusion level (partial, heavy). Each annotation is labeled by one annotator and cross-checked by two different people.    The proposals are generated by using Edgebox <ref type="bibr" target="#b39">[40]</ref>. Y-axis denotes for detection rate. X-axis denotes for average number of proposals per image. Lower detection rate implies higher difficulty. We show histograms of detection rate over the number of proposal for different settings (a) Different face detection datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Properties of WIDER FACE</head><p>WIDER FACE dataset is challenging due to large variations in scale, occlusion, pose, and background clutters. These factors are essential to establishing the requirements for a real world system. To quantify these properties, we use generic object proposal approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40]</ref>, which are specially designed to discover potential objects in an image (face can be treated as an object). Through measuring the number of proposals vs. their detection rate of faces, we can have a preliminary assessment on the difficulty of a dataset and potential detection performance. In the following assessments, we adopt EdgeBox <ref type="bibr" target="#b39">[40]</ref> as object proposal, which has good performance in both accuracy and efficiency as evaluated in <ref type="bibr" target="#b10">[11]</ref>. Overall. <ref type="figure" target="#fig_2">Fig. 3</ref>(a) shows that WIDER FACE has much lower detection rate compared with other face detection datasets. The results suggest that WIDER FACE is a more challenging face detection benchmark compared to exist-  <ref type="figure">Figure 4</ref>. Histogram of detection rate for different event categories. Event categories are ranked in an ascending order based on the detection rate when the number of proposal is fixed at 10, 000. Top 1 − 20, 21 − 40, 41 − 60 event categories are denoted in blue, red, and green, respectively. Example images for specific event classes are shown. Y-axis denotes for detection rate. X-axis denotes for event class name.</p><p>ing datasets. Following the principles in KITTI <ref type="bibr" target="#b8">[9]</ref> and MALF <ref type="bibr" target="#b33">[34]</ref> datasets, we define three levels of difficulty: 'Easy', 'Medium', 'Hard' based on the detection rate of EdgeBox <ref type="bibr" target="#b39">[40]</ref>, as shown in the <ref type="figure" target="#fig_2">Fig. 3</ref>(a). The average recall rates for these three levels are 92%, 76%, and 34%, respectively, with 8, 000 proposal per image. Scale. We group the faces by their image size (height in pixels) into three scales: small (between 10-50 pixels), medium (between 50-300 pixels), large (over 300 pixels). We make this division by considering the detection rate of generic object proposal and human performance. As can be observed from <ref type="figure" target="#fig_2">Fig 3(b)</ref>, the large and medium scales achieve high detection rate (more than 90%) with 8, 000 proposals per image. For the small scale, the detection rates consistently stay below 30% even we increase the proposal number to 10, 000.</p><p>Occlusion. Occlusion is an important factor for evaluating the face detection performance. Similar to a recent study <ref type="bibr" target="#b33">[34]</ref>, we treat occlusion as an attribute and assign faces into three categories: no occlusion, partial occlusion, and heavy occlusion. Specifically, we ask annotator to measure the fraction of occlusion region for each face. A face is defined as 'partially occluded' if 1%-30% of the total face area is occluded. A face with occluded area over 30% is labeled as 'heavily occluded'. <ref type="figure" target="#fig_0">Fig. 2</ref> shows some examples of partial/heavy occlusions. <ref type="figure" target="#fig_2">Fig. 3</ref>(c) shows that the detection rate decreases as occlusion level increases. The detection rates of faces with partial or heavy occlusions are below 50% with 8, 000 proposals.</p><p>Pose. Similar to occlusion, we define two pose deformation levels, namely typical and atypical. <ref type="figure" target="#fig_0">Fig. 2</ref> shows some faces of typical and atypical pose. Face is annotated as atypical under two conditions: either the roll or pitch degree is larger than 30-degree; or the yaw is larger than 90-degree. <ref type="figure" target="#fig_2">Fig. 3</ref>(d) suggests that faces with atypical poses are much harder to be detected.</p><p>Event. Different events are typically associated with different scenes. WIDER FACE contains 60 event categories covering a large number of scenes in the real world, as shown in <ref type="figure">Fig. 1</ref> and <ref type="figure" target="#fig_0">Fig. 2</ref>. To evaluate the influence of event to face detection, we characterize each event with three factors: scale, occlusion, and pose. For each factor we compute the detection rate for the specific event class and then rank the detection rate in an ascending order. Based on the rank, events are divided into three partitions: easy (41-60 classes), medium (21-40 classes) and hard (1-20 classes). We show the partitions based on scale in <ref type="figure">Fig. 9</ref>. Partitions based on occlusion and pose are included in the appendix.</p><p>Effective training source. As shown in the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multi-scale Detection Cascade</head><p>We wish to establish a solid baseline for WIDER FACE dataset. As we have shown in <ref type="table" target="#tab_0">Table 1</ref>, WIDER FACE contains faces with a large range of scales. <ref type="figure" target="#fig_2">Fig. 3</ref>(b) further shows that faces with a height between 10-50 pixels only achieve a proposal detection rate of below 30%. In order to deal with the high degree of variability in scale, we propose a multi-scale two-stage cascade framework and employ a divide and conquer strategy. Specifically, we train a set of face detectors, each of which only deals with faces in a relatively small range of scales. Each face detector consists of two stages. The first stage generates multi-scale proposals from a fully-convolutional network. The second stage is a multi-task convolutional network that generates face and non-face prediction of the candidate windows obtained from the first stage, and simultaneously predicts for face location. The pipeline is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. The two main steps are explained as follow.</p><p>Multi-scale proposal. In this step, we joint train a set of fully convolutional networks for face classification and scale classification. We first group faces into four categories by their image size, as shown in the an intersection-over-union (IoU) of smaller than 0.5 with any of the positive samples. We assign a value −1 as the scale class for negative samples, which will have no contribution to the gradient during training.</p><p>We take Network 2 as an example. Let</p><formula xml:id="formula_1">{x i } N i=1 be a set of image patches where ∀x i ∈ R 120×120 . Similarly, let {y f i } N i=1</formula><p>be the set of face class labels and</p><formula xml:id="formula_2">{y s i } N i=1</formula><p>be the set of scale class label, where ∀y f i ∈ R 1×1 and ∀y s i ∈ R 1×3 . Learning is formulated as a multi-variate classification problem by minimizing the cross-entropy loss.</p><formula xml:id="formula_3">L = N i=1 y i log p(y i = 1|x i ) + (1 − y i ) log 1 − p(y i = 1|x i ) , where p(y i |x i )</formula><p>is modeled as a sigmoid function, indicating the probability of the presence of a face. This loss function can be optimized by the stochastic gradient descent with back-propagation. Face detection. The prediction of proposed windows from the previous stage is refined in this stage. For each scale category, we refine these proposals by joint training face classification and bounding box regression using the same CNN structure in the previous stage with the same input size. For face classification, a proposed window is assigned with a positive label if the IoU between it and the ground truth bounding box is larger than 0.5; otherwise it is negative. For bounding box regression, each proposal is predicted a position of its nearest ground truth bounding box.</p><p>If the proposed window is a false positive, the CNN outputs a vector of [−1, −1, −1, −1]. We adopt the Euclidean loss and cross-entropy loss for bounding box regression and face classification, respectively. More details of face detection can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Benchmarks</head><p>As we discussed in Sec. 2, face detection algorithms can be broadly grouped into four representative categories. For each class, we pick one algorithm as a baseline method. We select VJ <ref type="bibr" target="#b28">[29]</ref>, ACF <ref type="bibr" target="#b32">[33]</ref>, DPM <ref type="bibr" target="#b20">[21]</ref>, and Faceness <ref type="bibr" target="#b35">[36]</ref> as baselines. The VJ <ref type="bibr" target="#b28">[29]</ref>, DPM <ref type="bibr" target="#b20">[21]</ref>, and Faceness <ref type="bibr" target="#b35">[36]</ref> detectors are either obtained from the authors or from open source library (OpenCV). The ACF <ref type="bibr" target="#b32">[33]</ref> detector is reimplemented using the open source code. We adopt the Scenario-Ext here (see Sec. 3.1), that is, these detectors were trained by using external datasets and are used 'as is' without re-training them on WIDER FACE. We employ PASCAL VOC <ref type="bibr" target="#b5">[6]</ref> evaluation metric for the evaluation. Following previous work <ref type="bibr" target="#b20">[21]</ref>, we conduct linear transformation for each method to fit the annotation of WIDER FACE. Overall. In this experiment, we employ the evaluation setting mentioned in Sec. 3.3. The results are shown in <ref type="figure" target="#fig_5">Fig. 6</ref> (a.1)-(a.3). Faceness <ref type="bibr" target="#b35">[36]</ref> outperforms other methods on three subsets, with DPM <ref type="bibr" target="#b20">[21]</ref> and ACF <ref type="bibr" target="#b32">[33]</ref> as marginal second and third. For the easy set, the average precision (AP) of most methods are over 60%, but none of them surpasses 75%. The performance drops 10% for all methods on the medium set. The hard set is even more challenging. The performance quickly decreases, with a AP below 30% for all methods. To trace the reasons of failure, we examine performance on varying subsets of the data. Scale. As described in Sec. 3.3, we group faces according to the image height: small (10-50 pixels), medium (50-300 pixels), and large (300 or more pixels) scales. The results of small scale are abysmal: none of the algorithms is able to achieve more than 12% AP. This shows that current face detectors are incapable to deal with faces of small scale. Occlusion. Occlusion handling is a key performance metric for any face detectors. In <ref type="figure" target="#fig_2">Fig. 6 (c.1)-(c.3)</ref>, we show the impact of occlusion on detecting faces with a height of at least 30 pixels. As mentioned in Sec. 3.3, we classify faces into three categories: un-occluded, partially occluded (1%-30% area occluded) and heavily occluded (over 30% area occluded). With partial occlusion, the performance drops significantly. The maximum AP is only 26.5% achieved by Faceness. The performance further decreases in the heavy occlusion setting. The best performance of baseline methods drops to 14.4%. It is worth noting that Faceness and DPM, which are part based models, already perform relatively better than other methods on occlusion handling. Pose. As discussed in Sec. 3.3, we assign a face pose as atypical if either the roll or pitch degree is larger than 30degree; or the yaw is larger than 90-degree. Otherwise a face pose is classified as typical. We show results in <ref type="figure" target="#fig_0">Fig. 6  (d.1)-(d.2)</ref>. Faces which are un-occluded and with a scale larger than 30 pixels are used in this experiment. The performance clearly degrades for atypical pose. The best performance is achieved by Faceness, with a recall below 20%.</p><p>The results suggest that current face detectors are only capable of dealing with faces with out-of-plane rotation and a small range of in-plane rotation. Summary. Among the four baseline methods, Faceness tends to outperform the other methods. VJ performs poorly on all settings. DPM gains good performance on medium/large scale and occlusion. ACF outperforms DPM on small scale, no occlusion and typical pose settings. However, the overall performance is poor on WIDER FACE, suggesting a large room of improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">WIDER FACE as an Effective Training Source</head><p>In this experiment, we demonstrate the effectiveness of WIDER FACE dataset as a training source. We adopt Scenario-Int here (see Sec. 3.1). We train ACF and Faceness on WIDER FACE to conduct this experiment. These two algorithms have shown relatively good performance on WIDER FACE previous benchmarks see (Sec. 5.1). Faces with a scale larger than 30 pixels in the training set are used to retrain both methods. We train the ACF detector using the same training parameters as the baseline ACF. The negative samples are generated from the training images. For the Faceness detector, we first employ models shared by the authors to generate face proposals from the WIDER FACE training set. After that, we train the classifier with the same procedure described in <ref type="bibr" target="#b35">[36]</ref>. We test these models (denoted as ACF-WIDER and Faceness-WIDER) on WIDER FACE testing set and FDDB dataset. WIDER FACE. As shown in <ref type="figure" target="#fig_7">Fig. 7</ref>, the retrained models perform consistently better than the baseline models. The average AP improvement of retrained ACF detector is 5.4% in comparison to baseline ACF detector. For the Faceness, the retrained Faceness model obtain 4.2% improvement on WIDER hard test set. FDDB. We further evaluate the retrained models on FDDB dataset. Similar to WIDER FACE dataset, the retrained models achieve improvement in comparison to the baseline methods. The retrained ACF detector achieves a recall rate of 87.48%, outperforms the baseline ACF by a considerable margin of 1.4%. The retrained Faceness detector obtains a  <ref type="table">Table 3</ref>. Comparison of per class AP. To save space, we only show abbreviations of category names here. The event category is organized based on the rank sequence in <ref type="figure">Fig. 9</ref> (from hard to easy events based on scale measure). We compare the accuracy of Faceness and ACF models retrained on WIDER FACE training set with the baseline Faceness and ACF. With the help of WIDER FACE dataset, accuracies on 56 out of 60 categories have been improved. The re-trained Faceness model wins 30 out of 60 classes, followed by the ACF model with 26 classes. Faceness wins 1 medium class and 3 easy classes.  high recall rate of 91.78%. The recall rate improvement of the retrained Faceness detector is 0.8% in comparison to the baseline Faceness detector. It worth noting that the retrained Faceness detector performs much better than the baseline Faceness detector when the number of false positive is less than 300. Event. We evaluate the baseline methods on each event class individually and report the results in <ref type="table">Table 3</ref>. Faces with a height larger than 30 pixels are used in this experiment. We compare the accuracy of Faceness and ACF models retrained on WIDER FACE training set with the baseline Faceness and ACF. With the help of WIDER FACE dataset, accuracies on 56 out of 60 event categories have been improved. It is interesting to observe that the accuracy obtained highly correlates with the difficulty levels specified in Sec. 3.3 (also refer to <ref type="figure">Fig. 9</ref>). For example, the best performance on "Festival" which is assigned as a hard class is no more than 46% AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation of Multi-scale Detection Cascade</head><p>In this experiment we evaluate the effectiveness of the proposed multi-scale cascade algorithm. Apart from the ACF-WIDER and Faceness-WIDER models (Sec. 5.2), we establish a baseline based on a "Two-stage CNN". This model differs to our multi-scale cascade model in the way it handles multiple face scales. Instead of having multiple networks targeted for different scales, the two-stage CNN adopts a more typical approach. Specifically, its first stage consists only a single network to perform face classification. During testing, an image pyramid that encompasses different scales of a test image is fed to the first stage to generate multi-scale face proposals. The second stage is similar to our multi-scale cascade model -it performs further refinement on proposals by simultaneous face classification and bounding box regression.</p><p>We evaluate the multi-scale cascade CNN and baseline methods on WIDER Easy/Medium/Hard subsets. As shown in <ref type="figure" target="#fig_8">Fig. 8</ref>, the multi-scale cascade CNN obtains 8.5% AP improvement on the WIDER Hard subset compared to the retrained Faceness, suggesting its superior capability in handling faces with different scales. In particular, having multiple networks specialized on different scale range is shown effective in comparison to using a single network to handle multiple scales. In other words, it is difficult for a single network to handle large appearance variations caused by scale. For the WIDER Medium subset, the multi-scale cascade CNN outperforms other baseline methods with a considerable margin. All models perform comparably on the WIDER Easy subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a large, richly annotated WIDER FACE dataset for training and evaluating face detection algorithms. We benchmark four representative face detection methods. Even considering an easy subset (typically with faces of over 50 pixels height), existing state-of-the-art algorithms reach only around 70% AP, as shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. With this new dataset, we wish to encourage the community to focusing on some inherent challenges of face detection -small scale, occlusion, and extreme poses. These factors are ubiquitous in many real world applications. For instance, faces captured by surveillance cameras in public spaces or events are typically small, occluded, and atypical poses. These faces are arguably the most interesting yet crucial to detect for further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Multi-scale Detection Cascade</head><p>Multi-scale detection cascade CNN consists of a set of face detectors, with each of them only deals with faces in a relatively small range of scale. Each face detector consists of two stages. The first stage generates multi-scale proposal from a fully-convolutional network. The second stage gives face and non-face prediction of the candidate windows generate from first stage. If the candidate window is classified as face, we further refine the location of the candidate window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Training Multi-scale Proposal Network</head><p>We provide details of the training process for multi-scale proposal networks. In this step, we train four fully convolutional networks for face classification and scale classification. The network structures are summarized in <ref type="table">Table 5</ref>, <ref type="table" target="#tab_8">Table 6</ref>, <ref type="table" target="#tab_9">Table 7</ref>, and <ref type="table" target="#tab_10">Table 8</ref>. As we described in the paper, we group faces into four categories by their image size, as shown in the <ref type="table">Table 4</ref> (each row in the table represents a category). For each group, we further divide it into three subclasses. Each network is trained with image patches with the size of their upper bound scale. For example, Proposal Network 1 and Proposal Network 2 are trained with 30×30 and 120×120 image patches respectively. For the Proposal Network 1, Proposal Network 2, and Proposal Network 3, we initialize the layers from Conv 1 to Conv 5 using the Imagenet 1, 000 categories pre-trained Clarifai net. For the Proposal Network 4, we initialize the layers from Conv 2 to Conv 5 using pre-trained Clarifai net. The remaining layers in each network are randomly initialized with weights drawn from a Gaussian distribution of µ = 0 and σ = 0.01. To account for the multi-label scenario, cross-entropy loss is adopted as shown below:</p><formula xml:id="formula_4">L = |D| i=1</formula><p>(y i log p(y i |I i ) + (1 − y i ) log(1 − p(y i |I i ))), and</p><formula xml:id="formula_5">p(y i = c|I i ) = 1 1 + exp(−f (I i )) ,<label>(1)</label></formula><p>Back propagation and SGD are also employed here for optimizing Eqn. <ref type="bibr" target="#b0">(1)</ref>. Similar to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>, we set the initial finetuning learning rate as one-tenth of the corresponding pretraining learning rate and drop it by a factor of 10 throughout training. After training, we conduct hard negative mining on the training set and further tune the proposal networks using hard negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Training Face Detector</head><p>In this section, we provide more details of the training process for face detection. As mentioned at beginning, we train    bounding box and the assigned ground truth bounding box is larger than 0.5; otherwise it is negative. For bounding box regression, we train the multi-task deep convolutional neural network to regress each proposal to predict the positions of its assigned ground truth bounding box.</p><p>During the training process, if the number of positive samples is less than 10% of the total samples, we randomly cropped the ground truth faces and add these samples as positive samples. We adopt Euclidean loss and crossentropy loss for bounding box regression and face classification, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">WIDER FACE Dataset</head><p>Event. We measure each event with three factors: scale, occlusion and pose. For each factor, we compute the detection rate for the specific event class and then rank the detection rate in the ascending order. Three classes are divided: easy (top 41-60 class), medium (top 21-40 class) and hard (top 1-20 class), as shown in the <ref type="figure">Fig. 9</ref>. The corresponding relationship between abbreviations of the events categories and full name of the event categories are shown in <ref type="table" target="#tab_0">Table 10</ref>.   <ref type="figure">Figure 9</ref>. Histogram of detection rate for different event categories. Event categories are ranked based on detection rate when number of proposal is 10, 000 in the ascending order. Top 1 − 20, 21 − 40, 41 − 60 event categories are denote in blue, red, green respectively. Example images for specific event class are shown. Y-axis denotes for detection rate. X-axis denotes for event class name.  <ref type="table" target="#tab_0">Table 10</ref>. Full name of abbreviation of event categories.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Examples of annotation in WIDER FACE dataset (Best view in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The detection rate with different number of proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(b) Face scale level. (c) Occlusion level. (d) Pose level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The pipeline of the proposed multi-scale cascade CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 (</head><label>6</label><figDesc>b.1)-(b.3) show the results for each scale on un-occluded faces only. For the large scale, DPM and Faceness obtain over 80% AP. At the medium scale, Faceness achieves the best relative result but the absolute performance is only 70% AP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Precision and recall curves of different subsets of WIDER FACES: (a.1)-(a.3) Overall Easy/Medium/Hard subsets. (b.1)-(b.3) Small/Medium/Large scale subsets. (c.1)-(c.3) None/Partial/Heavy occlusion subsets. (d.1)-(d.2) Typical/Atypical pose subsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>WIDER FACE as an effective training source. ACF-WIDER and Faceness-WIDER are retrained with WIDER FACE, while ACF and Faceness are the original models. (a)-(c) Precision and recall curves on WIDER Easy/Medium/Hard subsets. (d) ROC curve on FDDB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Evaluation of multi-scale detection cascade: (a)-(c) Precision and recall curves on WIDER Easy/Medium/Hard subsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of face detection datasets.</figDesc><table><row><cell></cell><cell cols="2">Training</cell><cell>Testing</cell><cell></cell><cell></cell><cell>Height</cell><cell></cell><cell></cell><cell>Properties</cell><cell></cell></row><row><cell>Dataset</cell><cell>#Image</cell><cell>#Face</cell><cell>#Image</cell><cell>#Face</cell><cell>10-50 pixels</cell><cell>50-300 pixels</cell><cell>≤300 pixels</cell><cell>Occlusion labels</cell><cell>Event labels</cell><cell>Pose labels</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>, existing datasets such as FDDB, AFW, and PASCAL FACE do not provide training data. Face detection algorithms tested on these datasets are frequently trained with ALFW<ref type="bibr" target="#b14">[15]</ref>, which is designed for face landmark localization. However, there are two problems. First, ALFW omits annotations of many faces with a small scale, low resolution, and heavy occlusion. Second, the background in ALFW dataset is relatively clean. As a result, many face detection approaches resort to generate negative samples from other datasets such as PASCAL VOC dataset. In contrast, all recognizable faces are labeled in the WIDER FACE dataset. Because of its event-driven nature, WIDER FACE dataset has a large number of scenes with diverse background, making it possible as a good training source with both positive and negative samples. We demonstrate the effectiveness of WIDER FACE as a training source in Sec. 5.2. Network 1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Detection</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Network 1</cell></row><row><cell></cell><cell>10-30 pixels</cell><cell></cell><cell></cell><cell>pixels</cell></row><row><cell></cell><cell>Proposal</cell><cell></cell><cell></cell><cell>Detection</cell></row><row><cell></cell><cell>Network 2</cell><cell></cell><cell></cell><cell>Network 2</cell></row><row><cell></cell><cell>30-120 pixels</cell><cell></cell><cell></cell><cell>pixels</cell></row><row><cell>Input Image X</cell><cell>Proposal Network 3</cell><cell></cell><cell></cell><cell>Detection Network 3</cell><cell>Final results</cell></row><row><cell></cell><cell>120-240 pixels</cell><cell></cell><cell></cell><cell>pixels</cell></row><row><cell></cell><cell>Proposal</cell><cell></cell><cell></cell><cell>Detection</cell></row><row><cell></cell><cell>Network 4</cell><cell></cell><cell></cell><cell>Network 4</cell></row><row><cell></cell><cell>240-480 pixels</cell><cell></cell><cell></cell><cell>pixels</cell></row><row><cell></cell><cell>Multiscale proposal networks</cell><cell>Response maps</cell><cell>Proposals</cell><cell>Multiscale detection networks</cell><cell>Detection results</cell></row><row><cell></cell><cell></cell><cell>Stage 1</cell><cell></cell><cell></cell><cell>Stage 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 (Table 2 .</head><label>42</label><figDesc>each row in the table represents a category). For each group, we further divide it into three subclasses. Each network is trained with image patches with the size of their upper bound scale. For example, Network 1 and Network 2 are trained with 30×30 and 120×120 image patches, respectively. We align a face at the center of an image patch as positive sample and assign a scale class label based on the predefined scale subclasses in each group. For negative samples, we randomly cropped patches from the training images. The patches should have Summary of face scale for multi-scale proposal networks.</figDesc><table><row><cell>Scale</cell><cell cols="3">Class 1 Class 2 Class 3</cell></row><row><cell>Network 1</cell><cell>10-15</cell><cell>15-20</cell><cell>20-30</cell></row><row><cell>Network 2</cell><cell>30-50</cell><cell>50-80</cell><cell>80-120</cell></row><row><cell cols="4">Network 3 120-160 160-200 200-240</cell></row><row><cell cols="4">Network 4 240-320 320-400 400-480</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Traf. Fest. Para. Demo. Cere. March. Bask. Shop. Mata. Acci. Elec. Conc. Awar. Picn. Riot. Fune. Chee. Firi. Raci. Vote. ACF .421 .368 .431 .330 .521 .381 .452 .503 .308 .254 .409 .512 .720 .475 .388 .502 .474 .320 .552 .457 ACF-WIDER .385 .435 .528 .464 .595 .490 .562 .603 .334 .352 .538 .486 .797 .550 .395 .568 .589 .432 .669 .532 Faceness .497 .376 .459 .410 .547 .434 .481 .575 .388 .323 .461 .569 .730 .526 .455 .563 .496 .439 .577 .535 Faceness-WIDER .535 .451 .560 .454 .626 .495 .525 .593 .432 .358 .489 .576 .737 .621 .486 .579 .555 .454 .635 .558 Stoc. Hock. Stud. Skat. Gree. Foot. Runn. Driv. Dril. Phot. Spor. Grou. Cele. Socc. Inte. Raid. Base. Patr. Angl. Resc. ACF .549 .430 .557 .502 .467 .394 .626 .562 .447 .576 .343 .685 .577 .719 .628 .407 .442 .497 .564 .465 ACF-WIDER .519 .591 .666 .630 .546 .508 .707 .609 .521 .627 .430 .756 .611 .727 .616 .506 .583 .529 .645 .546 Faceness .617 .481 .639 .561 .576 .475 .667 .643 .469 .628 .406 .725 .563 .744 .680 .457 .499 .538 .621 .520 Faceness-WIDER .611 .579 .660 .599 .588 .505 .672 .648 .519 .650 .409 .776 .621 .768 .686 .489 .607 .607 .629 .564 Gymn. Hand. Wait. Pres. Work. Parach. Coac. Meet. Aero. Boat. Danc. Swim. Fami. Ball. Dres. Coup. Jock. Tenn. Spa. Surg. ACF .749 .472 .722 .720 .589 .435 .598 .548 .629 .530 .507 .626 .755 .589 .734 .621 .667 .701 .386 .599 ACF-WIDER .750 .589 .836 .794 .649 .492 .705 .700 .734 .602 .524 .534 .856 .642 .802 .589 .827 .667 .418 .586 Faceness .756 .540 .782 .732 .645 .517 .618 .592 .678 .569 .558 .666 .809 .647 .774 .742 .662 .744 .470 .635 Faceness-WIDER .768 .577 .740 .746 .640 .540 .637 .670 .718 .628 .595 .659 .842 .682 .754 .699 .688 .759 .493 .632</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Summary of face scale for multi-scale proposal networks. Model structure of Network 1.</figDesc><table><row><cell cols="2">Scale</cell><cell cols="4">Class 1 Class 2 Class 3</cell></row><row><cell cols="2">Network 1</cell><cell>10-15</cell><cell cols="2">15-20</cell><cell>20-30</cell></row><row><cell cols="2">Network 2</cell><cell>30-50</cell><cell cols="2">50-80</cell><cell>80-120</cell></row><row><cell cols="6">Network 3 120-160 160-200 200-240</cell></row><row><cell cols="6">Network 4 240-320 320-400 400-480</cell></row><row><cell>Layer name</cell><cell>Filter Number</cell><cell>Filter Size</cell><cell>Stride</cell><cell>Padding</cell><cell>Activation Function</cell></row><row><cell>Conv 1</cell><cell>96</cell><cell>7×7</cell><cell>2</cell><cell>0</cell><cell>RELU</cell></row><row><cell>Pool 1</cell><cell>-</cell><cell>3×3</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>LRN 1</cell><cell>-</cell><cell>5×5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv 2</cell><cell>256</cell><cell>5×5</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>LRN 2</cell><cell>-</cell><cell>5×5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv 3</cell><cell>384</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Conv 4</cell><cell>384</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Conv 5</cell><cell>256</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Conv 6</cell><cell>256</cell><cell>4×4</cell><cell>1</cell><cell>0</cell><cell>RELU</cell></row><row><cell>Conv 7</cell><cell>256</cell><cell>1×1</cell><cell>1</cell><cell>0</cell><cell>RELU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Model structure of Network 2.</figDesc><table><row><cell>Layer name</cell><cell>Filter Number</cell><cell>Filter Size</cell><cell>Stride</cell><cell>Padding</cell><cell>Activation Function</cell></row><row><cell>Conv 1</cell><cell>96</cell><cell>7×7</cell><cell>2</cell><cell>0</cell><cell>RELU</cell></row><row><cell>Pool 1</cell><cell>-</cell><cell>3×3</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>LRN 1</cell><cell>-</cell><cell>5×5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv 2</cell><cell>256</cell><cell>5×5</cell><cell>2</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Pool 2</cell><cell>-</cell><cell>3×3</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>LRN 2</cell><cell>-</cell><cell>5×5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv 3</cell><cell>384</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Conv 4</cell><cell>384</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Conv 5</cell><cell>256</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Conv 6</cell><cell>256</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Conv 7</cell><cell>256</cell><cell>3×3</cell><cell>1</cell><cell>0</cell><cell>RELU</cell></row><row><cell>Conv 8</cell><cell>1024</cell><cell>4×4</cell><cell>1</cell><cell>0</cell><cell>RELU</cell></row><row><cell>Conv 9</cell><cell>1024</cell><cell>1×1</cell><cell>1</cell><cell>0</cell><cell>RELU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Model structure of Network 3.</figDesc><table><row><cell>Layer name</cell><cell>Filter Number</cell><cell>Filter Size</cell><cell>Stride</cell><cell>Padding</cell><cell>Activation Function</cell></row><row><cell>Conv 1</cell><cell>96</cell><cell>7×7</cell><cell>2</cell><cell>0</cell><cell>RELU</cell></row><row><cell>Pool 1</cell><cell>-</cell><cell>3×3</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>LRN 1</cell><cell>-</cell><cell>5×5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv 2</cell><cell>256</cell><cell>5×5</cell><cell>2</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Pool 2</cell><cell>-</cell><cell>3×3</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>LRN 2</cell><cell>-</cell><cell>5×5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv 3</cell><cell>384</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Conv 4</cell><cell>384</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Conv 5</cell><cell>256</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Pool 5</cell><cell>-</cell><cell>3×3</cell><cell>3</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv 6</cell><cell>4096</cell><cell>5×5</cell><cell>1</cell><cell>0</cell><cell>RELU</cell></row><row><cell>Conv 7</cell><cell>4096</cell><cell>1×1</cell><cell>1</cell><cell>0</cell><cell>RELU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Model structure of Network 4. neural networks for face detection. Each detection network is trained to conduct face classification and bounding box regression simultaneously. We finetune the detection networks in this stage using respective proposal network in the previous stage. For example, the Detection Network 1 is trained by fine-tuning the Proposal Network 1 in the previous stage with the same structure. The detection networks are trained with face proposals generated from respective proposal networks. For face classification, we assign a proposed bounding box to a ground truth bounding box based on the minimal Euclidean distances between the center of the proposed bounding box and the center of ground truth bounding box. The proposed bounding box is assigned as positive, if the IoU between proposed</figDesc><table><row><cell>Layer name</cell><cell>Filter Number</cell><cell>Filter Size</cell><cell>Stride</cell><cell>Padding</cell><cell>Activation Function</cell></row><row><cell>Conv 1</cell><cell>96</cell><cell>11×11</cell><cell>4</cell><cell>0</cell><cell>RELU</cell></row><row><cell>Pool 1</cell><cell>-</cell><cell>3×3</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>LRN 1</cell><cell>-</cell><cell>5×5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv 2</cell><cell>256</cell><cell>5×5</cell><cell>2</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Pool 2</cell><cell>-</cell><cell>3×3</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>LRN 2</cell><cell>-</cell><cell>5×5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv 3</cell><cell>384</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Conv 4</cell><cell>384</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Conv 5</cell><cell>256</cell><cell>3×3</cell><cell>1</cell><cell>1</cell><cell>RELU</cell></row><row><cell>Pool 5</cell><cell>-</cell><cell>3×3</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv 6</cell><cell>4096</cell><cell>4×4</cell><cell>1</cell><cell>0</cell><cell>RELU</cell></row><row><cell>Conv 7</cell><cell>4096</cell><cell>1×1</cell><cell>1</cell><cell>0</cell><cell>RELU</cell></row><row><cell cols="2">four fully convolutional</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>Full name of event categories. Traf. Fest. Para. Demo. Cere. March. Bask. Shop. Mata. Acci. Elec. Conc. Awar. Picn. Riot. Fune. Chee. Firi. Raci. Vote. Hock. Stud. Skat. Gree. Foot. Runn. Driv. Dril. Phot. Spor. Grou. Cele. Socc. Inte. Raid. Base. Patr. Angl. Resc. Hand. Wait. Pres. Work. Parach. Coac. Meet. Aero. Boat. Danc. Swim. Fami. Ball. Dres. Coup. Jock. Tenn. Spa. Surg.</figDesc><table><row><cell>Traffic</cell><cell>Festival</cell><cell>Parade</cell><cell>Demonstration</cell><cell>Ceremony</cell><cell>People Marching</cell><cell>Basketball</cell><cell>Shoppers</cell><cell>Matador Bullfighter</cell><cell>Car Accident</cell><cell>Election Campain</cell><cell>Concerts</cell><cell>Award Ceremony</cell><cell>Picnic</cell><cell>Riot</cell><cell>Funeral</cell><cell>Cheering</cell><cell>Soldier Firing</cell><cell>Car Racing</cell><cell>Voter</cell></row><row><cell>Stoc. Stock Market</cell><cell>Hockey</cell><cell>Students Schoolkids</cell><cell>Ice Skating</cell><cell>Greeting</cell><cell>Football</cell><cell>Running</cell><cell>People Driving Car</cell><cell>Soldier Drilling</cell><cell>Photographers</cell><cell>Sports Fan</cell><cell>Group</cell><cell>Celebration Or Party</cell><cell>Soccer</cell><cell>Interview</cell><cell>Raid</cell><cell>Baseball</cell><cell>Soldier Patrol</cell><cell>Angler</cell><cell>Rescue</cell></row><row><cell>Gymn. Gymnastics</cell><cell>Handshaking</cell><cell>Waiter Waitress</cell><cell>Press Conference</cell><cell>Worker Laborer</cell><cell>Parachutist Paratrooper</cell><cell>Sports Coach Trainer</cell><cell>Meeting</cell><cell>Aerobics</cell><cell>Row Boat</cell><cell>Dancing</cell><cell>Swimming</cell><cell>Family Group</cell><cell>Balloonist</cell><cell>Dresses</cell><cell>Couple</cell><cell>Jockey</cell><cell>Tennis</cell><cell>Spa</cell><cell>Surgeons</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Pascal visual object classes VOC challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view face detection using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Farfade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How good are detection proposals, really</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High-performance rotation invariant multiview face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">FDDB: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<pubPlace>University of Massachusetts, Amherst</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: IARPA janus benchmark A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, realworld database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning SURF cascade for fast and accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A fast and accurate unconstrained face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical face parsing via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Large-scale concept ontology for multimedia. MultiMedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tesic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Curtis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A deep pyramid deformable part model for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Robust real-time face detection. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Recognize complex events from static images by fusing deep channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Face detection by structural models. IVC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Aggregate channel features for multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fine-grained evaluation on face detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting faces in images: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A survey of recent advances in face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
