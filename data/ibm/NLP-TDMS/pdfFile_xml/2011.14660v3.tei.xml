<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
							<email>zhaoshuaimcc@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Artificial Intelligence and Robotics for Society (AIRS)</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liguang</forename><surname>Zhou</surname></persName>
							<email>liguangzhou@link.</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Artificial Intelligence and Robotics for Society (AIRS)</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
							<email>wenxiaowang@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin</forename><forename type="middle">Lun</forename><surname>Lam</surname></persName>
							<email>tllam@</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Artificial Intelligence and Robotics for Society (AIRS)</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangsheng</forename><surname>Xu</surname></persName>
							<email>ysxu@cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Artificial Intelligence and Robotics for Society (AIRS)</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The width of a neural network matters since increasing the width will necessarily increase the model capacity. However, the performance of a network does not improve linearly with the width and soon gets saturated. In this case, we argue that increasing the number of networks (ensemble) can achieve better accuracy-efficiency trade-offs than purely increasing the width. To prove it, one large network is divided into several small ones regarding its parameters and regularization components. Each of these small networks has a fraction of the original one's parameters. We then train these small networks together and make them see various views of the same data to increase their diversity. During this co-training process, networks can also learn from each other. As a result, small networks can achieve better ensemble performance than the large one with few or no extra parameters or FLOPs. Small networks can also achieve faster inference speed than the large one by concurrent running on different devices. We validate our argument with 8 different neural architectures on common benchmarks through extensive experiments. The code is available at https://github. com/mzhaoshuai/Divide-and-Co-training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Increasing the width of neural networks to pursue better performance is common sense in neural architecture engineering <ref type="bibr" target="#b55">[54,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b59">59]</ref>. However, the performance of a network does not improve linearly with its width. As shown in <ref type="figure">Figure 1</ref>, at the initial stage, increasing width can gain promising improvement of accuracy; at the later stage, the improvement becomes relatively slight and no longer matches the increasingly expensive cost. For example, EfficientNet baseline (w = 5.0, width factor) gains less than +0.5% accuracy improvement compared to EfficientNet baseline (w = 3.8) with nearly doubled FLOPs (floating-point operations). We call this the width saturation of a network. Increasing depth/resolution produces similar phenomena <ref type="bibr" target="#b55">[54]</ref>. * Tin Lun Lam is the corresponding author.  <ref type="figure">Figure 1</ref>: The width saturation of a network -the gain does not match the expensive extra cost when the width is already very large. w is the width factor. Data is shamelessly copied from ResNeXt <ref type="bibr" target="#b59">[59]</ref> and EfficientNet <ref type="bibr" target="#b55">[54]</ref>.</p><p>Besides the width saturation, we also observe that relatively small networks achieve close accuracies to very wide networks, i.e., ResNet-29 (w = 3.0 v.s. w = 5.0) and Effi-cientNet baseline (w = 2.6 v.s. w = 5.0) in <ref type="figure">Figure 1</ref>. In this case, an interesting question arises, can two small networks with a half width of a large one achieve or even surpass the performance of the latter? Firstly, ensemble is a practical technique and can improve the generalization performance of individual neural networks <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b56">55]</ref>. Kondratyuk et al. <ref type="bibr" target="#b26">[26]</ref> already demonstrate that the ensemble of several smaller networks is more efficient than a large model in some cases. Secondly, multiple networks can collaborate with their peers during training to achieve better individual or ensemble performance. This is verified by some deep mutual learning <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b68">68]</ref> and co-training <ref type="bibr" target="#b43">[42]</ref> works.</p><p>Based on the above analysis, we argue that increasing the number of networks (ensemble) can achieve better accuracyefficiency trade-offs than purely increasing the width. A  S is the number of networks. All networks are tested on Tesla V100(s) with mixed precision <ref type="bibr" target="#b38">[38]</ref> and batch size 100.</p><p>ning small networks are also generally faster than the large one. All evidence supports our previous argument and suggests people should reconsider the possibility and value of ensemble learning when designing a classification system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Neural network architecture design Since the success of AlexNet <ref type="bibr" target="#b28">[28]</ref> in the ILSVRC-2012 competition, many excellent architectures emerged, e.g., NIN <ref type="bibr" target="#b33">[33]</ref>, VGG-Net <ref type="bibr" target="#b48">[47]</ref>, Inception <ref type="bibr" target="#b53">[52]</ref>, ResNet <ref type="bibr" target="#b17">[17]</ref>, Xception <ref type="bibr" target="#b6">[6]</ref>. They explored different ways to design an effective and efficient model, e.g., 1 × 1 convolution kernels, stacked convolution layers with small kernels, combination of different convolution and pooling operations, residual connection, depthwise separable convolution, and so on. In recent years, neural architecture search (NAS) becomes more and more popular. People hope to automatically learn or search for the best neural architectures for certain tasks with machine learning methods. We name a few here, reinforcement learning based NAS <ref type="bibr" target="#b70">[70]</ref>, progressive neural architecture search (PNASNet <ref type="bibr" target="#b34">[34]</ref>), differentiable architecture search (DARTS <ref type="bibr" target="#b35">[35]</ref>), etc. Implicit ensemble methods Ensemble methods use multiple learning algorithms to obtain better performance than any of them. Some layer splitting methods <ref type="bibr" target="#b53">[52,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b67">67]</ref> adopt an implicitly "divide and ensemble" strategy, namely, they divide a single layer in a model and then fuse their outputs to get better performance. Dropout <ref type="bibr" target="#b52">[51]</ref> can also be interpreted as an implicit ensemble of multiple sub-networks within one full network. Slimmable Network <ref type="bibr" target="#b62">[62]</ref> derives several networks with different widths from a full network and trains them in a parameter-sharing way to achieve adaptive accuracy-efficiency trade-offs at runtime. MutualNet <ref type="bibr" target="#b61">[61]</ref> further trains these sub-networks mutually to make the full network achieve better performance. These methods get several dependent models after implicitly splitting, while our methods obtain independent models in terms of parameters after dividing. They are also compatible with our methods and can be applied to any model in our system. Collaborative learning Collaborative learning refers to a variety of educational approaches involving joint intellectual effort by students, or students and teachers together <ref type="bibr" target="#b49">[48]</ref>. It was formally introduced in deep learning by <ref type="bibr" target="#b51">[50]</ref>, which was used to describe the simultaneous training of multiple classifier heads of a network. Actually, according to its original definition, many works involving two or more models learning together can also be called collaborative learning, e.g., DML <ref type="bibr" target="#b68">[68]</ref>, co-training <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b43">42]</ref>, mutual mean-teaching (MMT) <ref type="bibr" target="#b11">[11]</ref>, cooperative learning <ref type="bibr" target="#b1">[1]</ref>, knowledge distillation <ref type="bibr" target="#b20">[20]</ref>. Their core idea is similar, i.e., enhancing the performance of one or all models by training them with some peers or teachers. They inspire our co-training algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given a neural model M and N training samples X = {x n } N n=1 from C classes, the objective is cross entropy:</p><formula xml:id="formula_0">L ce (p, y) = − 1 N N n=1 C c=1 y n,c log(p n,c ),<label>(1)</label></formula><p>where y ∈ {0, 1} is the ground truth label and p ∈ [0, 1] is the softmax normalized probability given by M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Division</head><p>Parameters In <ref type="figure">Figure 2</ref>, we divide one large network M into S small networks {M 1 , M 2 , . . . , M S }. The principle is keeping the metrics -the number of parameters or FLOPs roughly unchanged before and after dividing. M is usually a stack of convolutional (conv.) layers. Following the definition in PyTorch <ref type="bibr" target="#b42">[41]</ref>, its kernel size is K × K, numbers of channels of input and output feature maps are C in and C out , and the number of groups is d, which means every Cin d input channels are convolved with its own sets of filters, of size Cout d . In this case, the number of parameters and FLOPs are:</p><formula xml:id="formula_1">Params : K 2 × C in d × C out d × d,<label>(2)</label></formula><p>FLOPs :</p><formula xml:id="formula_2">(2 × K 2 × C in d − 1) × H × W × C out ,<label>(3)</label></formula><p>where H × W is the size of the output feature map and −1 occurs because the addition of Cin d ×K 2 numbers only needs ( Cin d × K 2 − 1) times operations. The bias is omitted for the sake of brevity. For depthwise convolution <ref type="bibr" target="#b47">[46]</ref>, d = C in .</p><p>Generally, C out = t 1 ×C in , where t 1 is a constant. Therefore, if we want to divide a conv. layer by a factor S, we just need to divide C in by √ S: . Each small block only has a quarter of the parameters and FLOPs of the original block. In practice, the numbers of output channels have a greatest common divisor (GCD). The GCD of most ResNet variants <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b21">21]</ref> is the C out of the first convolutional layer. For other networks, like EfficientNet <ref type="bibr" target="#b55">[54]</ref>, their GCD is a multiple of 8 or some other numbers. In general, when we want to divide a network by S, we just need to find its GCD, and replace it with GCD/ √ S, then it is done. For networks mainly consisted of group convolutions, like ResNeXt <ref type="bibr" target="#b59">[59]</ref>, we keep Cin d fixed, i.e., C in = t 2 × d, where t 2 is a constant. Namely, the number of channels per group is unchanged during dividing, and the number of groups will be divided. We substitute the C in in Eq. (4) with the above equation and get:</p><formula xml:id="formula_3">K 2 × C in × C out S × 1 d = K 2 × t 1 × ( C in √ S ) 2 × 1 d .<label>(4)</label></formula><formula xml:id="formula_4">K 2 × t 1 × t 2 2 × d S .<label>(5)</label></formula><p>Then the division can be easily achieved by dividing d by S. This way is more concise than the square root operations. For networks that have a constant global factor linearly related to the number of channels, we simply divide the factor by √ S. For example, the widen factor of WRN <ref type="bibr" target="#b63">[63]</ref>, the growth rate of DenseNet <ref type="bibr" target="#b23">[23]</ref>, and the additional rate of PyramidNet <ref type="bibr" target="#b14">[14]</ref>. More details can be found in Appendix A.</p><p>Regularization After dividing, the model capacity degrades and the regularization components in networks should change accordingly. Under the assumption that the model capacity is linearly dependent with the network width, we change the magnitude of dropping regularization linearly. Specifically, the dropping probabilities of dropout <ref type="bibr" target="#b52">[51]</ref>, ShakeDrop <ref type="bibr" target="#b60">[60]</ref>, and stochastic depth <ref type="bibr" target="#b24">[24]</ref> are divided by √ S in our experiments. As for weight decay <ref type="bibr" target="#b29">[29]</ref>, it is a little complex as its intrinsic mechanism is still vague <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b12">12]</ref>. We test some dividing manners and adopt two dividing strategies -no dividing and exponential dividing in this paper:</p><formula xml:id="formula_5">wd = wd × exp( 1 S − 1.0),<label>(6)</label></formula><p>where wd is the original weight decay value and wd is the new value after dividing. No dividing means the weight decay value keeps unchanged. The above two dividing strategies are empirical criteria. In practice, the best way now is trial and error. Besides, we adopt exponential dividing rather than directly dividing the wd by √ S because the latter may lead to too small weight decay value to maintain the generalization performance when S is large. Detailed discussions about dividing weight decay are in Appendix B.2.</p><p>The above dividing mechanism is usually not perfect, i.e., the number of parameters of M i may not be exactly 1/S of M . Firstly, √ S may not be an integer. In this case, we round C in / √ S in Eq. (4) to a nearby even number. Secondly, the division of the first layer and the last fully-connected layer is not perfect because the input channel (color channels) and output channel (number of object classes) are fixed. Concurrent running Despite better ensemble performance, small networks can also achieve faster inference speed than the large network by concurrent running on different devices as shown in <ref type="figure">Figure 2</ref>. Typically, a device is an NVIDIA GPU. Theoretically, if one GPU has enough processing units, e.g., streaming multiprocessor, small networks can also run concurrently within one GPU with multiple CUDA Streams <ref type="bibr" target="#b41">[40]</ref>. However, we find this is impractical on a Tesla V100 GPU in experiments. One small network is already able to occupy most of the computational resources, and different networks can only run in sequence. Therefore, we only discuss the multiple devices fashion in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Co-training</head><p>The generalization error of neural network ensemble (NNE) can be interpreted as Bias-Variance-Covariance Trade-off <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b45">44]</ref>. Let f be the function model learned, g(x) be the target function, and f en (x;</p><formula xml:id="formula_6">X ) = 1 S S i=1 f i (x; X ).</formula><p>Then the expected mean-squared ensemble error is:</p><formula xml:id="formula_7">EX [ fen(x; X ) − g(x) 2 ] = EX [fen(x) − g(x)] 2 + EX 1 S 2 S i=1 fi(x; X ) − EX [fi(x; X )] 2 + EX 1 S 2 S i=1 S j =i fi(x; X ) − EX [fi(x; X )] × fj(x; X ) − EX [fj(x; X )] ,<label>(7)</label></formula><p>where the first term is the square bias (Bias 2 ) of the combined system, the second and third terms are the variance (Var) and</p><formula xml:id="formula_8">covariance (Cov) of the outputs of individual networks. A clean form is E[ 1 S Var +(1 − 1 S ) Cov + Bias 2 ]</formula><p>. Data noise is omitted here. Detailed proof is given by Ueda et al. <ref type="bibr" target="#b57">[57]</ref>. Different initialization and data views Increasing the diversity of networks without increasing Var or Bias can decrease the correlation (Cov) between networks. To this end, small networks are initialized with different weights <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b31">31]</ref>. Then, when feeding the training data, we apply different data transformers D i on the same data for different networks as shown in <ref type="figure">Figure 2</ref>. In practice, different data views are generated by randomness in the procedure of data augmentation. Besides the commonly used random resize/crop/flip, we further introduce random erasing <ref type="bibr" target="#b69">[69]</ref> and AutoAugment <ref type="bibr" target="#b7">[7]</ref> policies. AutoAugment has 14 image transformation operations, e.g., shear, translate, rotate, and auto contrast. It searches tens of different policies which are consisted of two operations and randomly chooses one policy during the data augmentation process. By applying these random data augmentation operations multiple times, we can guarantee that {D 1 (x), D 2 (x), . . . , D S (x)} produce different views of x for corresponding networks in most cases. Co-training loss Knowledge distillation and DML <ref type="bibr" target="#b68">[68]</ref> show that one network can boost its performance by learning from a teacher or cohorts. Namely, a deep ensemble system can reduce its Bias in this way. Besides, following the co-training assumption <ref type="bibr" target="#b3">[3]</ref>, small networks are expected to have consistent predictions on x although they see different views of x. From the perspective of Eq. <ref type="bibr" target="#b7">(7)</ref>, this can reduce the variance of networks and avoid poor performance caused by overly decorrelated networks <ref type="bibr" target="#b45">[44]</ref>. Therefore, we adopt Jensen-Shannon (JS) divergence among predicted probabilities as the co-training objective <ref type="bibr" target="#b43">[42]</ref> :</p><formula xml:id="formula_9">L cot (p 1 , p 2 , . . . , p S ) = H( 1 S S i=1 p i ) − 1 S S i=1 H(p i ),<label>(8)</label></formula><p>where p i is the estimated probability of network M i , and</p><formula xml:id="formula_10">H(p) = E[− log(p)]</formula><p>is the Shannon entropy of the distribution of p. Through this co-training manner, one network can learn valuable information from its peers, which defines a rich similarity structure over objects. For example, a model classifies an object as Chihuahua may also give a high confidence about Japanese spaniel since they are both dogs <ref type="bibr" target="#b20">[20]</ref>. DML uses the Kullbac-Leibler (KL) divergence between predictions of every two networks. The overall objective function is a combination of the small networks' classification losses and the co-training loss:</p><formula xml:id="formula_11">L all = S i=1 L ce (p i , y) + λ cot L cot (p 1 , p 2 , . . . , p S ),<label>(9)</label></formula><p>where λ cot = 0.5 is a weight factor of L cot (·) and it is chosen by cross-validation. At the early stage of training, the out-puts of network are full of randomness, so we adopt a warmup scheme for λ cot <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b43">42]</ref>. Specifically, we use a linear scaling up strategy when the current training epoch is less than a certain number -40/60 on CIFAR <ref type="bibr" target="#b27">[27]</ref>/ImageNet <ref type="bibr" target="#b46">[45]</ref>. λ cot = 0.5 is also an equilibrium point between learning diverse networks and producing consistent predictions. During inference, we average the outputs before softmax layers as the final ensemble output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Datasets We adopt CIFAR-10, CIFAR-100 <ref type="bibr" target="#b27">[27]</ref>, and Im-ageNet 2012 <ref type="bibr" target="#b46">[45]</ref> datasets. CIFAR-10 and CIFAR-100 datasets contain 50K training and 10K test RGB images of size 32×32, labeled with 10 and 100 classes, respectively. ImageNet 2012 dataset contains 1.28 million training images and 50K validation images from 1000 classes. Learning rate and training epochs We apply warm up and cosine learning rate decay policy <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b19">19]</ref>. If the initial learning rate is lr and current epoch is epoch, for the first slow_epoch steps, the learning rate is lr × epoch slow_epoch ; for the rest epochs, the learning rate is 0.5 × lr × (1 + cos(π × epoch−slow_epochs max_epoch−slow_epoch )). Generally, lr is 0.1; {max_epoch, slow_epoch} is {300, 20}/{120, 5} for CIFAR/ImageNet, respectively. Batch size and crop size For CIFAR/ImageNet, crop size is 32/224 and batch size is 128/256, respectively. The output stride -the ratio of input image spatial resolution to final output resolution <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref>, is 4/32 for CIFAR/ImageNet. Data augmentation Random crop and resize (bicubic interpolation), random left-right flipping, AutoAugment <ref type="bibr" target="#b7">[7]</ref>, normalization, random erasing <ref type="bibr" target="#b69">[69]</ref>, and mixup <ref type="bibr" target="#b65">[65]</ref> are used during training. Label smoothing <ref type="bibr" target="#b54">[53]</ref> is only applied to models on ImageNet. Weight decay Generally, wd =1e-4. For {EfficientNet-B3, ResNeXt-29 (8×64d), WRN-28-10, WRN-40-10} on CIFAR datasets, wd =5e-4. It is only applied to weights of conv. and fc. layers <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b13">13]</ref>. Bias and parameters of batch normalization <ref type="bibr" target="#b25">[25]</ref> are left undecayed.</p><p>Besides, we use kaiming weight initialization <ref type="bibr" target="#b16">[16]</ref>. The optimizer is nesterov <ref type="bibr" target="#b40">[39]</ref> accelerated SGD with momentum <ref type="table">Table 1</ref>: Influence of various settings of ResNet-110 on CIFAR-100. step-lr means step learning rate decay policy as described in ResNet <ref type="bibr" target="#b17">[17]</ref>. When the erasing probability of random erasing p e = 1.0, it acts like cutout <ref type="bibr" target="#b9">[9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on CIFAR dataset</head><p>Results on CIFAR-100 dataset</p><p>Results on CIFAR-100 are shown in <ref type="table" target="#tab_2">Table 2</ref>. Dividing and co-training achieve consistent improvements with few extra or even fewer parameters or FLOPs. Additional cost occurs since the division of a network is not perfect, as mentioned in Sec. 3.1. Some conclusions can be drawn from the data.</p><p>Conclusion 1 Increasing the number, width, and depth of networks together is more efficient and effective than purely increasing the width or depth.</p><p>For all networks in <ref type="table" target="#tab_2">Table 2</ref>, dividing and co-training gain promising improvement. We also notice, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion 2</head><p>The ensemble performance is closely related to individual performance.</p><p>The relationship between the average accuracy and ensemble accuracy is shown in <ref type="figure" target="#fig_1">Figure 5</ref>. When calculating the average accuracy, we separately calculate the accuracies of small networks and average them. From the big picture, the average accuracy is positively correlated with the ensemble accuracy. The higher the average accuracy, the better the ensemble performance. This coincides with the Eq. (7) because the stronger the individual networks the smaller the Bias.</p><p>At the same time, we note that there is a big gap between the average accuracy and ensemble accuracy. The ensemble accuracy is higher than the average accuracy by a large margin. This gap may be filled by two factors according to Eq. <ref type="formula" target="#formula_7">(7)</ref>: the decayed variance of individual networks and learned diverse networks during the co-training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion 3 A necessary width/depth of networks matters.</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, ResNet-110 (S = 4) and SE-ResNet-110 (S = 4) get a drop in performance, 0.63%↓ and 0.42%↓, respectively.  In <ref type="figure" target="#fig_1">Figure 5</ref>, these two networks obtain the first two lowest average accuracies. If we take one step further to look at the architecture of these small networks, we will find they have input channels <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b32">32]</ref> at the first layer of their three blocks. These networks are too thin compared to the original one with <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b64">64]</ref> channels. In this case, the magnitude of decayed variance of individual networks is smaller than the magnitude of gained accuracy (decayed bias) from increasing the channels (width) of a single network. This demonstrates that a necessary width is essential. The conclusion also applies to PyramidNet-272, which gets a relatively slight improvement with dividing and co-training as its base channel is small, i.e., 16. Increasing the width of networks is still effective when the width is very small.</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, ResNet-164 (S = 4) and SE-ResNet-164 (S = 4) still get improvements, although their performance is not as good as applying (S = 2). In <ref type="figure" target="#fig_1">Figure 5</ref> In a word, after dividing, the small networks should maintain a necessary width or depth to guarantee their model capacity and achieve satisfying ensemble performance. This is consistent with Kondratyuk et al. <ref type="bibr" target="#b26">[26]</ref>, they show that the ensemble of some small models with limited depths and widths cannot achieve promising ensemble performance compared to some large models.</p><p>From conclusion 1-3, We can learn some best practices about effective model scaling. When the width/depth of networks is small, increasing width/depth can still get substantial improvement. However, when width/depth becomes large and increasing them yields little gain <ref type="figure">(Figure 1)</ref>, it is more effective to increase the number of networks. This comes to our core conclusion -Increasing the number, width, and depth of networks together is more efficient and effective than purely scaling up one dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on CIFAR-10 dataset</head><p>Results on the CIFAR-10 are shown in <ref type="table">Table 4</ref>. Although the Top-1 accuracy is very high, dividing and co-training can also achieve significant improvement. It is worth noting {WRN-28-10, epoch 1800} get worse performance  <ref type="table">Table 4</ref>: Results on CIFAR-10. S is the number of small networks after dividing. Divide weight decay as Eq. <ref type="bibr" target="#b6">(6)</ref>. No extra data and train from scratch. PyramidNet is trained with mixed precision <ref type="bibr" target="#b38">[38]</ref>. Methods compared here are AutoAugment <ref type="bibr" target="#b7">[7]</ref>, RandAugment <ref type="bibr" target="#b8">[8]</ref>, Fixup-init <ref type="bibr" target="#b66">[66]</ref>, Mixup <ref type="bibr" target="#b65">[65]</ref>, Cutout <ref type="bibr" target="#b9">[9]</ref>, ShakeDrop <ref type="bibr" target="#b60">[60]</ref>, and Fast AutoAugment <ref type="bibr" target="#b32">[32]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on ImageNet dataset</head><p>Results on ImageNet are shown in <ref type="table" target="#tab_3">Table 3</ref>. All experiments on ImageNet are conducted with mixed precision. WRN-50-2 and WRN-50-3 are 2× and 3× wide as ResNet-50. The results on ImageNet validates our argument again -Increasing the number, width, and depth of networks together is more efficient and effective. Specifically, EfficientNet-B7 (84.4%, 66M, crop 600, wider, deeper) vs. EfficientNet-B6 (84.2%, 43M, crop 528), WRN-50-3 (80.74%, 135.0M) vs. WRN-50-2 (80.66%, 68.9M), the former only produces 0.2%↑ and 0.08%↑ gain of accuracy, respectively. This shows that increasing width/depth can only yield little gain when the model is already very large, while it brings out an expensive extra cost. In contrast, increasing the number of networks rewards with much more tangible improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>Ensemble of ensemble models The ensemble of divided networks can also achieve better accuracy-efficiency tradeoffs than the original single model. The testing results are shown in <ref type="figure" target="#fig_6">Figure 6</ref>. For Top-1 accuracy, the ensemble of   ensemble models obtains better performance than single models when the number of networks is relatively small. Then the former also reach the saturation point (plateau) faster than the latter. As for top-5 accuracy, the ensemble of ensemble models always achieves higher accuracy than single models. This shows the robustness of the ensemble of ensemble models.</p><p>Training budgets At this time, the training time of dividing and co-training is longer than a single model. For example, the training time of WRN-50-3 on 8 RTX 2080Ti GPUs (in <ref type="table" target="#tab_3">Table 3</ref>) is 9d18h and 10d20h for a single model and divided models, respectively. This is caused by the doubled data pre-processing time and sequential running manner of multiple models during training. This can be alleviated by doing these operations parallelly as we partially demonstrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Theoretically, several small models can definitely achieve faster training speed than a single large model by parallelization. As for the memory budget, divided models need slightly more memories as they produce more feature maps than a single model. For the most memory-consuming model -EfficientNet-B7 (in <ref type="table" target="#tab_3">Table 3</ref>), the number is~2GB per GPU on 4 Tesla V100 GPUs. The memory consumption may fluctuate during training. This is a roughly averaged number.</p><p>Regularization The influence of dividing the regularization components is shown in <ref type="table" target="#tab_5">Table 5</ref> nd <ref type="table" target="#tab_6">Table 6</ref>. The results partially support our assumption: small models generally need less regularization. There are also some counterexamples: dividing weight decay of WRN-16-8 does not work.</p><p>Possibly 1e-4 is a small number for WRN-16-8 on CIFAR-100, and it should not be further divided. It is worth noting that 5e-4 and 1e-4 are already appropriate weight decay values for WRN-28-10 and WRN-16-8 as we tested in <ref type="table">Table 8</ref> in Appendix B.2. More experiments and discussions can be found in the appendix.</p><p>Co-training The influences of dividing and ensemble, different data views, and various value of weight factor λ cot of co-training loss in Eq. (9) are shown in <ref type="table" target="#tab_7">Table 7</ref>. The contribution of dividing and ensemble is the most significant, i.e., 1.28%↑ at best. Using different data transformers (0.14%↑) and co-training loss (0.24%↑) can also help the model improve performance during several runs. Besides, the effect of co-training is more significant when there are more networks, i.e., 0.06%↑ (S = 2) vs. 0.24%↑ (S = 4). Although co-training achieves relatively small improvement than dividing and ensemble, it does make changes in several runs. Considering the baseline is very strong, their improvement also matters, especially in practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>In this paper, we discuss the accuracy-efficiency tradeoffs of increasing the number of networks and demonstrate it is better than purely increasing the depth or width of networks. Along this process, a simple yet effective methoddividing and co-training is proposed to enhance the performance of a single large model. This work potentially introduces some interesting topics in neural network engineering, e.g., designing a flexible framework for asynchronous training of multiple models, more complex deep ensemble and cotraining methods, multiple models with different modalities, introducing the idea into NAS (given a specific constrain of parameters/FLOPs, how can we design one or several models to get the best performance on a certain task).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training Appendices</head><p>A. Details about dividing a large network S is the number of small networks after dividing.</p><p>ResNet For CIFAR-10 and CIFAR-100, the numbers of input channels of the three blocks are: WRN Suppose the widen factor is w, the new widen factor w after dividing is:</p><formula xml:id="formula_12">w = max( w √ S + 0.4 , 1.0).<label>(10)</label></formula><p>ResNeXt Suppose original cardinality (groups in convolution) is d, new cardinality d is:</p><formula xml:id="formula_13">d = max( d S , 1.0).<label>(11)</label></formula><p>Shake-Shake For Shake-Shake 26 2×96d, the numbers of output channels of the first convolutional layer and three blocks are: DenseNet Suppose the growth rate of DenseNet is g dense , the new growth rate after dividing is</p><formula xml:id="formula_14">g dense = 1 2 × 2 × g dense √ S .<label>(12)</label></formula><p>PyramidNet + ShakeDrop Suppose the additional rate of PyramidNet and final drop probability of ShakeDrop is g pyramid and p shake , respectively, we divide them as:</p><formula xml:id="formula_15">g pyramid = g pyramid √ S ,<label>(13)</label></formula><formula xml:id="formula_16">p shake = p shake √ S .<label>(14)</label></formula><p>To pursue better performance, we do not divide the base channel of PyramidNet on CIFAR since it is small -16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weight decay matters B.1. The value of weight decay matters</head><p>Weight decay is a simple technique that can prevent weights from growing too large and improve the generalization performance of a model <ref type="bibr" target="#b29">[29]</ref>. When using SGD in PyTorch, it is equivalent to 2 regularization.</p><p>It is commonly known that the generalization of a model depends on the balance between the model capacity and data complexity. In our experiments, we use many data augmentation methods, e.g., AutoAugment <ref type="bibr" target="#b7">[7]</ref>, mixup <ref type="bibr" target="#b65">[65]</ref>, random erasing <ref type="bibr" target="#b69">[69]</ref>. These regularization methods have relatively fixed hyper-parameters. In this way, to obtain good performance, it is necessary to find an appropriate value of weight decay to balance various forms of regularizations <ref type="bibr" target="#b50">[49]</ref>. Some weight decay values are tested on CIFAR datasets with WRN-16-8 and WRN-28-10. The results and corresponding training curves are shown in <ref type="table">Table 8</ref> and <ref type="figure" target="#fig_7">Figure 7</ref>.</p><p>For WRN-16-8 and WRN-28-10, a small weight decay factor like 1e-5 and 5e-5 is not suitable with initial learning rate 0.1. In contrast, 1e-4 and 5e-4 are good choices for these two networks on CIFAR datasets. From <ref type="table">Table 8</ref>, we also find the small model with fewer parameters, i.e., WRN-16-8, <ref type="table">Table 8</ref>: Influence of different values of weight decay on CI-FAR dataset. {lr = 0.1, max_epoch = 300, slow_epoch = 20} with cosine annealing learning rate strategy as shown in <ref type="figure" target="#fig_8">Figure 8a</ref>. Generally, weight decay is only applied to weights of conv. and fc. layers. wd-all means applying weight decay to all learnable parameters in the model.  is more sensitive to the value of weight decay. Specifically, compared to their best results, WRN-16-8 gets~2% drop of performance while WRN-28-10 only gets~1% drop when using weight decay=1e-5 on CIFAR-100. A small weight decay values (5e-5 and 1e-5) can help the model gain better performance than large values at the early stage of the training, however, the models with large weight decay values (5e-4 and 1e-4) finally achieve better generalization performance. This is partly because of the cosine annealing learning rate strategy we use. The decayed weight in an iteration is lr × wd × θ, where lr is the learning rate, wd denotes the value of weight decay, and θ is the learnable weight. At the early stage of training, the decayed value (lr × wd, wd =5e-4 / 1e-4) causes large disturbance to the model, and the model cannot fit the data well. In contrast, at the late stage, the decayed value (lr × wd, wd =5e-5 / 1e-5) maybe too small for the model and produce little positive regularization effect. In the latter case, the model may be trapped at some local minima with inferior performance compared to the model with a large weight decay value.</p><p>It is worth noting that weight decay is only applied to the weight parameters of the conv. and fc. layers. The parameters of the batch normalization layers and bias are left undecayed. According to the observation of <ref type="bibr" target="#b64">[64]</ref>, it makes a trivial difference with applying weight decay to the      <ref type="table" target="#tab_12">Table 9</ref> and <ref type="figure" target="#fig_9">Figure 9</ref>. A large weight decay value (5e-4 &amp; lr = 0.1) leads to a significant drop in performance (~6%) on Ima-geNet while WRNs with a large weight decay value work well on CIFAR. This shows that the weight decay value should not be too large on a large and complex dataset with a relatively small model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. The manner of dividing weight decay matters</head><p>As above said, the value of weight decay matters for the generalization performance of a neural network, consequently, the manner of dividing weight decay matters. Besides the exponential strategy in Eq. (6) and no dividing strategy, a linear dividing strategy is further introduced:</p><formula xml:id="formula_17">wd = wd S .<label>(15)</label></formula><p>Linear transformations can also be applied to Eq. (6) and Eq. (15) to get new decay strategies. However, this is a large topic, i.e., finding out the underlying relationship between model capacity and appropriate weight decay values. Therefore, in this work, discussions are restricted within these three manners.</p><p>The experimental results are shown in <ref type="table" target="#tab_13">Table 10</ref>. It is clear that there is not a best and universal solution for different models on different datasets. In most cases, dividing weight decay can gain performance improvement. This is reasonable because a general assumption is that a small model needs less regularization. However, diving weight decay sometimes lead to worse performance, i.e., WRN-16-8 with Eq. (6) and Eq. (15) performs worse than WRN-16-8 with wd = wd. It is somehow counterintuitive. WRN-28-10 (S = 2) with larger capacity can gain improvement with dividing weight decay. In this case, WRN-16-8 (S = 2) is expected to work well with dividing weight decay because WRN-16-8 (S = 2) is a relatively small model and should need less regularization. One possible reason is 1e-4 maybe already a small weight decay value for WRN-16-8 on CIFAR-100, and it should not be further minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ensemble and co-training methods</head><p>Ensemble Besides the averaging ensemble manner we used, we also test max ensemble, i.e., use the most confident prediction of small networks as the final prediction, and the geometric mean of model predictions <ref type="bibr" target="#b26">[26]</ref>:</p><formula xml:id="formula_18">p = S i p i 1 S .<label>(16)</label></formula><p>Results are shown in <ref type="table" target="#tab_14">Table 11</ref>. Simple averaging is the most effective way among the test methods in most cases. This is consistent with previous work. Co-training In this work, we just use a simple co-training method and make no further exploration in this direction. There do exist some other more complex co-training or mutual learning methods. For example, MutualNet <ref type="bibr" target="#b61">[61]</ref> derives several networks with various widths from a single full network, feeds them images at different resolutions, and trains them together in a weight-sharing way to boost the performance of the full network. Limited by the paper length, more complex ensemble and co-training methods are left as future topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. EfficientNet in PyTorch</head><p>We re-implement EfficientNet in PyTorch according to <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b37">37]</ref> and the official implementation in TensorFlow <ref type="bibr">[56]</ref>. Some attempts are shown in <ref type="table" target="#tab_2">Table 12</ref>. We cannot reproduce the original result due to some differences between two frameworks, limited epochs, or smaller batch size.</p><p>Original EfficientNet uses an exponential learning rate strategy, i.e., decays the learning rate by 0.97 every 2.4 epochs. To match the decay steps (the combination of a forward process and a backward propagation is called a step), when using exponential learning rate strategy with batch size 256 and epochs 120, we use decay epochs 0.8. A decay epoch 2.4 in our training settings will lead to much inferior performance. A surprising finding is that mixup will lead to worse performance with EfficientNet.</p><p>Wightman <ref type="bibr" target="#b58">[58]</ref> reproduces a similar result with the original EfficientNet-B2 using PyTorch, i.e., 80.4% Top-1 ac- Latency of concurrent running EfficientNet-B7 (S = 2) As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, concurrent running EfficientNet-B7 (S = 2) is slower than EfficientNet-B7. This is because our running time includes data loading, pre-processing (still runs in sequence), and transferring (CPU⇔GPU, GPU⇔GPU) time. For EfficientNet-B7, the speedup of concurrent running is not able to cover the extra time brought by additional data manipulating time. The speedup of concurrent running with different networks is shown in <ref type="figure">Figure 10</ref>. A v e r a g e L a t e n c y ( m s ) G F L O P S <ref type="figure">Figure 10</ref>: The difference of inference latency between sequential and concurrent running. Test on Tesla V100(s) with mixed precision and batch size 100.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4 R</head><label>4</label><figDesc>i c i e n t N e t B a s e l i n e o n I m a g e N e t T o p -1 A c c u r a c y ( % ) e s N e t -2 9 o n C I F A R -1 0 T o p -1 A c c u r a c y ( % ) # P a r a m s ( M )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5 R</head><label>5</label><figDesc>arXiv:2011.14660v3 [cs.CV] 20 Mar 2021 i c i e n t N e t -B 4 E f f i c i e n t N e t -B i d e i n t o 2 s m a l l n e t w o r k s o n e l a r g e n e t w o r k T o p -1 A c c u r a c y o n I m a g e N e t ( % ) A v e r a g e L a t e n c y ( m s ) G F L O P s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Networks and their inference latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ResNet-110 (S = 2) &gt; ResNet-164, SE-ResNet-110 (S = 2) &gt; SE-ResNet-164, EfficientNet-B0 (S = 4) &gt; EfficientNet-B3, and WRN-28-10 (S = 4) &gt; WRN-40-10 with fewer parameters, where &gt; means the former has better performance. By contrast, the latter is deeper or wider. This exactly demonstrates the superiority of increasing the number of networks. The relationship between network architectures and the maximal gain of accuracy with dividing and co-training is shown in Figure 4. Generally, with wider or deeper networks, dividing and co-training can gain more improvement, e.g., ResNet-110 (+0.64) vs. ResNet-164 (+1.26), SE-ResNet-110 (+0.54) vs. SE-ResNet-164 (+1.06), WRN-28-10 (+1.24) vs. WRN-40-10 (+1.60), and EfficientNet-B0 (+0.65) vs. EfficientNet-B3 (+1.48).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Network architecture and its maximal gain of accuracy with division and co-training on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>, small networks of ResNet-164 and SE-ResNet-164 with (S = 4) also get better performance than ResNet-110 and SE-ResNet-110 with (S = 4), respectively. This reveals that a necessary depth can help guarantee the model capacity and achieve better ensemble performance. The average of accuracies and ensemble accuracy of small networks on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Ensemble of ensemble models. {WRN, S = 2} is treated as a single model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>WRN-28-10 on CIFAR-100 wd = 5e 4 wd = 1e 4 wd = 5e 5 wd = 1e 5 (b) Accuracy of WRN-28-10 on CIFAR-100 Training curves of WRN-28-10 on CIFAR-10 and CIFAR-100. Best viewed in color with 800% zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Cosine learning rate strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Training curve of ResNet-50 on ImageNet. The training curve of ResNet-50 (wd-all) is similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>c u r r e n t r u n n i n g o n 2 G P U s s e q u e n t i a l r u n n i n g o n 1 G P U T o p -1 A c c u r a c y o n I m a g e N e t ( % )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ResNet variants adopt the modifications introduced in<ref type="bibr" target="#b19">[19]</ref>. The code is implemented in PyTorch<ref type="bibr" target="#b42">[41]</ref>. Influence of some settings is shown inTable 1. The baseline is solid.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>0.9.</cell></row><row><cell>Method</cell><cell>step-lr cos-lr random erasing</cell><cell>mixup</cell><cell>AutoAug Top-1 err. (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>24.71 ± 0.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell>24.15 ± 0.07</cell></row><row><cell>ResNet-110 [18]</cell><cell>, pe = 1.0</cell><cell></cell><cell>23.43 ± 0.01</cell></row><row><cell>original: 26.88%</cell><cell>, pe = 0.5</cell><cell></cell><cell>23.11 ± 0.29</cell></row><row><cell></cell><cell>, pe = 0.5</cell><cell>, λ = 0.2</cell><cell>21.22 ± 0.28</cell></row><row><cell></cell><cell>, pe = 0.5</cell><cell>, λ = 0.2</cell><cell>19.19 ± 0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on CIFAR-100. The last three rows are trained for 1800 epochs. S is the number of small networks after dividing. Train from scratch, no extra data. Weight decay value keeps unchanged except {WRN-40-10, 1800 epochs}, which applies Eq.<ref type="bibr" target="#b6">(6)</ref>. DenseNet and PyramidNet are trained with mixed precision<ref type="bibr" target="#b38">[38]</ref>. The smaller, the better.</figDesc><table><row><cell></cell><cell>original</cell><cell></cell><cell>re-implementation</cell><cell></cell><cell cols="2">(# Networks) S = 2</cell><cell></cell><cell cols="2">(# Networks) S = 4</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Top-1 err. Top-1 err. # params (M) GFLOPs</cell><cell>Top-1 err.</cell><cell cols="2"># params (M) GFLOPs</cell><cell>Top-1 err.</cell><cell cols="2"># params (M) GFLOPs</cell></row><row><cell>ResNet-110 [18]</cell><cell>26.88</cell><cell>18.96</cell><cell>1.17</cell><cell>0.17</cell><cell>18.32 (0.64 ↑)</cell><cell>1.33</cell><cell>0.20</cell><cell>19.56 (0.63 ↓)</cell><cell>1.21</cell><cell>0.18</cell></row><row><cell>ResNet-164 [18]</cell><cell>24.33</cell><cell>18.38</cell><cell>1.73</cell><cell>0.25</cell><cell>17.12 (1.26 ↑)</cell><cell>1.96</cell><cell>0.29</cell><cell>18.05 (0.33 ↑)</cell><cell>1.78</cell><cell>0.26</cell></row><row><cell>SE-ResNet-110 [21]</cell><cell>23.85</cell><cell>17.91</cell><cell>1.69</cell><cell>0.17</cell><cell>17.37 (0.54 ↑)</cell><cell>1.89</cell><cell>0.20</cell><cell>18.33 (0.42 ↓)</cell><cell>1.70</cell><cell>0.18</cell></row><row><cell>SE-ResNet-164 [21]</cell><cell>21.31</cell><cell>17.37</cell><cell>2.51</cell><cell>0.26</cell><cell>16.31 (1.06 ↑)</cell><cell>2.81</cell><cell>0.29</cell><cell>17.21 (0.16 ↑)</cell><cell>2.53</cell><cell>0.27</cell></row><row><cell>EfficientNet-B0 [54]  †</cell><cell>-</cell><cell>18.50</cell><cell>4.13</cell><cell>0.23</cell><cell>18.20 (0.30 ↑)</cell><cell>4.28</cell><cell>0.24</cell><cell>17.85 (0.65 ↑)</cell><cell>4.52</cell><cell>0.30</cell></row><row><cell>EfficientNet-B3 [54]</cell><cell>-</cell><cell>18.10</cell><cell>10.9</cell><cell>0.60</cell><cell>17.00 (1.10 ↑)</cell><cell>11.1</cell><cell>0.60</cell><cell>16.62 (1.48 ↑)</cell><cell>11.7</cell><cell>0.65</cell></row><row><cell>WRN-16-8 [63]</cell><cell>20.43</cell><cell>18.69</cell><cell>11.0</cell><cell>1.55</cell><cell>17.37 (1.32 ↑)</cell><cell>12.4</cell><cell>1.75</cell><cell>17.07 (1.62 ↑)</cell><cell>11.1</cell><cell>1.58</cell></row><row><cell>ResNeXt-29, 8×64d [59]</cell><cell>17.77</cell><cell>16.43</cell><cell>34.5</cell><cell>5.41</cell><cell>14.99 (1.44 ↑)</cell><cell>35.4</cell><cell>5.50</cell><cell>14.88 (1.55 ↑)</cell><cell>36.9</cell><cell>5.67</cell></row><row><cell>WRN-28-10 [63]</cell><cell>19.25</cell><cell>15.50</cell><cell>36.5</cell><cell>5.25</cell><cell>14.48 (1.02 ↑)</cell><cell>35.8</cell><cell>5.16</cell><cell>14.26 (1.24 ↑)</cell><cell>36.7</cell><cell>5.28</cell></row><row><cell>WRN-40-10 [63]</cell><cell>18.30</cell><cell>15.56</cell><cell>55.9</cell><cell>8.08</cell><cell>14.28 (1.28 ↑)</cell><cell>54.8</cell><cell>7.94</cell><cell>13.96 (1.60 ↑)</cell><cell>56.0</cell><cell>8.12</cell></row><row><cell>WRN-40-10 [63]</cell><cell>18.30</cell><cell>16.02</cell><cell>55.9</cell><cell>8.08</cell><cell>14.09 (1.93 ↑)</cell><cell>54.8</cell><cell>7.94</cell><cell>13.10 (2.92 ↑)</cell><cell>56.0</cell><cell>8.12</cell></row><row><cell>DenseNet-BC-190 [23]</cell><cell>17.18</cell><cell>14.10</cell><cell>25.8</cell><cell>9.39</cell><cell>12.64 (1.46 ↑)</cell><cell>25.5</cell><cell>9.24</cell><cell>12.56 (1.54 ↑)</cell><cell>26.3</cell><cell>9.48</cell></row><row><cell>PyramidNet-272 [14] + ShakeDrop [60]</cell><cell>14.96</cell><cell>11.02</cell><cell>26.8</cell><cell>4.55</cell><cell>10.75 (0.27 ↑)</cell><cell>28.9</cell><cell>5.24</cell><cell>10.54 (0.48 ↑)</cell><cell>32.8</cell><cell>6.33</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>† When training on CIFAR-100, the stride of EfficientNet at the stage 4&amp;7 is set to be 1. Original EfficientNet on CIFAR-100 is pre-trained while we train it from scratch.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Single crop results on ImageNet 2012 validation dataset. No extra data and train from scratch. S is the number of small networks after dividing. Acc. of M i is the accuracy of small networks. Only WRN applies Eq. (6); others keep weight decay unchanged. Train with mixed precision.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Influence of weight decay's dividing manners.</figDesc><table><row><cell>Method</cell><cell>wd</cell><cell cols="2">Top-1 err. (%) CIFAR-10 CIFAR-100</cell></row><row><cell>WRN-28-10</cell><cell>5e-4</cell><cell>2.28</cell><cell>15.50</cell></row><row><cell cols="2">WRN-28-10, S = 2 5e-4</cell><cell>2.15</cell><cell>14.48</cell></row><row><cell cols="2">WRN-28-10, S = 2 2.5e-4</cell><cell>2.15</cell><cell>14.43</cell></row><row><cell cols="2">WRN-28-10, S = 2 Eq. (6)</cell><cell>2.06</cell><cell>14.16</cell></row><row><cell cols="2">WRN-28-10, S = 4 5e-4</cell><cell>2.36</cell><cell>14.26</cell></row><row><cell cols="2">WRN-28-10, S = 4 1.25e-4</cell><cell>2.00</cell><cell>14.79</cell></row><row><cell cols="2">WRN-28-10, S = 4 Eq. (6)</cell><cell>2.01</cell><cell>14.04</cell></row><row><cell>WRN-16-8</cell><cell>1e-4</cell><cell></cell><cell>18.69</cell></row><row><cell>WRN-16-8, S = 2 WRN-16-8, S = 2</cell><cell>1e-4 0.5e-4</cell><cell>-</cell><cell>17.37 18.11</cell></row><row><cell>WRN-16-8, S = 2</cell><cell>Eq. (6)</cell><cell></cell><cell>17.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Influence of dividing dropping layers. p shake is the initial dropping probability of ShakeDrop modules.</figDesc><table><row><cell>Method</cell><cell>p shake</cell><cell cols="2">Top-1 err. (%) CIFAR-10 CIFAR-100</cell></row><row><cell>PyramidNet-272 + ShakeDrop</cell><cell>0.5</cell><cell>1.33</cell><cell>11.02</cell></row><row><cell cols="2">PyramidNet-272 + ShakeDrop, S = 2 PyramidNet-272 + ShakeDrop, S = 2 0.5/ 0.5 √ 2</cell><cell>1.42 1.31</cell><cell>11.15 10.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Influence of co-training components. Networks) S = 4 diff. views λcot Top-1 err. (%) diff. views λcot Top-1 err. (%)</figDesc><table><row><cell cols="3">Method (# WRN-16-8 [63] (# Networks) S = 2 17.64 ± 0.18 17.50 ± 0.05</cell><cell></cell><cell>17.41 ± 0.18 17.36 ± 0.12</cell></row><row><cell>original: 20.43%</cell><cell>0.1</cell><cell>17.56 ± 0.08</cell><cell>0.1</cell><cell>17.37 ± 0.04</cell></row><row><cell>re-impl.: 18.69%</cell><cell>0.5</cell><cell>17.44 ± 0.08</cell><cell>0.5</cell><cell>17.12 ± 0.05</cell></row><row><cell></cell><cell>1.0</cell><cell>17.51 ± 0.04</cell><cell>1.0</cell><cell>17.15 ± 0.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Influence of different values of weight decay on ImageNet validation dataset. We use {lr = 0.1, max_epoch = 120, slow_epoch = 5, batch size 256} with cosine annealing learning rate strategy. Generally, weight decay is only applied to weights of conv. and fc. layers. wd-all means applying weight decay to all learnable parameters in the model.</figDesc><table><row><cell>Method</cell><cell>wd</cell><cell cols="5">Top-1 / Top-5 Acc. (%) Top-1 / Top-5 Acc. (%) (wd-all)</cell></row><row><cell></cell><cell>1e-5</cell><cell cols="2">78.21 / 94.01</cell><cell></cell><cell>78.37 / 94.25</cell><cell></cell></row><row><cell>ResNet-50</cell><cell>5e-5 1e-4</cell><cell cols="2">78.84 / 94.47 78.32 / 94.17</cell><cell></cell><cell>78.81 / 94.47 78.53 / 94.34</cell><cell></cell></row><row><cell></cell><cell>5e-4</cell><cell cols="2">72.98 / 91.44</cell><cell></cell><cell>74.89 / 92.49</cell><cell></cell></row><row><cell>0</cell><cell>20</cell><cell>40</cell><cell>60 Training Epoch</cell><cell>80</cell><cell>100</cell><cell>120</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Influence of dividing manners of weight decay. Weight decay is only applied to weights of conv. and fc. layers. lr = 0.1/0.2 for WRNs / Shake-Shake. Three manners, wd = wd, wd = wd S , wd = wd × exp( 1 S − 1.0) (Eq.(6)).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Training epochs 300</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>wd</cell><cell cols="2">Top-1 err. (%) CIFAR-10 CIFAR-100</cell><cell>Method</cell><cell>wd</cell><cell>Top-1 err. (%) CIFAR-100</cell></row><row><cell>WRN-28-10</cell><cell>5e-4</cell><cell>2.28</cell><cell>15.50</cell><cell>WRN-16-8</cell><cell>1e-4</cell><cell>18.69</cell></row><row><cell cols="2">WRN-28-10, S = 2 5e-4</cell><cell>2.15</cell><cell>14.48</cell><cell cols="2">WRN-16-8, S = 2 1e-4</cell><cell>17.37</cell></row><row><cell cols="2">WRN-28-10, S = 2 2.5e-4</cell><cell>2.15</cell><cell>14.43</cell><cell cols="2">WRN-16-8, S = 2 0.5e-4</cell><cell>18.11</cell></row><row><cell cols="2">WRN-28-10, S = 2 Eq. (6)</cell><cell>2.06</cell><cell>14.16</cell><cell cols="2">WRN-16-8, S = 2 Eq. (6)</cell><cell>17.77</cell></row><row><cell cols="2">WRN-28-10, S = 4 5e-4</cell><cell>2.36</cell><cell>14.26</cell><cell cols="2">WRN-16-8, S = 4 1e-4</cell><cell>17.07</cell></row><row><cell cols="2">WRN-28-10, S = 4 1.25e-4</cell><cell>2.00</cell><cell>14.79</cell><cell cols="2">WRN-16-8, S = 4 0.25e-4</cell><cell>18.35</cell></row><row><cell cols="2">WRN-28-10, S = 4 Eq. (6)</cell><cell>2.01</cell><cell>14.04</cell><cell cols="2">WRN-16-8, S = 4 Eq. (6)</cell><cell>17.92</cell></row><row><cell></cell><cell></cell><cell cols="3">(b) Training epochs 1800</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>wd</cell><cell cols="2">Top-1 err. (%) Method CIFAR-10</cell><cell></cell><cell>wd</cell><cell>Top-1 err. (%) CIFAR-10</cell></row><row><cell>WRN-28-10</cell><cell>5e-4</cell><cell>2.41</cell><cell cols="2">Shake-Shake 26 2×96d</cell><cell>1e-4</cell><cell>2.00</cell></row><row><cell cols="2">WRN-28-10, S = 2 5e-4</cell><cell>1.95</cell><cell cols="3">Shake-Shake 26 2×96d, S = 2 1e-4</cell><cell>1.94</cell></row><row><cell cols="2">WRN-28-10, S = 2 2.5e-4</cell><cell>1.81</cell><cell cols="3">Shake-Shake 26 2×96d, S = 2 0.5e-4</cell><cell>1.74</cell></row><row><cell cols="2">WRN-28-10, S = 2 Eq. (6)</cell><cell>1.81</cell><cell cols="3">Shake-Shake 26 2×96d, S = 2 Eq. (6)</cell><cell>1.75</cell></row><row><cell cols="2">WRN-28-10, S = 4 1.25e-4</cell><cell>1.66</cell><cell cols="3">Shake-Shake 26 2×96d, S = 4 0.25e-4</cell><cell>1.69</cell></row><row><cell cols="2">WRN-28-10, S = 4 Eq. (6)</cell><cell>1.68</cell><cell cols="3">Shake-Shake 26 2×96d, S = 4 Eq. (6)</cell><cell>1.84</cell></row><row><cell cols="4">whole model. However, WRN-16-8 get better performance</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(17.75% vs. 18.69%) on CIFAR-100 when applying a large</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">weight decay (5e-4) to all parameters. This is interesting and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>needs further investigation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">We replicate some of the above experiments on Ima-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">geNet [45] with ResNet-50. Results are shown in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Influence of different ensemble manners. Softmax () means we ensemble the results after softmax operation. Generally, we do ensemble operations without softmax.</figDesc><table><row><cell>Method</cell><cell>softmax ensemble</cell><cell cols="2">Top-1 err. (%) CIFAR-10 CIFAR-100</cell></row><row><cell></cell><cell>max</cell><cell>1.85</cell><cell>15.04</cell></row><row><cell>WRN-28-10, S = 4</cell><cell>avg. max</cell><cell>1.77 1.90</cell><cell>14.45 15.27</cell></row><row><cell></cell><cell>avg.</cell><cell>1.68</cell><cell>14.26</cell></row><row><cell></cell><cell>geo. mean</cell><cell>1.68</cell><cell>14.26</cell></row><row><cell></cell><cell>max</cell><cell>1.94</cell><cell>15.58</cell></row><row><cell></cell><cell>avg.</cell><cell>1.93</cell><cell>15.08</cell></row><row><cell>ResNeXt-29, 8×64d, S = 2</cell><cell>max</cell><cell>2.01</cell><cell>15.78</cell></row><row><cell></cell><cell>avg.</cell><cell>1.94</cell><cell>14.99</cell></row><row><cell></cell><cell>geo. mean</cell><cell>1.94</cell><cell>15.00</cell></row><row><cell>Method</cell><cell>softmax ensemble</cell><cell cols="2">ImageNet Acc. (%) Top-1 Top-5</cell></row><row><cell></cell><cell>max</cell><cell>81.11</cell><cell>95.42</cell></row><row><cell></cell><cell>avg.</cell><cell>81.31</cell><cell>95.52</cell></row><row><cell>WRN-50-3, S = 2</cell><cell>max</cell><cell>80.84</cell><cell>95.42</cell></row><row><cell></cell><cell>avg.</cell><cell>81.42</cell><cell>95.62</cell></row><row><cell></cell><cell>geo. mean</cell><cell>81.42</cell><cell>95.56</cell></row><row><cell></cell><cell>max</cell><cell>81.92</cell><cell>95.82</cell></row><row><cell></cell><cell>avg.</cell><cell>82.03</cell><cell>95.91</cell></row><row><cell>ResNeXt-101, 64×4d, S = 2</cell><cell>max</cell><cell>81.78</cell><cell>95.80</cell></row><row><cell></cell><cell>avg.</cell><cell>82.13</cell><cell>95.98</cell></row><row><cell></cell><cell>geo. mean</cell><cell>82.12</cell><cell>95.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Some attempts of re-implementing EfficientNet-B1. Our re-implementation adopts RandAugment and mixed precision training. Weight decay is 1e-5. / 94.10 curacy. The original result of EfficientNet-B2 is 80.3%. However, they train the model for 450 epochs with batch size 128. This means much more training steps and leads to formidable training time. They also use RandAugment during training. TinyNet [15] gets a 76.7% Top-1 accuracy after training EfficientNet-B0 for 450 epochs with batch size 1024 and RandAugment 1 , while the original result of EfficientNet-B0 is 77.3%.</figDesc><table><row><cell>EfficientNet-B1</cell><cell>optim.</cell><cell>lr</cell><cell cols="2">mixup Crop / Batch / Epochs Top-1 / Top-5 Acc.</cell></row><row><cell>Original</cell><cell cols="2">RMSProp exp., 0.256</cell><cell>240 / 4096 / 350</cell><cell>79.20 / 94.50</cell></row><row><cell></cell><cell cols="2">RMSProp exp., 0.016</cell><cell>240 / 256 / 120</cell><cell>77.56 / 93.61</cell></row><row><cell>re-impl.</cell><cell>SGD SGD</cell><cell>exp., 0.256 exp., 0.256</cell><cell>240 / 256 / 120 240 / 256 / 120</cell><cell>77.61 / 93.80 77.36 / 93.61</cell></row><row><cell></cell><cell>SGD</cell><cell>cos., 0.256</cell><cell>240 / 256 / 120</cell><cell>78.31</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/huawei-noah/ghostnet/tree/ master/tinynet_pytorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This paper was supported by funding 2019-INT007 from the Shenzhen Institute of Artificial Intelligence and Robotics for Society. This work was also supported in part by The National Nature Science Foundation of China (Grant Nos: 62036009, 61936006). Thanks Yuejin Li for his support in using GPU clusters.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On each worker, batch size is 16 or 32. Batch size 128 on 4 Tesla V100 32GB GPUs with gradient accumulation step 2, i.e., 1 backpropagation per 2 forward processes. Re-implementation of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Refer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Github</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">tensorflow/models/pnasnet and GitHub: tensorflow/tpu/amoeba_net</title>
		<imprint/>
	</monogr>
	<note>They use 100 P100 workers. EfficientNet in PyTorch refers to Appendix D</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cooperative learning with visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno>abs/1705.05512</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">MultiGrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Iasonas Kokkinos, and Matthijs Douze. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Shake-shake regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno>abs/1705.07485</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mutual meanteaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics, matter little near convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Golatkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Model rubik&apos;s cube: Twisting resolution, depth and width for tinynets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiulin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno>abs/1812.01187</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Snapshot ensembles: Train 1, get M for free</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">When Ensembling Smaller Models is More Efficient than Single Large Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00570</idno>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Research and development of neural network ensembles: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifei</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Fast autoaugment</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DARTS: differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simultaneous training of negatively correlated neural networks in an ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Part B</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Efficientnet in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<ptr target="https://github.com/lukemelas/EfficientNet-PyTorch" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gregory F. Diamos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<imprint>
			<pubPlace>Erich Elsen, David García, Boris Ginsburg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A method for unconstrained convex minimization problem with the rate of convergence o (1/kˆ2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Doklady an ussr</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Cuda toolkit documentation v11</title>
		<ptr target="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>NVIDIA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep co-training for semi-supervised image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Ensemble learning using decorrelated neural networks. Connection science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruce E Rosen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">What is collaborative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Leigh</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean T</forename><surname>Macgregor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/1803.09820</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Collaborative learning for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Deep neural network ensembles. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Tao</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet,2020" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>56] TensorFlow. Efficientnet in tensorflow</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Generalization error of ensemble estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Nakano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Networks</title>
		<meeting>International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Shakedrop regularization for deep residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Kise</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mutualnet: Adaptive convnet via mutual learning from network width and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taojiannan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Willis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Slimmable neural networks. In ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Three mechanisms of weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fixup initialization: Residual learning without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
