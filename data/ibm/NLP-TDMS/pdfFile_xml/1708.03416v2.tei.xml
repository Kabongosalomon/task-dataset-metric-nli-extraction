<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Bytedance Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cairong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hand Pose Estimation</term>
					<term>Convolutional Neural Network</term>
					<term>Human Computer Interaction</term>
					<term>Depth Images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hand pose estimation from single depth images is an essential topic in computer vision and human computer interaction. Despite recent advancements in this area promoted by convolutional neural networks, accurate hand pose estimation is still a challenging problem. In this paper we propose a novel approach named as Pose guided structured Region Ensemble Network (Pose-REN) to boost the performance of hand pose estimation. Under the guidance of an initially estimated pose, the proposed method extracts regions from the feature maps of convolutional neural network and generates more optimal and representative features for hand pose estimation. The extracted feature regions are then integrated hierarchically according to the topology of hand joints by tree-structured fully connections to regress the refined hand pose. The final hand pose is obtained by an iterative cascaded method. Comprehensive experiments on public hand pose datasets demonstrate that our proposed method outperforms state-of-the-art algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate 3D hand pose estimation is one of the most important techniques in human computer interaction and virtual reality <ref type="bibr" target="#b0">[1]</ref>, since it can provide fundamental information for interacting with objects and performing gestures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Hand pose estimation from single depth images has attracted broad research interests in recent years <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> thanks to the availability of depth cameras <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, such as Microsoft Kinect, Intel Realsense Camera etc. However, hand pose estimation is an extremely challenging problem due to the severe selfocclusion, high complexity of hand articulation, noises and holes in depth image, large variation of viewpoints and self-similarity of fingers etc.</p><p>Hand pose estimation has achieved great advancements by convolutional neural networks (CNNs). CNN-based data-driven methods either predict heatmaps of hand joints <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> and infer hand pose from heatmaps, or directly regress the 3D coordinates of hand joints <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref>. In either ways, features are critical for the performance of hand pose estimation. Prior works mainly focused on incorporating prior knowledge into CNN <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref> or using error feedback <ref type="bibr" target="#b17">[18]</ref> and spatial attention design <ref type="bibr" target="#b6">[7]</ref>. However, few of prior works have paid attentions to extracting more $ Email addresses: chen-xh13@mails.tsinghua.edu.cn (X. Chen), guohengkaighk@gmail.com (H. Guo), zcr17@mails.tsinghua.edu.cn (C. Zhang). Work was done when H. Guo was with Dept. of EE, Tsinghua University. * Corresponding author: wangguijin@tsinghua.edu.cn (G. <ref type="bibr">Wang)</ref> optimal and representative features of CNN. Ye et al. <ref type="bibr" target="#b6">[7]</ref> used spatial attention module to select and transform features to a canonical space. Guo et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b51">51]</ref> proposed the region ensemble network (REN) that divides the feature maps of last convolutional layer into several spatial regions and integrates them in fully connected layers. All aforementioned works haven't fully exploit optimal features of CNN for hand pose estimation.</p><p>In this paper, we propose a novel method called pose guided structured region ensemble network (Pose-REN) to boost the performance of hand pose estimation, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Upon an iterative refinement procedure, our proposed method takes a previously estimated pose as input and predicts a more accurate result in each iteration. We present a novel feature extraction method under the guidance of previous predicted hand pose to get optimal and representative features for hand pose estimation. Furthermore, inspired by hierarchical recurrent neural network <ref type="bibr" target="#b20">[21]</ref>, we present a hierarchical method to fuse features of different joints according to the topology of hand. Features from joints that belong to the same finger are integrated in the first layer and features from all fingers are fused in the following layers to predict the final hand pose.</p><p>We evaluate our proposed method on three public hand pose benchmarks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23]</ref>. Compared with state-of-theart methods, our method has achieved the best performance. Extensive ablation analyses illustrate the contributions of different components of the framework and robustness of our proposed method.</p><p>The remainder of this paper is organized as follows. In Section 2, we review prior works that are highly related to our proposed method. In Section 3, we present details about our proposed pose guided structured region ensemble network. Evaluations on public datasets and ablation studies are provided in Section 4. Section 5 gives a brief conclusion of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section we briefly review related works of our proposed method. Firstly we will review recent algorithms for depth based hand pose estimation. Since our method basically builds upon cascaded framework, we will introduce the cascaded methods for hand pose estimation. Finally, we will review related works about the hierarchical structure of neural network, as the hierarchical structured connections are utilized in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Depth-based Hand Pose Estimation</head><p>Recent approaches of hand pose estimation are generally categorized into three classes: discriminative methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9]</ref>, generative methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> and hybrid methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref>. Comprehensive review and analysis on depth based 3D hand pose estimation can be found in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Generative methods fit a predefined hand model to the input data using optimization algorithms to obtain the optimized hand pose, such as PSO (particle swarm optimization) <ref type="bibr" target="#b33">[34]</ref>, ICP (Iterative Closest Point) <ref type="bibr" target="#b26">[27]</ref> and their combination (PSO-ICP) <ref type="bibr" target="#b35">[35]</ref>. Hand-crafted energy functions that describe the distance between the hand model and input image are utilized in prior works, such as golden energy <ref type="bibr" target="#b33">[34]</ref> and silver energy <ref type="bibr" target="#b5">[6]</ref>. Several kinds of hand model have been adopted, including sphere model <ref type="bibr" target="#b35">[35]</ref>, sphere-meshes model <ref type="bibr" target="#b27">[28]</ref>, cylinder model <ref type="bibr" target="#b26">[27]</ref> and mesh model <ref type="bibr" target="#b33">[34]</ref>. Generative methods are robust for self-occlusive areas or missing areas and ensure to output plausible hand pose. However, they need a complex and time-consuming optimizing procedure and are likely to trap into local optimizations.</p><p>Discriminative methods directly learn a predictor from the labelled training data. The predictor either predicts the probability maps (heatmaps) of each hand joints <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> or directly predicts the 3D hand joint coordinates <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9]</ref>. The most frequently used methods for predictor are random forest <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23]</ref> and convolutional neural network <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Discriminative methods do not require any complex hand model and are totally datadriven, which are fast and appropriate for real-time applications. Guo et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b51">51]</ref> proposed a region ensemble network (REN) that greatly promoted the performance of hand pose estimation based on a single network. Region ensemble network divides the feature maps of last convolutional layer into several spatial regions and integrates them in fully connected layers. However, REN extracts the feature regions using a uniform grid and all features are treated equally, which is not optimal to fully incorporate the spatial information of feature maps and obtain highly representative features.</p><p>Hybrid methods try to combine the discriminative and generative methods to achieve better hand pose estimation performance. Some works adopted the generative methods after obtaining initial results by discriminative methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Zhou et al. <ref type="bibr" target="#b19">[20]</ref> proposed to incorporate a hand model into the CNN, which exploits the constraints of the hand and ensures the geometric validity of the estimated pose. However, hybrid methods have to predefine the properties of the hand model, such as the length of bones. Oberweger et al. <ref type="bibr" target="#b17">[18]</ref> proposed a data-driven hybrid method, which learns to generate a depth image from hand pose. However, the generation of depth images is likely affected by the errors of annotations.</p><p>Our proposed method basically falls into the category of discriminative method and does not rely on any predefined hand model. Compared with prior CNN-based discriminative methods, our proposed method directly predicts the 3D locations of hand pose using a cascaded framework without any postprocessing procedure. What's more, our proposed pose guided structured region ensemble network (Pose-REN) can learn better features for hand pose estimation by incorporating guided information of previously estimated hand pose into the feature maps and improve the performance of our method.</p><p>Although our proposed Pose-REN follows the idea of feature region ensemble as REN <ref type="bibr" target="#b8">[9]</ref>, there are several essential differences between Pose-REN and REN [9]: 1) Different from REN that uses grid region feature extraction, the proposed Pose-REN fully exploits an initially estimated hand pose as the guided information to extract more representative features from CNN, which is shown to have a large impact for hand pose estimation problem, as discussed in Section 4.4.2. 2) Instead of simple feature fusion as adopted in REN, our Pose-REN presents a structured region ensemble strategy that better models the connections and constraints between different joints in the hand.</p><p>3) The Pose-REN is a common framework that can easily be compatible with any existing methods (for example, Feedback <ref type="bibr" target="#b17">[18]</ref>, DeepModel <ref type="bibr" target="#b19">[20]</ref> etc.) by using them to produce initial estimations for Pose-REN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cascaded Method</head><p>The cascaded framework has been widely used in face alignment <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40]</ref>, human pose estimation <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref> and has also shown good performances in the problem of hand pose estimation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Sun et al. <ref type="bibr" target="#b22">[23]</ref> proposed a method to iteratively refine the hand pose using hand-crafted 3D pose index features that are invariant to viewpoint transformation. Oberweger et al. <ref type="bibr" target="#b16">[17]</ref> proposed a post-refinement method to refine each joint independently using multiscale input regions centered on the initially estimated hand joints. These works have to train multi models for refinement and independently predict different parts of hand joints while our proposed needs only one model to iteratively improve the estimated hand pose.</p><p>Oberweger et al. <ref type="bibr" target="#b17">[18]</ref> presented a feedback loop framework for hand pose estimation. One discriminative network is used to produce initial hand pose. A depth image is then generated from the initial hand pose using a generative CNN and an updater network improves the hand pose by comparing the synthetic depth image and input depth image. However, the depth synthetic network is highly sensitive to the annotation errors of hand poses.</p><p>Ye et al. <ref type="bibr" target="#b6">[7]</ref> integrated cascaded and hierarchical regression into a CNN framework using spatial attention mechanism. The partial hand joints are iteratively refined using transformed features generated by spatial attention module. In their method, the features in cascaded framework are generated by a initial CNN and remain unchanged in each refinement stage except for the spatial transformation. In our proposed method, feature maps are updated in each cascaded stage using an end-to-end framework, which will help to learn more effective features for hand pose estimation.</p><p>Our Pose-REN also adopts the cascaded framework. Different from the above prior methods, we present a novel feature extraction method under the guidance of previous predicted hand pose to get optimal and representative features from CNN. What's more, Pose-REN explicitly models the constraints and relations between different hand joints using structured region ensemble strategy, which is a novel method to improve the robustness and performance of hand pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Hierarchical Structure of Neural Network</head><p>Du et al. <ref type="bibr" target="#b20">[21]</ref> proposed a hierarchical recurrent neural network (RNN) for skeleton-based human action recognition. The whole skeleton is divided into five parts and fed into different branches of the RNN. Different parts of skeleton are hierarchically fused to generated higher-level representations. Madadi et al. <ref type="bibr" target="#b18">[19]</ref> proposed a tree-shape structure of CNN which regresses local poses at different branches and fuses all features in the last layer. In their structure, features of different partial poses are learned independently except for sharing features in very early layers. In contrast, our method shares features in the convolutional layers for all joints and hierarchically fuses different regions from feature maps to finally estimate the hand pose. The shared features enables better representation of hand pose and the hierarchical structure of feature fusion can better model the correlation of different hand joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pose Guided Structured Region Ensemble Network</head><p>In this section, we first give an overview of Pose-REN in Section 3.1. After that we will provide detailed elaboration about extracting regions from the feature maps under the guidance of a hand pose in Section 3.2. In Section 3.3 we present the details of fusing feature regions using hierarchically structured connection. Finally, the training strategy and implementation details are given in Section 3.4 and Section 3.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The framework of our proposed method is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. A simple CNN (denoted as Init-CNN) predicts an initial hand pose pose 0 , which is used as the initialization of the cascaded framework. The proposed framework takes a previously estimated hand pose pose t−1 and the depth image as input. The depth image is fed into a CNN to generate feature maps. Feature regions are extracted from these feature maps under the guidance of the input hand pose pose t−1 . The insight of our proposed method is that features around the location of a joint contribute more while other features like corner regions are less important. Afterwards, features from different joints are hierarchically integrated using the structured connection to regress the refined hand pose pose t . The images in dash rectangles show the close-up results of pose t−1 and pose t . It can be seen that the network refines the hand pose gradually.</p><p>Our method aims to estimate the 3D hand pose from a single depth image in a cascaded framework. Specifically, given a depth image D, the 3D locations P = {p i = (p xi , p yi , p zi )} J i=1 of J hand joints are inferred. Given a previously estimated hand pose result P t−1 in stage t − 1, our method uses the learned regression model R to refine the hand pose in stage t.</p><formula xml:id="formula_0">P t = R(P t−1 , D)<label>(1)</label></formula><p>After T stages, we get the final estimated hand pose P T for the input depth image D.</p><formula xml:id="formula_1">P T = R(P T −1 , D)<label>(2)</label></formula><p>It should be noted that only one same model R is used in every stage of refinement in the inference phase, see Section 3.4 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pose Guided Region Extraction</head><p>We first use a standard convolutional neural network (CNN) with residual connections to generate feature maps. The backbone architecture of CNN for generating feature maps used in our method is the same as the baseline network in <ref type="bibr" target="#b8">[9]</ref>, with 6 convolutional layers and 2 residual connections. Each convolutional layer is followed by a Rectified Linear Unit (ReLU) <ref type="bibr" target="#b43">[43]</ref> as the activation function and every 2 convolutional layers are followed by a max pooling layer. The residual connections are added between max pooling layers.</p><p>Denote feature maps from the last convolutional layer as F and the estimated hand pose from previous stage as</p><formula xml:id="formula_2">P t−1 = {(p t−1 xi , p t−1 yi , p t−1 zi )} J i=1</formula><p>. We use P t−1 as the guidance to extract feature regions from F. Specifically, for the i th hand joint, We first project the real-world coordinates into the image pixel coordinates using the intrinsic parameters of the depth camera, as shown in Eq. 3.</p><formula xml:id="formula_3">(p t−1 ui , p t−1 vi , p t−1 di ) = proj(p t−1 xi , p t−1 yi , p t−1 zi )<label>(3)</label></formula><p>The feature region for this joint is then cropped using a rectangular window which can be defined by a tuple</p><formula xml:id="formula_4">(b t ui , b t vi , w, h), where b t ui and b t vi</formula><p>is the coordinates of topleft corner, w and h is the width and height of the cropped feature region. The coordinates of the rectangular window are calculated by normalizing and converting the original</p><formula xml:id="formula_5">coordinates (p t−1 ui , p t−1 vi , p t−1 di ) into coordinates in feature maps.</formula><p>The extracted feature region for hand joint i is then obtained by cropping the feature maps within the rectangular window: <ref type="figure" target="#fig_1">Figure 2</ref> gives an example of pose guided region extraction. The left image is a feature map from the last convolutional layer of the CNN. It should be noted the feature maps usually contains multiple channels, we only use one channel of them to depict how to crop a region guided by a joint. The green dot and red dot indicate two joints (palm center joint and Metacarpophalangeal joint for middle finger respectively) from the previously estimated hand pose. The green and red rectangles are the corresponding cropped windows. The images in middle and right columns show the extracted feature regions for these two joints.</p><formula xml:id="formula_6">F t i = crop(F; b t ui , b t vi , w, h) (4) where the function crop(F; b u , b v , w, h) means extracting the region specified by a rectangular window (b u , b v , w, h) from F.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Structured Region Ensemble</head><p>In the previous section we have described how to extract feature regions from the feature maps for each joint using the guidance of previously estimated hand pose. One intuitional way to fuse these feature regions is to connect each region with fully connected (f c) layers respectively and then fuse these layers to regress the final hand pose, which is adopted in REN <ref type="bibr" target="#b8">[9]</ref>.</p><p>Human hand is a highly complex articulated object. Therefore, there are many constraints and correlations between different joints <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b45">45]</ref>. Independently connecting feature regions with f c layers and fusing them in the last layer can not fully adopt these constraints. Inspired by hierarchical recurrent neural network <ref type="bibr" target="#b20">[21]</ref>, in this paper we  adopt hierarchically structured region ensemble strategy to better model the constraints of hand joints, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. First a set of feature regions {F t j } M j=1 are fed into f c layers respectively.</p><formula xml:id="formula_7">h l1 j = f c(F t j ), j = 1, . . . , M<label>(5)</label></formula><p>Where M is the number of regions extracted from the feature maps. Next, {h l1 j } M j=1 are integrated hierarchically according the topology structure of hand. Specifically, denote the indices of joints that belong to the i th finger as {I i j } Mi j=1 , where M i is the number of joints that belong to the i th finger. All joints that belong to the same finger are concatenated (denote as concate) and then fed into a f c layer, as shown in Eq. 6 and Eq. 7.</p><formula xml:id="formula_8">h l1 i = concate({h l1 I i j } Mi j=1 ), i = 1, . . . , 5<label>(6)</label></formula><p>h l2 i = f c(h l1 i ), i = 1, . . . , 5</p><p>Afterwards, features from different fingers {h l2 i } 5 i=1 are concatenated and fed into a f c layer to regress the final hand pose P t ∈ R 3×J .</p><formula xml:id="formula_10">h l2 = concate({h l2 i } 5 i=1 )<label>(8)</label></formula><formula xml:id="formula_11">P t = f c(h l2 )<label>(9)</label></formula><p>Each f c layer in Eq. 5 and Eq. 7 has a dimension of 2048 nodes. They are followed by ReLU layers and dropout layers with dropout rate of 0.5. The last f c layer output a 3 × J vector P t which represents the 3D locations of hand pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>Denote the original training set as</p><formula xml:id="formula_12">T 0 = {(D i , P 0 i , P gt i )} N T i=1<label>(10)</label></formula><p>where N T is the number of training samples, D i is the depth image, P 0 i is the initially estimated hand pose and P gt i is the corresponding ground truth of hand pose. In stage t, a regression model R t is trained using T t−1 . Using this model, we can obtain the refined hand pose for each sample in training set.</p><formula xml:id="formula_13">P t i = R t (P t−1 i , D)<label>(11)</label></formula><p>we add the refined samples</p><formula xml:id="formula_14">T t = {(D i , P t i , P gt i )} N T i=1</formula><p>to the training set, generating an augmented training set T t .</p><formula xml:id="formula_15">T t = T t−1 T t<label>(12)</label></formula><p>Again, we train a model R t+1 in stage t + 1 using T t and iteratively repeat this process until reaching the maximum iteration T . The trained model R T is the final model used in the inference phase to refine the initial hand pose iteratively, as described in Eq. 1 and Eq. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>We implemented our proposed method using Caffe <ref type="bibr" target="#b46">[46]</ref>. RoI Pooling layer <ref type="bibr" target="#b47">[47]</ref> was used to facilitate the implementation of pose guided region extraction.</p><p>We used the baseline network in <ref type="bibr" target="#b8">[9]</ref> as the Init-CNN to produce initial poses for our method. Generally speaking, any existing hand pose estimation algorithms can be adopted as the initialization method of Pose-REN. We will further discuss the effect of different initializations in Preprocessing. Similar to previous methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9]</ref>, we extracted a fix-sized cube from the input depth image. The center of the cube was determined by calculating the centroid of mass of the hand region. The extracted cube was then resized into a patch with size of 96 × 96 and the depth values within it were normalized into [−1, 1]. Besides, depth values that were outside the cube were truncated according to the size of cube, providing robustness to invalid depth values. The idea of extracting a fix-sized cube is to ensure invariance of the hand size to the distance to the camera.</p><p>Training. We first trained the Init-CNN to obtain initial hand pose. After that, we used the weights of trained Init-CNN to initialize Pose-REN and train the network. The whole network was trained using stochastic gradient descent (SGD) with a batch size of 128 and a momentum of 0.9. A weight decay of 0.0005 was also adopted for the network. The learning rate was set to 0.001 and divided by 10 after every 25 epochs. The model was trained for 100 epochs for each stage and totally trained for two stages.</p><p>We followed several good practices that have been proved to be quite effective for hand pose estimation <ref type="bibr" target="#b8">[9]</ref>, including random data augmentation, smooth L 1 loss. For data augmentation, we applied random scaling of [0.9, 1.1], random translation of [−10, 10] pixels and random rotation of [−180, 180] degrees to the depth image. We used smooth L 1 loss to achieve less sensitivity to the outliers.</p><p>Parameter settings. Different datasets have the different number of hand joints, e.g. 21 joints in MSRA dataset and 16 joints in ICVL dataset. To balance the complexity of model and accuracy, we only used part of joints as the guidance to extract feature regions. Specifically, 11 out of all joints were used, as shown in <ref type="figure" target="#fig_4">Figure 4</ref>. The joints circled by dash rectangles were used, with M i = 3 for each finger, including a joint for the palm, a joint for the root of finger and a joint the tip of finger. It should be noted that despite part of joints (M = 11) are utilized as the guidance to extract features, the network still predicts the locations of all joints. The insights behind are that the overlaps of different feature regions make sure the covering of almost all important features even only a part of the joints is used.</p><p>In our experiments, the size of extracted region was set to (w, h) = (7, 7). In inference phase, the number of iterations was set to T = 3, which will be further discussed in Section 4.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we will first introduce the datasets and evaluation metrics in the experiments. Afterwards we will evaluation our proposed method on three challenging public datasets: ICVL Hand Posture Dataset <ref type="bibr" target="#b21">[22]</ref>, NYU Hand Pose Dataset <ref type="bibr" target="#b4">[5]</ref> and MSRA Hand Pose Dataset <ref type="bibr" target="#b22">[23]</ref>. Finally we conduct extensive experiments for ablation study to discuss the effectiveness and robustness of different components of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>ICVL Hand Posture Dataset <ref type="bibr" target="#b21">[22]</ref>. This dataset was collected from 10 different subjects using Intel's Creative Interactive Gesture Camera <ref type="bibr" target="#b48">[48]</ref>. In-plane rotations are applied to the collected samples and the final dataset contains 330k samples for training. There are totally 1596 samples in the testset, including 702 samples for test sequence A and 894 samples for test sequence B. The annotation of hand pose contains 16 joints, including 3 joints for each finger and 1 joint for the palm.</p><p>NYU Hand Pose Dataset <ref type="bibr" target="#b4">[5]</ref>. The NYU hand pose dataset was collected using three Kinects from different views. The training set contains 72757 frames from 1 subject and the testing set contains 8252 frames from 2 subjects, while one of the subjects in testing set doesn't appear in training set. The annotation of hand pose contains 36 joints. Following the protocol of previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b8">9]</ref>, we only use frames from the frontal view and 14 out of 36 joints in evaluation.</p><p>MSRA Hand Pose Dataset <ref type="bibr" target="#b22">[23]</ref>. The MSRA hand pose dataset contains 76500 frames from 9 different subjects captured by Intel's Creative Interactive Camera. The leave one subject out cross validation strategy is utilized for evaluation. The annotation of hand pose consists of 21 joints, with 4 joints for each finger and 1 joint for the palm. This dataset has large viewpoint variation, which makes it a rather challenging dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metric</head><p>There are two evaluation metrics widely used in hand pose estimation: per-joint errors and success rate. Denote {p ij } as the predicted joint locations of test frames, where i is the index of frame and j is the index of joint. {p gt ij } is the corresponding groundtruth label. N is the number of test frames and J is the number of joints in a frame.</p><p>Per-joint Errors. Average euclidean distance between predicted joint location and groundtruth for each joint over all test frames. The error for the j th joint is calculated by:</p><formula xml:id="formula_16">err j = i ( p ij − p gt ij ) N<label>(13)</label></formula><p>Average joint error err = j errj J is also used to evaluate the overall performance of hand pose estimation.</p><p>Success Rate. The fraction of good frames. A frame is considered as good if the maximum joint error of this frame is within a distance threshold τ . The success rate for distance threshold τ is calculated as Eq. 14.</p><formula xml:id="formula_17">rate τ = i 1(max j ( p ij − p gt ij ) ≤ τ ) N<label>(14)</label></formula><p>where 1(cond) is an indicate function that equals to one if cond is true and equals to zero otherwise.       <ref type="bibr" target="#b8">[9]</ref> 13.39 6.78 REN (9x6x6) <ref type="bibr" target="#b51">[51]</ref> 12.69 6.32 Ours 11.81 5.53</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Arts</head><p>To demonstrate the effectiveness of our proposed method, we compare it against several state-of-the-art methods, including latent random forest (LRF) <ref type="bibr" target="#b21">[22]</ref>, DeepPrior with refinements (HandsDeep) <ref type="bibr" target="#b16">[17]</ref>, cascaded hand pose regression (Cascaded) <ref type="bibr" target="#b22">[23]</ref>, feedback loop (Feedback) <ref type="bibr" target="#b17">[18]</ref>, deep hand model (DeepModel) <ref type="bibr" target="#b19">[20]</ref>, Lie group based method (Lie-X) <ref type="bibr" target="#b7">[8]</ref>, multi-view CNN (Multiview) <ref type="bibr" target="#b15">[16]</ref>, 3D-CNN based method (3DCNN) <ref type="bibr" target="#b10">[11]</ref> , CrossingNets <ref type="bibr" target="#b9">[10]</ref>, local surface normals (LSN) <ref type="bibr" target="#b23">[24]</ref>, occlusion aware method (Occlusion) <ref type="bibr" target="#b52">[52]</ref>, JTSC <ref type="bibr" target="#b50">[50]</ref>, global to local CNN (Madadi et al.) <ref type="bibr" target="#b18">[19]</ref> and region ensemble network with 9 × 6 × 6 region setting (REN-9x6x6) <ref type="bibr" target="#b51">[51]</ref>.</p><p>It should be noted that some reported results of stateof-the-art methods are calculated using the predicted labels that are available online <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b51">51]</ref> and others    are estimated from the figures and tables of the original papers <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b18">19]</ref>. We also compare our method with Mask R-CNN <ref type="bibr" target="#b49">[49]</ref> due to its impressive performance on RGB human pose estimation. For fair comparison, we first crop the depth images and resize them into 96 × 96, which is the same preprocessing as our proposed method. We use similar setting with human pose estimation task in <ref type="bibr" target="#b49">[49]</ref> that exploits ResNet-50-FPN as the backbone network. To adopt Mask R-CNN for depth-based hand pose estimation, we first use Mask R-CNN to detect 2D hand pose in image coordinates and then infer depth values from the original depth images to recover 3D hand pose. To alleviate the impact of noises and holes in depth images, the inferred depth values are constrained within the 3D cube of hand and valid depth values from 9-neighbours are averaged to get the final depth coordinate.</p><p>On NYU dataset, we compare our proposed method with <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b49">49]</ref>. The success rate with respect to the worse case criteria and per-joint errors are given in <ref type="figure" target="#fig_6">Figure 5</ref>. As shown in the figure, our proposed outperforms all state-of-the-art methods. We further compare the overall 2D and 3D mean joint error in <ref type="table" target="#tab_2">Table 1</ref>. Our method obtain 0.88mm 3D error decrease compared with existing best performance by REN <ref type="bibr" target="#b51">[51]</ref>. Mask R-CNN performs 2D keypoint detection and the post-processing is used to lift 2D pose to 3D pose. It achieves comparable 2D error with prior methods. Nevertheless, our Pose-REN outperforms Mask R-CNN and reduces the 2D error by 2.7 pixels.</p><p>On ICVL dataset, we compare our proposed method against <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b51">51]</ref>. Results in <ref type="figure" target="#fig_8">Figure 6</ref> demonstrate that our proposed method outperforms all other methods with a large margin. Compared with REN <ref type="bibr" target="#b51">[51]</ref>, our method reduces the mean error by 0.514mm, which is a 7.04% relative improvement.</p><p>On MSRA dataset, we compare with several state-ofthe-art methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b51">51]</ref>. The success rate with respect to maximum allowed threshold and per-joint errors are shown in <ref type="figure" target="#fig_9">Figure 7</ref>. Our method achieves the best performance among all evaluated methods. Following the protocol of previous works <ref type="bibr" target="#b22">[23]</ref>, we also report the mean joint errors distributed over yaw and pitch viewpoint angles, as shown in <ref type="figure" target="#fig_11">Figure 8</ref>. Our method achieves the smallest errors in almost all angles. It should be noted that the LSN <ref type="bibr" target="#b23">[24]</ref> get slightly smaller errors when the yaw or pitch angle is relatively small. However, the performance of LSN decreases rapidly when the viewpoint becomes larger. These results demonstrate that our method is much more robust to viewpoint changes, which is a quite challenging problem in hand pose estimation.</p><p>The fraction of good frames of our method decreases slightly compared with REN <ref type="bibr" target="#b51">[51]</ref> when the errors are larger than around 30mm. This is mainly due to worse initial pose for these challenging samples. When regarding to the per-joint errors, our method achieves the best performance among all compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section we will provide extensive experiments to discuss the contributions of different components of our method and the effect of some parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Effect of the Number of Iteration T</head><p>First we will discuss how the number of iteration T affects the performance. The average joint errors on NYU dataset with using the different number of iterations are shown in <ref type="figure" target="#fig_0">Figure 10</ref>. The error for iteration 0 is the result of the initialization. After one iteration, the error drops rapidly. As the iteration increases, the error becomes stable and finally converges. To better balance the computation complexity and performance, we choose the number of iteration as T = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Effect of Pose Guided Region Extraction</head><p>One of the contributions of our proposed method is to extract feature regions under the guidance of hand pose from previous stage. We will show whether this strategy helps to improve the performance of hand pose estimation. In REN <ref type="bibr" target="#b8">[9]</ref>, feature regions are extracted using a uniformly distributed grid. We report the performances of our method that only adopts one iteration and sets the number of regions and the size of regions the same as REN-4x6x6 <ref type="bibr" target="#b8">[9]</ref> and REN-9x6x6 <ref type="bibr" target="#b51">[51]</ref>. Under such experimental settings, the number of parameters of our method and REN are the same, which ensures fair comparison. The first number in the suffix indicates the number of regions and the last two numbers represent the size of regions. Specifically, we use the palm joint, the root joint of thumb, middle, pinky finger in 4x6x6 setting (denoted <ref type="table">Table 2</ref>: Comparing average joint errors of our method with and without structured region ensemble strategy on three datasets. The numbers in the brackets indicate the percentages of error reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Ours w/o structure Ours (mm) (mm) NYU <ref type="bibr" target="#b4">[5]</ref> 11.869 11.811(−0.5%) ICVL <ref type="bibr" target="#b21">[22]</ref> 6.932 6.793(−2.0%) MSRA <ref type="bibr" target="#b22">[23]</ref> 8.728 8.649(−0.9%)</p><p>as Our-4x6x6) and use all joints except for two joints in thumb finger and the tip joint of pinky finger in 9x6x6 setting (denoted as Our-9x6x6). The success rate curve and per-joint errors on NYU dataset are shown in <ref type="figure" target="#fig_13">Figure 9</ref>.</p><p>With different region settings, our method both performs better than REN that adopts grid region ensemble, indicating the contributions of pose guided region extraction strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Effect of Structured Region Ensemble</head><p>We will demonstrate the effectiveness of another component of our proposed method: the hierarchically structured region ensemble. We compare our method with a network (denoted as Ours w/o structure) that use two simple f c layers as is adopted in REN <ref type="bibr" target="#b8">[9]</ref> instead of hierarchical f c layers. For fair comparison, we set the dimensions of the two f c layers as 2304 and 2048 respectively to ensure the similar number of parameters between our method and Ours w/o structure. The mean joint errors on NYU, ICVL and MSRA dataset are shown in <ref type="table">Table 2</ref>. It can be seen that our method performs better than Ours w/o structure, which illustrates the effectiveness of the hierarchically structured region ensemble strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4.">Effect of the Initialization</head><p>In this section we will demonstrate the robustness of our proposed method over different initializations. Our proposed method builds upon the cascaded framework, which takes an initial hand pose as input and iteratively refine the results. To explore the impact of initialization for our methods, we conduct several experiments on NYU dataset with different initializations.</p><p>Firstly we will discuss the impact of initialization in inference phase. Specifically, we chose four methods as initialization: Init-CNN (which is proposed in <ref type="bibr" target="#b8">[9]</ref> as a baseline network and also adopted as the initialization of our method), DeepPrior <ref type="bibr" target="#b16">[17]</ref>, Feedback <ref type="bibr" target="#b17">[18]</ref>, DeepModel <ref type="bibr" target="#b19">[20]</ref>. The results of different initializations and refined results (denoted as, e.g. Ours init deepprior) are shown in <ref type="figure" target="#fig_0">Figure 11</ref>. We can observe that our method can considerably boost the performances of the initializations. Even with some rather rough initialization (e.g. DeepPrior), the refined results boosted by our Pose-REN are quite competitive. With other better initializations (Feedback, Deep-Model), the final results are similar to our method, even if their initializations are slightly worse than ours. These    results indicate the robustness over initializations of our method. It should be noted that the model used above were trained using the samples with our initialization (Init-CNN). We used different initializations in inference to get the results above. Therefore, the results above also demonstrate the generalization of our model. Furthermore, we consider the case that uses a natural pose (denoted as meanpose) as initialization and discuss which performance can be expected. We used the model that was trained on our initialization to refine the hand pose with meanpose as the initialization. As shown in <ref type="figure" target="#fig_0">Figure 12</ref>, the initial meanpose is very poor and the results are boosted by adopting our method. We empirically find that the performance converges after 10 stages (Ours init meanpose), resulting the average joint error of 17.708mm, which is comparable with some state-of-the-are methods, as shown in <ref type="table" target="#tab_2">Table 1</ref>. We further trained a model using the meanpose as initialization and report the refined results (Ours init meanpose train) in <ref type="figure" target="#fig_0">Figure 12</ref>. It can be seen that the results are quite close to those of our method that uses a better initialization, which indicates that our proposed method is robust to different initializations.</p><p>As discussed above, the model trained on our initialization greatly generalize to other initializations. Furthermore, for a very poor initialization, our proposed method can still obtain satisfying results by training a model using this initialization. <ref type="figure" target="#fig_0">Figure 13</ref> shows some examples of the iterative process on NYU dataset. The first column shows the results of the initialize hand pose, the second to fourth columns show the refined results on stage 1 − 3. The rightmost column is the groundtruth annotation. Our method gradually improves the estimated hand pose and obtains accurate results after several iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>Some qualitative results on three datasets can be seen in <ref type="figure" target="#fig_0">Figure 14</ref>. For each dataset, the first row represents the results of REN-9x6x6 <ref type="bibr" target="#b51">[51]</ref>, the second row shows the results of our proposed method and the third row is the groundtruth. It can be seen that our method performs better than REN even in some challenging samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we propose a novel method called pose guided structured region ensemble network (Pose-REN) for accurate 3D hand pose estimation from a single depth image. Our method extracts regions from the feature maps under the guidance of an initially estimated hand pose to attain more optimal and representative features. Feature     regions are then integrated hierarchically by adopting a tree-like structured connection that models the topology of hand joints. Our method iteratively refines the hand pose to obtain the final estimated results. Experiments on public hand pose datasets demonstrate that our proposed method outperforms all state-of-the-art methods. In our future work, we intend to further improve our method for robust and accurate 3D hand pose estimation when hands are interacting with other hands or objects. We would like to research on integrating hand detection and hand pose estimation into a unified framework, based on Faster R-CNN <ref type="bibr" target="#b54">[54]</ref> or Mask R-CNN <ref type="bibr" target="#b49">[49]</ref> etc. It will also be interesting to apply our proposed method for more articulated pose estimation tasks, like human pose estimation and face alignment. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The framework of our proposed pose guided structured region ensemble network (Pose-REN). A simple CNN (Init-CNN) predicts pose 0 as the initialization of the cascaded framework. Feature regions are extracted from the feature maps generated by a CNN under the guidance of pose t−1 and hierarchically fused using a tree-like structure. poset is the refined hand pose obtained by our proposed Pose-REN and will be used as the guidance in next stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The scheme of our proposed pose guided region extraction. The green and red dots represent two hand joints from previously estimated hand pose. The rectangles of different colors are the corresponding feature regions extracted from the feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of the proposed structured region ensemble method. Features from the joints of the same finger (including the palm joint) are fused first. Afterwards, features of different fingers are fused to regress the final hand pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Section 4.4.4, including generalization of our pre-trained model to other initializations in inference phase and robustness of Pose-REN to other initializations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The subset of joints used in pose guided region extraction. The joints circled by dash rectangles are used when extracting feature regions under the guidance of previous joints. Totally M = 11 joints are used, including a joint for the palm, two joints for the root and tip of each finger.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of our approach with state-of-the-art methods on NYU dataset. Left: the proportion of good frames over different error thresholds. Right: per-joint errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of our approach with state-of-the-art methods on ICVL dataset. Left: the proportion of good frames over different error thresholds. Right: per-joint errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of our approach with state-of-the-art methods on MSRA dataset. Left: the proportion of good frames over different error thresholds. Right: per-joint errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>40</head><label>40</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of mean error distance over different yaw (left) and pitch (right) viewpoint angles on MSRA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Effect of pose guided region ensemble by comparing our method against grid region ensemble (REN<ref type="bibr" target="#b8">[9]</ref>). Left: the proportion of good frames over different error thresholds. Right: per-joint errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>r a g e J o i n t E r r o r ( m m ) I t e r a t i o n Effect of the number of iteration on NYU dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :</head><label>11</label><figDesc>Performance of our model with different initial hand pose used in inference phase on NYU dataset. Left: the proportion of good frames over different error thresholds. Right: per-joint errors. within distance (%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 :</head><label>12</label><figDesc>Performance of our method when using mean pose as the initialization on NYU dataset. Left: the proportion of good frames over different error thresholds. Right: per-joint errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 13 :</head><label>13</label><figDesc>Qualitative results on NYU dataset of different stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quantitative evaluation of different methods on the benchmark NYU dataset for hand pose estimation task. We report 2D average pixel errors and 3D average joint errors in mm.</figDesc><table><row><cell>Methods</cell><cell cols="2">3D error (mm) 2D error (pixels)</cell></row><row><cell>HandsDeep [17]</cell><cell>19.73</cell><cell>9.81</cell></row><row><cell>Feedback [18]</cell><cell>15.97</cell><cell>8.20</cell></row><row><cell>DeepModel [20]</cell><cell>16.90</cell><cell>8.76</cell></row><row><cell>Mask R-CNN [49]</cell><cell>27.61</cell><cell>8.25</cell></row><row><cell>JTSC [50]</cell><cell>16.80</cell><cell>8.02</cell></row><row><cell>Madadi et al. [19]</cell><cell>15.60</cell><cell>-</cell></row><row><cell>Lie-X [8]</cell><cell>14.51</cell><cell>7.48</cell></row><row><cell>REN (4x6x6)</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vision-based hand pose estimation: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Twombly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="73" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Motion feature augmented recurrent neural network for skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2885" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vandeborre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Depth-based hand pose estimation: data, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1868" to="1876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shotton, Opening the black box: Hierarchical sampling optimization for estimating human hand pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3325" to="3333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatial attention deep net with partial pso for hierarchical hybrid hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lie-x: Depth image based articulated object pose estimation, tracking, and action recognition on lie groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Govindarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-017-0998-6</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Region ensemble network: Improving convolutional network for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Image Processing (ICIP</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crossing nets: Dual generative models with a shared latent space for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for efficient and robust hand pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1991" to="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth estimation for speckle projection system using progressive reliable points growing matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied optics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="516" to="524" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High-accuracy stereo matching based on adaptive ground control points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1412" to="1423" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Keselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Woodfill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grunnet-Jepsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhowmik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05548</idno>
		<title level="m">Intel realsense stereoscopic depth cameras</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3593" to="3601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hands deep in deep learning for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision Winter Workshop</title>
		<meeting>Computer Vision Winter Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training a feedback loop for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3316" to="3324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">End-to-end global to local cnn learning for hand pose recovery in depth data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09606</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Model-based deep hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2421" to="2427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d articulated hand posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3786" to="3793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hand pose estimation from local surface normals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="554" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to navigate the energy landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="323" to="332" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00596</idno>
		<title level="m">Learning to search on manifolds for 3d pose estimation of articulated objects</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust articulated-icp for real-time hand tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="101" to="114" />
			<date type="published" when="2015" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sphere-meshes for realtime hand modeling and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">222</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fits like a glove: Rapid and reliable hand shape personalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D. Joseph</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5610" to="5619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bordeaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Corish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">143</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Combining discriminative and model based approaches for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krejov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A collaborative filtering approach to real-time hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Hee</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2336" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast and robust hand tracking using detection-guided optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oulasvirta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3213" to="3221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">ICVL NYU MSRA REN Ours GT REN Ours GT REN Ours GT Figure 14: Qualitative results. For each dataset, three rows show the results from region ensemble network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Leichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>REN-9x6x6) [51], our method (Ours) and groundtruth (GT) respectively</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3633" to="3642" />
		</imprint>
	</monogr>
	<note>Accurate, robust, and flexible real-time hand tracking</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Realtime and robust hand tracking from depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1106" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Real-time articulated hand pose estimation using semi-supervised transductive regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3224" to="3231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Parsing the hand in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1241" to="1253" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="109" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01789</idno>
		<title level="m">Deep alignment network: A convolutional neural network for robust face alignment</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Rectifier nonlinearities improve neural network acoustic models</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modeling the constraints of human hand motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings. Workshop on</title>
		<imprint>
			<biblScope unit="page" from="121" to="126" />
			<date type="published" when="2000" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hand modeling, analysis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dynamics based 3d skeletal hand tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Keselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orsten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface</title>
		<meeting>Graphics Interface</meeting>
		<imprint>
			<publisher>Canadian Information Processing Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-task, multi-domain learning: application to semantic segmentation and pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fourure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trémeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="page" from="68" to="80" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Region ensemble network: Towards good practices for deep 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvcir.2018.04.005</idno>
		<ptr target="https://doi.org/10.1016/j.jvcir.2018.04.005" />
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representationdoi</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Occlusion aware hand pose recovery from sequences of depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carruesco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Andujar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzàlez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
	<note>Automatic Face &amp; Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust hand pose estimation by completing a matrix imputed with deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deephand</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4150" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
