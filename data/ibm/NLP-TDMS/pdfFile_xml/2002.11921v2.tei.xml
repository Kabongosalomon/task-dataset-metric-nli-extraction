<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RNNPool: Efficient Non-linear Pooling for RAM Constrained Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oindrila</forename><surname>Saha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research India</orgName>
								<orgName type="institution" key="instit2">‡ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
							<email>kusupati@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research India</orgName>
								<orgName type="institution" key="instit2">‡ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><forename type="middle">Vardhan</forename><surname>Simhadri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research India</orgName>
								<orgName type="institution" key="instit2">‡ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research India</orgName>
								<orgName type="institution" key="instit2">‡ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
							<email>prajain@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research India</orgName>
								<orgName type="institution" key="instit2">‡ University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RNNPool: Efficient Non-linear Pooling for RAM Constrained Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Standard Convolutional Neural Networks (CNNs) designed for computer vision tasks tend to have large intermediate activation maps. These require large working memory and are thus unsuitable for deployment on resource-constrained devices typically used for inference on the edge. Aggressively downsampling the images via pooling or strided convolutions can address the problem but leads to a significant decrease in accuracy due to gross aggregation of the feature map by standard pooling operators. In this paper, we introduce RNNPool, a novel pooling operator based on Recurrent Neural Networks (RNNs), that efficiently aggregates features over large patches of an image and rapidly downsamples activation maps. Empirical evaluation indicates that an RNNPool layer can effectively replace multiple blocks in a variety of architectures such as MobileNets, DenseNet when applied to standard vision tasks like image classification and face detection. That is, RNNPool can significantly decrease computational complexity and peak memory usage for inference while retaining comparable accuracy. We use RNNPool with the standard S3FD [50] architecture to construct a face detection method that achieves state-ofthe-art MAP for tiny ARM Cortex-M4 class microcontrollers with under 256 KB of RAM. Code is released at https://github.com/Microsoft/EdgeML.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks (CNNs) have become ubiquitous for computer vision tasks such as image classification and face detection. Steady progress has led to new CNN architectures that are increasingly accurate, but also require larger memory and more computation for inference. The increased inference complexity renders these models unsuitable for resource-constrained processors that are commonplace on the edge in IoT systems and battery-powered and privacy-centric devices.</p><p>To reduce inference complexity, several techniques like quantization <ref type="bibr" target="#b43">[44]</ref>, sparsification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>, cheaper CNN blocks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b21">22]</ref>, or neural architecture search <ref type="bibr" target="#b40">[41]</ref> have been proposed to train CNN models with lower inference cost and model size while retaining accuracy. However, these models still require large working memory for inference. Memory tends to be the most constrained resource on low power devices as it occupies a large fraction of the device die and has high sustained power requirement <ref type="bibr" target="#b23">[24]</ref>. Most low power ARM Cortex-M* microcontrollers have less than 256 KB RAM.</p><p>Typical CNNs have large intermediate activation maps, as well as many convolution layers, which put together require large amount of RAM for inference (see <ref type="bibr">Proposition 1)</ref>. A standard approach to reducing working memory is to use pooling operators or strided convolution to bring down size of the activation map. In fact, standard CNNs have multiple such layers. However, such pooling operators aggregate the underlying activation map in a simplistic manner, which can lead to a significant loss of accuracy. As a result, their use is limited to small receptive fields, typically no larger than 3 × 3, and they can not be used to aggressively reduce the activation map by aggregating larger receptive fields. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNNPool( r, c, k, h 1 , h 2 )</head><p>Hidden state size of RNN1 Hidden state size of RNN2 k #Input channels h 1 h 2 <ref type="figure" target="#fig_3">Figure 1</ref>: The RNNPool operator applied to patches of size r × c with stride s. It summarizes the patch with two RNNs into a vector of size 4h 2 .</p><p>In this paper, we propose a novel pooling operator RNNPool that uses Recurrent Neural Networks (RNNs) to perform a more refined aggregation over a large receptive field of the activation map without compromising on accuracy. RNNPool can be applied to any tensor structured problem, but we focus on 2D images for ease of exposition. For images, RNNPool uses RNNs to aggregate information along rows &amp; columns in a given patch. RNNPool has three parameters -patch size or receptive field, stride, and output dimension -to control its expressiveness and ability to downsample. The RNNPool operator matches standard pooling operators syntactically, so can be used to replace them in convolutional networks.</p><p>RNNPool allows rapid down-sampling of images and activation maps, eliminating the need for many memory-intensive intermediate layers. RNNPool is most effective when used to replace multiple CNN blocks in the initial stages of the network where the activation map sizes are large, and hence, require the most memory and compute. There, a single layer of RNNPool can down-sample by a factor of 4 or 8. For example, RNNPool applied to a 640 × 640 × 3 image with patch-size 16, stride 8, and 32 output channels results in a 80 × 80 × 32 activation map, which can be stored in about 200 KB, and can be computed one patch at a time without significant memory cost. Replacing a few blocks using RNNPool reduces peak memory requirement significantly for typical CNN architectures without much loss of accuracy.</p><p>Our experiments demonstrate that RNNPool can be used as an effective replacement for multilayered, expensive CNN blocks in a variety of architectures such as MobileNets, DenseNets, S3FD, and for varied tasks such as image classification and face detection. For example, in a 10-class image classification task, RNNPool+MobileNetV2 reduces the peak memory requirement of MobileNetV2 by up to 10× and MAdds (MAdds refers to Multiply-Adds as in MobileNetV2 <ref type="bibr" target="#b36">[37]</ref>) by about 25%, while maintaining the same accuracy. Additionally, due to its general formulation, RNNPool can replace pooling layers anywhere in the architecture. For example, it can replace the final average pool layer in MobileNetV2 and improve accuracy by ∼ 1%.</p><p>Finally, we modify the S3FD <ref type="bibr" target="#b49">[50]</ref> architecture with RNNPool to construct an accurate face detection model which needs only 225 KB RAM -small enough to be deployed on a Cortex-M4 based deviceand achieves 0.78 MAP on the medium category of the WIDER FACE dataset <ref type="bibr" target="#b46">[47]</ref> using 80× fewer MAdds than EXTD <ref type="bibr" target="#b47">[48]</ref> -a state-of-the-art resource-constrained face detection method.</p><p>In summary, we make the following contributions:</p><p>• A novel pooling operator that can rapidly down-sample input in a variety of standard CNN architectures, e.g., MobileNetV2, DenseNet121 while retaining the expressiveness. • Demonstrate that RNNPool can reduce working memory and compute requirements for image classification and Visual Wake Words significantly while retaining comparable accuracy. • By combining RNNPool with S3FD, we obtain a state-of-the-art face detection model for ARM</p><p>Cortex-M4 class devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Pooling: Max-pooling, average-pooling and strided convolution layers <ref type="bibr" target="#b28">[29]</ref> are standard techniques for feature aggregation and for reducing spatial resolution in CNNs. Existing literature on rethinking pooling <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10]</ref> focuses mainly on increasing accuracy and does not take compute/memory efficiency into consideration which is the primary focus of this paper.</p><p>Efficient CNN architectures: Most existing research on efficient CNN architectures aims at reducing model size and number of operations per inference. These methods include designing new architectures such as DenseNet <ref type="bibr" target="#b20">[21]</ref>, MobileNets <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref> or searching for them (ProxylessNAS <ref type="bibr" target="#b2">[3]</ref>, EfficientNets <ref type="bibr" target="#b40">[41]</ref>, SqueezeNAS <ref type="bibr" target="#b37">[38]</ref>). These architectures do not primarily optimize for the peak working memory, which is a critical constraint on devices powered by tiny microcontrollers. Previ-ous work on memory-optimized inference manipulates existing convolution operator by reordering computations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref> or performing them in place <ref type="bibr" target="#b12">[13]</ref>. However, most of these methods provide relatively small memory savings and are validated on low-resolution images like CIFAR-10 <ref type="bibr" target="#b24">[25]</ref>. Channel pruning <ref type="bibr" target="#b16">[17]</ref> is a method that tries to reduce memory requirement by pruning out multiple convolution kernels in every layer. While effective, channel/filter pruning does not tackle gradual spatial downsampling and thus is a complementary technique to RNNPool.</p><p>Visual Wake Words: Visual cues (visual wake word) to "wake-up" AI-powered home assistant devices require real-time inference on relatively small devices. Chowdhery et al. <ref type="bibr" target="#b5">[6]</ref> proposed a Visual Wake Words dataset and a resource-constrained setting to evaluate various methods. Section 5.2 discusses the efficient RNNPool based models and their performance for this task.</p><p>Face-detection on tiny devices: Recent work including EXTD <ref type="bibr" target="#b47">[48]</ref>, LFFD <ref type="bibr" target="#b17">[18]</ref>, FaceBoxes <ref type="bibr" target="#b48">[49]</ref> and EagleEye <ref type="bibr" target="#b51">[52]</ref> address the problem of accurate real-time face detection on resource-constrained devices. EXTD and LFFD are the most accurate but have high compute and memory requirements. On the other hand, EagleEye and FaceBoxes have lower inference complexity but also suffer from lower MAP scores. Face detection using RNNPool is discussed in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNNs for Computer Vision:</head><p>RNNs have been successful for sequential tasks but haven't been extensively explored in the context of computer vision. An early work, ReNet <ref type="bibr" target="#b41">[42]</ref>, uses RNN based layer as a replacement for a convolution layer but does not aim at improving efficiency. RNNPool contrasts with ReNet as follows:</p><p>a. ReNet is designed to replace a convolutional layer by capturing the global context and leaves the local context to be captured by flattening non-overlapping patches. RNNPool, on the other hand, uses overlapping patches and strongly captures local features and relies on subsequent standard convolutions to capture the global context. Hence, RNNPool and ReNet are complementary methods and can be combined. b. Semantically, RNNPool is a generalized pooling operator and can replace any pooling layer or strided convolution. However, ReNet does not correspond to any pooling abstraction, making it hard to combine with existing CNN models. For example, RNNPool can modify S3FD architecture to achieve state-of-the-art real-time face detection with &lt; 1 MB RAM while ReNet fails to fit in that context as a replacement layer since the receptive field of the output of ReNet layer varies across spatial positions. c. ReNet can still be used as a rapid downsampling layer. <ref type="table" target="#tab_3">Table 2</ref> shows that RNNPool outperforms ReNet with lower model size and fewer MAdds across datasets and architectures. E.g. ReNet+MobileNetV2 applied to ImageNet-1K is almost 4% less accurate than RNNPool +MobileNetV2, despite the same working RAM requirement and more MAdds per inference.</p><p>Inside-Outside Net <ref type="bibr" target="#b1">[2]</ref> uses a ReNet based layer for extracting context features in object detection while PiCANet <ref type="bibr" target="#b30">[31]</ref> uses it as a global attention function for salient object detection. L-RNN <ref type="bibr" target="#b44">[45]</ref> inserts multiple ReNet based layers but in a cascading fashion. See Appendix B for more discussion.</p><p>PolygonRNN <ref type="bibr" target="#b0">[1]</ref>, CNN-RNN <ref type="bibr" target="#b42">[43]</ref> and Conv-LSTM <ref type="bibr" target="#b45">[46]</ref> also use RNNs in their architectures but only to model certain sequences in the respective tasks rather than tackling pooling and efficiency.</p><p>3 What is RNNPool?</p><p>Consider the output of an intermediate layer in a CNN of size R × C × f , where R and C are the number of rows and columns and f is the number of channels. A typical 2 × 2 pooling layer (e.g. max or average) with stride 2 would halve the number of rows and columns. So, reducing dimensions by a factor of 4 would require two such blocks of convolutions and pooling. Our goal is to reduce the activation of size R × C × f to, say, R/4 × C/4 × f or smaller in a single layer while retaining the information necessary for the downstream task. We do so using an RNNPoolLayer illustrated in <ref type="figure" target="#fig_3">Figure 1</ref> that utilizes strided RNNPool operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The RNNPool Operator and the RNNPoolLayer</head><p>An RNNPool operator of size (r, c, k, h 1 , h 2 ) takes as input an activation patch of size r × c × k corresponding to k input channels, and uses a pair of RNNs -RNN 1 of hidden dimension h 1 and RNN 2 with hidden dimension h 2 -to sweep the patch horizontally and vertically to produce a summary of size 1 × 1 × 4h 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 RNNPool Operation</head><p>Input: X : [x1,1 . . . xr,c]; xi,j ∈ R k Output: RNNPool(X)</p><formula xml:id="formula_0">1: function FastGRNN(P, x) 2: [W, U, bz, b h ] ← P, h0 ← randn 3: for k ← 1 to length(x) do 4: z ← σ(Wx k + Uh k−1 + bz) 5:h k ← tanh(Wx k + Uh k−1 + b h ) 6: h k ← z h k−1 + (1 − z) h k 7:</formula><p>end for 8:</p><p>return hT 9: end function</p><formula xml:id="formula_1">10: RNNi(_) ← FastGRNN(Pi, _), for i ∈ {1, 2} 11: function RNNPool(X) 12: p r i ← RNN1(X i,1≤j≤c ), for all 1 ≤ i ≤ r 13: q r 1 ← RNN2(p r 1≤i≤r ) 14:p r ← reverse(p r ), q r 2 ← RNN2(p r 1≤i≤r ) 15: p c j ← RNN1(X 1≤i≤r,j ), for all 1 ≤ j ≤ c 16: q c 1 ← RNN2(p c 1≤j≤c ) 17:p c ← reverse(p c ), q c 2 ← RNN2(p c 1≤j≤c ) 18: return [q r 1 , q r 2 , q c 1 , q c 2 ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19: end function</head><p>Algorithm 1 describes the RNNPool operator wich applies two parallel pipelines to a patch and concatenates their outputs. In the first, RNN 1 traverses each row and summarizes the patch horizontally (Line 12) and then RNN 2 trverses the outputs of RNN 1 (Lines 13-14) bidirectionally. In the second pipeline RNN 1 first traverses along columns to summarize the patch vertically (Line 15) and then RNN 2 (Lines 16-17) summarizes bi-directionally.</p><p>While it is possible to use GRU <ref type="bibr" target="#b3">[4]</ref> or LSTM <ref type="bibr" target="#b18">[19]</ref> for the two instances of RNN in RNNPool, we use FastGRNN <ref type="bibr" target="#b25">[26]</ref> for its compact size and fewer MAdds (see Appendix H).</p><p>An RNNPoolLayer consists of a single RNNPool operator strided over an input activation map and takes as input two more parameters: patch size and the stride length. Note that there are only two RNNs (RNN 1 &amp; RNN 2 ) in an RNNPool operator, thus weights are shared for both the row-wise and column-wise passes (RNN 1 ) and all bidirectional passes (RNN 2 ) across every instance of RNNPool in an RNNPoolLayer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Probing the Efficacy of RNNPool</head><p>Capturing edges, orientations, and shapes: To demonstrate the capabilities of RNNs as spatial operators for vision tasks such as capturing edges, orientations, and shapes, we performed experiments on synthetic data. RNNPool learns how to capture edges, orientations, and shapes as effectively as convolutional layers which reinforces the choice of RNNs as spatial operators. Appendix C.1 provides further details of these experiments.</p><p>Comparing performance with pooling operators: We also performed experiments to contrast the down-sampling power of RNNPool against standard pooling operators on CIFAR-10 <ref type="bibr" target="#b24">[25]</ref>. As discussed in Appendix C.2, RNNPool significantly outperforms standard pooling operators in terms of accuracy. RNNPool can be used to modify standard CNN architectures and reduce their working memory as well as computational requirements. Typically, such modifications involve replacing one or more stacks of convolutions and pooling layers of the "base" (original) architecture with an RNNPoolLayer and retraining from scratch. We describe architecture modification strategies here and demonstrate their effectiveness through extensive experimentation in Section 5.</p><p>Replacement for a Sequence of Blocks: Consider the DenseNet121 <ref type="bibr" target="#b20">[21]</ref> architecture in <ref type="figure">Figure 2</ref>. It consists of one convolutional layer, followed by repetitions of "Dense" (D), "Transition" (T) and "Pooling" (P) blocks which gradually reduce the size of the image while increasing the number of channels. Of all these layers, the first block following the initial convolutional layer (D1) requires the most working memory and compute as it works on large activation maps that are yet to be down-sampled. Further, the presence of 6 layers within each dense block makes it harder to work with small memory (see Proposition 1). This is also true of other architectures such as MobileNetV2, EfficientNet, and ResNet. We can use an RNNPoolLayer to rapidly down-sample the image size and bypass intermediate large spatial resolution activations. In DenseNet121, we can replace 4 blocks -P1, D1, T1, D2 -spanning 39 layers with a single RNNPoolLayer to reduce the activation map from size 112 × 112 × 64 to 28 × 28 × 128 (see <ref type="figure">Figure 2</ref>). The replacement RNNPoolLayer can be executed patch-by-patch without re-computation, thus reducing the need to store the entire activation map across the image. These two factors greatly reduce the working memory size as well as the number of computations. DenseNet121-RNNPool achieves an accuracy of 94.8% on ImageNet-10 (see Appendix A for dataset details) which is comparable to 95.4% of the original DenseNet121 model.</p><p>A similar replacement of functional blocks with RNNPoolLayer can be performed for MobileNetV2 as specified in <ref type="table" target="#tab_2">Table 10</ref> of Appendix F, and leads to a similar reduction in the size of the largest activation map while retaining accuracy. These results extend to other networks like EfficientNet, ResNet and GoogLeNet <ref type="bibr" target="#b39">[40]</ref>, where residual connection based functional blocks in the initial parts can be effectively replaced with the RNNPoolLayer with improvements in working memory and compute, while retaining comparable accuracy. These results are listed in <ref type="table" target="#tab_2">Table 1</ref>. Appendix H presents further ablation studies on RNNPool and its base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replacement for Pooling Layers:</head><p>RNNPool has the same input and output interface as any pooling operator and hence, RNNPoolLayer can replace any standard pooling layer while providing more accurate aggregation. For example, DenseNet121-RNNPool has three pooling layers one each in T2, T3, and the final average pool layer. <ref type="table" target="#tab_2">Table 1</ref> shows that, on ImageNet-10, DensetNet121-RNNPool loses 0.6% accuracy compared to its base model. But, replacing all three remaining pooling layers in DenseNet121-RNNPool with a RNNPoolLayer results in almost the same accuracy as the base DenseNet121 but with about 2× and 4× lower compute and RAM requirement respectively. We can further drop 14 dense layers in D3 and 10 layers in D4 to bring down MAdds and RAM requirement to 0.79G MAdds and 0.43 MB, respectively, while still ensuring 94.2% accuracy.</p><p>Replacement in Face Detection models: As in the above architectures, we can use RNNPoolLayer to rapidly down-sample the image by a factor of 4 × 4 in the early phase of an S3FD face detector <ref type="bibr" target="#b49">[50]</ref>. The resulting set of architectures (with different parameters) are described in Appendix F.2. For example, the RNNPool-Face-Quant model has a state-of-the-art MAP for methods that are constrained to at most 256 KB of working RAM <ref type="table" target="#tab_5">(Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference memory requirements:</head><p>Computing exact memory and compute requirement of a large CNN model is challenging as the execution order of activations in various layers can be re-organized to trade-off memory and compute. For example, in the memory-optimized column of <ref type="table" target="#tab_2">Table 1</ref> we present the compute usage of a variety of baseline architectures when their execution order (EO) is restricted to using no more memory than the corresponding RNNPool based architecture. That is, we identify the memory bottleneck layers in various architectures whose activation map size is almost same as that of the corresponding RNNPool-based model. We then compute every voxel of this layer by re-computing the required set of convolutions, without storing them. CNNs, in general, have significant compute requirement and such re-compute intensive optimizations make the architecture infeasible even for large devices, e.g. DenseNet121 requires 24.41G MAdds in this scheme ( <ref type="table" target="#tab_2">Table 1)</ref>.</p><p>A standard approach is to restrict execution orders that do not require any re-computation of intermediate activation maps. A straightforward and standard EO is the one where the computation is executed layer-by-layer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref>. The memory requirement of such a scheme would correspond to the largest activation map in the architecture, except the output of 1x1 convolution layers which can be computed on the fly. This approach mimics the memory requirement of existing platforms like TF-lite <ref type="bibr" target="#b10">[11]</ref> and is proposed as a standard benchmark for comparing resource-constrained inference methods <ref type="bibr" target="#b5">[6]</ref>. Following this prior convention, we list the inference complexity for various architectures under the compute-optimized columns in <ref type="table" target="#tab_2">Table 1</ref>, unless the operation is easy to compute on the fly like 1x1 convolution or patch-by-patch computation of RNNPool. Appendix E.2 provides more details about these calculations.</p><p>The above scheme is easy to implement and allows an inference pipeline that is more modular and easy to debug and could allow faster inference on neural network accelerators <ref type="bibr" target="#b22">[23]</ref>. But, in principle, one can design execution orders (EO) that do not re-compute any intermediate layers, but are still not required to store entire activation maps, especially the largest ones. So, a rigorous quantification of the memory requirement of a model (without any re-compute) needs to show that any valid execution order requires a certain amount of working memory at some point in its execution, and also demonstrate a valid EO with the same memory requirement as a matching upper bound. We achieve this with the following proposition, whose proof and corollaries are in Appendix D.</p><p>Proposition 1 Consider an l-layer (l &gt; 1) convolutional network with a final layer of size m × n.</p><p>Suppose the for each node in the output layer, the size of receptive field in intermediate layer q ∈ [l−1] is (2k q + 1) × (2k q + 1), k q &gt; 0 and that this layer has c q channels and stride 1. Any serial execution order of this network that disallows re-computation requires at least 2 l−1 q=1 c q k q ×min(m−1, n−1) memory for nodes in the intermediate layers.</p><p>The above proposition shows that for a CNN with receptive field k q at the q-th layer, the memory requirement scales linearly with the height/width of the activation map and with the number of layers. As networks like MobileNetV2 or DenseNet have blocks with a significant number of convolution layers and large receptive field, this proposition implies that it is not possible to significantly reduce the memory requirement over the standard layer-by-layer approach. For example, our un-optimized calculations for RNNPool architectures still give us 3 − 4x reduction in peak RAM usage when compared to the minimum RAM requirement of the corresponding base architecture (see Appendix E.1). Further, similar optimization can be applied to RNNPool based architectures, so the relative reduction in memory by RNNPool does not change significantly. The implications of the above proposition, i.e., the peak memory of various networks without re-compute is calculated in Appendix E.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation of RNNPool on Vision Tasks</head><p>We present empirical evidence that RNNPool operator is compatible with popular CNN architectures for vision tasks, and can push the envelope of compute/memory usage vs accuracy curve. Further, we show that RNNPool combined with MobileNetV2 <ref type="bibr" target="#b36">[37]</ref> generates accurate models for Visual wake words and face detection problems that can be deployed on tiny Cortex-M4 microcontrollers. See Appendix G for more details about model training and hyperparameters used for the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">RNNPool for Image Classification</head><p>We first focus on ImageNet-10, a 10 class subset of ImageNet-1K <ref type="bibr" target="#b6">[7]</ref> where the classes correspond to the categories in CIFAR-10 <ref type="bibr" target="#b24">[25]</ref>. We study this dataset because in several realistic tiny devices scenario, like intrusion detection, we are interested in identifying the presence/absence of a few, rather than 1000, classes of objects. The dataset is divided into 1300 images for training and 50 for validation per class. More details and rationale about the dataset can be found in the Appendix A. <ref type="table" target="#tab_3">Table 2</ref> compares RNNPoolLayer against other standard pooling operators as used in MobileNetV2 and DenseNet121 base networks (see Appendix F.1 for description of the architecture). It shows that with the same memory usage, RNNPool is up to 4% more accurate than the standard pooling operators. While standard pooling operators are cheaper than RNNPool, the overall compute requirement of RNNPool based architectures is similar to pooling based architectures. Furthermore, replacing the last average pooling layer in the base network with RNNPool further increases accuracy, thus demonstrating the flexibility of RNNPoolLayer. <ref type="table" target="#tab_3">Table 2</ref> also contrasts RNNPool with ReNet <ref type="bibr" target="#b41">[42]</ref> as a downsampling layer. We observe that RNNPool is a much better alternative for downsampling layers in terms of accuracy (better by up to 2%), model size, and MAdds for the same amount of working memory.</p><p>Next, we study the compatibility of RNNPool with different architectures. <ref type="table" target="#tab_2">Table 1</ref> shows that RNNPool based architectures maintain the accuracy of base models while significantly decreasing memory and compute requirement. See Section 4 and Appendix E for a discussion on the calculation of memory and compute requirements of different models.  <ref type="table" target="#tab_4">Table 3</ref> presents results on the complete ImageNet-1K <ref type="bibr" target="#b6">[7]</ref> dataset with MobileNetV1 and MobileNetV2 as the base architectures. ReNet and RNNPool based models are constructed in a manner similar to the models in <ref type="table" target="#tab_2">Table 1</ref>. See <ref type="table" target="#tab_2">Table 10</ref> for the complete specification of the MobileNetV2+RNNPool model. MobileNetV1+RNNPool model is constructed similarly with h 1 = h 2 = 16. Consistent with the results on ImageNet-10, RNNPool retains almost same accuracy as the base models while decreasing memory usage significantly. Furthermore, RNNPool based models are also 3 − 4% more accurate than ReNet based models. In this work, we focus on state-of-the-art resource-constrained models that do not require neural architecture search (NAS); we leave extension of RNNPool for NAS based architectures like EfficientNets <ref type="bibr" target="#b40">[41]</ref> for future work. The Visual Wake Words challenge <ref type="bibr" target="#b5">[6]</ref> presents a relevant use case for computer vision on tiny microcontrollers. It requires detecting the presence of a human in the frame with very little resources -no more than 250 KB peak RAM usage and model size, and no more than 60M MAdds/image. The existing state-of-the-art method <ref type="bibr" target="#b5">[6]</ref> is MobileNetV2-0.35× with 8 channels for the first convolution and 320 channels for the last convolution layer. We use this as our baseline and replace convolutions with an RNNPoolLayer. After training a floating-point model with the best validation accuracy, we perform per-channel quantization to obtain 8-bit integer weights and activations. <ref type="table" target="#tab_3">Table 2</ref> compares the accuracy of the baseline and new architectures on this task. Replacing the last average pool layer with RNNPool increases the accuracy by ≥ 1%. Inserting RNNPool both at the beginning of the network and at the end provides a model whose accuracy is within 0.6% of the  <ref type="figure" target="#fig_0">Figure 3</ref> shows that RNNPool models are significantly cheaper during inference in terms of compute and memory while offering the same accuracy as the baselines. For example, peak memory usage of MobileNetV2-0.35× with the lowest resolution images is ∼40 KB, while our model requires only 34 KB RAM despite using the highest resolution image and providing ∼4% higher accuracy. Note that ProxylessNAS <ref type="bibr" target="#b13">[14]</ref> was the winner of the Visual Wake Words challenge. We report it's accuracy on the final network provided by the authors. To be consistent, we train the model only on the training data provided, instead of pretraining with ImageNet-1K used by ProxylessNAS in the wake word challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RNNPool for Visual Wake Words</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">RNNPool for Face Detection</head><p>We experiment with multiple architectures we call RNNPool-Face-* for face detection suggested in Section 4 and described in greater detail in Appendix F.2. We train and validate these architectures with the WIDER FACE dataset <ref type="bibr" target="#b46">[47]</ref>. Versions Quant, A, B, and C of the RNNPool-Face use RNNPoolLayer of hidden dimensions 4, 4, 6 and 16, respectively. <ref type="table" target="#tab_5">Table 4</ref> compares validation Mean Average Precision (MAP) for easy, medium, and hard subsets. MAP is a standard metric for face detection and measures the mean area under the precision-recall curve. We report MAP scores for baselines based on the official open-source code or pre-trained models. For Eagle-Eye <ref type="bibr" target="#b51">[52]</ref>, we re-implemented the method as the source code was not available. For EXTD <ref type="bibr" target="#b47">[48]</ref>, we report MAdds of the EXTD-32 version -the computationally cheapest. EXTD and LFFD <ref type="bibr" target="#b17">[18]</ref> are accurate but are computationally expensive. In contrast, RNNPool-Face-C achieves better MAP in the easy and medium subsets despite using ∼ 4.5× less compute and ∼ 3× less RAM.</p><p>FaceBoxes <ref type="bibr" target="#b48">[49]</ref> and Eagle-Eye reduce MAdds and peak memory usage by aggressively downsampling the image or by decreasing the number of channels leading to inaccurate models. In contrast, RNNPool-Face-A and RNNPool-Face-B achieve significantly higher MAPs than these methods while still ensuring smaller MAdds and peak RAM usage. We also compare MAP scores for images that have ≤ 3 faces, which is a more realistic face-detection setting for tiny devices. Here also, RNNPool-Face-C is more accurate than all the baselines.  To not overshoot RAM for storing input image, we use 320×240×1 monochrome images for training and testing. For evaluation, we first train on the WIDER FACE dataset and then fine-tune on the SCUT-HEAD dataset <ref type="bibr" target="#b34">[35]</ref> which consists of images in conference/class rooms. We then use the SeeDot <ref type="bibr" target="#b11">[12]</ref> compiler to quantize our model to 8 bits and generate C code for deployment. <ref type="table" target="#tab_7">Table 5</ref> compares the resource requirements and MAP on the SCUT-HEAD validation set (random 80%-20% split) of RNNPool-Face-M4 against a similarly trained MobileNetV2-SSDLite model which is a state-of-the-art architecture for low-cost detection.</p><p>Note that MobileNetV2-SSDLite cannot be deployed on a Cortex-M4 device even with 8-bit quantization as the peak RAM requirement is much more than the 256 KB limit of the device. RNNPool-Face-M4 model processes a single image in 10.45 seconds on an ARM Cortex-M4 microcontroller based STM32F439-M4 device clocked at 168 MHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we proposed RNNPool, an efficient RNN-based pooling operator that can be used to rapidly downsample activation map sizes thus significantly reduce inference-time memory and compute requirements for a variety of standard CNNs. Due to syntax level similarity with pooling layers, we can use RNNPool in most existing CNN based architectures. These replacements retain accuracy for tasks like image classification and visual wake words. Our S3FD based RNNPool model for face detection provided accurate models that can be deployed on tiny Cortex-M4 microcontrollers. Finally, we showed with Proposition 1 that calculations of minimum memory requirement for standard architectures can be made rigorous and demonstrate that despite such optimizations of standard CNNs, RNNPool based models can be significantly more efficient in terms of inferencetime working memory. Using neural architecture search for RNNPool based models to further reduce inference cost is an immediate and interesting direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Pros: ML models are compute-intensive and are typically served on power-intensive cloud hardware with a large resource footprint that adds to the global energy footprint. Our models can help reduce this footprint by (a) allowing low power edge sensors with small memory to analyze images and admit only interesting images for cloud inference, and (b) reducing the inference complexity of the cloud models themselves. Further, edge-first inference enabled by our work can reduce reliance on networks and also help provide privacy guarantees to end-user. Furthermore, vision models on tiny edge devices enables accessible technologies, e.g., Seeing AI <ref type="bibr" target="#b32">[33]</ref> for people with visual impairment.</p><p>Cons: While our intentions are to enable socially valuable use cases, this technology can enable cheap, low-latency and low-power tracking systems that could enable intrusive surveillance by malicious actors. Similarly, abuse of technology in certain wearables is also possible.</p><p>Again, we emphasize that it depends on the user to see the adaptation to either of these scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Information</head><p>A.1 ImageNet-10 The ImageNet-10 is a subset of images from ILSVRC 2012 ImageNet-1K dataset <ref type="bibr" target="#b35">[36]</ref> of 1000 classes. All images corresponding to the 10 classes from CIFAR-10 as listed in <ref type="table" target="#tab_8">Table 6</ref> are sampled from the full dataset. The classes in CIFAR-10 are: airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck.</p><p>The class n02430045: 'deer' is not present in the ImageNet-1K subset and was scraped from the full ImageNet-22K database <ref type="bibr" target="#b6">[7]</ref>. Each class is divided into 1300 images for training and 50 images for validation.</p><p>Typical on-device models for real-world applications deal with limited classes (e.g. intruder detection). ImageNet-10 is a good proxy for this task with medium resolution natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Visual Wake Words</head><p>This is a binary classification dataset <ref type="bibr" target="#b5">[6]</ref> dealing with the presence and absence of a person in the image. The dataset is derived by re-labeling the images available in the MS COCO dataset <ref type="bibr" target="#b29">[30]</ref> with labels corresponding to whether a person is present or not. The training set has 115K images and the validation set has 8K images. The labels are balanced between the two classes: 47% of the images in the training dataset of 115k images are labeled as 'person'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 WIDER FACE</head><p>This is a face detection dataset <ref type="bibr" target="#b46">[47]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 SCUT HEAD</head><p>This is a head detection dataset <ref type="bibr" target="#b34">[35]</ref>. We use PartB of this dataset for our experiments. PartB includes 2405 images with 43940 heads annotated. 1905 images of PartB are for training and 500 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RNN as a spatial operator and comparison with ReNet</head><p>Since ReNet <ref type="bibr" target="#b41">[42]</ref>, there have been a few methods that have been built upon it to solve various vision tasks. The fundamental difference, mathematically, between these approaches, and ours is how the RNN is used to extract spatial information. In ReNet based methods, the RNN is used to find a pixel-wise mapping from a voxel of the input activation map to that of the output map. However, in our method, we are using RNNs to spatially summarize a big patch of the input activation map to a 1×1 voxel of the output activation map. Note that in ReNet the hidden states of every timestep of RNN contribute to one voxel of the output, whereas in our case only the last hidden states of the traversals are taken for both row/column-wise summarizations and bidirectional summarizations.</p><p>ReNet based approaches either insert RNN based layers in existing networks or replace a single convolution layer (thus resulting in increasing computations). In ReNet, the RNNs are applied over the whole input map, whereas RNNPool is applied patch by patch, which is semantically similar to a pooling operator. Our usage of RNN for spatial information extraction is so powerful that we can eliminate a large amount of RAM and compute heavy convolution layers and still preserve accuracy.  <ref type="formula">(1)</ref>. Some test images (plotted using black and brown dots) were modified by randomly permuting rows and columns.</p><p>For ReNet to do the same, patches of size equal to the stride have to be flattened to construct an input to the RNN, which makes it further inefficient in terms of compute and parameters and results in loss of spatial dependencies. RNNPool results in a decrease in computations and parameters while ReNet based methods will increase the same with respect to the baseline model. The comparisons in <ref type="table" target="#tab_3">Table 2</ref> &amp; 3 show that ReNet in fact results in a significant loss in accuracy too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Probing the Efficacy of RNNPool</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Capturing Edges, Orientations and Shapes</head><p>To probe RNNPool's efficacy at capturing edges, orientation, and shapes, we attempt to fit an RNNPool operator to the following synthetic datasets of small 8-bit monochrome images with background noise as shown in <ref type="figure">Figure 4</ref>. We conduct experiments on synthetic datasets to prove that RNNPoolLayer can learn spatial representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">A multi-class dataset consisting of images with one line segment of varying lengths and positions.</head><p>There are 9 classes corresponding to lines ranging from 0 to 160°at 20°intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A multi-label dataset with images consisting of multizple line segments with varying lengths and positions. There are 9 labels corresponding to lines with orientations of 0 to 160°at 20°intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>A multi-label dataset consisting of images with a subset of shapes (5 in total) -circle, triangle, square, pentagon, and hexagon. We sweep over the h 1 , h 2 parameters in powers of 2 for the smallest RNNPool operator that can enable a single FC layer to classify or label the test set with 100% accuracy. We do so with and without a preceding CNN layer of 8 convolutions of 3 × 3 size and stride 2. <ref type="table" target="#tab_9">Table 7</ref> lists the least h 1 , h 2 required for each task. We observe that a single RNNPool module fits to 100% accuracy on all these datasets.</p><p>We conclude that the horizontal and the vertical passes of the RNN allows a single RNNPool operator to capture the orientation of edges and simple shapes over patches of size up to 64 × 64. Further, adding a single convolutional layer before the RNNPool layer makes the model much more parameter efficient. In effect, the convolution layer detects gradients in a local 3 × 3 patch, while the RNNPool detects whether gradients across 3 × 3 patches aggregate into a target shape.</p><p>Further, we use multi-dimensional scaling <ref type="bibr" target="#b31">[32]</ref> to visualize the 4 · h 2 = 128 dimensional output of RNNPool operator on the multi-class dataset (1) in <ref type="figure">Figure 4</ref> (left). Dataset (1) consists of various lines in the image at a discrete set of angles, and the classification task is to detect the angle of the line. Some images from the test set of classes 80°and 100°are multiplied with a permutation matrix to randomly permute rows and columns. These resulting images are added to the original test dataset and the output of the RNNPool is plotted in <ref type="figure">Figure 4</ref> (right). The outputs for each class form well-separated tight clusters indicating RNNPool indeed learns various orientations, while the outputs for the permuted images are scattered across the plot indicating that it is not exploiting certain gross aggregations in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Comparing Performance with Pooling Operators</head><p>We now contrast the down-sampling power of RNNPool against standard pooling operators. That is, we investigate if the pooling operators maintain accuracy for a downstream task even when the pooling receptive field is large. To this end, we consider the image classification task with CIFAR-10 dataset <ref type="bibr" target="#b24">[25]</ref> but the pooling operator is required to down-sample the input 32 × 32 image to a 1 × 1 voxel in one go i.e. both patch size and stride are 32. This is followed by a fully connected (FC) layer. The number of output channels after pooling was ensured to be the same. For Max and Average pooling models, a 1 × 1 convolution is used to ensure the same output dimension. For this task, RNNPool achieves an accuracy of 70.63%, while the convolution layer, max pooling, and average pooling's accuracy are 53.13%, 20.04% and 26.53%, respectively. This demonstrates the modeling power of the RNNPool operator over other pooling methods. Details. We use h 1 = h 2 = 32 for the RNNPool operator with patch size and stride as 32. For the strided convolution we use a convolution layer of 4 × h 2 = 128 filters. For Max and Average pooling first we pool down to 1 × 1 × 3 from input of 32 × 32 × 3 and then use a 1 × 1 convolution of 128 filters. All the above have the same patch size and stride size and are followed by a fully connected layer projection to 10 from 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Lower bounds on space required for multi-layer networks</head><p>We now lower bound the memory requirements of computation of multi-layer convolutional networks when recomputation is not permitted. Suppose we have an l-layer (l &gt; 1) convolutional network. Let With this claim, we are ready to prove Proposition 1.</p><p>Proof of Proposition 1. Fix any execution order of the network, and label the nodes in the final layer Y in the order they are evaluated: (p 1 , q 1 ), (p 2 , q 2 ), . . . , (p mn , q mn ). That is y p1,q1 is evaluated before y p2,q2 and so on. Let</p><formula xml:id="formula_2">I t = ∪ t τ =1 p τ , J t = ∪ t τ =1 q τ , and t * = min t {|I t | = m or |J t | = n}.</formula><p>That is, once y p t * ,q t * is executed, either (a) at least one node in each row of the final layer has been executed, or (b) at least one node in each column of the final layer has been executed, and at the moment y p t * −1 ,q t * −1 is computed, there is an entire row, say r, and an entire column, say c, in the final layer where no nodes have been executed.</p><p>Suppose that case (b) holds. Then, at step t * − 1, nodes in n − 1 columns [n] \ {c} have been executed, and in each column, at least one row has not been executed. By Claim 1, each such column would need to have 2k q activations at layer q in memory at this point of execution, and all these nodes are unique (that the nodes required to be in memory by Claim 1 for different columns are non-overlapping). Therefore, at least 2 l−1 q=1 c q k q ×(n−1) memory is required to hold the necessary nodes in each intermediate layer for this execution.</p><p>A similar analysis of case (a) yields a lower bound of 2 l−1 q=1 c q k q × (m − 1) from which the lemma follows.</p><p>If convolution operators have a stride larger than 1, then we can similarly state the following claim based on the overlap between the nodes in an intermediate layer that are common dependencies across two consecutive rows/columns of the output.</p><formula xml:id="formula_3">Claim 2 Fix column j ∈ [n]. Suppose that nodes y i,j , i ∈ I [m]</formula><p>have been completed at some point in an execution order. Suppose that the stride at layer q is s q . Restrict s q to 1 in a layer with 1 × 1 convolutions, i.e., assume activations are not simply thrown away. Then at the same point in the execution order, at least k = 2k + 1 − Π l r=q s r contiguous activations x (l)</p><formula xml:id="formula_4">i * − k /2 +1,j , x (l) i * −k+2,j , . . . x (l)</formula><p>i * + k /2 ,j for some i * ∈ [m] will need to be saved in memory until another node from column j is computed.</p><p>This allows us to restate Proposition 1 in networks where stride is greater than 1.</p><p>Proposition 2 Consider an l-layer (l &gt; 1) convolutional network with a final layer of size m × n. Suppose the for each node in the output layer, the size of receptive field in intermediate layer q ∈ [l−1] is (2k q + 1) × (2k q + 1), k q &gt; 0 and that this layer has c q channels and stride s q . Restrict s q to 1 in a layer with 1 × 1 convolutions. Suppose that k q = 2k q + 1 − Π l−1 r=q s r . Any serial execution order of this network that disallows re-computation requires at least l−1 q=1 c q k q × min((Π l−1 r=q s r )m − 1, (Π l−1 r=q s r )n − 1) memory for nodes in the intermediate layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 3</head><p>The lower bound in Proposition 1 is matched by an execution order that computes the network in a row or column-first order, whichever is smaller. That is, execute all the intermediate nodes needed to compute the first row of the output, retain those intermediate nodes required for the calculation of the second row of the output, compute the second row of output, and so on. Let S q = Π l r=q s r , and restrict s q to 1 in a layer with 1 × 1 convolutions. This schedule has a memory requirement of l−1 q=1 c q (2k q + 1 − S q )min(Q q m − 1 + 2k q , S q n − 1 + 2k q ) if we account for the padding at either ends of the row in each intermediate layer, and</p><formula xml:id="formula_5">l−1 q=1 c q (2k q + 1 − S q )min(S q m − 1, S q n − 1),</formula><p>if the padding is not counted.</p><p>Claim 4 Suppose we follow the row (or column)-wise execution order in Claim 3, and that each row in the output depends on k 0 layers at the input. Suppose that the input is required to be in memory before the start of the execution and the output is required to be in memory at the end of the execution. Let c in and c out denote the number of channels in the input and output. Let S q = Π l r=q s r , and let k 0 = k 0 − S 1 be the number of rows/columns in the input layer that are common dependencies between two consecutive rows/columns of the output. The memory requirement including those of the input and output layers is max{m in n in c in +k 0 n out c out , m out n out c out +k 0 n in c in }+ l−1 q=1 c q (2k q +1−S q )min(S q m−1, S q n−1), with padding added on the fly for convolutions at the boundaries of activation maps. This is obtained by reclaiming the footprint of the input for the output one row at time (with a lag of k 0 rows) once all the nodes that depend on it are completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Details about Compute and Peak RAM Calculation</head><p>In this section, we quantify the memory requirements of the networks analyzed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Optimal memory requirements without recomputation</head><p>First, we analyze the minimum memory requirements and optimal execution orders of componentsinverted residual block, separable residual block, dense block, and inception block -assuming that no re-computation is allowed. That is, we wish to find the minimum value, over all valid execution orders E of the block, of the maximum memory requirement of the execution order. Then, we analyze the memory requirement of image classification architectures discussed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.1 Memory requirements of various block</head><p>We assume that the execution always starts with the input of the block in memory, and terminates with output in memory. We denote that the size of input I is h in × w in × C, where h in and w in are the height and the width of the activation and c in is the number of channels. Likewise, denote the size of O to be h out × w out × c out . In what follows, suppose also that h in ≥ w in and h out ≥ w out . Otherwise we can flip rows and columns and meet the same constraints.</p><p>1. Inverted bottleneck residual block (a.k.a. MBConv, see <ref type="figure" target="#fig_0">Fig. 3b</ref> of <ref type="bibr" target="#b36">[37]</ref>) : The first layer is a point-wise convolution (C1) that expands the number of channels to c in × t where t is expansion factor. Then there is a depth-wise separable 3 × 3 convolution (C2) with stride either 1 or 2, followed by another point-wise convolution (C3) which reduces the number of output channels. We can use the row-wise order suggested in Claim 4, which results in a schedule where the first row of the output is generated, then the second row and so on. This schedule has a memory footprint of max{h in w in c in + (3 − s)w out c out , h out w out c out + (3 − s)w in c in } + (3 − s)tc in w in , where s is the stride of the 3 × 3 convolution. 2. Residual Block (see <ref type="figure" target="#fig_5">Fig. 5</ref>(left) of <ref type="bibr" target="#b15">[16]</ref>) : We consider a residual block consisting of two convolution layers with 3 × 3 kernels, of which the first has a stride s of 1 or 2, and the second has stride 1. The we have w out = w in /s and h out = h in /s. Using Claim 4, we can see that the best case memory footprint is max{h in w in c in + (5 − s)w in c out /s, h in w in c out /s 2 + (5 − s)w in c in } + 2w in c out /s, assuming that the number of channels of intermediate layer is equal to c out as is the norm here.</p><p>3. Inception block (see <ref type="figure">Fig. 2b</ref> of <ref type="bibr" target="#b39">[40]</ref>): Denote the output of each of the 4 paths in the block by O 1 , O 2 , O 3 and O 4 . We consider the case where all convolutions are of stride 1. We can apply the arguments of Section D simultaneously for all four paths with slight modification. We consider a minimal set of contiguous rows at the start of the input -which would be first 5 row in the referenced image as its the largest convolution size -and compute all channels in the first row of the output of all four paths. We then drop the first row of input, materialize the second row of output on all four paths and so on. If we denote by c out the number of output channels of all four networks, then the memory requirement is  <ref type="figure">Fig. 4</ref> of URL) : At any point in the execution of a dense block, we need to store the input to the dense block and outputs of all previous dense layers, since the last layer needs all the activation maps concatenated as its input. The total activation maps being stored will reach the peak just after the last dense layer. Therefore the peak memory requirement is the output of the dense block.</p><formula xml:id="formula_6">max{h in w in c in + 4w out c out , h out w out c out + 4w in c in } + (2c 2 + 4c 3 )w in ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.2 Memory requirements of image classification networks</head><p>We calculate the lowest possible memory requirements of networks using calculations in the previous subsection for individual blocks and the following methodology: find a partitioning of a multi-layer network into disjoint contiguous sets of layers that minimizes the least memory requirement of the most memory-intensive partition. Using this, we calculate the memory requirements of networks in <ref type="table" target="#tab_2">Table 1</ref> and list the requirements in <ref type="table" target="#tab_12">Table 8</ref>. We now discuss the specifics of each network, and in particular, the partition of the layers of the network that requires the maximum memory (and thus lower bonds the memory requirement of a network).</p><p>GoogLeNet has a initial convolution layer (C1) of stride 2, followed by a max pooling layer (P1), another convolution layer (C2) of stride 2 and then a max pooling layer (P2). Output of P2 is of size 28 × 28 × 192. Applying Proposition 2 to the set of layers starting with the input image (I) and output of P2 (O), the RAM required is 112 × (11-4) × 64 + 56 × (5-2) × 64 + 56 × (3-2) × 192 added to O and 7 rows of input, is lesser than the requirement for inception (3b). For the inception (3b) block, the input is ( 28 × 28 × 256) and the output is of size 14 × 14 × 480. Therefore using Proposition 2, the RAM required is 28 × (7-2) × 32 + 28 × (5-2) × 128 + 28 × (3-2) × 64 + 28 × (3-2) × 480 (the first three terms are intermediate activations of the inception block and have different receptive fields), added to the input size (28 × 28 × 256) + 14 × (7-2) × 480, results in 1.01MB.</p><p>DenseNet121 has a 2-strided convolution layer (C1) in the beginning followed by a max pool of stride 2 (P1) and then D1-the first Dense block which has 6 Dense layers. Each Dense layer has 1 × 1 convolution with 128 output channels followed by a 3 × 3 convolution with 128 input and 32 output channels. The output of each Dense layer is concatenated to the input to form the input to the next Dense layer which is why the 1 × 1 convolution in each Dense layer has different input channels. D1 is followed by a 1 × 1 convolution which reduces channels of activation map to half followed by P2, another Max Pool layer. For determining the peak RAM required, we apply Proposition 2 to the set of layers starting with the output of P1 (I) until the output of P2 (O), so that we can go from 56 × 56 × 64 to 28 × 28 × 128 directly bypassing 56 × 56 × 256 sized O D1 . The receptive field of O on I can be calculated to be 14×14. The RAM for intermediate activations will be 56 × (14-2) × 128 + 56 × (12-2) × 32 + 56 × (12-2) × 128 + 56 × (10-2) × 32 + . . . + 56 × (4-2) × 32. The total peak RAM along with I (56× 56 × 64) + 28 × (14-2) × 128, which is 2.38MB.</p><p>ResNet18. A similar calculation as above can be done for ResNet18. The architecture consists of a convolution layer (C1) of stride 2 followed by a max pool layer (P1), followed by residual blocks. In this case, let us apply Proposition 2 to the block of layers starting with the input RGB image of size 224 × 224 × 3 (denoted I) until the output of P1 (denoted O). Between I and O we have 2 layers: C1 and P1. Therefore the total RAM requirement will be 112 × (3-2) × 64 added to O (56 × 56 × 64) + 224 × (11-4) × 3, which is 0.81MB.</p><p>MobileNetV2 has a convolution layer C1 of stride 2 followed by a MBConv block MB1 which has stride 1. MB1 contributes to the peak memory (2.29MB). Denote by I the input RGB image of size 224 × 224 × 3 and denote by O the output of MB1. The receptive field of O on output of C1 is 3, on output of first layer of MB1 is 3 and after the 1 for the rest two layers of MB1. Therefore, using Proposition 1, the RAM required is 112 × (3-1) × 32 + 112 × (3-1) × 32 added to O ( 112 × 112 × 16 )) + 224 × (7-2) × 3, which is 0.84MB.</p><p>EfficientNet-B0 has exactly the same calculation as MobileNetV2 as the first convolution block and first MBConv block are identical.</p><p>RNNPool Versions : Similar to GoogLeNet we can also reduce peak RAM of GoogLeNet-RNNPool.</p><p>Here inception (4e) is the bottleneck. Lets take I as the input to inception (3b)( 14 × 14 × 528) and O as the output of the pooling layer after inception (3b). Size of O is 7 × 7 × 832. Therefore using Proposition 1, the RAM required is 14 × (7-2) × 32 + 14 × (5-2) × 160 + 14 × (3-2) × 128 + 14 × (3-2) × 832, added to input (14 × 14 × 528) + 7 × (7-2) × 832, resulting in 0.59MB.</p><p>The peak memory requirements of RNNPool versions of ResNet18, DenseNet121, MobileNetV2 and EfficientNet-B0 in <ref type="table" target="#tab_2">Table 1</ref> cannot be reduced further by better schedules as we replace the most memory-intensive blocks and operate patch-by-patch, which is more local and granular that row-by-row schedules used above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Memory requirement (without recomputation) estimates according to prior conventions</head><p>In this subsection, we follow the scheduling convention of Chowdhery et al. <ref type="bibr" target="#b5">[6]</ref> to estimate the memory requirements of individual blocks and networks that use them. Note that the memory requirements listed here can be higher than in Section E.1 as the schedules may not be optimal from memory requirement perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.1 Memory requirements of individual blocks</head><p>1. Inverted bottleneck residual block (a.k.a. MBConv) : Give input I of size h in ×w in ×C, a pointwise convolution (C1) first expands the number of channels to C × t where t is expansion factor. Then there is a depthwise separable 3 × 3 convolution (C2) with stride either 1 or 2, followed by another pointwise convolution (C3) which reduces the channel to the number of output channels (O) associated with the MBConv block. To avoid storing the large output (O C1 ) of C1 and bloating the memory, O C1 is constructed channel by channel, so at first 1 filter of the C × t filters of C1 will be convolved with I, then this single 2D vector will be convolved by C2. Since C2 is depthwise separable and input channels independently contribute to an output channel, we again get a 2D map. This map is convolved with all filters of C3 and we get an output of O number of channels. We keep doing this, going one by one through each filter of C1 and adding to the output of the MBConv block of O channels, to get the final output. Hence, the memory requirement is the size of input added to that of the output of the MBConv block. 2. Residual Block : The memory requirement is the maximum of input and output maps of the block. As the residual connection adds the input to the output values can be discarded after being added to the output values being computed. 3. Inception block: Denote the input to the inception block I and the outputs of each of the 4 paths in the block O 1 , O 2 , O 3 and O 4 . Since we can get rid of the input I after computing the last output, we can order the computation in increasing order of the number of channels in O i . Therefore, the peak RAM while computing the full block will be the sum of input added to the sum of the 3 smallest outputs. 4. Dense block: A dense block needs to store the input as well as outputs of all previous dense layers since the last layer needs all the activation maps concatenated. The volume activation maps stored will reach the peak just after the last dense layer. Therefore the peak RAM usage is the size of the output of the dense block. <ref type="table" target="#tab_2">Table 1</ref> We now use the above results to compute the memory requirements of image classification networks, assuming all computations are in 32-bit floating-point. We assume the layer-by-layer convention of <ref type="bibr" target="#b5">[6]</ref> for RAM computation. The peak memory requirement of both MobileNetV2 and EfficientNet-B0 is contributed by the first MBConv block in these architectures. The input map size to the block is 112 × 112 × 32 and the output map size is 112 × 112 × 16, adding up to a peak memory requirement of 2.29MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.2 Memory requirements of image classification networks in</head><p>The peak memory requirement of the RNNPool inserted versions is the MBConv block right after the RNNPool replacement. The input size is 28 × 28 × 64 and output size is 14 × 14 × 64 for MobileNetV2-RNNPool, adding up to 0.24MB. The input size is 28 × 28 × 64 and output size is 14 × 14 × 80 for EfficientNetB0-RNNPool, adding up to 0.25MB.</p><p>For ResNet18, DenseNet121, and GoogLeNet the maximum memory requirement is to host the activation map just after the first convolution layer which is of size 112 × 112 × 64. For ResNet18-RNNPool, the maximum requirement comes from the residual block just after RNNPool, i.e., the first residual block out of the two of conv4_x. The input to this is of size 28 × 28 × 128 and the output size is 14 × 14 × 256. The maximum of these two is 0.38MB. For DenseNet121-RNNPool, the largest memory requirement comes from the output of D3 (see <ref type="figure">Figure 2</ref>), the size of which  <ref type="table" target="#tab_5">Table 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>without recomputation</head><p>We use convention of considering the largest activation map to be the peak RAM requirement. For EagleEye, FaceBoxes, EXTD and LFFD architectures, the largest activation map is the output of the first convolution, their sizes being 320 × 240 × 4 (=1.17MB), 160 × 120 × 24 (=1.76MB), 320×240×64 (=18.75MB) and 320×240×64 (=18.75MB) respectively. For RNNPool-Face-A and RNNPool-Face-B, the largest activation map is the output of the RNNPool, which is 160 × 120 × 16 (=1.17MB) and 160 × 120 × 24 (=1.76MB) respectively. For RNNPool-Face-C and RNNPool-Face-Quant, peak memory requirement is contributed by the MBConv block right after the RNNPool. The input size of this block for RNNPool-Face-C is 160 × 120 × 64 and output size is 160 × 120 × 24, the total being 6.44MB. The input size of this block for RNNPool-Face-Quant is 80 × 60 × 32 and output size is 80 × 60 × 16, the total being 224KB as we quantize to 1 byte unsigned integer. <ref type="table" target="#tab_2">Table 1</ref> with recomputation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Memory requirements of image classification networks in</head><p>As explained in Section E.2.2, the RAM calculations for RNNPool based models revealed that the convolution block after RNNPoolLayer contributes to the peak RAM. Let's denote this block in both the base architecture and RNNPool-based version as ConvBlock-A. In the memory-optimized scheme, we fix the peak RAM of the base model to be that of the convolution block whose RAM usage is a bit more than that of the RNNPool version. We denote by ConvBlock-B the convolution block that lies before ConvBlock-A, and such that there exists no block that lies between this block and ConvBlock-A which has a RAM usage less than that of ConvBlock-A. Note that ConvBlock-B is present only in the base model and not the RNNPool model. Since we fix the peak RAM, we have to reconstruct an activation map (denoted by Activation-A) that comes before ConvBlock-B patch by patch. Note that Activation-A need not necessarily be the activation map just before ConvBlock-B. Activation-A is chosen as the earliest occurring activation map (nearer to the input image) which ensures that there is no intermediate layer or block between it and ConvBlock-B which can contribute to more RAM usage. We do construct Activation-A by loading a patch of the image (one at a time), which is of the size of the receptive field of Activation-A w.r.t. the input image, and feed it forward to get a 1 × 1 × channel Activation−A voxel of Activation-A. When we load the next patch we have to re-compute some convolution and pooling outputs which come in the overlapping region of the two consecutive patches. We keep doing this until we reconstruct Activation-A completely. The total number of MAdds is the sum of the MAdds of the base network and the extra re-computations in order to compute patch-by-patch.  As discussed in <ref type="figure">Figure 2</ref>, we can use RNNPoolLayer in the beginning of the architecture to rapidly downsample the image leading to smaller working RAM and compute requirement. <ref type="table" target="#tab_14">Table 9</ref> presents the hidden state size and patch size used by RNNPoolLayer when applied to various models discussed in <ref type="table" target="#tab_2">Table 1</ref>. Note that the last row refers to the model used for Visual Wake Words experiments ( <ref type="figure" target="#fig_0">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Architectures</head><p>Furthermore, <ref type="table" target="#tab_2">Table 10</ref> presents the exact architecture used by MobileNet-v2-RNNPool(0.35x) architecture applied to the Visual Wakeword problem (Section 5.2). used. The rest of the layers are defined as in <ref type="bibr" target="#b36">[37]</ref>. Each line denotes a sequence of layers, repeated n times. The first layer of each bottleneck sequence has stride s and rest use stride 1. Expansion factor t is multiplied to the input channels to change the width. The number of output classes is l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Operator t c n s Typical image classification models use average pooling before the final feed-forward layer to produce the class probabilities. As RNNPoolLayer is syntactically equivalent to standard pooling layers, we can use it to perform the pooling in the penultimate layer, replacing the average pool layer. To this end, we use RNNPool operator with h 1 = h 2 = l/4 where l is the number of channels in the last activation map before the average pooling layer. Such a replacement does not significantly contribute to the number of parameters and MAdds. In <ref type="table" target="#tab_3">Table 2</ref>, Row 2 refers to such a replacement in the base MobilnetV2, DenseNet121, and MobilenetV2-0.35x models, while Row 7 refers to similar replacement in the corresponding RNNPool models. In <ref type="figure" target="#fig_0">Figure 3</ref>, all RNNPool based architectures use RNNPool both in the beginning layer and in the penultimate layer of the network.</p><formula xml:id="formula_7">224 2 × 3 conv2d 3 ×</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.3 RNNPoolLayer replacing intermediate Pooling layers</head><p>These experiments have been tried on DenseNet121 as the base model (Section-4), where we are replacing single max-pooling layers appearing in intermediate positions in the network with RNNPool. Given r in × c in × k in size input activation map to the pooling layer, the hidden sizes for RNNPool is taken as h 1 = h 2 = k in /4, patch size as 4 and stride as 2. Note that we also further drop dense layers (1 × 1 convolution followed by 3 × 3 convolution) in D3 and D4. The number of channels in the output of any dense block is the sum of the number of input channels and output of each dense layer. Hence, reducing the number of dense layers reduces the number of channels of the output activation maps of these dense blocks and hence the input to the pooling layer. However, for the RNNPool the same strategy of h 1 = h 2 = k in /4 is followed where k in is lesser now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Face Detection</head><p>Our detection network builds upon the backbone structure of S3FD <ref type="bibr" target="#b49">[50]</ref>. Following S3FD architecture, we fix the required receptive field size of each of the detection layers, which is then used to compute the number of MBConv Blocks or convolution layers after RNNPool and before each detection layer. We also use S3FD's anchor matching strategy and the max-out background label technique. Images are trained on 640 × 640 images. A multi-task loss is used where cross-entropy loss is used for classification of anchor box and smooth L1 loss is used as regression loss for bounding box coordinate offsets. We use multi-scale testing and Non-Maximal Suppression during inference to determine final bounding boxes. <ref type="table" target="#tab_2">Table 11</ref> contains the architecture of RNNPool-Face-C. There is a detection layer after every bottleneck stack. The detection layer contains two 3 × 3 constitutional kernels which predict the class probability (2 outputs per pixel) and bounding box offsets(4 outputs per pixel). The convention followed in the table below is the same as in <ref type="table" target="#tab_2">Table 10</ref>. t is the expansion coefficient, c is the number of output channels, n is the number of repetitions of the MBConv 1 layer and s is the stride associated with the first of those stack of layers. RNNPool's hidden state sizes are fixed to be: h 1 = h 2 = 16. Architecture for RNNPool-Face-A is shown in <ref type="table" target="#tab_2">Table 13</ref>. The detection heads are after the second row of the table and then after each stack of bottleneck layers. RNNPool's hidden state sizes are fixed to be: h 1 = h 2 = 16. Depthwise+Pointwise refers to a depthwise separable 3 × 3 convolution followed by a pointwise 1 × 1 convolution.</p><p>The architecture for RNNPool-Face-Quant is shown in <ref type="table" target="#tab_2">Table 14</ref>. The detection heads are after the second row of the table and then after each stack of bottleneck layers. The first detection head has a strided 3 × 3 convolution to reach a total stride of 4 (following S3FD). RNNPool's hidden state sizes are fixed to be: h 1 = h 2 = 4. <ref type="bibr" target="#b0">1</ref> We use the terms 'bottleneck', MBConv, and inverted residual interchangeably, they refer to the same block.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Hyperparameters</head><p>Models are trained in PyTorch <ref type="bibr" target="#b33">[34]</ref> using SGD with momentum optimizer <ref type="bibr" target="#b38">[39]</ref> with weight decay 4 × 10 −5 and momentum 0.9. We do data-parallel training with 4 NVIDIA P40 GPUs and use a batch size of 256 for classification and 32 for face detection. We use a cosine learning rate schedule with an initial learning rate of 0.05 for classification tasks, and 0.01 with 5 warmup epochs for face detection tasks. All convolution layers use learnable batch normalization. We use the EdgeML <ref type="bibr" target="#b7">[8]</ref> implementation of FastGRNN. All ImageNet-10 and face detection experiments were trained for 300 epochs. Both Visual Wake Words and ImageNet-1K experiments were run for 150 epochs. Best top-1 validation accuracy is reported in all the classification datasets and test MAP was reported for face detection.</p><p>We use FastGRNN as both the RNNs in RNNPool. We usually use the same hidden dimension for both the RNNs. We fix ζ as 1 and ν as 0 for all models, for stability, and use piecewise linear non-linearities quantTanh and quantSigmoid for the Visual Wake Word models, so we can quantize it without loss of information.</p><p>Various image augmentations were used for training each network. For the ImageNet experiments, the training images were cropped to a random size of 0.08 to 1.0 times the original size and reshaped to a random aspect ratio of 3/4 to 4/3. This was then resized to 224 × 224. This image was further flipped horizontally randomly and then normalized by the mean and standard deviation. For the validation set, we resize the input image to 256 × 256 and then take a center crop of 224 × 224. For the Visual Wake Word experiment, we follow a similar process except during training we crop the input image first to a random size of 0.2 to 1.0 times the original size. For varying resolutions from 96 to 224 as reported in <ref type="figure" target="#fig_0">Figure 3</ref>, the ratio of resizing resolution of the input image and center crop size is kept the same during validation. All other augmentations are kept the same with output size changed from 96 to 224. For Face Detection experiments we use augmentations like in S3FD <ref type="bibr" target="#b49">[50]</ref>. This includes color distortion, random cropping: specifically zooming in to smaller faces to get larger faces to train on, and horizontal flipping after cropping to 640 × 640. Note that the same augmentation strategies were used for the baseline models also for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H RNNPool Ablation</head><p>In this section, we first discuss the changes in accuracy, peak RAM, MAdds, and the number of parameters on varying hyperparameters of RNNPool like patch size, hidden dimensions, and stride. We also compare the same for multiple layers of RNNPool. We use MobileNetV2 as the base network and the dataset is ImageNet-10. Note that the first row refers to the MobileNetV2-RNNPool architecture in <ref type="table" target="#tab_2">Table 10</ref>, and the other rows (b)-(e) of <ref type="table" target="#tab_2">Table 16</ref> are variations on it. <ref type="table" target="#tab_2">Table 16</ref> (f) and (g) have another 4 MBConv blocks replaced in the MobileNetV2-RNNPool architecture (Row 3 of <ref type="table" target="#tab_2">Table 10</ref>). (f) uses a single RNNPool to do this replacement whereas (g) uses two consecutive RNNPool Blocks. All variations have ∼2M parameters (even (g) which has 2 RNNPool layers has a very minimal model size overhead). This suggests that a finer hyperparameter and architecture search could lead to a better trade-off between accuracy and compute requirements. In <ref type="table" target="#tab_2">Table 18</ref>, we ablate over the choice of RNN cell (LSTM, GRU and FastGRNN) in RNNPool for the MobileNetV2-RNNPool model <ref type="table" target="#tab_2">(Table 10</ref>) on the ImageNet-10 dataset. We show that the choice of FastGRNN results in significantly lower MAdds than LSTM or GRU while having about 1% higher accuracy. Finally, <ref type="table" target="#tab_2">Table 17</ref> has the training curve for the MobileNetV2-RNNPool on ImagetNet-10 showing that training with RNNPool is not harder than the base models.    with Eagle-Eye and RNNPool-Face-Quant. The confidence threshold is set to 0.6 for both models. EagleEye misses faces when there is makeup, occlusion, blurriness and in grainy pictures, while our method detects them. However, in the case of some hard faces, RNNPool-Face-Quant misses a few of them or does not draw a bounding box over the full face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXTD_32</head><p>RNNPool-Face-C <ref type="figure">Figure 7</ref>: Comparison of performance on test images with EXTD_32 and RNNPool-Face-C. The confidence threshold is set to 0.6 for both models. The EXTD model has more false positives and misses more faces. In the first image, EXTD makes a faulty prediction at the top right. In the second image, EXTD mistakes regions in leaves for faces, while our model detects just the two correct faces. In the next image, both the models have some wrong detections, but the EXTD model detects a large bounding box that is a false positive. In the next image EXTD misses a face with an unnatural pose that our model detects. However, our model detects a face within a face which in general can be removed easily. In the next image (last row above), both the models detect the two faces, which weren't detected by the models on the left. Our model detects a slightly better bounding box than EXTD.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Visual Wake Word: MobileNetV2-RNNPool requires 8× less RAM and 40% less compute than baselines. We cap the number of parameters at ≤ 250K instead of the 290K allowed by MobileNetV2 (0.35×). ProxylessNAS has 242K parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>with 32,203 images containing 393,703 labeled faces varying in scale, pose, and occlusion. It is organized based on 61 event classes. Each event class has 40%/10%/50% data as training, validation, and testing sets. The images in the dataset are divided into Easy, Medium, and Hard cases. The Hard case includes all the images of the dataset, and the Easy and Medium cases are subsets of the Hard case. The hard case includes images with a large number of faces or tiny faces along with the data from Easy and Medium cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 2 )Figure 4 :</head><label>24</label><figDesc>Instances of 32 x 32 and 64 x 64 images with smoothed line segments and (left) Examples from three multi-class and multi-label synthetic datasets used for probing RNNPool. (right) A 2-dimensional Multi-Dimensional Scaling visualization of the 128 dimensional output of RNNPool operator for the multi-class dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Claim 1</head><label>1</label><figDesc>Y denote the nodes in the final layer which form a grid of size m × n. Suppose that the size of the receptive field of each node in Y in an intermediate layer l is (2k + 1) × (2k + 1), k &gt; 0 and that y i,j ∈ Y depends on the activations of nodes x(l) i j , i ∈ {i − k, . . . , i, . . . i + k}, j ∈ {j − k, . . . , j, . . . j + k} in the intermediate layer l. Suppose further that the convolution operations have stride 1 and are generic and not separable, i.e., can not in the general case be factored into depth-wise separable operations. An execution of this network "disallows recomputation" if once a node x in an intermediate layer (layers that are neither the input nor output of the network) is computed, all nodes y ∈ Y that depend on x must be computed before x is evicted from memory. Fix column j ∈ [n]. Suppose that nodes y i,j , i ∈ I [m] have been completed at some point in an execution order. Then at the same point in the execution order, at least 2k contiguous activations x (l) i * −k+1,j , x (l) i * −k+2,j , . . . x (l) i * +k,j for some i * ∈ [m] will need to be saved in memory until another node from column j is computed. Proof. Since I [m], there exists index i * ∈ [m] \ I such that either i * + 1 ∈ I or i * − 1 ∈ I. Suppose without loss of generality that i * − 1 ∈ I. Then, nodes x (l) i * −k+1,j , x (l)i * −k+2,j , . . . , x (l) i * +k−1,j must have been loaded into memory. However, y i * ,j also depends on these intermediate nodes, and has not yet been computed. So these 2k intermediate nodes must be retained in memory, thus proving the statement. The case where i * + 1 ∈ I is similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F. 1 F. 1 . 1</head><label>111</label><figDesc>Image Classification RNNPoolLayer in the beginning replacing multiple blocks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>WIDER Face Dataset: MAdds vs MAP of various methods including RNNPool +S3FD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figures 6 and 7 Figure 6 :</head><label>76</label><figDesc>show the qualitative results where RNNPool based models outperform the current state-of-the-art real-time face detection models. Comparison of performance on test images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>:2002.11921v2 [cs.CV] 22 Oct 2020 First sweep with RNN1 (blue arrows share weights) Second sweep with RNN2 (green arrows share weights)</figDesc><table><row><cell>r</cell></row><row><cell>c</cell></row><row><cell>4*h 2</cell></row><row><cell>k</cell></row></table><note>34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.arXiv</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>4</head><label></label><figDesc>How to use the RNNPoolLayer?</figDesc><table><row><cell>C1 112 64 3 DenseNet121 P1 224 56 64</cell><cell>D1</cell><cell>T1 256 128 D2 56 28</cell><cell cols="2">28 512</cell><cell>T2</cell><cell>14 256</cell><cell>D3</cell><cell>14 1024</cell><cell>T3</cell><cell>7 512</cell><cell>D4</cell><cell>7 1024 1024 1 Avg Pooling FC + Softmax</cell></row><row><cell cols="4">C1 112 64 3 DenseNet121-RNNPool RNNPoolLayer (8, 8, 112, 112, 4, 64, 48, 48) 224</cell><cell>192 28</cell><cell>T2</cell><cell>14 256</cell><cell>D3</cell><cell>14 1024</cell><cell>T3</cell><cell>7 512</cell><cell>D4</cell><cell>7 1024 1024 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC + Softmax</cell></row><row><cell cols="13">Figure 2: DenseNet121-RNNPool: obtained by re-</cell></row><row><cell cols="13">placing P1, D1, T1 and D2 blocks in DenseNet121</cell></row><row><cell cols="6">with an RNNPoolLayer.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of inference complexity and accuracy with and without RNNPoolLayer on ImageNet-10.</figDesc><table><row><cell>Model</cell><cell>Accuracy (%)</cell><cell>Parameters</cell><cell cols="4">Base Memory Optimised Peak RAM MAdds Peak RAM Standard Calculation [6, 37] MAdds</cell><cell cols="4">RNNPool Accuracy (%) Parameters Peak RAM MAdds</cell></row><row><cell>MobileNetV2</cell><cell>94.20</cell><cell>2.20M</cell><cell>0.38 MB</cell><cell>1.00G</cell><cell>2.29 MB</cell><cell>0.30G</cell><cell>94.40</cell><cell>2.00M</cell><cell>0.24 MB</cell><cell>0.23G</cell></row><row><cell>EfficientNet-B0</cell><cell>96.00</cell><cell>4.03M</cell><cell>0.40 MB</cell><cell>1.09G</cell><cell>2.29 MB</cell><cell>0.39G</cell><cell>96.40</cell><cell>3.90M</cell><cell>0.25 MB</cell><cell>0.33G</cell></row><row><cell>ResNet18</cell><cell>94.80</cell><cell>11.20M</cell><cell>0.38 MB</cell><cell>21.58G</cell><cell>3.06 MB</cell><cell>1.80G</cell><cell>94.40</cell><cell>10.60M</cell><cell>0.38 MB</cell><cell>0.95G</cell></row><row><cell>DenseNet121</cell><cell>95.40</cell><cell>6.96M</cell><cell>1.53 MB</cell><cell>24.41G</cell><cell>3.06 MB</cell><cell>2.83G</cell><cell>94.80</cell><cell>5.60M</cell><cell>0.77 MB</cell><cell>1.04G</cell></row><row><cell>GoogLeNet</cell><cell>96.00</cell><cell>9.96M</cell><cell>1.63 MB</cell><cell>3.32G</cell><cell>3.06 MB</cell><cell>1.57G</cell><cell>95.60</cell><cell>9.35M</cell><cell>0.78 MB</cell><cell>0.81G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Impact of various downsampling and pooling operators on the accuracy, inference complexity and the model size of three base architectures: MobileNetV2 and DenseNet121 for ImageNet-10 dataset, and MobileNetV2-0.35x for Visual Wake Word dataset. First block of the table represents the base network and a modified network where the last average pooling layer in the network is replaced by RNNPoolLayer. Second block represent modified networks where the image is passed through a convolution layer followed by various downsampling methods to reduce the size of image by a factor of 4 × 4. The last row represents the architecture from the second block with RNNPoolLayer with an additional RNNPool replacing the last layer. Peak RAM usage computed using standard convention of<ref type="bibr" target="#b5">[6]</ref> is the same for all methods in the second block. Note that RNNPoolLayer +Last layer RNNPool has accuracy similar to the base network while other methods like ReNet are 2-3% less accurate.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">ImageNet-10</cell><cell></cell><cell></cell><cell cols="2">Visual Wake Words</cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>MobileNetV2</cell><cell></cell><cell></cell><cell>DenseNet121</cell><cell></cell><cell cols="2">MobileNetV2-0.35×</cell><cell></cell></row><row><cell></cell><cell cols="9">Accuracy (%) MAdds Parameters Accuracy (%) MAdds Parameters Accuracy (%) MAdds Parameters</cell></row><row><cell>Base Network</cell><cell>94.20</cell><cell>0.300G</cell><cell>2.2M</cell><cell>95.40</cell><cell>2.83G</cell><cell>6.96M</cell><cell>90.20</cell><cell>53.2M</cell><cell>296K</cell></row><row><cell>Last layer RNNPool</cell><cell>95.00</cell><cell>0.334G</cell><cell>2.9M</cell><cell>95.40</cell><cell>3.05G</cell><cell>7.41M</cell><cell>91.14</cell><cell>53.4M</cell><cell>300K</cell></row><row><cell>Average Pooling</cell><cell>90.80</cell><cell>0.200G</cell><cell>2.0M</cell><cell>92.80</cell><cell>0.71G</cell><cell>5.59M</cell><cell>86.85</cell><cell>31.9M</cell><cell>255K</cell></row><row><cell>Max Pooling</cell><cell>92.80</cell><cell>0.200G</cell><cell>2.0M</cell><cell>93.40</cell><cell>0.71G</cell><cell>5.59M</cell><cell>86.92</cell><cell>31.9M</cell><cell>255K</cell></row><row><cell>Strided Convolution</cell><cell>93.00</cell><cell>0.258G</cell><cell>2.1M</cell><cell>93.80</cell><cell>1.33G</cell><cell>6.38M</cell><cell>88.08</cell><cell>39.2M</cell><cell>264K</cell></row><row><cell>ReNet</cell><cell>92.20</cell><cell>0.296G</cell><cell>2.3M</cell><cell>93.00</cell><cell>1.35G</cell><cell>6.41M</cell><cell>88.10</cell><cell>46.4M</cell><cell>277K</cell></row><row><cell>RNNPoolLayer</cell><cell>94.40</cell><cell>0.226G</cell><cell>2.0M</cell><cell>94.80</cell><cell>1.04G</cell><cell>5.60M</cell><cell>89.57</cell><cell>37.7M</cell><cell>255K</cell></row><row><cell>RNNPoolLayer + Last layer RNNPool</cell><cell>95.60</cell><cell>0.260G</cell><cell>2.7M</cell><cell>95.00</cell><cell>1.26G</cell><cell>6.06M</cell><cell>89.65</cell><cell>37.9M</cell><cell>259K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of resources and accuracy withMobileNets for ImageNet-1K.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Finally,</cell></row><row><cell>Method</cell><cell cols="4">Peak RAM Parameters MAdds Accuracy (%)</cell></row><row><cell>MobileNetV1</cell><cell>3.06MB</cell><cell>4.2M</cell><cell>569M</cell><cell>69.52</cell></row><row><cell>MobileNetV1-ReNet</cell><cell>0.77MB</cell><cell>4.2M</cell><cell>487M</cell><cell>66.90</cell></row><row><cell>MobileNetV1-RNNPool</cell><cell>0.77MB</cell><cell>4.1M</cell><cell>417M</cell><cell>69.39</cell></row><row><cell>MobileNetV2</cell><cell>2.29MB</cell><cell>3.4M</cell><cell>300M</cell><cell>71.81</cell></row><row><cell>MobileNetV2-ReNet</cell><cell>0.24MB</cell><cell>3.6M</cell><cell>296M</cell><cell>66.72</cell></row><row><cell>MobileNetV2-RNNPool</cell><cell>0.24MB</cell><cell>3.2M</cell><cell>226M</cell><cell>70.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of memory requirement, no. of parameters and validation MAP of various Face Detectionarchitectures when applied to 640 × 480 RGB images from the Wider Face dataset. RNNPool-Face-C achieves higher accuracy than the baselines despite using 3× less RAM and 4.5× less MAdds. RNNPool-Face-Quant enables deployment on Cortex-M4 class devices with 6-7% accuracy gains over the cheapest baselines. baseline but with far smaller memory requirement (250 → 33.68 KB), model size, and MAdds. Peak memory usage is calculated using the same convention as<ref type="bibr" target="#b5">[6]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Peak RAM Parameters MAdds</cell><cell></cell><cell>MAP</cell><cell></cell><cell cols="3">MAP for ≤ 3 faces</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Easy Medium Hard Easy Medium Hard</cell></row><row><cell>EXTD</cell><cell>18.75 MB</cell><cell>0.07M</cell><cell>8.49G</cell><cell>0.90</cell><cell>0.88</cell><cell cols="2">0.82 0.93</cell><cell>0.93</cell><cell>0.91</cell></row><row><cell>LFFD</cell><cell>18.75 MB</cell><cell>2.15M</cell><cell>9.25G</cell><cell>0.91</cell><cell>0.88</cell><cell cols="2">0.77 0.83</cell><cell>0.83</cell><cell>0.82</cell></row><row><cell>RNNPool-Face-C</cell><cell>6.44 MB</cell><cell>1.52M</cell><cell>1.80G</cell><cell>0.92</cell><cell>0.89</cell><cell cols="2">0.70 0.95</cell><cell>0.94</cell><cell>0.92</cell></row><row><cell>FaceBoxes</cell><cell>1.76 MB</cell><cell>1.01M</cell><cell>2.84G</cell><cell>0.84</cell><cell>0.77</cell><cell>0.39</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RNNPool-Face-B</cell><cell>1.76 MB</cell><cell>1.12M</cell><cell>1.18G</cell><cell>0.87</cell><cell>0.84</cell><cell cols="2">0.67 0.91</cell><cell>0.90</cell><cell>0.88</cell></row><row><cell>EagleEye</cell><cell>1.17 MB</cell><cell>0.23M</cell><cell>0.08G</cell><cell>0.74</cell><cell>0.70</cell><cell cols="2">0.44 0.79</cell><cell>0.78</cell><cell>0.75</cell></row><row><cell>RNNPool-Face-A</cell><cell>1.17 MB</cell><cell>0.06M</cell><cell>0.10G</cell><cell>0.77</cell><cell>0.75</cell><cell cols="2">0.53 0.81</cell><cell>0.79</cell><cell>0.77</cell></row><row><cell>RNNPool-Face-Quant</cell><cell>225 KB</cell><cell>0.07M</cell><cell>0.12G</cell><cell>0.80</cell><cell>0.78</cell><cell cols="2">0.53 0.84</cell><cell>0.83</cell><cell>0.81</cell></row></table><note>Further, we sweep across input image resolutions of {96, 128, 160, 192, 224} to trade-off between accuracy and efficiency.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Finally, RNNPool-Face-Quant uses byte quantization to reduce the model size so it can be deployed on Cortex-M4 devices which typically have ≤ 256 KB RAM, while still having &gt; 0.80 MAP accuracy on images with ≤ 3 faces. See Appendix I for a qualitative evaluation of our method against the baselines.</figDesc><table /><note>5.4 RNNPool based Model for ARM Cortex-M4 Microcontrollers Finally, we develop a face detection model for conference/class room settings that can be deployed on ARM Cortex-M4 class devices. To this end, we develop a more compact version of the face detection model, RNNPool-Face-M4 (Table 15 in Appendix F.2), which has only 4 MBConv blocks. For</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of resources and MAP on the SCUT-HEAD dataset. RNNPool-Face-M4 can be effectively deployed on an M4 device with &lt;256 KB RAM in contrast to MobileNetV2-SSDLite low-cost detection model. MAdds and model-size, we train the RNNPool parameters to be sparse. That is, W matrix of RNN 1 is 50% non-zeros while the rest of the matrices in RNNPool are 30% non-zeros.</figDesc><table><row><cell>Model</cell><cell cols="4">MAP Peak RAM MAdds Model Size</cell></row><row><cell cols="2">MobileNetV2-SSDLite 0.63</cell><cell>3.51 MB</cell><cell>540M</cell><cell>11.32 MB</cell></row><row><cell>RNNPool-Face-M4</cell><cell>0.58</cell><cell>188 KB</cell><cell>70M</cell><cell>160 KB</cell></row><row><cell>further reduction in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Classes in ImageNet-10 dataset.</figDesc><table><row><cell cols="2">Class no. ImageNet id</cell><cell>Class name</cell></row><row><cell>1</cell><cell>n02690373</cell><cell>'airliner'</cell></row><row><cell>2</cell><cell>n04285008</cell><cell>'sports car'</cell></row><row><cell>3</cell><cell>n01560419</cell><cell>'bulbul'</cell></row><row><cell>4</cell><cell>n02124075</cell><cell>'Egyptian cat'</cell></row><row><cell>5</cell><cell>n02430045</cell><cell>'deer'</cell></row><row><cell>6</cell><cell cols="2">n02099601 'golden retriever'</cell></row><row><cell>7</cell><cell>n01641577</cell><cell>'bullfrog'</cell></row><row><cell>8</cell><cell>n03538406</cell><cell>'horse cart'</cell></row><row><cell>9</cell><cell>n03673027</cell><cell>'ocean liner'</cell></row><row><cell>10</cell><cell>n04467665</cell><cell>'trailer truck'</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Minimum required hyperparameter configurations for synthetic experiments.</figDesc><table><row><cell cols="2">Data Image Size</cell><cell>With Conv.</cell><cell>Without Conv.</cell></row><row><cell>(1)</cell><cell>32 × 32</cell><cell>h 1 = 4, h 2 = 16</cell><cell>h 1 = 16, h 2 = 32</cell></row><row><cell>(2)</cell><cell>32 × 32</cell><cell>h 1 = h 2 = 8</cell><cell>h 1 = h 2 = 32</cell></row><row><cell>(2)</cell><cell>64 × 64</cell><cell>h 1 = 8, h 2 = 16</cell><cell>h 1 = h 2 = 32</cell></row><row><cell>(3)</cell><cell>64 × 64</cell><cell>h 1 = 8 = h 2 = 16</cell><cell>h 1 = h 2 = 32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note>(Rows 2-5) reinforces the same but on bigger image classification datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>where c 2 and c 3 are the number of intermediate channels in O 2 and O 3 respectively.</figDesc><table /><note>4. Dense block (see</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Comparison of accuracy, compute and minimum memory requirement for inference with and without RNNPoolLayer on ImageNet-10. The memory calculations reflect the application of Proposition 2 and Claim 4 .</figDesc><table><row><cell>Model</cell><cell></cell><cell>Base</cell><cell></cell><cell></cell><cell></cell><cell>RNNPool</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">Accuracy (%) Parameters Peak RAM MAdds Accuracy (%) Parameters Peak RAM MAdds</cell></row><row><cell>MobileNetV2</cell><cell>94.20</cell><cell>2.20M</cell><cell>0.84MB</cell><cell>0.30G</cell><cell>94.40</cell><cell>2.00M</cell><cell>0.24MB</cell><cell>0.23G</cell></row><row><cell>EfficientNet-B0</cell><cell>96.00</cell><cell>4.03M</cell><cell>0.84MB</cell><cell>0.39G</cell><cell>96.40</cell><cell>3.90M</cell><cell>0.24MB</cell><cell>0.33G</cell></row><row><cell>ResNet18</cell><cell>94.80</cell><cell>11.20M</cell><cell>0.81MB</cell><cell>1.80G</cell><cell>94.40</cell><cell>10.60M</cell><cell>0.38MB</cell><cell>0.95G</cell></row><row><cell>DenseNet121</cell><cell>95.40</cell><cell>6.96M</cell><cell>2.38MB</cell><cell>2.83G</cell><cell>94.80</cell><cell>5.60M</cell><cell>0.77MB</cell><cell>1.04G</cell></row><row><cell>GoogLeNet</cell><cell>96.00</cell><cell>9.96M</cell><cell>1.01MB</cell><cell>1.57G</cell><cell>95.60</cell><cell>9.35M</cell><cell>0.59MB</cell><cell>0.81G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>14 × 14 × 1024 i.e. 0.77MB. For GoogLeNet, the peak requirement comes from the last inception block on the spatial resolution of 14 × 14 -inception (4e). Here the size of the input is 14 × 14 × 528 and sizes of the 3 smallest outputs are 14 × 14 × 128, 14 × 14 × 128 and 14 × 14 × 256, totaling 0.78MB. E.2.3 Memory requirement of face detection networks in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>RNNPool settings for image classification.</figDesc><table><row><cell>Model</cell><cell>Hidden Size</cell><cell>Patch Size</cell></row><row><cell>MobileNetV2-RNNPool</cell><cell>h 1 = h 2 = 16</cell><cell>6</cell></row><row><cell>EfficientNet-B0-RNNPool</cell><cell>h 1 = h 2 = 16</cell><cell>6</cell></row><row><cell>ResNet18-RNNPool</cell><cell>h 1 = h 2 = 32</cell><cell>8</cell></row><row><cell>DenseNet121-RNNPool</cell><cell>h 1 = h 2 = 48</cell><cell>8</cell></row><row><cell>GoogLeNet-RNNPool</cell><cell>h 1 = h 2 = 32</cell><cell>8</cell></row><row><cell cols="2">MobileNetV2-RNNPool (0.35×) h 1 = h 2 = 8</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>MobileNetV2-RNNPool: RNNPool Block with patch-size 6×6 and hidden sizes h1 = h2 = 16 is</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Each RNNPool-Face model is created by placing RNNPool Block directly after the input image or after a strided convolution (RNNPool-Face-Quant). Following the RNNPoolLayer, we apply standard S3FD architecture for detection. Detection layers are placed at strides of 4, 8, 16, 32, 64, and 128, for square anchor boxes of sizes 16, 32, 64, 128, 256, and 512 as in S3FD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>The architecture of RNNPool-Face-C</figDesc><table><row><cell>Input</cell><cell>Operator</cell><cell>t</cell><cell>c</cell><cell>n s</cell></row><row><cell cols="4">640 × 480 × 3 RNNPoolLayer 1 64</cell><cell>1 4</cell></row><row><cell>160 × 120 × 64</cell><cell>bottleneck</cell><cell cols="2">6 24</cell><cell>2 1</cell></row><row><cell>160 × 120 × 24</cell><cell>bottleneck</cell><cell cols="2">6 32</cell><cell>3 2</cell></row><row><cell>80 × 60 × 32</cell><cell>bottleneck</cell><cell cols="2">6 64</cell><cell>4 2</cell></row><row><cell>40 × 30 × 64</cell><cell>bottleneck</cell><cell cols="2">6 96</cell><cell>3 2</cell></row><row><cell>20 × 15 × 96</cell><cell>bottleneck</cell><cell cols="3">6 160 2 2</cell></row><row><cell>10 × 7 × 160</cell><cell>bottleneck</cell><cell cols="3">6 320 1 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>The architecture of RNNPool-Face-BArchitecture for RNNPool-Face-B is shown inTable 12. The detection heads are after the second row of the table and then after each stack of bottleneck layers. RNNPool's hidden state sizes are fixed to be: h 1 = h 2 = 6.</figDesc><table><row><cell>Input</cell><cell>Operator</cell><cell>t</cell><cell>c</cell><cell>n s</cell></row><row><cell cols="4">640 × 480 × 3 RNNPoolLayer 1 24</cell><cell>1 4</cell></row><row><cell>160 × 120 × 24</cell><cell>conv2d 3 × 3</cell><cell cols="2">1 24</cell><cell>4 1</cell></row><row><cell>160 × 120 × 24</cell><cell>conv2d 3 × 3</cell><cell cols="2">1 96</cell><cell>1 2</cell></row><row><cell>80 × 60 × 96</cell><cell>conv2d 1 × 1</cell><cell cols="2">1 32</cell><cell>1 1</cell></row><row><cell>80 × 60 × 32</cell><cell>bottleneck</cell><cell cols="2">6 32</cell><cell>3 1</cell></row><row><cell>80 × 60 × 32</cell><cell>bottleneck</cell><cell cols="2">6 64</cell><cell>3 2</cell></row><row><cell>40 × 30 × 64</cell><cell>bottleneck</cell><cell cols="3">6 128 2 2</cell></row><row><cell>20 × 15 × 128</cell><cell>bottleneck</cell><cell cols="3">6 160 1 2</cell></row><row><cell>10 × 7 × 160</cell><cell>bottleneck</cell><cell cols="3">6 320 1 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 13 :</head><label>13</label><figDesc>The architecture of RNNPool-Face-A</figDesc><table><row><cell>Input</cell><cell>Operator</cell><cell>t</cell><cell>c</cell><cell>n s</cell></row><row><cell>640 × 480 × 3</cell><cell>RNNPoolLayer</cell><cell cols="2">1 16</cell><cell>1 4</cell></row><row><cell cols="4">160 × 120 × 16 Depthwise+Pointwise 1 16</cell><cell>4 1</cell></row><row><cell cols="4">160 × 120 × 16 Depthwise+Pointwise 1 16</cell><cell>1 2</cell></row><row><cell>80 × 60 × 16</cell><cell>bottleneck</cell><cell cols="2">1 16</cell><cell>3 1</cell></row><row><cell>80 × 60 × 16</cell><cell>bottleneck</cell><cell cols="2">1 24</cell><cell>3 2</cell></row><row><cell>40 × 30 × 24</cell><cell>bottleneck</cell><cell cols="2">1 32</cell><cell>2 2</cell></row><row><cell>20 × 15 × 32</cell><cell>bottleneck</cell><cell cols="3">2 128 1 2</cell></row><row><cell>10 × 7 × 128</cell><cell>bottleneck</cell><cell cols="3">2 160 1 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 14 :</head><label>14</label><figDesc>The architecture of RNNPool-Face-Quant</figDesc><table><row><cell>Input</cell><cell>Operator</cell><cell>t</cell><cell cols="2">c n s</cell></row><row><cell>640 × 480 × 3</cell><cell>conv2d 3 × 3</cell><cell cols="2">1 4</cell><cell>1 2</cell></row><row><cell>320 × 240 × 4</cell><cell>conv2d 3 × 3</cell><cell cols="2">1 4</cell><cell>1 1</cell></row><row><cell cols="5">320 × 240 × 4 RNNPoolLayer 1 32 1 4</cell></row><row><cell>80 × 60 × 32</cell><cell>bottleneck</cell><cell cols="3">2 16 4 1</cell></row><row><cell>80 × 60 × 16</cell><cell>bottleneck</cell><cell cols="3">2 24 4 2</cell></row><row><cell>40 × 30 × 24</cell><cell>bottleneck</cell><cell cols="3">2 32 2 2</cell></row><row><cell>20 × 15 × 32</cell><cell>bottleneck</cell><cell cols="3">2 64 1 2</cell></row><row><cell>10 × 7 × 64</cell><cell>bottleneck</cell><cell cols="3">2 96 1 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 15 :</head><label>15</label><figDesc>The architecture of RNNPool-Face-M4</figDesc><table><row><cell>Input</cell><cell>Operator</cell><cell>t</cell><cell cols="2">c n s</cell></row><row><cell>320 × 240 × 1</cell><cell>conv2d 3 × 3</cell><cell cols="2">1 4</cell><cell>1 2</cell></row><row><cell cols="5">160 × 120 × 4 RNNPoolLayer 1 64 1 4</cell></row><row><cell>40 × 30 × 64</cell><cell>bottleneck</cell><cell cols="3">2 32 1 1</cell></row><row><cell>40 × 30 × 32</cell><cell>bottleneck</cell><cell cols="3">2 32 1 1</cell></row><row><cell>40 × 30 × 32</cell><cell>bottleneck</cell><cell cols="3">2 64 1 2</cell></row><row><cell>20 × 15 × 64</cell><cell>bottleneck</cell><cell cols="3">2 64 1 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 15</head><label>15</label><figDesc>shows the RNNPool-Face-M4 architecture for our cheapest model deployed on a M4 device. The model has 4 detection layers after each MBConv Block. RNNPool's hidden state sizes are fixed to be:h 1 = h 2 = 16.The RNNPool models decrease MAdds drastically while maintaining performance.Figure 5,shows the difference we are making. When restricted to the methods with &lt;2G MAdds requirement, our model attains even better MAP (for easy and medium dataset) than the state-of-the-art EXTD and LFFD architectures (which need about 10G MAdds per inference.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 16 :</head><label>16</label><figDesc>Comparison of accuracy, peak RAM and MAdds for variations in hidden dimensions, patch size and stride in RNNPool for MobileNetV2 and on ImageNet-10 dataset. Parameters are same as the base if not mentioned. (f) and (g) are further replacements in MobileNetV2-RNNPool (Row 3 ofTable 10).</figDesc><table><row><cell>#</cell><cell>Hyperparameters</cell><cell cols="3">Accuracy (%) Peak RAM MAdds</cell></row><row><cell>(a)</cell><cell>Reported (Patch Size = 6; h 1 = h 2 = 16, Stride = 4)</cell><cell>94.4</cell><cell>0.24MB</cell><cell>0.23G</cell></row><row><cell>(b)</cell><cell>Patch size = 8</cell><cell>94.0</cell><cell>0.24MB</cell><cell>0.24G</cell></row><row><cell>(c)</cell><cell>Patch size = 4</cell><cell>93.2</cell><cell>0.24MB</cell><cell>0.22G</cell></row><row><cell>(d)</cell><cell>h 1 = h 2 = 8</cell><cell>92.8</cell><cell>0.14MB</cell><cell>0.21G</cell></row><row><cell>(e)</cell><cell>h 1 = h 2 = 32</cell><cell>95.0</cell><cell>0.43MB</cell><cell>0.29G</cell></row><row><cell>(f)</cell><cell>Stride = 8; Patch Size = 12</cell><cell>94.0</cell><cell>0.14MB</cell><cell>0.17G</cell></row><row><cell cols="2">(g) Stride = 4; Patch Size = 6 and Stride = 2; Patch Size = 4</cell><cell>93.2</cell><cell>0.19MB</cell><cell>0.17G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 17 :</head><label>17</label><figDesc>Training curve of MobileNetV2-RNNPool on ImageNet-10.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 18 :</head><label>18</label><figDesc>Ablation over RNN cell in RNNPool forMobileNetV2-RNNPool on ImageNet-10.</figDesc><table><row><cell>RNN cell</cell><cell>Parameters</cell><cell>MAdds</cell><cell>Accuracy (%)</cell></row><row><cell>LSTM</cell><cell>2.0M</cell><cell>266M</cell><cell>93.4</cell></row><row><cell>GRU</cell><cell>2.0M</cell><cell>246M</cell><cell>93.0</cell></row><row><cell>FastGRNN</cell><cell>2.0M</cell><cell>226M</cell><cell>94.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to Shikhar Jaiswal and Aayan Kumar for their assistance in the deployment of RNNPool models on Cortex-M4 devices. We also thank Sahil Bhatia, Ali Farhadi, Sachin Goyal, Max Horton, Sham Kakade and Ajay Manchepalli for helpful discussions and feedback. Aditya Kusupati did a part of this work during his research fellowship at Microsoft Research India.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient interactive annotation of segmentation datasets with polygon-rnn++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="859" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mec: memory-efficient convolution for deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rhodes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05721</idno>
		<title level="m">Visual wake words dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gaurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lovett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Simhadri</surname></persName>
		</author>
		<ptr target="https://github.com/Microsoft/EdgeML" />
		<title level="m">Machine Learning for resource</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09574</idno>
		<title level="m">The state of sparsity in deep neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ML for mobile and edge devices -tensorflow lite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/lite" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compiling kb-sized machine learning models to tiny iot devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghanathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="79" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Memory-optimal direct convolutions for maximizing classification accuracy in embedded applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Murmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2515" to="2524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/mit-han-lab/VWW" />
		<title level="m">Solution to Visual Wakeup Words Challenge&apos;19</title>
		<imprint/>
	</monogr>
	<note>first place</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">LFFD: A light and fast face detector for edge devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10633</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and&lt; 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluating energy efficiency of internet of things software architecture based on reusable software components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Distributed Sensor Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1550147716682738</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FastGRNN: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9017" to="9028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Soft threshold weight reparameterization for learnable sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06601</idno>
		<title level="m">Cmsis-nn: Efficient neural network kernels for arm cortex-m cpus</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep learning. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Review of the development of multidimensional scaling methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series D (The Statistician)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="39" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Seeing</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/ai/seeing-ai" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Detecting heads using feature refine net and cascaded multi-scale architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09256</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">The IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SqueezeNAS: Fast neural architecture search for faster semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Neural Architects Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00393</idno>
		<title level="m">Renet: A recurrent neural network based alternative to convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">CNN-RNN: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Haq: Hardware-aware automated quantization with mixed precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8612" to="8620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Layer recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">EXTD: Extremely tiny face detector via iterative filter reuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06579</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Faceboxes: A CPU real-time face detector with high accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Joint Conference on Biometrics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multiactivation pooling method in convolutional neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wireless Communications and Mobile Computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Real-time multi-scale face detector on embedded devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">2158</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
