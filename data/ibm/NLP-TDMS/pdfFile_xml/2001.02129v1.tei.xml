<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Video Super-Resolution using HR Optical Flow Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20191">JANUARY 2019 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Deep Video Super-Resolution using HR Optical Flow Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<date type="published" when="20191">JANUARY 2019 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video Super-Resolution</term>
					<term>Optical Flow Estima- tion</term>
					<term>Temporal Consistency</term>
					<term>Scale-Recurrent Architecture</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video super-resolution (SR) aims at generating a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The key challenge for video SR lies in the effective exploitation of temporal dependency between consecutive frames. Existing deep learning based methods commonly estimate optical flows between LR frames to provide temporal dependency. However, the resolution conflict between LR optical flows and HR outputs hinders the recovery of fine details. In this paper, we propose an end-to-end video SR network to super-resolve both optical flows and images. Optical flow SR from LR frames provides accurate temporal dependency and ultimately improves video SR performance. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed using HR optical flows to encode temporal dependency. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate SR results. Extensive experiments have been conducted to demonstrate the effectiveness of HR optical flows for SR performance improvement. Comparative results on the Vid4 and DAVIS-10 datasets show that our network achieves the state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S UPER-resolution (SR) aims at generating high-resolution (HR) images from their low-resolution (LR) counterparts. As a typical low-level computer vision problem, SR has been investigated for decades <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Recently, converting LR videos into HR ones, namely video SR, is under great demand due to the prevalence of high-definition displays. Compared to a single image, adjacent frames in a video clip provide additional information for SR. Therefore, exploiting temporal dependency between consecutive frames plays an important role in video SR.</p><p>To exploit temporal dependency between consecutive frames, traditional video SR (or multi-image SR) methods detect recurrent patches across images using patch similarities <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. However, these methods can only employ pixel-level dependency and their computational cost is high. To employ sub-pixel dependency, several methods have been proposed to use sub-pixel motion information through optical flow estimation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. These methods formulate the video SR task as an optimization problem and estimate HR images, optical flows and blur kernels alternately. Since a large number of iterations are required to reach convergence, these methods also suffer from high computational costs.</p><p>Motivated by the success of deep learning in single image SR <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, numerous deep learning based video SR methods have been proposed recently <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. These methods first estimate optical flows from LR frames for motion compensation, and then learn a direct mapping from compensated LR frames to the HR output. Motion compensation encodes temporal dependency in compensated LR frames and facilitates these methods to exploit temporal information from consecutive frames. However, the accuracy of temporal dependency provided by LR optical flows is still low for video SR <ref type="bibr" target="#b14">[15]</ref>, especially for scenarios with large upscaling factors.</p><p>Since video SR aims at generating high-quality videos with plausible and temporally consistent details, both temporal details and spatial details are important for video SR. Although existing deep learning based video SR methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> can successfully hallucinate spatial details from consecutive LR frames, the restoration of temporal details is still under investigated. To address this limitation, we use a convolutional neural network (CNN) to recover HR temporal details in LR frames for video SR.</p><p>In this paper, we propose an end-to-end network to Superresolve Optical Flows for Video SR (namely, SOF-VSR). Our SOF-VSR network can recover temporal details through optical flow SR, which improves both the accuracy and consistency of video SR. Specifically, we first propose an optical flow reconstruction net (OFRnet) to reconstruct HR optical flows in a coarse-to-fine manner. Different from previous methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref> that use optical flows to align LR frames, our OFRnet learns to infer HR optical flows to align latent HR frames. These HR optical flows are then used to perform motion compensation on LR frames. Meanwhile, a spaceto-depth transformation is used to bridge the resolution gap between HR optical flows and LR frames. Finally, these compensated LR frames are fed to a super-resolution net (SRnet) to generate an HR frame. Ablation study is performed to test the effectiveness of HR optical flows for SR performance improvement. Comparative results show that our SOF-VSR network achieves the state-of-the-art performance on the Vid4 and DAVIS-10 datasets.</p><p>The major contributions of our work can be summarized as follows:</p><p>• We incorporate the SR of both optical flows and images arXiv:2001.02129v1 [cs.CV] 6 Jan 2020 into a unified SOF-VSR network. The SR of optical flows contributes to the SR of images. Consequently, better performance can be achieved by our SOF-VSR network. <ref type="bibr">•</ref> We propose an OFRnet to infer HR optical flows from LR frames in a coarse-to-fine manner. It is demonstrated that OFRnet can recover accurate temporal details for SR performance improvement. • Our SOF-VSR network achieves the state-of-the-art performance as compared to recent video SR methods. This work is an extension of our previous conference version <ref type="bibr" target="#b16">[17]</ref> with four notable improvements. First, we introduce a more lightweight and compact architecture for SOF-VSR in this paper. Specifically, techniques including channel split, channel shuffle and depth-wise convolution <ref type="bibr" target="#b17">[18]</ref> are employed to update our building blocks, and the OFRnet is rebuilt using a scale-recurrent network. Our lightweight SOF-VSR network achieves comparable performance to the original one <ref type="bibr" target="#b16">[17]</ref> with parameters being reduced by over 30%. Second, we have included additional analyses on the design of our network, including ablation studies on HR optical flows, scale-recurrent architecture and building block. Third, we have conducted additional experiments on different upscaling factors and performed additional evaluation on computational complexity. Fourth, additional experiments have been provided to further test the video SR performance through a face recognition task.</p><p>The rest of this paper is organized as follows. In Section II, we briefly review the related works. In Section III, we describe the proposed network in details. In Section IV, experimental results are presented. Finally, we conclude this paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we briefly review several methods that are closely related to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Single Image SR</head><p>Interpolation-based approaches (e.g., bilinear, bicubic and Lanczos <ref type="bibr" target="#b18">[19]</ref>) are initially used to increase the size of a single image. However, these methods cannot recover high-frequency details <ref type="bibr" target="#b19">[20]</ref>. Later, numerous reconstruction-based approaches have been proposed for single image SR <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b21">[22]</ref>. These methods formulate the single image SR task as an optimization problem and introduce different regularization techniques to reconstruct HR images. However, these methods require a large number of iterations and thus suffer from a very high computational cost. To learn a direct mapping between LR and HR images, exemplars are collected from the input image <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b2">[3]</ref> and external datasets <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. These exemplarbased methods usually use machine learning approaches (e.g., Markov random field) to achieve promising performance <ref type="bibr" target="#b25">[26]</ref>. For comprehensive reviews on traditional single image SR methods, we refer the readers to <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b25">[26]</ref>.</p><p>Recently, deep learning has been extensively investigated for SR. Dong et al. <ref type="bibr" target="#b8">[9]</ref> proposed the pioneering work to use deep learning for single image SR. They used a three-layer CNN (namely, SRCNN) to approximate the non-linear mapping from an LR image to its corresponding HR image. Kim et al. <ref type="bibr" target="#b26">[27]</ref> proposed a very deep super-resolution network (i.e., VDSR) with 20 convolutional layers. The deep architecture of VDSR improves the approximating capacity of CNN to achieve better performance. To achieve a compromise between model size and SR performance, Tai et al. <ref type="bibr" target="#b27">[28]</ref> developed a deep recursive residual network (DRRN) to deepen the network without obvious increase in model parameters. Shi et al. <ref type="bibr" target="#b28">[29]</ref> proposed an efficient sub-pixel convolutional neural network (ESPCN) to increase the resolution of an LR image at the end of the network. Its computational complexity is significantly reduced. More recently, Zhang et al. <ref type="bibr" target="#b29">[30]</ref> proposed a residual dense network (RDN) to facilitate effective feature learning using a contiguous memory mechanism.</p><p>B. Video SR 1) Traditional Video SR: Since the seminal work proposed by Tsai and Huang <ref type="bibr" target="#b30">[31]</ref>, significant progresses have been achieved in multi-image SR and video SR. Early methods <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> focus on videos with only affine transforms exist between adjacent frames, which is usually not the real case. To handle complex motion patterns in video clips, Protter et al. <ref type="bibr" target="#b3">[4]</ref> generalized the non-local means framework for video SR. They performed adaptive fusion of multiple frames using patch-wise spatio-temporal similarities. Takeda et al. <ref type="bibr" target="#b4">[5]</ref> further introduced a 3D kernel regression to exploit patchwise spatio-temporal neighborhood relationship. However, HR images produced by these two methods are usually oversmoothed. To exploit pixel-wise correspondences, optical flow estimation was used in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. These methods formulate the video SR task as an optimization problem and use iterative frameworks to estimate HR images, optical flows and blur kernels alternately. However, these methods are time-consuming.</p><p>2) Deep Video SR with Separated Motion Compensation: Inspired by the success of SRCNN in single image SR, deep learning has been investigated for video SR. Kappelar et al. <ref type="bibr" target="#b12">[13]</ref> proposed a two-step framework to perform video SR. Specifically, optical flow estimation is first performed for motion compensation. Then, the compensated frames are concatenated and fed to a CNN to reconstruct an HR frame. Following the same two-step framework as <ref type="bibr" target="#b12">[13]</ref>, Liao et al.</p><p>[12] estimated multiple optical flows using different parameter settings. These optical flows are then used for motion compensation to generate an ensemble of SR-drafts. Finally, a CNN is employed to recover high-frequency details from the ensemble. The two-step framework separates motion estimation and compensation from the CNN network. Therefore, it is difficult for these methods to obtain an overall optimal solution.</p><p>3) Deep Video SR with Integrated Motion Compensation: Recently, Caballero et al. <ref type="bibr" target="#b13">[14]</ref> proposed the first end-to-end CNN (namely, VESPCN) for video SR to integrate both motion estimation and compensation. Their VESPCN network comprises a motion estimation module and a spatio-temporal ESPCN module <ref type="bibr" target="#b28">[29]</ref>. Since then, end-to-end framework with integrated motion compensation dominates the research of video SR. Tao et al. <ref type="bibr" target="#b15">[16]</ref> used the motion estimation module in VESPCN and then designed a new layer to achieve both subpixel motion compensation (SPMC) and resolution enhancement. They also proposed an encode-decoder network with   <ref type="bibr" target="#b35">[36]</ref> proposed a bidirectional recurrent CNN to avoid explicit motion estimation and compensation. This recurrent-like architecture can capture long-term contextual information within temporal sequences. However, this method fails to handle large displacements and other complicated motions. Jo et al. <ref type="bibr" target="#b36">[37]</ref> introduced a CNN to generate dynamic upsampling filters for video SR. These dynamic upsampling filters are computed using local spatio-temporal neighborhood to avoid explicit motion compensation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Space-to-depth</head><formula xml:id="formula_0">OFRnet 1 L t I  L t I 1 L t I  1 H t t F   1 H t t F  </formula><p>Since temporal dependency between consecutive frames is important for video SR, existing deep learning based video SR methods focus on explicit or implicit exploitation of temporal dependency. However, these methods model temporal dependency in LR space, their limited accuracy in dependency hinders the restoration of fine details. Different from previous works, we propose an end-to-end video SR network to recover both temporal details and spatial details. Specifically, we first super-resolve optical flows to recover temporal details. These HR optical flows provide accurate temporal dependency and contribute to the restoration of spatial details. It is demonstrated that optical flow SR facilitates our network to achieve the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we introduce our SOF-VSR network in details. We first give an overview of our SOF-VSR network, and then describe the OFRnet, the motion compensation module and the SRnet of our network. Finally, we present the loss function for the training of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Given T consecutive LR frames (I L t−N , ..., I L t , ..., I L t+N ) of a video clip as the input of SOF-VSR, our task is to superresolve the central frame. Here, T = 2N + 1. Following <ref type="bibr" target="#b33">[34]</ref>, we convert input LR frames into YCbCr color space and only process the luminance channel. Input LR frames are first fed to OFRnet to infer HR optical flows. Specifically, our OFRnet takes the central LR frame I L t and one neighboring frame I L i as input to generate an HR optical flow F H i→t . Then, a spaceto-depth transformation <ref type="bibr" target="#b34">[35]</ref> is employed to shuffle the HR optical flows into LR grids, resulting in LR flow cubes. Next, motion compensation is performed to generate a draft cube using these flow cubes. Finally, the draft cube is fed to SRnet to infer the HR frame. The overview of our network is shown in <ref type="figure" target="#fig_2">Fig. 1</ref>. For simplicity, we only show the architecture with T = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optical Flow Reconstruction Net (OFRnet)</head><p>It has already been demonstrated by deep learning based SR methods (e.g., SRCNN <ref type="bibr" target="#b8">[9]</ref>, VDSR <ref type="bibr" target="#b26">[27]</ref> and RDN <ref type="bibr" target="#b29">[30]</ref>) that CNN is able to learn the non-linear mapping between LR and HR images. Recent CNN-based optical flow estimation methods (e.g., FlowNet <ref type="bibr" target="#b37">[38]</ref>, PWCNet <ref type="bibr" target="#b38">[39]</ref> and LiteFlowNet <ref type="bibr" target="#b39">[40]</ref>) have also shown the potential for motion estimation. Therefore, we incorporate these two tasks into a unified OFRnet to infer HR optical flows directly from LR images. Specifically, our OFRnet takes a pair of LR frames I L i and I L j as inputs, and reconstruct an optical flow F H i→j between their corresponding HR frames I H i and I H j :</p><formula xml:id="formula_1">F H i→j = Net OF R (I L i , I L j ; Θ OF R ),<label>(1)</label></formula><p>where F H i→j represents the HR optical flow and Θ OF R denotes the set of parameters.</p><p>Multi-scale mechanism has been demonstrated to be effective in optical flow estimation <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b39">[40]</ref>, stereo matching <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> and many other vision tasks <ref type="bibr" target="#b43">[44]</ref>. To reduce model size and training difficulty, a scale-recurrent architecture with shared parameters across scales is used in SRN-DeblurNet <ref type="bibr" target="#b43">[44]</ref>. Inspired by this, we introduce a scale-recurrent network for optical flow reconstruction, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>  first two levels, we use a recurrent module to estimate optical flows for inputs with different scales. For level 3, we first use the recurrent structure to generate deep representations, and then introduce an SR module to recover HR optical flows from the LR feature representations. The scale-recurrent architecture enables OFRnet to handle complex motion patterns (especially large displacements) while being lightweight and compact. and then fed to a feature extraction layer with 320 kernels of size 3 × 3. Then, three efficient residual blocks are used to generate deep features. Channel split, channel shuffle and depth-wise convolution techniques <ref type="bibr" target="#b17">[18]</ref> are used in these residual blocks to improve the efficiency. Next, these features are fed to a flow estimation layer with 2 kernels of size 3 × 3 to generate optical flow F LD i→j at this level. All convolutional layers are followed by a leaky rectified linear unit (ReLU) except the middle layer in each residual block and the last flow estimation layer.</p><formula xml:id="formula_2">E-ResB 2 E-ResB 2 E-ResB 1 E-ResB 2 E-ResB 1 Concat LD i I LD j I LDU i j F  L i I L j I L i j F  L i I L j I Warp Warp H i j F  L i j F  LD i j F  0 i j F  3x3 Conv E-ResB 6 E-ResB 5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared weights</head><p>Level 2: Once the optical flow F LD i→j is obtained from level 1, it is upscaled by a factor of 2 using bilinear interpolation. Note that, the magnitude of optical flow is also doubled with the resolution. The upscaled flow F LDU i→j is then used to warp I L i , resulting in I L i→j . Next, I L i→j , I L j and F LDU i→j are concatenated and fed to the recurrent module (which is the same as the one used in level 1) to generate optical flow F L i→j at this level.</p><p>Level 3: Since the output optical flow F L i→j of level 2 has the same size as the LR input I L j , level 3 works as an SR module to reconstruct HR optical flows. Similar to level 2, I L i→j , I L j and F L i→j are first concatenated and fed to the recurrent module (which is the same as the one used in levels 1 and 2) to extract features. These features are then fed to three additional residual blocks to generate deep representations.</p><p>Next, the resulting feature representations are fed to a subpixel layer <ref type="bibr" target="#b28">[29]</ref> for resolution enhancement. Finally, a flow estimation layer is used to generate the final HR optical flow F H i→j .</p><p>Although numerous networks for SR <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b44">[45]</ref> and optical flow estimation <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> can be found in literature, our OFRnet is, to the best of our knowledge, the first unified network to integrate these two tasks. Specifically, our OFRnet learns to infer HR optical flows between latent HR images from LR inputs. Though some existing video SR methods can also obtain optical flows of full resolution by performing interpolation on LR inputs <ref type="bibr" target="#b45">[46]</ref> or LR optical flows <ref type="bibr" target="#b34">[35]</ref>, their flow estimation is still performed in LR space since interpolation does not introduce additional information for SR <ref type="bibr" target="#b28">[29]</ref>. Note that, inferring HR optical flows from LR images is quite challenging, our OFRnet has demonstrated the potential of CNN to address this challenge. It is further demonstrated in Sec. IV-C that our SOF-VSR network is benefited from HR optical flows in terms of both accuracy and consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Motion Compensation Module</head><p>Once HR optical flows are produced by OFRnet, spaceto-depth transformation is used to bridge the resolution gap between HR optical flows and LR frames. As shown in <ref type="figure" target="#fig_3">Fig.  3</ref>, regular LR grids are extracted from the HR flow and placed into the channel dimension to derive a flow cube with the same resolution as LR frames:</p><formula xml:id="formula_3">F H i→j sH×sW ×2 → F H i→j H×W ×2s 2 ,<label>(2)</label></formula><p>where H and W represent the size of the LR frame, s is the upscaling factor. Note that, the magnitude of optical flow is divided by a scalar s during the transformation to match the spatial resolution of LR frames. Then, slices are extracted from the LR flow cube to warp the LR frame I LR i , resulting in multiple warped drafts:</p><formula xml:id="formula_4">Space-to-depth 2 sH sW H i j F        2 2 H W s H i j F        HR optical flow LR flow cube</formula><formula xml:id="formula_5">C L i→j = W(I L i , F H i→j H×W ×2s 2 ),<label>(3)</label></formula><p>where W(·) denotes the warping operation using bilinear interpolation and C L i→j ∈ R H×W ×s 2 represents the concatenation of multiple warped drafts. Note that, although motion compensation is performed on LR frames, accurate temporal dependency can be encoded in compensated frames since HR optical flows are employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Super-Resolution Net (SRnet)</head><p>Our SOF-VSR takes T consecutive LR frames (I L t−N , ..., I L t , ..., I L t+N ) as inputs to super-resolve the central frame. After motion compensation, multiple drafts are produced for each neighboring frame. As shown in <ref type="figure" target="#fig_2">Fig. 1</ref>, all the drafts are concatenated with the central LR frame and fed to SRnet to infer the HR frame:</p><formula xml:id="formula_6">I SR 0 = Net SR (C L ; Θ SR ),<label>(4)</label></formula><p>where I SR 0 is the SR result of the central frame and Θ SR is the set of parameters. C L ∈ R H×W ×(2N s 2 +1) represents the concatenation of all drafts after motion compensation, namely, draft cube.</p><p>As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, the draft cube is first passed to a feature extraction layer with 320 kernels of size 3 × 3 for feature extraction. The output features are then fed to 8 efficient residual blocks to generate deep features. Once features are generated by these residual blocks, they are fed to a sub-pixel layer for resolution enhancement. Finally, a 3×3 convolutional layer is used to generate the HR frame. Since our SOF-VSR network only works on the luminance channel, the number of kernels in the last layer is set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss Function</head><p>We design two loss terms L SR and L OFR for SRnet and OFRnet, respectively. For the training of SRnet, we use the mean square error (MSE) loss: </p><formula xml:id="formula_7">L SR = I SR 0 − I H 0 2 2 . (5) LR C 3x3 Conv E-ResB 1 E-ResB 2 E-ResB 8 Sub-pixel Conv SR I 3x3 Conv ...</formula><formula xml:id="formula_8">L OFR = i∈[−N, N ], i =0 L level3,i +λ 2 L level2,i +λ 1 L level1,i 2N ,<label>(6)</label></formula><formula xml:id="formula_9">where      L level3,i = W(I H i , F H i→0 )−I H 0 1 +λ 3 ∇F H i→0 1 L level2,i = W(I L i , F L i→0 )−I L 0 1 +λ 3 ∇F L i→0 1 L level1,i = W(I LD i , F LD i→0 )−I LD 0 1 +λ 3 ∇F LD i→0 1 , (7) ∇F H i→0 1 , ∇F L i→0 1 and ∇F LD i→0</formula><p>1 are L1 regularization terms to constrain the smoothness of the optical flows at different scales. We empirically set λ 2 = 0.2 and λ 1 = 0.1 to make our OFRnet focus on the last level. We also set λ 3 = 0.1 as the regularization coefficient.</p><p>Finally, the total loss for joint training is defined as L = L SR + λ 4 L OFR , where λ 4 is empirically set to 0.01 to balance these two loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first introduce the datasets and implementation details. Next, ablation study is performed on the Vid4 dataset to test our network. Our SOF-VSR is then compared to the state-of-the-art methods on the Vid4 and DAVIS-10 datasets. Finally, face recognition task is used to further demonstrate the effectiveness of our network for highlevel vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>For training, we collected 145 1080P HD video clips from the CDVL Database 1 . These video clips cover diverse natural and urban scenes. Similar to <ref type="bibr" target="#b36">[37]</ref>, we used 4 video clips including Coastguard, Foreman, Garden, and Husky from the Derf's collection 2 for validation. For fair comparison to the state-of-the-arts, we used the widely used Vid4 benchmark dataset to test our method. We also used a subset of the DAVIS dataset <ref type="bibr" target="#b46">[47]</ref> with 10 video clips for further comparison, which will be referred to as DAVIS-10 in this paper. Note that, each video clip in the test dataset contains 31 consecutive frames, the same as in <ref type="bibr" target="#b11">[12]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Following <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref>, we downsampled the original video clips to the size of 540 × 960 as the HR groundtruth using Matlab function imresize in bicubic mode. These HR videos were further downsampled to generate LR video clips with different upscaling factors. During the training phase, we randomly extracted T consecutive frames from an LR video clip, and randomly cropped a 32 × 32 patch as the input. Meanwhile, its corresponding patch in the HR video clip was cropped as the groundtruth. Data augmentation was performed through rotation and reflection to improve the generalization capability of our network.</p><p>For evaluation, we used peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) to test the accuracy of each individual frame. The overall PSNR/SSIM values were then calculated by aggregating PSNRs/SSIMs over all frames in a video clip. To test the consistency performance, we used the temporal motion-based video integrity evaluation index (T-MOVIE) <ref type="bibr" target="#b47">[48]</ref>. Moreover, MOVIE <ref type="bibr" target="#b47">[48]</ref> was used to test the overall quality of a video. This metric is correlated to human perception and has been widely applied in video quality assessment. All metrics are computed in the luminance channel. Following <ref type="bibr" target="#b48">[49]</ref>, borders of 6 + s are cropped for fair comparison.</p><p>Our SOF-VSR was implemented in PyTorch on a PC with an Nvidia GTX 1080Ti GPU. We used the Adam solver <ref type="bibr" target="#b49">[50]</ref> with β 1 = 0.9, β 2 = 0.999 and a batch size of 32 for training. The initial learning rate was set to 1 × 10 −3 and divided by 10 after every 80K iterations. The training was stopped after 200K iterations since more iterations do not provide further consistent improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of the Network Architecture</head><p>In this section, we present ablation experiments on the Vid4 dataset to analyze the architecture of our SOF-VSR network. All variants in the experiment were retrained following the configuration of the original SOF-VSR network.</p><p>1) Motion Compensation: To handle complex motion patterns in video sequences, optical flows are used for motion compensation in our network. To test the effectiveness of motion compensation for video SR, we removed the whole OFRnet module and fed LR frames directly to our SRnet. Note that, replicated LR frames were used to match the dimension of the draft cube C L . Results achieved on the Vid4 dataset are listed in <ref type="table" target="#tab_2">Table I</ref>.</p><p>It can be observed that the performance of our SOF-VSR significantly benefits from motion compensation. If OFRnet is removed, the PSNR/SSIM values are decreased from 26.00/0.772 to 25.70/0.753. Besides, the consistency performance is also degraded, with T-MOVIE value being increased from 19.35 to 20.03. That is because, it is difficult for SRnet to learn the non-linear mapping between LR and HR images under complex motion patterns.</p><p>2) LR Flow vs. HR Flow: Optical flow SR provides accurate temporal dependency for video SR. To test the effectiveness of HR optical flows, we replaced the sub-pixel convolution at level 3 in our OFRnet with a normal convolution. Then, the resulting LR optical flows were directly used for motion compensation and subsequent processing. To match the dimension of the draft cube, compensated LR frames were also replicated before feeding to SRnet.</p><p>It can be observed from <ref type="table" target="#tab_2">Table I</ref> that if LR optical flows were generated for motion compensation, the PSNR/SSIM values are increased to 25.85/0.765. However, the performance is still inferior to our SOF-VSR using HR optical flows. That is because, HR optical flows provide more accurate temporal dependency for performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Upsampled Flow vs. Super-resolved Flow:</head><p>Optical flow sup-resolution can also be simply achieved by interpolation. However, our OFRnet can recover more accurate optical flow details. To demonstrate this, we replaced the sub-pixel convolution at level 3 in our OFRnet with a normal convolution, and upsampled the resulting LR optical flows using bilinear interpolation. Then, we used the modules in our original network for subsequent processing. From the comparative results shown in <ref type="table" target="#tab_2">Table I</ref>, we can see that if bilinear interpolation is used to upsample LR optical flows, no significant improvement can be observed (25.85/0.765 vs. 25.83/0.766). That is because, the upsampling operator cannot recover temporal dependency reliably. If optical flow SR is performed, the PSNR/SSIM values are increased to 26.00/0.772. That is because, optical flow SR can recover finer temporal details and facilitate our SOF-VSR network to achieve better video SR performance.</p><p>We further compare the super-resolved optical flows and upsampled optical flows to the groundtruth on the Sintel <ref type="bibr" target="#b50">[51]</ref>, Middlebury <ref type="bibr" target="#b51">[52]</ref>, KITTI 2012 <ref type="bibr" target="#b52">[53]</ref> and KITTI 2015 <ref type="bibr" target="#b53">[54]</ref> datasets. We also include two dedicated optical flow estimation methods for comparison, including FlowNet <ref type="bibr" target="#b37">[38]</ref> and SpyNet   <ref type="bibr" target="#b40">[41]</ref>. Note that, the optical flows estimated from LR frames are upsampled for fair evaluation. We use the average endpoint error (EPE) for quantitative comparison, and present the results in <ref type="table" target="#tab_2">Table II</ref>. It can be observed that super-resolved optical flows significantly outperform upsampled ones, with EPE results being reduced by over 0.3. Note that, FlowNet (30.58M) and SpyNet (1.14M) are trained on a much larger dataset (i.e. the Flying Chairs dataset with 22872 image pairs) in a supervised manner. Therefore, they achieve better performance than our OFRnet (0.41M) in terms of EPE. Since groundtruth optical flows are unavailable for the Vid4 dataset, we warped frames using the  <ref type="figure">Fig. 7</ref>: Visual comparison of error maps (difference between the warped image and the reference image) achieved on the Vid4 dataset for 4× SR. The results generated with superresolved optical flows achieve higher accuracy.</p><p>OFRnet produces visually comparable flow estimation results to SpyNet. This has clearly demonstrated the effectiveness of our OFRnet in recovering temporal details. Error maps achieved on two scenes of the Vid4 dataset are further shown in <ref type="figure">Fig. 7</ref>. It can be observed that super-resolved optical flows produce fewer erroneous pixels, i.e, finer temporal details are recovered.</p><p>In summary, the superior performance achieved on the Sintel, Middlebury, KITTI 2012, KITTI 2015 and Vid4 datasets demonstrates that finer temporal details can be recovered in super-resolved optical flows than upsampled ones. Note that, the task of our work is not to design a superior optical flow estimation network. Instead, we focus on the design of a lightweight sub-network, which is sufficiently effective to provide fine temporal details for the improvement of overall video SR performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) SISR before Optical Flow Estimation.:</head><p>To obtain HR optical flows from LR inputs, an alternative is to perform single image super-resolution (SISR) on separated LR frames first and then estimate HR optical flows from these SR results. To test the performance of this option, we designed a variant to perform SISR before optical flow estimation. Specifically, input LR frames were first super-resolved separately before being fed to the OFRnet for HR optical flow estimation. Note that, the sub-pixel convolution in level 3 of the OFRnet was replaced with a normal convolution. Next, SISR results were compensated and passed to the SRnet for fusion. It can be observed from <ref type="table" target="#tab_2">Table I</ref> that this variant does not introduce significant performance improvement against our SOF-VSR in terms of PSNR and SSIM. Meanwhile, this variant requires much higher computational cost than our SOF-VSR, with FLOPs being increased from 108.90G to 1.12T. Since SISR is first used to enhance the resolution of LR inputs, optical flow estimation and fusion of multiple frames are performed on HR images. Therefore, the computational complexity is significantly increased. In contrast, our SOF-VSR directly infers HR optical flows from LR inputs and fuses multiple frames in LR space. Therefore, our network has a much lower computational cost and is more suitable for applications on mobile computing devices. 5) Scale-recurrent vs. Scale-cascaded Architectures: Since the task of each level in our OFRnet is similar, we employ a scale-recurrent architecture in our OFRnet to reduce model size. To demonstrate its effectiveness, we replaced the scalerecurrent architecture with a scale-cascaded one by using independent networks at 3 levels. Results achieved on the Vid4 dataset are presented in <ref type="table" target="#tab_2">Table I</ref>.</p><p>Our SOF-VSR achieves comparable performance to the scale-cascaded architecture with the overall model size being reduced from 1.33M to 1.00M. Since the tasks of different levels are similar, the scale-cascaded architecture contains redundant parameters. In contrast, using a scale-recurrent structure, our SOF-VSR is more lightweight and compact while achieving comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) Efficient Residual Block vs. Vanilla Residual Block:</head><p>Efficient residual block is used in our SOF-VSR network to reduce model size and computational complexity. To demonstrate its effectiveness, we designed a variant by replacing efficient residual blocks with vanilla ones. Comparative results are listed in <ref type="table" target="#tab_2">Table I</ref>.</p><p>It can be observed that the variant with vanilla residual blocks achieves slightly better performance than our SOF-VSR. However, its model size and FLOPs are increased from 1.00M to 1.56M and from 108.90G to 143.14G, respectively. Using efficient residual blocks, our SOF-VSR is more lightweight and compact without obvious performance drop. Therefore, our SOF-VSR network is more suitable for applications on mobile computing devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison to the State-of-the-Art</head><p>We compared our SOF-VSR to 4 single image SR methods including Bicubic, Deeply Recursive Convolutional Network (DRCN) <ref type="bibr" target="#b9">[10]</ref>, Laplacian Pyramid Super-Resolution Network (LapSRN) <ref type="bibr" target="#b10">[11]</ref>, and Cascading Residual Network (CARN) <ref type="bibr" target="#b54">[55]</ref> and 6 video SR methods including Video Super-Resolution Network (VSRnet) <ref type="bibr" target="#b12">[13]</ref>, VESCPN <ref type="bibr" target="#b13">[14]</ref>, TDVSR <ref type="bibr" target="#b33">[34]</ref>, TDVSR-L <ref type="bibr" target="#b55">[56]</ref>, SPMC <ref type="bibr" target="#b15">[16]</ref>, and FRVSR <ref type="bibr" target="#b34">[35]</ref> on the Vid4 and DAVIS-10 datasets. For DRCN, LapSRN, CARN, VSRnet, and SPMC, we used the codes provided by the authors to produce their results. For TDVSR, we used the super-resolved images provided by the authors. For VESCPN, TDVSR-L and FRVSR, the results reported in their papers are used. Here, we only report the performance of FRVSR-3-64 since its network size are comparable to ours. For each test video clip with 31 frames, the first and last two frames are not used for performance evaluation.</p><p>Note that, the methods selected for comparison are trained on two different degradation models. Specifically, the degradation model used in DRCN, LapSRN, CARN, VSRnet, VESCPN, TDVSR and TDVSR-L is bicubic downsampling (denoted as BI), which is implemented using Matlab function imresize. For SPMC and FRVSR, HR images are first blurred using a Gaussian kernel and then downsampled by selecting every s th pixel (denoted as BD). Consequently, we retrained TABLE IV: Comparative results achieved on the Vid4 dataset. Note that, the first and last two frames are not used in our evaluation. FLOPs is computed based on HR frames with a resolution of 720p (1280×720). Results marked with * are directly copied from the corresponding papers. Best results are shown in boldface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Scale Method</head><p>Frames our SOF-VSR network on the BD degradation model (denoted as SOF-VSR-BD) to achieve fair comparison with SPMC and FRVSR. In this work, a Gaussian kernel with a standard deviation σ = 1.6 is used, which is the same as the one used in SPMC, but slightly larger than the one used in FRVSR (σ = 1.5). 1) Evaluation on the Vid4 Dataset: Quantitative Evaluation. Quantitative results achieved on the Vid4 dataset are shown in <ref type="table" target="#tab_2">Table IV</ref>. For the BI degradation model, our SOF-VSR achieves the best performance for 2× and 3× SR. For 4× SR, our SOF-VSR outperforms TDVSR-L in terms of PSNR, SSIM and MOVIE with halved parameters and FLOPs. Compared to the conference version, our SOF-VSR achieves comparable performance with much fewer parameters (1.0M vs. 1.5M). Moreover, our network outperforms other methods in terms of T-MOVIE and MOVIE. That means our results are temporally more consistent. That is because, more accurate temporal dependency details can be provided by HR optical flows and therefore improved accuracy and consistency performance can be achieved.</p><formula xml:id="formula_10">PSNR(↑) SSIM(↑) T-MOVIE(↓) (×10 −3 ) MOVIE(↓) (×10 −</formula><p>For the BD degradation model, our SOF-VSR-BD network outperforms SPMC, with PSNR, SSIM and T-MOVIE values being improved by a notable margin. Although FRVSR-3-64 achieves a higher SSIM value, our SOF-VSR-BD method still achieves comparable performance in terms of other metrics with halved parameters. Compared to our conference version, our SOF-VSR-BD achieves comparable performance with parameters being reduced by over 30%.</p><p>We further show the trade-off between accuracy and consistency of different methods in <ref type="figure">Fig. 8</ref>. It can be observed that our SOF-VSR and SOF-VSR-BD networks achieve better PSNR and T-MOVIE performance on the Vid4 dataset, while being lightweight and compact.</p><p>Qualitative Evaluation. Several qualitative results on two scenarios of the Vid4 dataset are shown in <ref type="figure">Fig. 9</ref>. We can see from the zoom-in regions that our SOF-VSR and SOF-VSR-BD networks recover finer details, such as the word "MAREE" and the stripes of the building. Moreover, it can be observed from the temporal profiles that the word "MAREE" can hardly be recognized in the SR results achieved by Bicubic, DRCN, LapSRN, CARN, VSRnet and TDVSR. Although finer results are produced by SPMC, the word is still distorted and blurred. In contrast, smooth and clear patterns with fewer artifacts can be observed in the temporal profiles of our results. In summary, our network produces temporally more consistent results and better perceptual quality. 2) Evaluation on the DAVIS-10 Dataset: Quantitative Evaluation. Quantitative results achieved on the DAVIS-10 dataset are shown in <ref type="table" target="#tab_6">Table V</ref>. For the BI degradation model, our SOF-VSR network achieves the stateof-the-art performance in terms of PSNR and SSIM. Although suffering from a slight PSNR performance drop as compared to the conference version for 4× SR, our network achieves better performance in terms of other metrics with much fewer parameters (1.0M vs. 1.5M). In terms of T-MOVIE, our network achieves comparable or better performance than other approaches. In summary, our SOF-VSR network produces SR results with the best overall video quality in terms of MOVIE.</p><p>For the BD degradation model, our SOF-VSR-BD network outperforms SPMC in terms of all metrics. Specifically, the PSNR/T-MOVIE values achieved by our network are better than SPMC by 1.26/3.15. That is, better accuracy and consistency performance is achieved by our network. Since the DAVIS-10 dataset comprises scenes with fast moving objects, complex motion patterns (especially large displacements) lead to performance deterioration of existing video SR methods. In contrast, more accurate temporal dependency is provided by HR optical flows in our network. Therefore, complex motion patterns can be handled more robustly and better performance can be achieved.</p><p>Qualitative Evaluation. Qualitative comparison on two scenarios of the DAVIS-10 dataset is shown in <ref type="figure" target="#fig_2">Fig. 10</ref>. Compared to other methods, our SOF-VSR and SOF-VSR-BD networks recover more accurate details and achieve better </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. High-Level Vision Tasks</head><p>Rich details in a video clip are beneficial to high-level vision tasks such as face recognition and digit recognition <ref type="bibr" target="#b56">[57]</ref>. Here, we further compare our network to LapSRN, CARN, and SPMC by integrating a video SR module into the face recognition task.</p><p>Data preparation. Following <ref type="bibr" target="#b55">[56]</ref>, we form a subset of the YouTube Face dataset <ref type="bibr" target="#b57">[58]</ref> by choosing 167 subject classes that contain more than three video sequences. For each class, we randomly select one video for test and the rest for training. We first cropped face regions and resized them to the size of 60×60 to generate the HR data. Then, these HR data were downsampled to 15×15 to form the LR data. For each test video, we splitted it into clips of 50 frames. In total, we have about 600 clips.</p><p>Classifier. We used a customized AlexNet in <ref type="bibr" target="#b58">[59]</ref> as the  <ref type="bibr" target="#b10">[11]</ref>, CARN <ref type="bibr" target="#b54">[55]</ref>, VSRnet <ref type="bibr" target="#b12">[13]</ref>, and SOF-VSR are based on the BI degradation model, while SPMC <ref type="bibr" target="#b15">[16]</ref> and SOF-VSR-BD are based on the BD degradation model.  The prediction probabilities were aggregated over all frames in each video clip. The top-1 and top-5 accuracy metrics were used for quantitative evaluation and the comparative results are shown in <ref type="table" target="#tab_2">Table VII</ref>. It can be observed that our SOF-VSR network achieves the highest top-1 and top-5 accuracy on both BI and BD degradation models. Specifically, our network outperforms CARN by 1.2%/1.7% in terms of top-1/top-5 accuracy. That is because, our SOF-VSR network can recover richer details such that better face recognition performance can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have proposed an end-to-end deep network for video SR. Our OFRnet first super-resolves optical flows to provide accurate temporal dependency. Motion compensation is then performed based on HR optical flows. Finally, SRnet is used to infer SR results from these compensated LR frames. Extensive experimental results show that our SOF-VSR network can recover accurate temporal details for the improvement of both SR accuracy and consistency. Comparison to existing video SR methods has also demonstrated the state-of-the-art performance of our SOF-VSR network. Yulan Guo received the B.Eng. and Ph.D. degrees from National University of Defense Technology (NUDT) in 2008 and 2015, respectively. He was a visiting Ph.D. student with the University of Western Australia from 2011 to 2014. He worked as a postdoctorial research fellow with the Institute of Computing Technology, Chinese Academy of Sciences from 2016 to 2018. He has authored over 80 articles in journals and conferences, such as the IEEE TPAMI and IJCV. His current research interests focus on 3D vision, particularly on 3D feature learning, 3D modeling, 3D object recognition, and 3D biometrics. Dr. Guo received the CAAI Outstanding Doctoral Dissertation Award in 2016. He served as an associate editor for IET Computer Vision and IET Image Processing, a guest editor for IEEE TPAMI, a PC member for several conferences (e.g., CVPR and ICCV), a reviewer for over 30 journals, and an organizer for a tutorial in CVPR 2016 and a workshop in CVPR 2019.</p><p>Li Liu received the BSc degree in communication engineering, the MSc degree in photogrammetry and remote sensing and the Ph.D. degree in information and communication engineering from the National University of Defense Technology (NUDT), <ref type="bibr">Changsha, China, in 2003</ref><ref type="bibr" target="#b20">, 2005</ref> and 2012, respectively. She joined the faculty at NUDT in 2012, where she is currently an Associate Professor with the College of System Engineering. During her PhD study, she spent more than two years as a Visiting Student at the University of Waterloo, Canada, from 2008 to 2010. From 2015 to 2016, she spent ten months visiting the Multimedia Laboratory at the Chinese University of Hong Kong. From 2016 to 2018, she is working at the Machine Vision Group at the University of Oulu, Finland. She was a cochair of International Workshops at ACCV2014, CVPR2016, ICCV2017 and ECCV2018. She was a guest editor of special issues for IEEE TPAMI and IJCV. Her research interests include facial behavior analysis, texture analysis, image classification, object detection and recognition.</p><p>Zaiping Lin received the B.Eng. and Ph.D. degrees from the National University of Defense Technology (NUDT) in 2007 and 2012, respectively. He is currently an Assistant Professor with the College of Electronic Science and Technology, NUDT. His current research interests include infrared image processing and signal processing.</p><p>Xinpu Deng received the B.Eng. and Ph.D. degrees from the National University of Defense Technology (NUDT). He is currently an Associate Professor with the College of Electronic Science and Technology, NUDT. His current research interests include signal processing and remote sensing.</p><p>Wei An received the Ph.D. degree from the National University of Defense Technology (NUDT), Changsha, China, in 1999. She was a Senior Visiting Scholar with the University of Southampton, Southampton, U.K., in 2016. She is currently a Professor with the College of Electronic Science and Technology, NUDT. She has authored or co-authored over 100 journal and conference publications. Her current research interests include signal processing and image processing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The architecture of our OFRnet. Our OFRnet works in a coarse-to-fine manner. At each level, the output of its previous level is used to generate a residual optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Level 1 :</head><label>1</label><figDesc>The pair of input LR images I L i and I L j are first downsampled by a factor of 2 to produce I LD i and I LD j . Meanwhile, an initial flow map F 0 i→j with all elements of 0 is generated. The initial flow map F 0 i→j is concatenated with I LD i and I LD j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>An illustration of space-to-depth transformation. The space-to-depth transformation folds an HR optical flow in LR space to generate an LR flow cube.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>The architecture of our SRnet. For the training of OFRnet, intermediate supervision is used at each level of the pyramid:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>12 Fig. 5 : 24 Fig. 6 :</head><label>125246</label><figDesc>Visual comparison of optical flow estimation results achieved on the Sintel dataset for 4× SR. The super-resolved optical flows recover finer correspondences with more clear edges and fewer artifacts than the upsampled optical flows. Visual comparison of optical flow estimation results achieved on the Middlebury dataset for 4× SR. The super-resolved optical flows recover finer correspondences with more clear edges and fewer artifacts than the upsampled optical flows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :</head><label>10</label><figDesc>Visual comparison of 4× SR results on Boxing and Demolition. Bicubic, DRCN [10], LapSRN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Longguang</head><label></label><figDesc>Wang received the B.E. degree in electric engineering from Shandong University (SDU), Jinan, China, in 2015, and the M.E. degree in information and communication engineering from National University of Defense Technology (NUDT), Changsha, China, in 2017. He is currently pursuing the Ph.D. degree with the College of Electronic Science and Technology, NUDT. His current research interests include low-level vision and deep learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Comparative results achieved by our network and its variants on the Vid4 dataset for 4× SR. FLOPs is computed based on HR frames with a resolution of 720p (1280×720). 38M 0.59M 0.97M 35.19G 36.10G 106.48G SOF-VSR w/o sub-pixel conv + upsampling25.83   </figDesc><table><row><cell></cell><cell cols="2">PSNR(↑) SSIM(↑)</cell><cell>T-MOVIE(↓) (×10 −3 )</cell><cell>MOVIE(↓) (×10 −3 )</cell><cell cols="6">Params. OFRnet SRnet Overall OFRnet SRnet Overall FLOPs</cell></row><row><cell>SOF-VSR w/o OFRnet</cell><cell>25.70</cell><cell>0.753</cell><cell>20.03</cell><cell>4.47</cell><cell>-</cell><cell cols="2">0.59M 0.59M</cell><cell>-</cell><cell cols="2">36.10G 36.10G</cell></row><row><cell>SOF-VSR w/o sub-pixel conv</cell><cell>25.85</cell><cell cols="9">0.765 0.0.766 19.69 4.41 19.65 4.39 0.38M 0.59M 0.97M 35.19G 36.10G 106.48G</cell></row><row><cell>SOF-VSR SISR</cell><cell>25.96</cell><cell>0.772</cell><cell>19.32</cell><cell>4.24</cell><cell>-</cell><cell>-</cell><cell>1.05M</cell><cell>-</cell><cell>-</cell><cell>1.12T</cell></row><row><cell>SOF-VSR w scale-cascaded architecture</cell><cell>26.02</cell><cell>0.773</cell><cell>19.16</cell><cell>4.23</cell><cell cols="6">0.74M 0.59M 1.33M 36.40G 36.10G 108.90G</cell></row><row><cell>SOF-VSR w vanilla residual blocks</cell><cell>26.04</cell><cell>0.773</cell><cell>19.02</cell><cell>4.20</cell><cell cols="6">0.67M 0.89M 1.56M 45.51G 52.12G 143.14G</cell></row><row><cell>SOF-VSR</cell><cell>26.00</cell><cell>0.772</cell><cell>19.35</cell><cell>4.25</cell><cell cols="6">0.41M 0.59M 1.00M 36.40G 36.10G 108.90G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Average EPE results achieved on the training sets of Sintel, Middlebury, KITTI 2012 and KITTI 2015 for 4× SR. Best results are shown in boldface.</figDesc><table><row><cell></cell><cell></cell><cell>Upsampled</cell><cell>Super-resolved</cell><cell>FlowNet-S</cell><cell>SpyNet</cell></row><row><cell></cell><cell></cell><cell>flow</cell><cell>flow</cell><cell>[38]</cell><cell>[41]</cell></row><row><cell cols="2">Sintel clean</cell><cell>10.96</cell><cell>10.58</cell><cell>7.21</cell><cell>4.63</cell></row><row><cell cols="2">Sintel final</cell><cell>11.21</cell><cell>10.83</cell><cell>8.17</cell><cell>6.02</cell></row><row><cell cols="2">Middlebury</cell><cell>1.69</cell><cell>1.30</cell><cell>1.18</cell><cell>0.81</cell></row><row><cell>KITTI 2012</cell><cell>Noc All</cell><cell>23.02 30.03</cell><cell>22.24 29.28</cell><cell>13.55 19.24</cell><cell>7.62 13.30</cell></row><row><cell>KITTI 2015</cell><cell>Noc All</cell><cell>24.64 33.27</cell><cell>23.82 32.47</cell><cell>20.39 28.52</cell><cell>15.34 23.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Average RMSE and PSNR results achieved on the Vid4 dataset for 4× SR. Best results are shown in boldface. From Table III we can also see that images warped using super-resolved optical flows have lower RMSE values (3.26 vs. 3.46) and higher PSNR values (30.05 vs. 29.51).Visual comparison of optical flow estimation results achieved on the Sintel and Middlebury datasets is shown in Figs. 5 and 6, respectively. It can be observed that upsampled optical flows produce distorted and blurred edges (e.g., the hand inFig. 5and the bush inFig. 6) with notable artifacts. In contrast, more clear edges can be observed in super-resolved optical flows, with finer details being recovered. Moreover, our</figDesc><table><row><cell>RMSE: 3.05x10 -2</cell><cell>RMSE: 2.65x10 -2</cell></row><row><cell>RMSE: 3.56x10 -2</cell><cell>RMSE: 2.54x10 -2</cell></row><row><cell cols="2">Upsampled flow RMSE (×10 −2 ) PSNR 4.76 26.51 3.09 30.49 3.00 30.49 2.99 30.56 3.46 29.51 generated flows and then calculated root mean square error Super-resolved flow RMSE (×10 −2 ) PSNR Calendar 4.56 26.89 City 3.04 30.62 Foliage 2.71 31.37 Walk 2.74 31.31 Average 3.26 30.05 (RMSE) for quantitative evaluation. Reference Image Upsampled Flow Super-resolved Flow</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Comparison of accuracy and consistency performance achieved on the DAVIS-10 dataset. Note that, the first and last two frames are not used in our evaluation. FLOPs is computed based on HR frames with a resolution of 720p (1280×720). Best results are shown in boldface.</figDesc><table><row><cell cols="5">Model Scale Method</cell><cell></cell><cell cols="5">Frames PSNR(↑) SSIM(↑)</cell><cell>T-MOVIE(↓) (×10 −3 )</cell><cell>MOVIE(↓) (×10 −3 )</cell><cell>Params.</cell><cell>FLOPs</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Bicubic</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>36.43</cell><cell>0.958</cell><cell>4.63</cell><cell>0.70</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">DRCN [10]</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>40.62</cell><cell>0.979</cell><cell>1.09</cell><cell>0.13</cell><cell>1.8M</cell><cell>9,788.7G</cell></row><row><cell></cell><cell></cell><cell>×2</cell><cell cols="3">LapSRN [11]</cell><cell></cell><cell>1</cell><cell></cell><cell>40.30</cell><cell>0.978</cell><cell>1.05</cell><cell>0.12</cell><cell>0.8M</cell><cell>29.9G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CARN [55]</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>40.99</cell><cell>0.981</cell><cell>0.85</cell><cell>0.11</cell><cell>1.6M</cell><cell>222.8G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">VSRnet [13]</cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell>39.00</cell><cell>0.972</cell><cell>1.31</cell><cell>0.20</cell><cell>266K</cell><cell>242.7G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SOF-VSR</cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell>41.38</cell><cell>0.983</cell><cell>0.92</cell><cell>0.09</cell><cell>0.9M</cell><cell>342.8G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Bicubic</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>32.94</cell><cell>0.912</cell><cell>13.55</cell><cell>2.63</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">DRCN [10]</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>36.08</cell><cell>0.947</cell><cell>5.26</cell><cell>0.92</cell><cell>1.8M</cell><cell>9,788.7G</cell></row><row><cell>BI</cell><cell></cell><cell>×3</cell><cell cols="2">CARN [55] VSRnet [13]</cell><cell></cell><cell></cell><cell>1 5</cell><cell></cell><cell>36.70 34.94</cell><cell>0.952 0.936</cell><cell>4.44 6.11</cell><cell>0.79 1.20</cell><cell>1.6M 266K</cell><cell>118.8G 242.7G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SOF-VSR</cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell>36.80</cell><cell>0.955</cell><cell>4.36</cell><cell>0.68</cell><cell>1.1M</cell><cell>205.0G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Bicubic</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>30.97</cell><cell>0.870</cell><cell>22.73</cell><cell>4.75</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">DRCN [10]</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>33.49</cell><cell>0.911</cell><cell>13.51</cell><cell>2.48</cell><cell>1.8M</cell><cell>9,788.7G</cell></row><row><cell></cell><cell></cell><cell>×4</cell><cell cols="3">LapSRN [11]</cell><cell></cell><cell>1</cell><cell></cell><cell>33.54</cell><cell>0.911</cell><cell>12.83</cell><cell>2.43</cell><cell>0.8M</cell><cell>149.4G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CARN [55]</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>34.12</cell><cell>0.921</cell><cell>11.41</cell><cell>2.05</cell><cell>1.6M</cell><cell>90.9G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">VSRnet [13]</cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell>32.63</cell><cell>0.897</cell><cell>14.63</cell><cell>2.85</cell><cell>266K</cell><cell>242.7G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">SOF-VSR [17]</cell><cell></cell><cell>3</cell><cell></cell><cell>34.32*</cell><cell>0.925*</cell><cell>11.77*</cell><cell>1.96*</cell><cell>1.5M</cell><cell>105.2G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SOF-VSR</cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell>34.28</cell><cell>0.926</cell><cell>11.72</cell><cell>1.94</cell><cell>1.0M</cell><cell>112.5G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SPMC [16]</cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell>33.02</cell><cell>0.911</cell><cell>14.06</cell><cell>1.96</cell><cell>1.7M</cell><cell>160.8G</cell></row><row><cell>BD</cell><cell></cell><cell>×4</cell><cell cols="4">SOF-VSR-BD [17]</cell><cell>3</cell><cell></cell><cell>34.27*</cell><cell>0.925*</cell><cell>10.93*</cell><cell>1.90*</cell><cell>1.5M</cell><cell>105.2G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">SOF-VSR-BD</cell><cell></cell><cell>3</cell><cell></cell><cell>34.28</cell><cell>0.927</cell><cell>10.91</cell><cell>1.87</cell><cell>1.0M</cell><cell>112.5G</cell></row><row><cell></cell><cell>26.50</cell><cell cols="2">SOF-VSR-BD</cell><cell></cell><cell></cell><cell cols="2"># of Params.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>26.25</cell><cell></cell><cell></cell><cell>SOF-VSR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>26.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">250K 500K 1M</cell><cell cols="2">2M</cell></row><row><cell>PSNR</cell><cell>25.50 25.75</cell><cell cols="2">SPMC</cell><cell></cell><cell></cell><cell>TDVSR</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>25.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DRCN</cell></row><row><cell></cell><cell>25.00</cell><cell></cell><cell></cell><cell>CARN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">16 24.75</cell><cell>18</cell><cell>20</cell><cell>22</cell><cell>24 LapSRN</cell><cell></cell><cell>26</cell><cell>28 VSRnet</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">T-MOVIE (x10 -3 )</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Fig. 8: Consistency and accuracy performance achieved on the</cell></row><row><cell cols="10">Vid4 dataset for 4× SR. Solid and hollow circles represent</cell></row><row><cell cols="10">methods developed for the BI and BD degradation models,</cell></row><row><cell cols="10">respectively. A lower T-MOVIE value represents a better con-</cell></row><row><cell cols="10">sistency performance, while a higher PSNR value represents</cell></row><row><cell cols="10">a better accuracy performance. The size of a circle represents</cell></row><row><cell cols="5">the number of parameters.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Visual comparison of 4× SR results on Calendar and City. Bicubic, LapSRN<ref type="bibr" target="#b10">[11]</ref>, CARN<ref type="bibr" target="#b54">[55]</ref>, VSRnet<ref type="bibr" target="#b12">[13]</ref>, TDVSR<ref type="bibr" target="#b33">[34]</ref>, and SOF-VSR are based on the BI degradation model, while SPMC<ref type="bibr" target="#b15">[16]</ref> and SOF-VSR-BD are based on the BD degradation model. Blue boxes represent corresponding temporal profiles.visual quality, such as the pattern on the shorts and the word "PEUA". Specifically, the patterns on the shorts recovered by Bicubic and VSRnet are obviously blurred. Although finer details can be recovered by DRCN, LapSRN and SPMC, their resulting patterns are still distorted. In contrast, our networks produce more clear details with fewer artifacts.</figDesc><table><row><cell></cell><cell>Calendar</cell><cell></cell><cell></cell><cell>City</cell><cell></cell></row><row><cell>Groundtruth</cell><cell>Bicubic</cell><cell>LapSRN</cell><cell>Groundtruth</cell><cell>Bicubic</cell><cell>LapSRN</cell></row><row><cell>CARN</cell><cell>VSRnet</cell><cell>TDVSR</cell><cell>CARN</cell><cell>VSRnet</cell><cell>TDVSR</cell></row><row><cell>SOF-VSR</cell><cell>SPMC</cell><cell>SOF-VSR-BD</cell><cell>SOF-VSR</cell><cell>SPMC</cell><cell>SOF-VSR-BD</cell></row><row><cell>Fig. 9:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI :</head><label>VI</label><figDesc>Network architecture of the classifier used for face recognition. architecture details are shown inTable VI. The classification network takes a 60×60 facial image as input and predicts the class of the subject.Implementation details. During test, we first superresolved each LR test clip using a specific SR method and then fed the SR results to the classifier. Note that, we did not fine-tune these SR methods on the Youtube Face dataset.</figDesc><table><row><cell>Layer Settings</cell><cell>Output Size</cell></row><row><cell>Input</cell><cell>1 × 60 × 60</cell></row><row><cell>9×9 Conv, stride 1, padding 0, ReLU</cell><cell>64 × 52 × 52</cell></row><row><cell>5×5 Conv, stride 1, padding 0, ReLU</cell><cell>32 × 48 × 48</cell></row><row><cell>4×4 Conv, stride 1, padding 0, ReLU</cell><cell>60 × 45 × 45</cell></row><row><cell cols="2">Max pooling, kernel 2, stride 2, padding 0 60 × 23 × 23</cell></row><row><cell>3×3 Conv, stride 1, padding 0, ReLU</cell><cell>80 × 21 × 21</cell></row><row><cell cols="2">Max pooling, kernel 2, stride 2, padding 0 80 × 11 × 11</cell></row><row><cell>Fully connected</cell><cell>167</cell></row><row><cell>classifier, whose</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII :</head><label>VII</label><figDesc>Face recognition performance achieved by different SR methods on a subset of the YouTube Face dataset under 4× SR scenario. Best results are shown in boldface.</figDesc><table><row><cell>Model</cell><cell cols="3">Model Top-1 Accuracy Top-5 Accuracy</cell></row><row><cell>Original</cell><cell>-</cell><cell>62.7%</cell><cell>75.1%</cell></row><row><cell>Bicubic</cell><cell></cell><cell>41.1%</cell><cell>58.8%</cell></row><row><cell>LapSRN [11] CARN [55]</cell><cell>BI</cell><cell>55.9% 56.9%</cell><cell>71.4% 72.4%</cell></row><row><cell>SOF-VSR</cell><cell></cell><cell>58.1%</cell><cell>74.1%</cell></row><row><cell>SPMC [16] SOF-VSR-BD</cell><cell>BD</cell><cell>55.8% 57.6%</cell><cell>71.4% 72.9%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">www.cdvl.org 2 media.xiph.org/video/derf/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A computationally efficient superresolution image reconstruction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="583" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image upsampling via imposed edge statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local selfexamples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2011-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalizing the nonlocal-means to super-resolution reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="51" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super-resolution without explicit subpixel motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1958" to="1975" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optical flow based super-resolution: A probabilistic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fransens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="115" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On bayesian adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="346" to="360" />
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Handling motion blur in multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5224" to="5232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5835" to="5843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video super-resolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="531" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video superresolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2848" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous super-resolution of depth and images using a single camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4482" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning for video super-resolution through HR optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Filters for common resampling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Turkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics gems</title>
		<imprint>
			<publisher>Academic Press Professional, Inc</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="147" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Super-resolution image reconstruction: a technical overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="36" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image up-sampling using total-variation regularization with a new observation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1647" to="1659" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient profile prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single-image super-resolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8692</biblScope>
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2790" to="2798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiframe image restoration and registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Vision and Image Processing</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="317" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Extraction of high-resolution frames from video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="996" to="1011" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint MAP registration and high-resolution image estimation using a sequence of undersampled images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Armstrong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1621" to="1633" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6626" to="6634" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video super-resolution and via bidirectional and recurrent convolutional and networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep video superresolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jaeyeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1709.02371" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2720" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno>abs/1802.01770</idno>
		<ptr target="http://arxiv.org/abs/1802.01770" />
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-end learning of video superresolution with motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in GCPR</title>
		<imprint>
			<biblScope unit="volume">10496</biblScope>
			<biblScope unit="page" from="203" to="214" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1704.00675</idno>
		<title level="m">The 2017 DAVIS challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Motion tuned spatio-temporal quality assessment of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename></persName>
		</author>
		<title level="m">NTIRE 2017 challenge on single image super-resolution: Methods and results</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
	<note>CVPR Workshops</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, ser. Lecture Notes in Computer Science</title>
		<editor>A. W. Fitzgibbon, S. Lazebnik, P. Perona, Y. Sato, and C. Schmid</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fast, accurate, and lightweight superresolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning temporal dynamics for video super-resolution: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Is image super-resolution helpful for other vision tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in WACV</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
