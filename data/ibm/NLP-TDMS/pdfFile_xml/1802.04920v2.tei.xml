<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DVAE++: Discrete Variational Autoencoders with Overlapping Transformations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbing</forename><surname>Bian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Khoshaman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Andriyash</surname></persName>
						</author>
						<title level="a" type="main">DVAE++: Discrete Variational Autoencoders with Overlapping Transformations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training of discrete latent variable models remains challenging because passing gradient information through discrete units is difficult. We propose a new class of smoothing transformations based on a mixture of two overlapping distributions, and show that the proposed transformation can be used for training binary latent models with either directed or undirected priors. We derive a new variational bound to efficiently train with Boltzmann machine priors. Using this bound, we develop DVAE++, a generative model with a global discrete prior and a hierarchy of convolutional continuous variables. Experiments on several benchmarks show that overlapping transformations outperform other recent continuous relaxations of discrete latent variables including Gumbel-Softmax (Maddison et al., 2016;<ref type="bibr" target="#b13">Jang et al., 2016)</ref>, and discrete variational autoencoders (Rolfe, 2016).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training of discrete latent variable models remains challenging because passing gradient information through discrete units is difficult. We propose a new class of smoothing transformations based on a mixture of two overlapping distributions, and show that the proposed transformation can be used for training binary latent models with either directed or undirected priors. We derive a new variational bound to efficiently train with Boltzmann machine priors. Using this bound, we develop DVAE++, a generative model with a global discrete prior and a hierarchy of convolutional continuous variables. Experiments on several benchmarks show that overlapping transformations outperform other recent continuous relaxations of discrete latent variables including Gumbel-Softmax <ref type="bibr" target="#b22">(Maddison et al., 2016;</ref><ref type="bibr" target="#b13">Jang et al., 2016)</ref>, and discrete variational autoencoders (Rolfe, 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have seen rapid progress in generative modeling made possible by advances in deep learning and stochastic variational inference. The reparameterization trick <ref type="bibr" target="#b16">Rezende et al., 2014)</ref> has made stochastic variational inference efficient by providing lower-variance gradient estimates. However, reparameterization, as originally proposed, does not easily extend to semi-supervised learning, binary latent attribute models, topic modeling, variational memory addressing, hard attention models, or clustering, which require discrete latentvariables.</p><p>Continuous relaxations have been proposed for accommodating discrete variables in variational inference <ref type="bibr" target="#b22">(Maddison et al., 2016;</ref><ref type="bibr" target="#b13">Jang et al., 2016;</ref><ref type="bibr">Rolfe, 2016</ref> Softmax technique <ref type="bibr" target="#b22">(Maddison et al., 2016;</ref><ref type="bibr" target="#b13">Jang et al., 2016)</ref> defines a temperature-based continuous distribution that in the zero-temperature limit converges to a discrete distribution. However, it is limited to categorical distributions and does not scale to multivariate models such as Boltzmann machines (BM). The approach presented in <ref type="bibr">(Rolfe, 2016)</ref> can train models with BM priors but requires careful handling of the gradients during training.</p><p>We propose a new class of smoothing transformations for relaxing binary latent variables. The method relies on two distributions with overlapping support that in the zero temperature limit converge to a Bernoulli distribution. We present two variants of smoothing transformations using a mixture of exponential and a mixture of logistic distributions.</p><p>We demonstrate that overlapping transformations can be used to train discrete directed latent models as in <ref type="bibr" target="#b22">(Maddison et al., 2016;</ref><ref type="bibr" target="#b13">Jang et al., 2016)</ref>, and models with BMs in their prior as in <ref type="bibr">(Rolfe, 2016)</ref>. In the case of BM priors, we show that the Kullback-Leibler (KL) contribution to the variational bound can be approximated using an analytic expression that can be optimized using automatic differentiation without requiring the special treatment of gradients in <ref type="bibr">(Rolfe, 2016)</ref>.</p><p>Using this analytic bound, we develop a new variational autoencoder (VAE) architecture called DVAE++, which uses a BM prior to model discontinuous latent factors such as object categories or scene configuration in images. DVAE++ is inspired by <ref type="bibr">(Rolfe, 2016)</ref> and includes continuous local latent variables to model locally smooth features in the data. DVAE++ achieves comparable results to the state-of-the-art techniques on several datasets and captures semantically meaningful discrete aspects of the data. We show that even when all continuous latent variables are removed, DVAE++ still attains near state-of-the-art generative likelihoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Training of models with discrete latent variables z z z requires low-variance estimates of gradients of the form ∇ φ E q φ (z z z) [f (z z z)]. Only when z z z has a modest number of configurations (as in semi-supervised learning  or semi-supervised generation <ref type="bibr" target="#b21">(Maaløe et al., 2017))</ref> arXiv:1802.04920v2 <ref type="bibr">[cs.</ref>LG] 25 May 2018 can the gradient of the expectation be decomposed into a summation over configurations.</p><p>The REINFORCE technique <ref type="bibr">(Williams, 1992)</ref> is a more scalable method that migrates the gradient inside the expectation:</p><formula xml:id="formula_0">∇ φ E q φ (z z z) f (z z z) = E q φ (z z z) [f (z z z)∇ φ log q φ (z z z)].</formula><p>Although the REINFORCE estimate is unbiased, it suffers from high variance and carefully designed "control variates" are required to make it practical. Several works use this technique and differ in their choices of the control variates. NVIL <ref type="bibr">(Mnih &amp; Gregor, 2014)</ref> uses a running average of the function, f (z z z), and an input-dependent baseline. VIMCO <ref type="bibr">(Mnih &amp; Rezende, 2016</ref>) is a multi-sample version of NVIL that has baselines tailored for each sample based on all the other samples. MuProp <ref type="bibr" target="#b7">(Gu et al., 2015)</ref> and DARN <ref type="bibr" target="#b6">(Gregor et al., 2013)</ref> are two other REINFORCE-based methods (with non-zero biases) that use a Taylor expansion of the function f (z z z) to create control variates.</p><p>To address the high variance of REINFORCE, other work strives to make discrete variables compatible with the reparametrization technique. A primitive form arises from estimating the discrete variables by a continuous function during back-propagation. For instance, in the case of Bernoulli distribution, the latent variables can be approximated by their mean value. This approach is called the straight-through (ST) estimator <ref type="bibr">(Bengio et al., 2013</ref>). Another way to make discrete variables compatible with the reparametrization is to relax them into a continuous distribution. Concrete <ref type="bibr" target="#b22">(Maddison et al., 2016)</ref> or Gumbel-Softmax <ref type="bibr" target="#b13">(Jang et al., 2016)</ref> adopt this strategy by adding Gumbel noise to the logits of a softmax function with a temperature hyperparameter. A slope-annealed version of the ST estimator is proposed by <ref type="bibr" target="#b3">(Chung et al., 2016)</ref> and is equivalent to the Gumbel-Softmax approach for binary variables. <ref type="bibr">RE-BAR (Tucker et al., 2017)</ref> is a recent method that blends REINFORCE with Concrete to synthesize control variates. (Rolfe, 2016) pairs discrete variables with auxiliary continuous variables and marginalizes out the discrete variables.</p><p>Both overlapping transformations and Gumbel-based approaches offer smoothing through non-zero temperature; however, overlapping transformations offer additional freedom through the choice of the mixture distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Let x x x represent observed random variables and z z z latent variables. The joint distribution over these variables is defined by the generative model p(x x x, z z z) = p(z z z)p(x x x|z z z), where p(z z z) is a prior distribution and p(x x x|z z z) is a probabilistic decoder. Given a dataset X X X = {x x x (1) , . . . , x x x (N ) }, the parameters of the model are trained by maximizing the log-likelihood:</p><formula xml:id="formula_1">log p(X X X) = N i=1 log p(x x x (i) ).</formula><p>Typically, computing log p(x x x) requires an intractable marginalization over the latent variables z z z. To address this problem, the VAE ) introduces an inference model or probabilistic encoder q(z z z|x x x) that infers latent variables for each observation. In the VAE, instead of the maximizing the marginal log-likelihood, a variational lower bound (ELBO) is maximized:</p><formula xml:id="formula_2">log p(x x x) ≥ E q(z z z|x x x) log p(x x x|z z z) − KL q(z z z|x x x)||p(z z z) . (1)</formula><p>The gradient of this objective is computed for the parameters of both the encoder and decoder using the reparameterization trick. With reparametrization, the expectation with respect to q(z z z|x x x) in Eq. (1) is replaced with an expectation with respect to a known optimization-parameterindependent base distribution and a differentiable transformation from the base distribution to q(z z z|x x x). This transformation may be a scale-shift transformation, in the case of Gaussian base distributions, or rely on the inverse cumulative distribution function (CDF) in the general case. Following the law of the unconscious statistician, the gradient is then estimated using samples from the base distribution.</p><p>Unfortunately, the reparameterization trick cannot be applied directly to the discrete latent variables because there is no differentiable transformation that maps a base distribution to a discrete distribution. Current remedies address this difficulty using a continuous relaxation of the discrete latent variables <ref type="bibr" target="#b22">(Maddison et al., 2016;</ref><ref type="bibr" target="#b13">Jang et al., 2016)</ref>. The discrete variational autoencoder (DVAE) (Rolfe, 2016) develops a different approach which applies the reparameterization trick to a marginal distribution constructed by pairing each discrete variable with an auxiliary continuous random variable.</p><p>For example, let z ∈ {0, 1} represent a binary random variable with the probability mass function q(z|x). A smoothing transformation is defined using spike-and-exponential transformation r(ζ|z), where r(ζ|z = 0) = δ(ζ) is a Dirac δ distribution and r(ζ|z = 1) ∝ exp(βζ) is an exponential distribution defined for ζ ∈ [0, 1] with inverse temperature β that controls the sharpness of the distribution. (Rolfe, 2016) notes that the autoencoding term can be defined as:</p><formula xml:id="formula_3">z q(z|x) dζ r(ζ|z) log p(x|ζ) = dζ q(ζ|x) log p(x|ζ),</formula><p>where the marginal</p><formula xml:id="formula_4">q(ζ|x) = z q(z|x)r(ζ|z)<label>(2)</label></formula><p>is a mixture of two continuous distributions. By factoring the inference model so that x depends on ζ rather than z, the discrete variables can be explicitly eliminated from the ELBO and the reparameterization trick applied. for ρ = 0.5 in comparison to the spike-and-exp smoothing <ref type="bibr">(Rolfe, 2016)</ref>. The inverse CDF resulting from the mixture of exponential distributions approximates the step function that samples from the Bernoulli distribution.</p><p>The smoothing transformations in (Rolfe, 2016) are limited to spike-and-X type of transformations (e.g., spike-and-exp and spike-and-Gaussian) where r(ζ|z = 0) is assumed to be a Dirac δ distribution. This property is required for computing the gradient of the KL term in the variational lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overlapping Transformations</head><p>A symmetric smoothing transformation of binary variables can also be defined using two exponential distributions:</p><p>r(ζ|z = 0) = e −βζ Z β and r(ζ|z = 1) = e β(ζ−1) Z β , for ζ ∈ [0, 1], where Z β = (1−e −β )/β. These conditionals, visualized in <ref type="figure">Fig. 1(a)</ref>, define the mixture distribution q(ζ|x) of Eq.</p><p>(2). The scalar β acts as an inverse temperature as in the Gumbel softmax relaxation, and as β → ∞, q(ζ|x) approaches q(z = 0|x)δ(ζ) + q(z = 1|x)δ(ζ − 1).</p><p>Application of the reparameterization trick for q(ζ|x) requires the inverse CDF of q(ζ|x). In Appendix A of the supplementary material, we show that the inverse CDF is</p><formula xml:id="formula_5">F −1 q(ζ|x) (ρ) = − 1 β log −b + √ b 2 − 4c 2 (3) where b = [ρ + e −β (q − ρ)]/(1 − q) − 1 and c = −[qe −β ]/(1 − q). Eq.</formula><p>(3) is a differentiable function that converts a sample ρ from the uniform distribution U(0, 1) to a sample from q(ζ|x). As shown in <ref type="figure">Fig. 1(b)</ref> the inverse CDF approaches a step function as β → ∞. However, to benefit from gradient information during training, β is set to a finite value. Appendix C provides further visualizations comparing overlapping transformations to Concrete smoothing <ref type="bibr" target="#b22">(Maddison et al., 2016;</ref><ref type="bibr" target="#b13">Jang et al., 2016)</ref>.</p><p>The overlapping exponential distributions defined here can be generalized to any pair of smooth distributions converging to δ(ζ) and δ(ζ − 1). In Appendix B, we provide analogous results for logistic smoothing distributions.</p><formula xml:id="formula_6">z z z 1 z z z 2 x x x (a) x x x z z z 1 z z z 2 (b) z z z 1 ζ ζ ζ 1 z z z 2 ζ ζ ζ 2 x x x (c) x x x z z z 1 ζ ζ ζ 1 z z z 2 ζ ζ ζ 2 (d) ζ ζ ζ 1 ζ ζ ζ 2 x x x (e) x x x ζ ζ ζ 1 ζ ζ ζ 2 (f) z z z 1 z z z 1 ζ ζ ζ 1 z z z 2 ζ ζ ζ 2 x x x (g)</formula><p>Next, we apply overlapping transformations to the training of generative models with discrete latent variables. We consider both directed and undirected latent variable priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Directed Prior</head><p>The simplest discrete prior is factorial; however, with conditioning, we can build complex dependencies. To simplify presentation, we illustrate a VAE prior with one or two groups of conditioning variables, but note that the approach straight-forwardly generalizes to many conditioning groups.</p><p>Our approach parallels the method developed in (Rolfe, 2016) for undirected graphical models. Consider the generative model in <ref type="figure" target="#fig_1">Fig. 2</ref>(a) and its corresponding inference model in <ref type="figure">Fig</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Joint ELBO</head><p>Assuming that p(z z z 1 ), p(z z z 2 |ζ ζ ζ 1 ), q(z z z 1 |x x x), q(z z z 2 |x x x, ζ ζ ζ 1 ), r(ζ ζ ζ 1 |z z z 1 ), and r(ζ ζ ζ 2 |z z z 2 ) are factorial in both the inference and generative models, then q(ζ ζ ζ 1 |x x x) and q(ζ ζ ζ 2 |ζ ζ ζ 1 , x x x) are also factorial with q(ζ ζ ζ 1 |x x x) = i q(ζ 1,i |x x x) where q(ζ 1,i |x x x) = z1,i r(ζ 1,i |z 1,i )q(z 1,i |x x x), and q(ζ ζ ζ 2 |ζ ζ ζ 1 , x x x) = i q(ζ 2,i |ζ ζ ζ 1 , x x x) where q(ζ 2,i |ζ ζ ζ 1 , x x x) = z2,i r(ζ 2,i |z 2,i )q(z 2,i |ζ ζ ζ 1 , x x x). In this case, the ELBO for the model in <ref type="figure" target="#fig_1">Fig. 2</ref>(c) and 2(d) is</p><formula xml:id="formula_7">E q(ζ ζ ζ 1 |x x x) E q(ζ ζ ζ 2 |ζ ζ ζ 1 ,x x x) [log p(x x x|ζ ζ ζ 1 , ζ ζ ζ 2 )] − KL(q(z z z 1 |x x x)||p(z z z 1 )) − E q(ζ ζ ζ 1 |x x x) [KL(q(z z z 2 |x x x, ζ ζ ζ 1 )||p(z z z 2 |ζ ζ ζ 1 ))] .<label>(4)</label></formula><p>The KL terms corresponding to the divergence between factorial Bernoulli distributions have a closed form. The expectation over ζ ζ ζ 1 and ζ ζ ζ 2 is reparameterized using the technique presented in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Marginal ELBO</head><p>The ELBO for the marginal graphical model of <ref type="figure" target="#fig_1">Fig. 2</ref>(e) and <ref type="figure" target="#fig_1">Fig. 2</ref></p><formula xml:id="formula_8">(f) is E q(ζ ζ ζ 1 |x x x) E q(ζ ζ ζ 2 |x x x,ζ ζ ζ 1 ) [log p(x x x|ζ ζ ζ 1 , ζ ζ ζ 2 )] − KL(q(ζ ζ ζ 1 |x x x)||p(ζ ζ ζ 1 )) − E q(ζ ζ ζ 1 |x x x) [KL(q(ζ ζ ζ 2 |x x x, ζ ζ ζ 1 )||p(ζ ζ ζ 2 |ζ ζ ζ 1 ))] (5) with p(ζ ζ ζ 1 ) = i p(ζ 1,i ) where p(ζ 1,i ) = zi r(ζ 1,i |z 1,i )p(z 1,i ) and p(ζ ζ ζ 2 |ζ ζ ζ 1 ) = i p(ζ 2,i |ζ ζ ζ 1 ) where p(ζ 2,i |ζ ζ ζ 1 ) = z2,i r(ζ 2,i |z 2,i )p(z 2,i |ζ ζ ζ 1 ).</formula><p>The KL terms no longer have a closed form but can be estimated with the Monte Carlo method. In Appendix D, we show that Eq. (5) provides a tighter bound on log p(x x x) than does Eq. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Boltzmann Machine Prior</head><p>(Rolfe, 2016) defined an expressive prior over binary latent variables by using a Boltzmann machine. We build upon that work and present a simpler objective that can still be trained with a low-variance gradient estimate.</p><p>To simplify notation, we assume that the prior distribution over the latent binary variables is a restricted Boltzmann machine (RBM), but these results can be extended to general BMs. An RBM defines a probability distribution over binary random variables arranged on a bipartite graph as p(z z z 1 , z z z 2 ) = e −E(z z z1,z z z2) /Z where E(z z z 1 , z z z 2 ) = −a a a T 1 z z z 1 − a a a T 2 z z z 2 − z z z T 1 W W Wz z z 2 is an energy function with linear biases a a a 1 and a a a 2 , and pairwise interactions W W W . Z is the partition function. The autoencoding contribution to the ELBO with an RBM prior is again the first term in Eq. (4) since both models share the same inference model structure. However, computing the KL term with the RBM prior is more challenging. Here, a novel formulation for the KL term is introduced. Our derivation can be used for training discrete variational autoencoders with a BM prior without any manual coding of gradients.</p><formula xml:id="formula_9">We use E q(z z z,ζ ζ ζ|x x x) [f ] = E q(ζ ζ ζ|x x x) E q(z z z|x x x,ζ ζ ζ) [f ] to compute the KL contribution to the ELBO: KL q(z z z1, z z z2, ζ ζ ζ 1 , ζ ζ ζ 2 |x x x) p(z z z1, z z z2, ζ ζ ζ 1 , ζ ζ ζ 2 ) = log Z − H q(z z z1|x x x) − E q(ζ ζ ζ 1 |x x x) H q(z z z2|x x x, ζ ζ ζ 1 ) + (6) + E q(ζ ζ ζ 1 |x x x) E q(ζ ζ ζ 2 |x x x,ζ ζ ζ 1 ) E q(z z z 1 |x x x,ζ ζ ζ 1 ) E q(z z z 2 |x x x,ζ ζ ζ 1 ,ζ ζ ζ 2 ) E(z z z1, z z z2) cross-entropy .</formula><p>Here, H(q) is the entropy of the distribution q, which has a closed form when q is factorial Bernoulli. The conditionals q(z z z 1 |x x x, ζ ζ ζ 1 ) and q(z z z 2 |x x x, ζ ζ ζ 1 , ζ ζ ζ 2 ) are both factorial distributions that have analytic expressions. Denoting</p><formula xml:id="formula_10">µ 1,i (x x x) ≡ q(z 1,i = 1|x x x), ν 1,i (x x x, ζ ζ ζ 1 ) ≡ q(z 1,i = 1|x x x, ζ ζ ζ 1 ), µ 2,i (x x x, ζ ζ ζ 1 ) ≡ q(z 2,i = 1|x x x, ζ ζ ζ 1 ), ν 2,i (x x x, ζ ζ ζ 1 , ζ ζ ζ 2 ) ≡ q(z 2,i = 1|x x x, ζ ζ ζ 1 , ζ ζ ζ 2 ),</formula><p>it is straightforward to show that</p><formula xml:id="formula_11">ν 1,i (x x x, ζ ζ ζ 1 ) = q(z 1,i = 1|x x x)r(ζ 1,i |z 1,i = 1) z1,i q(z 1,i |x x x)r(ζ 1,i |z 1,i ) = = σ g(µ 1,i (x x x)) + log r(ζ 1,i |z = 1) r(ζ 1,i |z = 0) , where σ(x) = 1/(1 + e −x )</formula><p>is the logistic function, and g(µ) ≡ log µ/ 1 − µ is the logit function. A similar expression holds for ν ν ν 2 (x x x, ζ ζ ζ 1 , ζ ζ ζ 2 ). The expectation marked as cross-entropy in Eq. (6) corresponds to the cross-entropy between a factorial distribution and an unnormalized Boltzmann machine which is</p><formula xml:id="formula_12">−a a a T 1 ν ν ν 1 (x x x, ζ ζ ζ 1 )−a a a T 2 ν ν ν 2 (x x x, ζ ζ ζ 1 , ζ ζ ζ 2 )−ν ν ν 1 (x x x, ζ ζ ζ 1 ) T W W Wν ν ν 2 (x x x, ζ ζ ζ 1 , ζ ζ ζ 2 ). Finally, we use the equalities E q(ζ ζ ζ 1 |x x x) [ν ν ν 1 (x x x, ζ ζ ζ 1 )] = µ µ µ 1 (x x x)</formula><p>and E q(ζ ζ ζ 2 |x x x,ζ ζ ζ 1 ) [ν ν ν 2 (x x x, ζ ζ ζ 1 , ζ ζ ζ 2 )] = µ µ µ 2 (x x x, ζ ζ ζ 1 ) to simplify the cross-entropy term which defines the KL as</p><formula xml:id="formula_13">KL q(z z z 1 , z z z 2 , ζ ζ ζ 1 , ζ ζ ζ 2 |x x x) p(z z z 1 , z z z 2 , ζ ζ ζ 1 , ζ ζ ζ 2 ) = log Z − H q(z z z 1 |x x x) − E q(ζ ζ ζ 1 |x x x) H q(z z z 2 |x x x, ζ ζ ζ 1 ) − a a a T 1 µ µ µ 1 (x x x) − E q(ζ ζ ζ 1 |x x x) a a a T 2 µ µ µ 2 (x x x, ζ ζ ζ 1 ) − E q(ζ ζ ζ 1 |x x x) ν ν ν 1 (x x x, ζ ζ ζ 1 ) T W W Wµ µ µ 2 (x x x, ζ ζ ζ 1 ) .</formula><p>All terms contributing to the KL other than log Z can be computed analytically given samples from the hierarchical encoder. Expectations with respect to q(ζ ζ ζ 1 |x x x) are reparameterized using the inverse CDF function. Any automatic differentiation (AD) library can then back-propagate gradients through the network. Only log Z requires special treatment. In Appendix E, we show how this term can also be included in the objective function so that its gradient is computed automatically. The ability of AD to calculate gradients stands in contrast to <ref type="bibr">(Rolfe, 2016)</ref> where gradients must be manually coded. This pleasing property is a result of r(ζ|z) having the same support for both z = 0 and z = 1, and having a probabilistic q(z|x, ζ) which is not the case for the spike-and-X transformations of (Rolfe, 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DVAE++</head><p>In previous sections, we have illustrated with simple examples how overlapping transformations can be used to train discrete latent variable models with either directed or undirected priors. Here, we develop a network architecture (DVAE++) that improves upon convolutional VAEs for generative image modeling. DVAE++ features both global discrete latent variables (to capture global properties such as scene or object type) and local continuous latent variables (to capture local properties such as object pose, orientation, or style). Both generative and inference networks rely on an autoregressive structure defined over groups of latent and observed variables. As we are modeling images, conditional dependencies between groups of variables are captured with convolutional neural networks. DVAE++ is similar to the convolutional VAEs used in <ref type="bibr" target="#b2">Chen et al., 2016)</ref>, but does not use normalizing flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Graphical Model</head><p>The DVAE++ graphical model is visualized in <ref type="figure" target="#fig_4">Fig. 3</ref>. Global and local variables are indicated by z z z and h h h respectively. Subscripts indicate different groups of random variables. The conditional distribution of each group is factorialexcept for z z z 1 and z z z 2 in the prior, which is modeled with an RBM. Global latent variables are represented with boxes and local variables are represented with 3D volumes as they are convolutional.</p><p>Groups of local continuous variables are factorial (independent). This assumption limits the ability of the model to capture correlations at different spatial locations and different depths. While the autoregressive structure mitigates this defect, we rely mainly on the discrete global latent variables to capture long-range dependencies. The discrete nature of the global RBM prior allows DVAE++ to capture richlycorrelated discontinuous hidden factors that influence data generation. <ref type="figure" target="#fig_4">Fig. 3</ref>(a) defines the generative model as</p><formula xml:id="formula_14">p(z z z, ζ ζ ζ, h h h, x x x) = p(z z z) i r(ζ 1,i |z 1,i )r(ζ 2,i |z 2,i )× j p(h h h j |h h h &lt;j , ζ ζ ζ)p(x x x|ζ ζ ζ, h h h)</formula><p>where p(z z z) is an RBM, ζ ζ ζ = [ζ ζ ζ 1 , ζ ζ ζ 2 ], and r is the smoothing transformation that is applied elementwise to z z z. The conditional p(h h h j |h h h &lt;j , ζ ζ ζ) is defined over the j th local variable group using a factorial normal distribution. Inspired by <ref type="bibr">(Reed et al., 2017;</ref><ref type="bibr" target="#b5">Denton et al., 2015)</ref>, the conditional on the data variable p(x x x|ζ ζ ζ, h h h) is decomposed into several factors defined on different scales of x x x:</p><formula xml:id="formula_15">p(x x x|ζ ζ ζ, h h h) = p(x x x 0 |ζ ζ ζ, h h h) i p(x x x i |ζ ζ ζ, h h h, x x x &lt;i )</formula><p>Here, x x x 0 is of size 4 × 4 and it represents downsampled x x x in the lowest scale. Conditioned on x x x 0 , we generate x x x 1 in the next scale, which is of the size 8 × 8. This process is continued until the full-scale image is generated (see Appendix G.1 for more details). Here, each conditional is represented using a factorial distribution. For binary images, a factorial Bernoulli distribution is used; for colored images a factorial mixture of discretized logistic distributions is used <ref type="bibr">(Salimans et al., 2017)</ref>.</p><p>The inference model of <ref type="figure" target="#fig_4">Fig. 3</ref>(b) conditions over latent variables in a similar order as the generative model:</p><formula xml:id="formula_16">q(z z z, ζ ζ ζ, h h h|x x x) = q(z z z 1 |x x x) i r(ζ 1,i |z 1,i )× q(z z z 2 |x x x, ζ ζ ζ 1 ) k r(ζ 2,k |z 2,k ) j q(h h h j |ζ ζ ζ, h h h &lt;j ).</formula><p>The conditionals q(z z z 1 |x x x) and q(z z z 2 |x x x, ζ ζ ζ 1 ) are each modeled with a factorial Bernoulli distribution, and q(h h h j |ζ ζ ζ, h h h &lt;j ) represents the conditional on the j th group of local variables. DVAE++ is related to VAEs with mixture priors <ref type="bibr" target="#b23">(Makhzani et al., 2015;</ref><ref type="bibr">Tomczak &amp; Welling, 2017)</ref>. The discrete variables z z z 1 and z z z 2 take exponentially many joint configurations where each configuration corresponds to a mixture component. These components are mixed by p(z z z 1 , z z z 2 ) in the generative model. During training, the inference model maps each data point to a small subset of all the possible mixture components. Thus, the discrete prior learns to suppress the probability of configurations that are not used by the inference model. Training results in a multimodal p(z z z 1 , z z z 2 ) that assigns similar images to a common discrete mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Neural Network Architecture</head><p>We use a novel neural network architecture to realize the conditional probabilities within the graphical model <ref type="figure" target="#fig_4">Fig. 3</ref>. The network uses residual connections <ref type="bibr" target="#b8">(He et al., 2016)</ref> with squeeze-and-excitation (SE) blocks <ref type="bibr" target="#b9">(Hu et al., 2017)</ref> that have shown state-of-the-art image classification performance. Our architecture is explained fully in Appendix G, and here we sketch the main components. We refer to a SE-ResNet block as a residual block, and the network is created by combining either residual blocks, fully-connected layers, or convolutional layers.</p><p>The encoder uses a series of downsampling residual blocks to extract convolutional features from an input image. This residual network is considered as a pre-processing step that extracts convolutional feature maps at different scales. The output of this network at the highest level is fed to fullyconnected networks that define q(z z z i |x x x, ζ ζ ζ &lt;i ) successively for In the data space, a distribution on the smallest scale x x x 0 is formed using a residual network. Given samples at this scale, the distribution at the next scale is formed using another upsampling residual network. This process is repeated until the image is generated at full scale.</p><formula xml:id="formula_17">h h h 1 h h h 2 h h h 3 x x x z z z 1 ζ ζ ζ 1 z z z 2 ζ ζ ζ 2 (a) generative model h h h 1 h h h 2 h h h 3 x x x ζ ζ ζ 1 ζ ζ ζ 2 z z z 1 z z z 2 (b) inference model</formula><p>With many layers of latent variables, the VAE objective often turns off many of the latent variables by matching their distribution in the inference model to the prior. The latent units are usually removed differentially across different groups. Appendix H presents a technique that enables efficient use of latent variables across all groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>To provide a comprehensive picture of overlapping transformations and DVAE++, we conduct three sets of experiments. In Sec. 7.1 and Sec. 7.2 we train a VAE with several layers of latent variables with a feed-forward encoder and decoder. This allows to compare overlapping transformations with previous work on discrete latent variables. In Sec. 7.3, we then compare DVAE++ to several baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Comparison with Previous Discrete Latent Variable Models</head><p>We compare overlapping transformations to NVIL (Mnih &amp; Gregor, 2014), MuProp <ref type="bibr" target="#b7">(Gu et al., 2015)</ref>, <ref type="bibr">REBAR (Tucker et al., 2017)</ref>, and Concrete <ref type="bibr" target="#b22">(Maddison et al., 2016)</ref> for training discrete single-layer latent variable models. We follow the structure used by <ref type="bibr">(Tucker et al., 2017)</ref> in which the prior distribution and inference model are factorial Bernoulli with 200 stochastic variables. In this setting, the inference and generative models are either linear or nonlinear functions. In the latter case, two layers of deterministic hidden units of the size 200 with tanh activation are used.</p><p>We use the settings in <ref type="bibr">(Tucker et al., 2017)</ref> to initialize the parameters, define the model, and optimize the parameters for the same number of iterations. However, <ref type="bibr">(Tucker et al., 2017)</ref> uses the Adam optimizer with β 2 = 0.99999 in training. We used Adam with its default parameters except for which is set to 10 −3 . The learning rate is selected from the set {1 · 10 −4 , 5 · 10 −4 }. The inverse temperature β for smoothing is annealed linearly during training with initial and final values chosen using cross validation from {5, 6, 7, 8} and {12, 14, 16, 18} respectively. In <ref type="table" target="#tab_1">Table 1</ref>, the performance of our model is compared with several stateof-the-art techniques proposed for training binary latent models on (statically) binarized MNIST <ref type="bibr">(Salakhutdinov &amp; Murray, 2008)</ref> and OMNIGLOT <ref type="bibr" target="#b19">(Lake et al., 2015)</ref>. At test time, all models are evaluated in the binary limit (β = ∞).</p><p>Smoothing transformations slightly outperform previous  <ref type="table">Table 2</ref>: The performance of the VAE model with an RBM prior trained with the overlapping transformation is compared against (Rolfe, 2016) as well as the directed VAE models <ref type="figure" target="#fig_1">(Fig. 2)</ref>. The performance is measured by 4000 importance weighted samples <ref type="bibr" target="#b1">(Burda et al., 2015)</ref>. Mean ± standard deviation for five runs are reported. techniques in most cases. In the case of the nonlinear model on OMNIGLOT, the difference is about 2.8 nats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Comparison with Previous RBM Prior VAE</head><p>Techniques such as KL annealing (Sønderby et al., 2016), batch normalization <ref type="bibr" target="#b12">(Ioffe &amp; Szegedy, 2015)</ref>, autoregressive inference/prior, and learning-rate decay can significantly improve the performance of a VAE beyond the results reported in Sec. 7.1. In this second set of experiments, we evaluate overlapping transformations by comparing the training of a VAE with an RBM prior to the original DVAE (Rolfe, 2016), both of which include these improvements. For a fair comparison, we apply only those techniques that were also used in <ref type="bibr">(Rolfe, 2016)</ref>. We examine VAEs with one and two latent layers with feed-forward linear or nonlinear inference and generative models. In the one-latent-layer case, the KL term in both our model and (Rolfe, 2016) reduces to the mean-field approximation. The only difference in this case lies in the overlapping transformations used here and the original smoothing method of (Rolfe, 2016). In the two-latent-layer case, our inference and generative model have the forms depicted in <ref type="figure" target="#fig_1">Fig. 2(d)</ref> and <ref type="figure" target="#fig_1">Fig. 2(g)</ref>. Again, all models are evaluated in the binary limit at the test time.</p><p>Comparisons are reported in <ref type="table">Table 2</ref>. For reference, we also provide the performance of the directed VAE models with the structures visualized in <ref type="figure" target="#fig_1">Fig. 2(c)</ref> to <ref type="figure" target="#fig_1">Fig. 2(f)</ref>. Implementation details are provided in Appendix F. Two observations can be made from <ref type="table">Table 2</ref>. First, our smoothing transformation outperforms (Rolfe, 2016) in most cases. In some cases the difference is as large as 5.1 nats. Second, the RBM prior performs better than a directed prior of the same size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Experiments on DVAE++</head><p>Lastly, we explore the performance of DVAE++ for density estimation on 2D images. In addition to statically binarized MNIST and OMNIGLOT, we test dynamically binarized <ref type="bibr">MNIST (LeCun et al., 1998)</ref> and Caltech-101 silhouettes <ref type="bibr" target="#b24">(Marlin et al., 2010)</ref>. All datasets have 28 × 28 binary pixel images. We use the same architecture for the MNIST and OMNIGLOT datasets, but because the Caltech-101 silhouettes dataset is smaller, our model easily overfits. Consequently, we use a shallower architecture for Caltech-101. We also evaluate DVAE++ on the CIFAR10 dataset, which consists of 32 × 32 pixel natural images. Appendix G lists the details of our architecture for different datasets.</p><p>Our goal is to determine whether we can use overlapping transformations to train a convolutional VAE with an RBM prior, and whether the RBM prior in DVAE++ captures global discrete hidden factors. In addition to DVAE++ (which uses binary global latent variables and continuous local latent variables), four different baselines are introduced by modifying the global and local distributions. These baselines are listed in <ref type="table" target="#tab_3">Table 3</ref>. For RBM (Rolfe), the spikeand-exp smoothing transformation is used and the ELBO is optimized using the derivation supplied in (Rolfe, 2016). For Bernoulli latent variables, we used the marginal distributions proposed in Sec. 4.2. For all the models, we used 16 layers of local latent variables each with 32 random variables at each spatial location. For the RBM global variables, we used 16 binary variables for all the binary datasets and 128 binary variables for CIFAR10. We cross-validated the number of the hierarchical layers in the inference model for the global variables from the set {1, 2, 4}. We used an unconditional decoder (i.e., factorial p(x x x|ζ ζ ζ, h h h)) for the MNIST  datasets. We measure performance by estimating test set log-likelihood (again, according to the binary model) with 4000 importance weighted samples. Appendix I presents additional ablation experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>We have introduced a new family of smoothing transformations consisting of a mixture of two overlapping distributions and have demonstrated that these transformations can be used for training latent variable models with either directed or undirected priors. Using variational bounds derived for both cases, we developed DVAE++ having a global RBM prior and local convolutional latent variables. All experiments used exponential mixture components, but it would be interesting to explore the efficacy of other choices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overlapping Transformation with the Mixture of Exponential Distributions</head><p>The CDF for each conditional distribution is given by</p><formula xml:id="formula_18">F r(ζ|z=0) (ζ) = 1 − e −βζ 1 − e −β F r(ζ|z=1) (ζ) = e β(ζ−1) − e −β 1 − e −β .</formula><p>To simplify notation, the mean of the Bernoulli distribution q(z = 1|x) is denoted by q. The CDF for the mixture q(ζ|x) = z r(ζ|z)q(z|x) is</p><formula xml:id="formula_19">F q(ζ|x) (ζ) = 1 − q 1 − e −β 1 − e −βζ + q 1 − e −β e β(ζ−1) − e −β .</formula><p>Defining m ≡ e −βζ and d ≡ e −β , the inverse CDF is found by solving F q(ζ) (ζ) − ρ = 0 which gives rise to the quadratic equation <ref type="figure">0 and d &gt; 0)</ref>, there are two real solutions. Further, m = (−b + √ b 2 − 4c)/2 is the valid solution since m must be positive (recall m = e −βζ ) and √ b 2 − 4c ≥ |b|. Lastly, the inverse CDF is obtained using ζ = − log m/β. The inverse CDF is a differentiable mapping from uniform samples ρ ∼ U (0, 1) to samples from q(ζ|x).</p><formula xml:id="formula_20">m 2 + −1 + ρ + d(q − ρ) 1 − q b m − qd 1 − q c = 0, which has solutions m = (−b± √ b 2 − 4c)/2. Since −4c ≥ 0 (as q ≥</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overlapping Transformation with the Mixture of Logistic Distributions</head><p>The Dirac δ distribution can be approximated by a normal distribution whose variance approaches to zero. We use this observation to define a smoothing transformation where each r(ζ|z) is modeled with a Normal distribution (with ζ ∈ R):</p><formula xml:id="formula_21">r(ζ|z = 0) = N (ζ|0, σ 2 ) r(ζ|z = 1) = N (ζ|1, σ 2 ).</formula><p>The resulting mixture q(ζ|x) = z r(ζ|z)q(z|x) converges to a Bernoulli distribution as σ goes to 0, but its CDF (which is a mixture of error functions) cannot be inverted in closed form.</p><p>To derive a Normal-like distribution with an invertible CDF and support ζ ∈ R, we define a smoothing transformation using the logistic distribution r(ζ|z = 0) = L(ζ|µ 0 , s)</p><formula xml:id="formula_22">r(ζ|z = 1) = L(ζ|µ 1 , s) where L(ζ|µ, s) = e − ζ−µ s s(1 + e − ζ−µ s ) 2 .</formula><p>For the mixture distribution 1</p><formula xml:id="formula_23">q(ζ|x) = (1 − q)L(ζ, µ 0 , s) + qL(ζ, µ 1 , s),</formula><p>the inverse CDF is derived by solving</p><formula xml:id="formula_24">F q(ζ) (ζ) = 1 − q 1 + e − ζ−µ 0 s + q 1 + e − ζ−µ 1 s = ρ (1 − q)(1 + e − ζ−µ 1 s ) + q(1 + e − ζ−µ 0 s ) (1 + e − ζ−µ 1 s )(1 + e − ζ−µ 0 s ) = ρ.</formula><p>Defining m ≡ e −ζ/s , d 0 ≡ e µ0/s , and d 1 = e µ1/s yields a quadratic in m</p><formula xml:id="formula_25">ρd 0 d 1 a m 2 + [ρ(d 0 + d 1 ) − d 0 q − d 1 (1 − q)] b m + ρ − 1 c = 0</formula><p>which has the valid solution</p><formula xml:id="formula_26">m * = −b + √ b 2 − 4ac 2a .</formula><p>This gives the inverse CDF as F −1 q(ζ|x) (ρ) = −s log m * . When s is very small and µ 1 &gt; µ 0 , d 1 is suceptible to overflow. A numerically stable solution can be obtained by applying the change of variable m = √ d 0 d 1 m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization of Inverse CDFs</head><p>To provide insight into the differences between overlapping transformations, Concrete <ref type="bibr" target="#b22">(Maddison et al., 2016;</ref><ref type="bibr" target="#b13">Jang et al., 2016)</ref>, and spike-and-exponential (Rolfe, 2016) smoothing, <ref type="figure">Fig.5</ref> visualizes the inverse CDFs at different temperatures. In cases where Concrete and spike-and-exponential have small gradients with respect to q(z = 1|x) (thereby slowing learning), overlapping transformations provide a larger gradient signal (for faster learning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Joint versus Marginal ELBOs</head><p>We have presented two alternative ELBO bounds, one based on a joint inference model q(ζ ζ ζ 1 , ζ ζ ζ 2 , z z z 1 , z z z 2 |x x x) and the other based on q(ζ ζ ζ 1 , ζ ζ ζ 2 |x x x) obtained by marginalizing the discrete variables. Here, we show that variational bound obtained with the marginal model is tighter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Adding the Gradient of log Z to the Objective Function</head><p>For training the DVAE++ model with an RBM prior, the gradient of log Z is needed for each parameter update.</p><p>Since log Z only depends on the prior parameters θ = {a a a 1 , a a a 2 , W W W }, its gradient is</p><formula xml:id="formula_27">∂ log Z ∂θ = ∂ ∂θ log z z z1,z z z2 e −E θ (z z z1,z z z2) = − z z z1,z z z2 e −E θ (z z z1,z z z2) ∂E θ (z z z1,z z z2) ∂θ z z z 1 ,z z z 2 e −E θ (z z z 1 ,z z z 2 ) = − z z z1,z z z2 p θ (z z z 1 , z z z 2 ) ∂E θ (z z z 1 , z z z 2 ) ∂θ = −E p θ (z z z1,z z z2) ∂E θ (z z z 1 , z z z 2 ) ∂θ .</formula><p>This expectation is estimated using Monte Carlo samples from the RBM. We maintain persistence chains and run block Gibbs updates for a fixed number of iterations (40) to update the samples after each parameter update. This approach is known as persistent contrastive divergence (PCD) <ref type="bibr">(Younes, 1989;</ref><ref type="bibr">Tieleman, 2008)</ref>.</p><p>Instead of manually coding the gradient of the negative energy function for each sample and modifying the gradient of whole objective function, we compute the negative average energy on L samples (indexed by l) generated from PCD chains (−</p><formula xml:id="formula_28">L l=1 E θ (z z z (l) 1 , z z z (l) 2 )/L where z z z (l) 1 , z z z (l) 2 ∼ p θ (z z z 1 , z z z 2 )</formula><p>). This gives a scalar tensor whose gradient is the sample-based approximation to ∂Z/∂θ. By adding this tensor to the objective function, an automatic differentiation (AD) library backpropagates through this tensor and computes the appropriate gradient estimate. Note that an AD library cannot backpropagate the gradients through the samples generated from the PCD computation graph because of the discrete nature of the process. However, one can use stop gradient commands on the samples to prevent unnecessary AD operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Implementation Details for the RBM Prior Experiments</head><p>In this section, we summarize the implementation details for the experiments reported in Sec.7.3 on the RBM prior VAE. During training, the KL term is annealed linearly from 0 to 1 in 300K iterations. The learning rate starts at 3 · 10 −3 and is multiplied by 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Implementation Details for the DVAE++ Experiments</head><p>The network architecture visualized in <ref type="figure">Fig. 7</ref> is divided into three parts: i) the inference network or encoder that represents q(z z z, ζ ζ ζ, h h h|x x x), ii) the prior network that models p(z z z, ζ ζ ζ, h h h), and iii) the decoder network that implements p(x x x|ζ ζ ζ, h h h). Our architecture consists of many modules that may differ for different datasets. The details here describe the network architecture used for CIFAR10 data. <ref type="table" target="#tab_8">Table 4</ref> lists the specifics of the networks for all the datasets.</p><p>For the encoder side, "down 1" denotes a series of downsampling residual blocks that extract convolutional features from the input image. The output of this network is a feature of size 8 × 8 × 256 (expressed by height × width × depth). The module "down 2" denotes another residual network that takes the output of "down 1" and progressively reduces the spatial dimensions of the feature maps until it reaches to a feature map of size 1 × 1 × 1024. This feature map is flattened and is iteratively fed to a series of fully connected networks that model q(z z z i |x x x, ζ ζ ζ &lt;i ). In addition to the feature map, each network accepts the concatenation of samples drawn from the smoothed variables. These networks have identical architecture with no parameter sharing and all model factorial Bernoulli distributions in their output.</p><p>The concatenation of samples from smoothed variables ζ ζ ζ i is fed to "upsample", a residual network that upsamples its input to 8 × 8 × ×32 dimensions using transposed convolutions. These features are concatenated with the feature map generated by "down 1". The concatenated feature is then iteratively fed to a series of residual network that defines q(h h h j |x x x, ζ ζ ζ, h h h &lt;j ). Each network accepts the concatenation samples from local variables in the previous group (h h h &lt;j ) in addition to the concatenated feature. When the local latent variables are modeled by normal distributions, these networks return mean and logarithm of the standard deviation of the elements in h h h j similar to the original VAE .</p><p>In the prior network, the same "upsample" network defined above is used to scale-up the global latent variables to the intermediate scale <ref type="bibr">(8 × 8 × 32)</ref>. Then, the output of this network is fed to a set of residual networks that defines p(h h h j |ζ ζ ζ, h h h &lt;j ) one at a time in the same scale. Similar to the encoder, these network accept the concatenation of all the local latent variables and they generate the parameters of the same type of distribution.</p><p>In the decoder network, the "context" residual network first maps the concatenation of all the local latent variables and upsampled global latent variables to a feature space. The output of this network is fed to a convolutional layer that generated parameters of a distribution on x x x 0 , which is subsampled x x x at scale 4 × 4. In the case of CIFAR10, the output of this layer correspond to the mixture of discretized logistic distribution (Salimans et al., 2017) with 10 mixtures. In the binary datasets, it is the parameters of a factorial Bernoulli distribution. The residual network input 4 × 4 is applied to the sample from this scale and its output is concatenated with the output of "context". The distribution on the next scale is formed similarly using another upsampling residual network. This process is repeated until we generate the image in the full scale.</p><p>The residual blocks in our work consist of two convolutional layers with an skip connection. Resizing the dimensions is always handled in the first convolutional layer. Downsampling is done using stride of 2 and upsampling is implemented using transposed convolution. The squeeze and excitation unit is applied with the reduction ratio r = 4 <ref type="bibr" target="#b9">(Hu et al., 2017)</ref>.</p><p>The SELU <ref type="bibr" target="#b18">(Klambauer et al., 2017)</ref> and ELU <ref type="bibr" target="#b4">(Clevert et al., 2015)</ref> activation functions are used in all the fully connected and convolutional layers respectively. No batch normalization was used except in the input of the encoder, the output of "down 1", and "down 2". AdaMax <ref type="bibr" target="#b14">(Kingma &amp; Ba, 2014)</ref> is used for training all the models. The learning rate is set to 0.001 and is decreased when the value of the variational bound on the validation set plateaus. The batch size is 100 for all the experiments. In all experiments, the β smoothing parameter is set to 8.</p><p>We use parallel tempering <ref type="bibr" target="#b10">(Hukushima &amp; Nemoto, 1996;</ref><ref type="bibr" target="#b11">Iba, 2001)</ref> to approximate the partition function of the RBM which is required to evaluate generative log-likelihoods.  <ref type="figure">Figure 7</ref>: The DVAE++ architecture is divided into three parts: a) the inference network or encoder that represents q(z z z, ζ ζ ζ, h h h|x x x), b) the prior network that models p(z z z, ζ ζ ζ, h h h), and c) the decoder network that implements p(x x x|ζ ζ ζ, h h h). Each part consists of different modules colored differently based on their type. The specific detail for each module is listed in <ref type="table" target="#tab_8">Table 4</ref>.  <ref type="figure" target="#fig_5">4)</ref>.</p><formula xml:id="formula_29">x x x down 1 down 2 q(z z z 1 |x x x) ζ ζ ζ 1 + q(z z z 2 |x x x, ζ ζ ζ 1 ) ζ ζ ζ 2 + upsample + q(h h h 1 |x x x, ζ ζ ζ) h h h 1 + q(h h h 2 |x x x, ζ ζ ζ, h h h 1 ) h h h 2<label>(a)</label></formula><p>Conditioned on x x x 0 , we generate the 8 × 8 subset, x x x 1 , at the next larger scale. Conditioned on x x x 0 and x x x 1 , we generate the 16 × 16 subset, x x x 2 , at the next larger scale. This process is repeated until all pixels are covered. In this figure, pixels in x x x 0 /x x x 1 /x x x 2 are indicated by their corresponding color. In order to have a consistent probabilistic model, given x x x &lt;i , we only generate the remaining pixels, x x x i , at scale i.</p><p>We use chains at an adaptive number of temperatures, and perform 100,000 sweeps over all variables to ensure that the log Z estimate is reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Conditional Decoder</head><p>The conditional decoder for a 16 × 16-pixel image is illustrated in <ref type="figure">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Balancing the KL Term</head><p>With many layers of latent variable, the VAEs tend to disable many stochastic variables <ref type="bibr" target="#b0">(Bowman et al., 2016;</ref><ref type="bibr">Sønderby et al., 2016)</ref>. Common mitigations to this problem include KL annealing <ref type="bibr">(Sønderby et al., 2016)</ref>, free bits , and soft free bit .</p><p>In our experiments, we observe that annealing the KL term is more effective in maintaining active latent variables than the free bits method. Nevertheless, at the end of training, the units tend to be disabled unevenly across different groups. Some are completely inactive while other groups have many active variables. To address this, we modify the VAE objective function to γ is annealed from zero to one during training, and α i is introduced to balance the KL term across variable groups. As in soft free bits, we reduce α i if the i th group has a lower KL value in comparison to other groups and increase it if  <ref type="table">Table 6</ref>: The performance of DVAE++ improves with the number of global variable groups in the inference model (i.e. q(z z z i |x x x, ζ ζ ζ &lt;i )). Performance is measured by test set log-likelihood in bits per dim. # groups 1 2 4 Bits per dim. 3.39 3.38 3.37 the KL value is higher for the group. In each parameter update α i is determined as</p><formula xml:id="formula_30">α i = Nα i jα j whereα i = E x x x∼M [KL(q(z z z i |x x x)||p(z z z i ))]+ .</formula><p>N is the number of latent groups, M is the current minibatch, and = 0.1 is a small value that softens the coefficients for very small values of KL. In this way, a group is penalized less in the KL term if it has smaller a KL value, thereby encouraging the group to use more latent variables. We apply a stop gradient operation on α i to prevent the AD from backpropagating through these coefficients. The α i are included in the objective only while γ is annealed. After γ saturates at one, we set all α i = 1 to allow the model to maximize the variational lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Additional Ablation Experiments</head><p>In this section, we provide additional ablation experiments that target individual aspects of DVAE++. The test-set evaluations reported in this section do not use the binary model (β = ∞), but instead use the same β that was used during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1. Hierarchical Models Help</head><p>As expected, we observe that increasing the number of local variable groups (the number of hierarchical levels) improves the performance of the generative model. <ref type="table" target="#tab_9">Table 5</ref> summarizes the performance of the DVAE++ for CIFAR10 as the hierarchy of continuous local variables is increased. Similarly, when global latent variables are modeled by an  RBM, dependencies between discrete latent variables can develop. Modeling of these dependencies can require a deeper hierarchical inference model. <ref type="table">Table 6</ref> summarizes the performance of DVAE++ on CIFAR10. In this experiment the number of local groups is fixed to 16. The RBM consists of 128 binary variables and the number of hierarchical levels in the inference model is varied from 1 to 4. Deeper inference models generate high log-likelihoods (low bits per dimmension).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2. Conditional vs. Unconditional Decoder</head><p>In <ref type="table" target="#tab_10">Table 7</ref>, the performance of DVAE++ with and without conditional decoder is reported. Multi-scale conditional decoders improve generative performance in all the datasets but MNIST. <ref type="table" target="#tab_11">Table 8</ref> compares the performance of DVAE++ with global latent variables trained with KL balancing with two baselines on three datasets. In the first baseline, the global latent variables are completely removed from the model. In the second baseline, the KL balancing coefficient (α i in Sec. H) is removed from original DVAE++. Removing either the global latent variables or the balancing coefficients typically decreases the performance of our generative model. However, on binarized MNIST, DVAE++ without a global prior attains a new state-of-the-art result at -78.96 nats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3. Global Latent Variables and KL Balancing</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a) Smoothing transformations using exponential distributions. b) Inverse CDF as a function of q(z = 1|x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) A generative model with binary latent variables z z z 1 and z z z 2 , and (b) the corresponding inference model. In (c) and (d), the continuous ζ is introduced and dependencies on z z z are transferred to dependencies on ζ. In (e) and (f) the binary latent variables z z z are marginalized out. (g) A generative model with a Boltzmann machine (dashed) prior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. 2(b). To train this model using smoothing transformations, we introduce the continuous ζ in Figs. 2(c) and 2(d) in which dependencies on z are transferred to dependencies on ζ. In this way, binary latent variables influence other variables only through their continuous counterparts. In Figs. 2(e) and 2(f) we show the same model but with z z z marginalized out. The joint (z z z, ζ ζ ζ) model of Figs. 2(c) and 2(d) gives rise to a looser ELBO than the marginal ζ ζ ζ model of Figs. 2(e) and 2(f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2</head><label>2</label><figDesc>(g) visualizes a generative model with a BM prior. As in Figs. 2(c) and 2(d), conditionals are formed on the auxiliary variables ζ ζ ζ instead of the binary variables z z z. The inference model in this case is identical to the model inFig. 2(d)and it infers both z z z and ζ ζ ζ in a hierarchical structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3</head><label>3</label><figDesc>: a) In the generative model, binary global latent variables z z z 1 and z z z 2 are modeled by an RBM (dashed) and a series of local continuous variables are generated in an autoregressive structure using residual networks. b) After forming distributions over the global variables, the inference model defines the conditional on the local latent variables similarly using residual networks.all the global latent variables. The feature maps at an intermediate scale are fed to another set of residual networks that define q(h h h j |x x x, ζ ζ ζ, h h h &lt;j ) successively for all the local latent variables.The decoder uses an upsampling network to scale-up the global latent variables to the intermediate scale. Then, the output of this network is fed to a set of residual networks that define p(h h h j |ζ ζ ζ, h h h &lt;j ) one at a time at the same scale. Finally, another set of residual networks progressively scales the samples from the latent variables up to the data space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of samples generated from our model trained on different datasets. In each figure, every five successive samples in each row are generated from a fixed sample drawn from the global RBM prior. Our global latent variables typically capture discontinuous global structures such as digit classes in MNIST or scene configuration in CIFAR10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>ForFigure 5 :Figure 6 :</head><label>56</label><figDesc>simplicity we consider a model with only one latent variable. Figs. 6(a) and (b) visualize the generative and inference models. Figs. 6(c) and (d) show the joint models, and Figs. 6(e) and (f) show the marginal models. The Visualization of inverse CDF as a function of q(z = 1|x) at ρ = 0.25 for different smoothing transformations with three different temperatures (λ) and inverse temperatures (β). We have selected temperature values that are often used in practice. (a) A generative model with binary latent variable z. (b) The corresponding inference model. In (c) and (d), the continuous ζ is introduced and dependency on z is transferred to dependency on ζ. In (e) and (f) the binary latent variable z is marginalized out. respective variational bounds are L 1 (x x x) = log p(x) − E q(z z z,ζ ζ ζ|x x x) q(z z z, ζ ζ ζ|x x x) p(z z z, ζ ζ ζ|x x x) and L 2 (x x x) = log p(x x x) − E q(ζ ζ ζ|x x x) log q(ζ ζ ζ|x x x) p(ζ ζ ζ|x x x) . Subtracting we find L 2 (x x x) − L 1 (x x x) = E q(ζ ζ ζ,z z z|x x x) log q(ζ ζ ζ, z z z|x x x) p(ζ ζ ζ, z z z|x x x) − E q(ζ ζ ζ|x x x) log q(ζ ζ ζ|x x x) p(ζ ζ ζ|x x x) = E q(ζ ζ ζ,z z z|x x x) log q(ζ ζ ζ, z z z|x x x) p(ζ ζ ζ, z z z|x x x) − log q(ζ ζ ζ|x x x) p(ζ ζ ζ|x x x) = E q(ζ ζ ζ,z z z|x x x) log q(z z z|ζ ζ ζ, x x x) p(z z z|ζ ζ ζ, x x x) = E q(ζ ζ ζ|x x x) KL q(z z z|ζ ζ ζ, x x x) p(z z z|ζ ζ ζ, x x x) ,which is clearly positive since KL(· ·) ≥ 0. Thus, L 2 (x x x), the marginal ELBO, provides a tighter bound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>E</head><label></label><figDesc>q(z z z|x x x) [log p(x x x|z z z)] − γ i α i KL(q(z z z i |x x x)||p(z z z i )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). The Gumbel-1 Quadrant.ai, D-Wave Systems Inc., Burnaby, BC, Canada. Correspondence to: Arash Vahdat &lt;arash@quadrant.ai&gt;. Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Overlapping transformations are compared against different single-sample based approaches proposed for training binary latent variable models. The performance is measured by 100 importance weighted samples<ref type="bibr" target="#b1">(Burda et al., 2015)</ref>. Mean ± standard deviation for five runs are reported. Baseline performances are taken from(Tucker et al., 2017). ± 0.06 -108.03 ± 0.07 -107.65 ± 0.08 -107.00 ± ± ± 0.10 -107.98 ± 0.10 -108.57 ± 0.10 Nonlinear -100.00 ± 0.10 -100.66 ± 0.08 -100.69 ± 0.08 -99.54 ± ± ± 0.06 -99.16 ± ± ± 0.12 -99.10 ± ± ± 0.21 OMNIGLOT Linear -117.59 ± ± ± 0.04 -117.64 ± 0.04 -117.65 ± 0.04 -117.65 ± 0.05 -117.38 ± ± ± 0.08 -118.35 ± 0.06 Nonlinear -116.57 ± 0.08 -117.51 ± 0.09 -118.02 ± 0.05 -116.69 ± 0.08 -113.83 ± ± ± 0.11 -113.76 ± ± ± 0.18</figDesc><table><row><cell>MNIST (static)</cell><cell>NVIL</cell><cell>MuProp</cell><cell>REBAR</cell><cell>Concrete</cell><cell>Joint ELBO</cell><cell>Marg. ELBO</cell></row><row><cell>Linear</cell><cell>-108.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Joint ELBO Marg. ELBO RBM (ours) RBM (Rolfe) Joint ELBO Marg. ELBO 1 Linear -91.21± ± ±0.11 -91.55±0.08 -106.70±0.08 -106.80±0.19 -109.66± ± ±0.09 -109.83±0.17 -117.62±0.09 -117.78±0.07 2 Linear -94.15± ± ±0.45 -91.06± ± ±0.21 -98.16±0.11 -98.56±0.10 -109.01± ± ±0.45 -110.35±0.14 -111.21±0.12 -111.49±0.08 1 Nonlin. -85.41± ± ±0.04 -85.57±0.03 -95.04±0.10 -95.06±0.08 -102.62± ± ±0.07 -103.12±0.06 -108.77±0.24 -108.82±0.20 2 Nonlin. -84.27± ± ±0.05 -84.52±0.05 -87.96±0.13 -88.23±0.11 -100.55± ± ±0.05 -105.60±0.68 -103.57±0.15 -104.05±0.22</figDesc><table><row><cell>MNIST (static)</cell><cell>OMNIGLOT</cell></row><row><cell>RBM (ours) RBM (Rolfe)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>DVAE++ compared against different baselines on several datasets. The performance is reported in terms of the log-likelihood values for all the dataset except for CIFAR10, in which bits per dimension is reported. In general, DVAE++ with RBM global prior and normal local variables outperforms the baselines.</figDesc><table><row><cell cols="2">Latent type Global</cell><cell>Local</cell><cell cols="5">MNIST (static) MNIST (dynamic) OMNIGLOT Caltech-101 CIFAR10</cell></row><row><cell>All cont.</cell><cell>Normal</cell><cell>Normal</cell><cell>-79.40</cell><cell>-78.59</cell><cell>-92.51</cell><cell>-82.24</cell><cell>3.40</cell></row><row><cell>Mixed</cell><cell cols="2">RBM (Rolfe) Normal RBM (ours) Normal</cell><cell>-79.04 -79.17</cell><cell>-78.65 -78.49</cell><cell>-92.56 -92.38</cell><cell>-81.95 -81.88</cell><cell>3.39 3.38</cell></row><row><cell>All disc.</cell><cell cols="2">RBM (ours) Bernoulli Bernoulli Bernoulli</cell><cell>-79.72 -79.90</cell><cell>-79.55 -79.62</cell><cell>-93.95 -93.87</cell><cell>-85.40 -86.57</cell><cell>3.59 3.62</cell></row><row><cell cols="2">Unconditional decoder</cell><cell></cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>For example, in the case of statically and dynamically binarized MNIST dataset, we achieve −79.72 and −79.55 respectively with unconditional decoder and 3.59 on CIFAR10 with conditional decoder. To the best of our knowledge these are the best reported results on these datasets with binary latent variables. Samples generated from DVAE++ are visualized inFig. 4. As shown, the discrete global prior clearly captures discontinuous latent factors such as digit category or scene configuration.</figDesc><table><row><cell></cell><cell>makes independence assumptions in each scale, whereas</cell></row><row><cell></cell><cell>the state-of-the-art techniques are based on PixelCNN (Van</cell></row><row><cell></cell><cell>Den Oord et al., 2016), which assumes full autoregressive</cell></row><row><cell></cell><cell>dependencies. Second, methods such as VLAE use nor-</cell></row><row><cell>groups the baselines into three categories: all con-tinuous latent, discrete global and continuous local (mixed), and all discrete. Within the mixed group, DVAE++ with RBM prior generally outperforms the same model trained</cell><cell>malizing flows for flexible inference models that reduce the KL cost on the convolutional latent variables. Here, the independence assumption in each local group in DVAE++ can cause a significant KL penalty.</cell></row><row><cell>with (Rolfe, 2016)'s. Replacing the continuous normal local</cell><cell></cell></row><row><cell>variables with Bernoulli variables does not dramatically hurt</cell><cell></cell></row><row><cell>the performance. DVAE++ results are comparable to current state-of-the-</cell><cell></cell></row><row><cell>art convolutional latent variable models such as Vamp-</cell><cell></cell></row><row><cell>Prior (Tomczak &amp; Welling, 2017) and variational lossy au-</cell><cell></cell></row><row><cell>toencoder (VLAE) (Chen et al., 2016). We note two features</cell><cell></cell></row><row><cell>of these models that may offer room for further improve-</cell><cell></cell></row><row><cell>ment for DVAE++. First, the conditional decoder used here</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Mnih, Andriy andGregor, Karol. Neural variational  inference and learning in belief networks. arXiv preprint arXiv:1402.0030, 2014. Mnih, Andriy and Rezende, Danilo. Variational inference for Monte Carlo objectives. In International Conference on Machine Learning, pp. 2188-2196, 2016. Williams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pp. 5-32. Springer, 1992. Younes, Laurent. Parametric inference for imperfectly observed Gibbsian fields. Probability theory and related fields, 1989.</figDesc><table><row><cell>Reed, Scott E, van den Oord, Aäron, Kalchbrenner, Nal,</cell></row><row><cell>Gómez, Sergio, Wang, Ziyu, Belov, Dan, and de Fre-</cell></row><row><cell>itas, Nando. Parallel multiscale autoregressive density</cell></row><row><cell>estimation. In Proceedings of The 34th International</cell></row><row><cell>Conference on Machine Learning, 2017.</cell></row><row><cell>Rezende, Danilo Jimenez, Mohamed, Shakir, and Wier-</cell></row><row><cell>stra, Daan. Stochastic backpropagation and approximate</cell></row><row><cell>inference in deep generative models. In International</cell></row><row><cell>Conference on Machine Learning, pp. 1278-1286, 2014.</cell></row><row><cell>Rolfe, Jason Tyler. Discrete variational autoencoders. arXiv</cell></row><row><cell>preprint arXiv:1609.02200, 2016.</cell></row><row><cell>Salakhutdinov, Ruslan and Murray, Iain. On the quantitative</cell></row><row><cell>analysis of deep belief networks. In Proceedings of the</cell></row><row><cell>25th international conference on Machine learning, pp.</cell></row><row><cell>872-879. ACM, 2008.</cell></row><row><cell>Salimans, Tim, Karpathy, Andrej, Chen, Xi, and Kingma,</cell></row><row><cell>Diederik P. PixelCNN++: Improving the pixelCNN with</cell></row><row><cell>discretized logistic mixture likelihood and other modifi-</cell></row><row><cell>cations. arXiv preprint arXiv:1701.05517, 2017.</cell></row><row><cell>Sønderby, Casper Kaae, Raiko, Tapani, Maaløe, Lars,</cell></row><row><cell>Sønderby, Søren Kaae, and Winther, Ole. Ladder varia-</cell></row><row><cell>tional autoencoders. In Advances in neural information</cell></row><row><cell>processing systems, pp. 3738-3746, 2016.</cell></row><row><cell>Tieleman, Tijmen. Training restricted Boltzmann machines</cell></row><row><cell>using approximations to the likelihood gradient. In Pro-</cell></row><row><cell>ceedings of the 25th international conference on Machine</cell></row><row><cell>learning, pp. 1064-1071. ACM, 2008.</cell></row><row><cell>Tomczak, Jakub M and Welling, Max. VAE with a Vamp-</cell></row><row><cell>Prior. arXiv preprint arXiv:1705.07120, 2017.</cell></row><row><cell>Tucker, George, Mnih, Andriy, Maddison, Chris J, Lawson,</cell></row><row><cell>John, and Sohl-Dickstein, Jascha. Rebar: Low-variance,</cell></row><row><cell>unbiased gradient estimates for discrete latent variable</cell></row><row><cell>models. In Advances in Neural Information Processing</cell></row><row><cell>Systems, pp. 2624-2633, 2017.</cell></row></table><note>Bengio, Yoshua, Léonard, Nicholas, and Courville, Aaron. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprintVan Den Oord, Aäron, Kalchbrenner, Nal, and Kavukcuoglu, Koray. Pixel recurrent neural networks. In Proceedings of the 33rd International Conference on International Conference on Machine Learning, pp. 1747-1756. JMLR. org, 2016.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>The architecture specifics for each module in DVAE++ for different datasets. The numbers correspond to the number of filters used in residual and convolutional blocks or the number of units used in the fully connected layers starting from the first hidden layer up to the last layer in each module. The arrows ↓ and ↑ in front of each number indicate that the corresponding block is downsampling or upsampling its input. For Binarized MNIST and MNIST, we used an identical architecture to OMNIGLOT except that the autoregressive connections are disabled when forming p(x x x|ζ ζ ζ, h h h) in the decoder.</figDesc><table><row><cell>Module</cell><cell>Type</cell><cell>OMNIGLOT</cell><cell>Caltech-101</cell><cell>CIFAR10</cell></row><row><cell>down 1</cell><cell>residual</cell><cell>32↓, 32, 64↓, 64, 128↓</cell><cell>8↓, 8↓</cell><cell>64↓, 64, 256↓, 256</cell></row><row><cell>down 2</cell><cell>residual</cell><cell>256↓, 512↓</cell><cell>8↓, 16↓, 16↓</cell><cell>512↓, 512, 1024↓, 1024↓</cell></row><row><cell>q(z z z i |x x x, ζ ζ ζ &lt;i )</cell><cell>fully connected</cell><cell>4</cell><cell>4</cell><cell>16</cell></row><row><cell>upsample</cell><cell>residual</cell><cell>128↑, 128↑</cell><cell>32↑, 32↑, 32↑</cell><cell>128↑, 64↑, 32↑</cell></row><row><cell>q(h h h i |x x x, ζ ζ ζ, h h h &lt;i )</cell><cell>residual</cell><cell>32, 32, 32, 32, 64</cell><cell>32, 32, 32, 32, 64</cell><cell>32, 64</cell></row><row><cell>p(h h h i |ζ ζ ζ, h h h &lt;i )</cell><cell>residual</cell><cell>32, 32, 64</cell><cell>32, 32, 32, 32, 64</cell><cell>32, 32, 32, 32, 64</cell></row><row><cell>context</cell><cell>residual</cell><cell>16</cell><cell>16</cell><cell>256</cell></row><row><cell>upsample 1</cell><cell>residual</cell><cell>16↑, 16,</cell><cell>8, 8</cell><cell>128, 128,</cell></row><row><cell>upsample 2</cell><cell>residual</cell><cell>16↑, 8</cell><cell>8↑, 4</cell><cell>64↑, 64</cell></row><row><cell>upsample 3</cell><cell>residual</cell><cell>8↑</cell><cell>4↑</cell><cell>32↑</cell></row><row><cell cols="2">p(x x x i |ζ ζ ζ, h h h, x x x &lt;i ) convolutional</cell><cell>1</cell><cell>1</cell><cell>100</cell></row><row><cell>input h × w</cell><cell>residual</cell><cell>32</cell><cell>32</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>The generative performance of DVAE++ improves with the number of local variable groups. Performance is measured by test set log-likelihood in bits per dim. Bits per dim. 3.45 3.41 3.40 3.40 3.39</figDesc><table><row><cell># groups</cell><cell>8</cell><cell>12 16 20 24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>DVAE++ with and without conditional decoder</figDesc><table><row><cell>Baseline</cell><cell cols="3">MNIST MNIST OMNI-Caltech-CIFAR10 (static) (dynamic) GLOT 101</cell></row><row><cell>Conditional</cell><cell>-79.37</cell><cell>-78.62 -92.36 -81.85</cell><cell>3.37</cell></row><row><cell cols="2">Unconditional -79.12</cell><cell>-78.47 -92.94 -82.40</cell><cell>3.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>DVAE++ is compared against baselines with either no global latent variables or no KL balancing coefficients.</figDesc><table><row><cell>Baseline</cell><cell cols="3">MNIST OMNIGLOT CIFAR10</cell></row><row><cell></cell><cell>(static)</cell><cell></cell><cell></cell></row><row><cell>DVAE++</cell><cell>-79.12</cell><cell>-92.36</cell><cell>3.37</cell></row><row><cell cols="2">DVAE++ w/o global latent -78.96</cell><cell>-92.60</cell><cell>3.41</cell></row><row><cell cols="2">DVAE++ w/o kl balancing -79.72</cell><cell>-92.74</cell><cell>3.42</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">q is again shorthand for q(z = 1|x).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Importance weighted autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prafulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02731</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Variational lossy autoencoder. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01704</idno>
		<title level="m">Hierarchical multiscale recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Djork-Arné</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sepp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep generative image models using a Laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename><surname>Soumith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andriy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.8499</idno>
		<title level="m">Deep autoregressive networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Muprop</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05176</idno>
		<title level="m">Unbiased backpropagation for stochastic neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<title level="m">Squeeze-and-excitation networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exchange Monte Carlo method and application to spin glass simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Hukushima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Nemoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Physical Society of Japan</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1604" to="1608" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extended ensemble Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukito</forename><surname>Iba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Modern Physics C</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with Gumbel-Softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shakir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Günter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="972" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Léon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semisupervised generation with cluster-aware generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00637</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inductive principles for restricted Boltzmann machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
