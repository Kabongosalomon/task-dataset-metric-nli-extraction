<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 1 -6, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
							<email>hazarika@comp.nus.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<email>sporia@ihpc.a-star.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<email>cambria@ntu.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
							<email>morency@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
							<email>rogerz@comp.nus.edu.sg</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Artificial Intelligence Initiative</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Language Technologies Institute, CMU</orgName>
								<address>
									<country>USA, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Language Technologies Institute, CMU</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of NAACL-HLT 2018</title>
						<meeting>NAACL-HLT 2018 <address><addrLine>New Orleans, Louisiana</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2122" to="2132"/>
							<date type="published">June 1 -6, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Emotion recognition in conversations is crucial for the development of empathetic machines. Present methods mostly ignore the role of inter-speaker dependency relations while classifying emotions in conversations. In this paper, we address recognizing utterance-level emotions in dyadic conversational videos. We propose a deep neural framework, termed conversational memory network, which leverages contextual information from the conversation history. The framework takes a multimodal approach comprising audio, visual and textual features with gated recurrent units to model past utterances of each speaker into memories. Such memories are then merged using attention-based hops to capture inter-speaker dependencies. Experiments show an accuracy improvement of 3−4% over the state of the art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Development of machines with emotional intelligence has been a long-standing goal of AI. With the increasing infusion of interactive systems in our lives, the need for empathetic machines with emotional understanding is paramount. Previous research in affective computing has looked at dialogues as an essential basis to learn emotional dynamics ( <ref type="bibr" target="#b39">Sidnell and Stivers, 2012;</ref><ref type="bibr" target="#b30">Poria et al., 2017a</ref>; <ref type="bibr" target="#b53">Zhou et al., 2017)</ref>.</p><p>Since the advent of Web 2.0, dialogue videos have proliferated across the internet through platforms like movies, webinars, and video chats. Emotion detection from such resources can benefit numerous fields like counseling <ref type="bibr" target="#b14">(De Choudhury et al., 2013)</ref>, public opinion mining ( , financial forecasting ( <ref type="bibr" target="#b48">Xing et al., 2018)</ref>, and intelligent systems such as smart homes and chatbots ( <ref type="bibr" target="#b50">Young et al., 2018)</ref>.</p><p>In this paper, we analyze emotion detection in videos of dyadic conversations. A dyadic conversation is a form of a dialogue between two entities. We propose a conversational memory network (CMN), which uses a multimodal approach for emotion detection in utterances (a unit of speech bound by breathes or pauses) of such conversational videos.</p><p>Emotional dynamics in a conversation is known to be driven by two prime factors: self and interspeaker emotional influence <ref type="bibr" target="#b27">(Morris and Keltner, 2000</ref>; <ref type="bibr" target="#b26">Liu and Maitlis, 2014)</ref>. Self-influence relates to the concept of emotional inertia, i.e., the degree to which a person's feelings carry over from one moment to another <ref type="bibr" target="#b24">(Koval and Kuppens, 2012)</ref>. Inter-speaker emotional influence is another trait where the other person acts as an influencer in the speaker's emotional state. Conversely, speakers also tend to mirror emotions of their counterparts ( <ref type="bibr" target="#b28">Navarretta et al., 2016)</ref>. <ref type="figure">Figure 1</ref> provides an example from the dataset showing the presence of these two traits in a dialogue.</p><p>Existing works in the literature do not capitalize on these two factors. Context-free systems infer emotions based only on the current utterance in the conversation ( <ref type="bibr" target="#b5">Bertero et al., 2016)</ref>. Whereas, state-of-the-art context-based networks like <ref type="bibr" target="#b31">Poria et al., 2017b</ref>, use long short-term memory (LSTM) networks to model speaker-based context that suffers from incapability of long-range summarization and unweighted influence from context, leading to model bias.</p><p>Our proposed CMN incorporates these factors by using emotional context information present in the conversation history. It improves speakerbased emotion modeling by using memory networks which are efficient in capturing long-term dependencies and summarizing task-specific details using attention models ( <ref type="bibr">Weston et al., 2014;</ref><ref type="bibr" target="#b19">Graves et al., 2014;</ref><ref type="bibr" target="#b51">Young et al., 2017)</ref>.</p><p>Specifically, the memory cells of CMN are continuous vectors that store the context information found in the utterance histories. CMN also models interplay of these memories to capture interspeaker dependencies.</p><p>CMN first extracts multimodal features (audio, visual, and text) for all utterances in a video. In order to detect the emotion of a particular utterance, say u i , it gathers its histories by collecting previous utterances within a context window. Separate histories are created for both speakers. These histories are then modeled into memory cells using gated recurrent units (GRUs).</p><p>After that, CMN reads both the speaker's memories and employs attention mechanism on them, in order to find the most useful historical utterances to classify u i . The memories are then merged with u i using an addition operation weighted by the attention scores. This is done to model inter-speaker influences and dynamics. The whole cycle is repeated for multiple hops and finally, this merged representation of utterance u i is used to classify its emotion category. The contributions of this paper can be summarized as follows:</p><p>1. We propose an architecture, termed CMN, for emotion detection in a dyadic conversation that considers utterance histories of both the speaker to model emotional dynamics. The architecture is extensible to multi-speaker conversations in formats such as textual dialogues or conversational videos.</p><p>2. When applied to videos, we adopt a multimodal approach to extract diverse features from utterances. It also makes our model robust to missing information.</p><p>3. CMN provides a significant increase in accuracy of 3 − 4% over previous state-of-the-art networks. One variant called CMN self which does not consider the inter-speaker relation in emotion detection also outperforms the state of the art by a significant margin.</p><p>The remainder of the paper is organized as follows: Section 2 provides a brief literature review; Section 3 formalizes the problem statement; Section 4 describes the proposed method in detail; exSo you're leaving tomorrow. <ref type="bibr">[sad]</ref> Yeah, they just called. <ref type="bibr">[sad]</ref> I don't know what to say. I don't want to go but I don't have a choice. <ref type="bibr">[sad]</ref> I am afraid when you leave you won't come back. <ref type="bibr">[sad]</ref> I have to do this. Do you think I want to miss seeing her (their daughter) grow? <ref type="bibr">[sad]</ref> You don't have to do this. Its not gonna work out. We're not a complete family without you being here. <ref type="bibr">[sad]</ref> Well I ll come back, what are you not willing to wait for me? <ref type="bibr">[ang]</ref> Thanks that helps. I feel much better now.</p><p>[ang]</p><p>We are not a complete family here, you don't understand. <ref type="bibr">[ang]</ref> Person B Person A Time <ref type="figure">Figure 1</ref>: An abridged dialogue from the dataset. Person A (wife) is leaving B (husband) for a work assignment. Initially both A and B are emotionally driven by their own emotional inertia. In the end, emotional influence can be seen when B, despite being sad, reacts angrily to A's angry statement.</p><p>perimental results are covered in Section 5; finally, Section 6 provides concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Over the years, emotion recognition as an area of research has seen contributions from researchers across varied fields like signal processing, machine learning, cognitive and social psychology, natural language processing, etc. <ref type="bibr" target="#b29">(Picard, 2010)</ref>. Ekman, 1993, provided initial findings that related facial expressions as universal indicators of emotions. <ref type="bibr" target="#b12">Datcu and</ref><ref type="bibr">Rothkrantz, 2008, 2011</ref>, showed the importance of acoustic cues in affect modeling.</p><p>A large section of researchers approaches emotion recognition from a multimodal learning perspective. Hence, many works used visual and audio features together for detecting affect ( <ref type="bibr" target="#b7">Busso et al., 2004;</ref><ref type="bibr" target="#b10">Castellano et al., 2008;</ref><ref type="bibr" target="#b34">Ranganathan et al., 2016</ref>). An in-depth review of the literature in these systems is provided by <ref type="bibr" target="#b16">D'mello and Kory, 2015</ref>. Our work, which performs context-sensitive recognition <ref type="bibr">(Wöllmer et al., 2010</ref>) uses three modalities: audio, visual and text. Recently, this combination of modalities has provided the best performance in affect recognition systems (Poria et al., 2017b; <ref type="bibr" target="#b44">Wang et al., 2017;</ref><ref type="bibr" target="#b43">Tzirakis et al., 2017)</ref>, thus motivating the use of a multimodal approach.</p><p>Previous works have focused on conversations as a resourceful event for emotion analysis. <ref type="bibr">Ru- usuvuori, 2013</ref>, provides an in-depth analysis on how emotions affect social interactions and conversations. In fact, significant works have attributed emotional dynamics as an interactive phe-nomenon, rather than being within-person and one-directional ( <ref type="bibr" target="#b35">Richards et al., 2003;</ref><ref type="bibr" target="#b20">Hareli and Rafaeli, 2008)</ref>. Such emotional dynamics are modeled by observing transition properties. <ref type="bibr" target="#b49">Yang et al., 2011</ref>, study patterns for emotion transitions and show the evidence of emotional inertia. <ref type="bibr">Xiaolan et al., 2013, use</ref> finite state machines to model transitions using stimuli and personality characteristics. Our work also tries to model emotional transitions using multimodal features. Unlike these works, however, we use memory networks to achieve the same.</p><p>The use of memory networks have been instrumental in the progress of multiple research problems, e.g., question-answering ( <ref type="bibr">Weston et al., 2014;</ref><ref type="bibr" target="#b41">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b25">Kumar et al., 2016)</ref>, machine translation ( ), speech recognition ( <ref type="bibr" target="#b19">Graves et al., 2014)</ref>, and commonsense reasoning . The repeated read and write to their memory cells is often coupled with attention modules, thus allowing it to filter only relevant memories.</p><p>Our model is loosely inspired from Sukhbaatar et al., 2015. Unlike their model, which directly encodes sentences into memories, we perform temporal sequence processing on our utterance histories using GRUs. We also extend their architecture to handle two speakers while keeping the possibility to add more. Finally, our model is different in the fact that we use multimodal features for input and processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>Our goal is to infer the emotion of utterances present in a dyadic conversation. Let us define a dyadic conversation to be an asynchronous exchange of utterances between two persons P a and P b . Both the speakers speak a sequence of utterances U a and U b , respectively. Here,</p><formula xml:id="formula_0">U λ = (s 1 λ , s 2 λ , ..., s l λ λ )</formula><p>is ordered temporally, where s i λ is the i th utterance by P λ and l λ is the total number of utterances spoken by person P λ , λ ∈ {a, b}. Overall, the utterances by both speakers can be linearly ordered based on temporal occurrence as</p><formula xml:id="formula_1">(u 1 , u 2 , ...u la+l b ) , where, u j ∈ U a or U b .</formula><p>Our model takes as input an utterance u i whose emotion category (Section 5.1) needs to be classified. To get its history, preceding K utterances of each person are separately collected as hist a and hist b . Here, K serves as the length of the context window for history of u i . Thus, for λ ∈ {a, b}:</p><formula xml:id="formula_2">hist λ = {u j u j ∈ U λ , j &lt; i} , hist λ ≤ K (1)</formula><p>hist λ is also ordered temporally. At the beginning of the conversation, histories would have lesser than K utterances, i.e., hist λ &lt; K.</p><p>In the remaining sections, for brevity, we explain the processes using a subscript λ which can instantiate to either a or b, i.e., λ ∈ {a, b}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>We start by detailing the multimodal feature extraction scheme for all utterances followed by the mechanism to model emotional context using memory networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multimodal Feature Extraction</head><p>The first phase of CMN is to extract multimodal features of all utterances in the conversations. The dyadic conversations are present in the form of videos. Each utterance of a particular conversation is thus a small segment of the full video. For each utterance, we extract features for the modes: audio, visual and text. The process of feature extraction for each mode is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Textual Features Extraction</head><p>We extract features from the transcript of an utterance video using convolutional neural networks (CNNs). CNNs are effective in learning high level abstract representations of sentences from constituting words or n-grams ( <ref type="bibr" target="#b22">Kalchbrenner et al., 2014</ref>). To get our sentence representation, we use a simple CNN with one convolutional layer followed by max-pooling <ref type="bibr" target="#b23">(Kim, 2014;</ref><ref type="bibr" target="#b33">Poria et al., 2016)</ref>.</p><p>Specifically, the convolution layer consists filters of sizes 3, 4 and 5 with 50 feature maps each. Max-pooling is employed on these feature maps with a pooling window of size 2. Finally, a fully connected layer is used with 100 neurons. The activations of this layer form our sentence representation t u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Audio Feature Extraction</head><p>To extract audio features we use openSMILE <ref type="bibr">(Ey- ben et al., 2010)</ref>. It is an open-source software which provides high dimensional audio vectors. These vectors comprise of features like loudness, Mel-spectra, MFCC, pitch, etc. Audio features play a significant role in providing information on the emotional state of a speaker ( <ref type="bibr" target="#b40">Song et al., 2004</ref>).</p><p>In fact, the literature shows that there exists a high correlation between many statistical measures of speech with speakers' emotion. For example, high pitch and fast speaking rate often denote anger while sadness associates low standard deviation of pitch and slow speech rate <ref type="bibr" target="#b15">(Dellaert et al., 1996;</ref><ref type="bibr" target="#b1">Amir, 1998)</ref>. In this work, we use the IS13 ComParE 1 config file which extracts a total of 6373 features for each utterance video. Zstandardization is performed for voice normalization and dimension of the audio vector is reduced to 100 using a fully-connected neural layer. This provides the final audio feature vector a u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Visual Feature Extraction</head><p>Facial expressions and visual surrounding provide rich emotional indicators. We use a 3D-CNN to capture these details from the utterance video. Apart from the benefits of extracting relevant features from each image frame, 3D-CNN also extracts spatiotemporal features across frames ( <ref type="bibr" target="#b42">Tran et al., 2015)</ref>. This leads to the identification of emotional expressions like a smile or frown.</p><p>The working of a 3D-CNN is identical to its 2D counterpart with an input being a video v of dimension: (3, f, h, w). Here, 3 represents the RGB channels and f, h, w are the number of frames, height, and width of each frame, respectively. For the convolution operation, a 3D filter f l of dimension (f m , 3, f d , f h , f w ) is used where, f <ref type="bibr">[mdhf ]</ref> represents number of feature maps, depth, height and width of the filter, respectively. Max-pooling is applied to the output of this convolution across a 3D sliding window of dimension (m p , m p , m p ).</p><p>In our model, we use 128 feature maps for 3D filters of size 5. For pooling, we set m p to be 3 whose output is fed to a fully connected layer with 100 neurons. All the values are decided using hyperparameter tuning (see Section 5). For the input utterance, the activations of this layer form the video representation v u .</p><p>Fusion: We perform feature level fusion to map the individual modalities to a joint space. This is done through a simple feature concatenation. Thus, the extracted features t u , a u and v u are joined to form the utterance representation u = [t u ; a u ; v u ] of dimension d in = 300. This multimodal representation is generated for all utterances in a conversation.</p><p>Literature consists of numerous fusion techniques for multimodal data <ref type="bibr" target="#b2">(Atrey et al., 2010;</ref><ref type="bibr" target="#b52">Zadeh et al., 2017;</ref><ref type="bibr" target="#b32">Poria et al., 2017c</ref>). Exploring these on CMN, however, is beyond the scope of this paper and left as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Conversational Memory Network</head><p>For classifying the emotion of an utterance u i , its corresponding histories (hist a and hist b ) are taken. Each history hist λ contains the preceding K utterances by person P λ (see Section 3). Here, both u i and utterances in the histories are represented using their multimodal feature vectors of dimension R d in <ref type="figure" target="#fig_0">(Figure 2)</ref>.</p><p>The histories are first modeled into memory cells using GRUs. This provides the memories with context information summarized by the GRU. We call this step as memory representation. Following cognitive evidence of self-emotional dynamics, we model separate memory cells for each person. Thus, identical but separate computations are performed on both histories. From these memories, content relevant to utterance u i is then filtered out using attention mechanism over multiple input/output hops. At each hop, both memories are accumulated and merged with u i to model interspeaker emotional dynamics. First, we describe our model as a single layer memory network which runs one hop operation on the memories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Single Layer</head><p>Here, we explain the representation scheme of the memories for both histories and the input/output operations on them along with attention mechanism. The memory representation for each history is generated using a GRU for modeling emotion transitions. First, we define the GRU cell. Gated Recurrent Unit: GRUs are a gating mechanism in recurrent neural networks introduced by ). Similar to an LSTM (Hochreiter and Schmidhuber, 1997), GRU provides a simpler computation with similar performance. At any timestep t, it utilizes two gates r t (reset gate) and z t (update gate) to control the combination criteria with current input utterance u t and previous hidden state s t−1 .</p><p>The new state s t is computed as:  λ for all R hops. Then, attention based filtering using multiple memory hops is performed. Finally, Person A's utterance u i is classified to predict its emotion category.</p><formula xml:id="formula_3">z t = σ(V z .u t + W z .s t−1 + b z )<label>(2)</label></formula><formula xml:id="formula_4">r t = σ(V r .u t + W r .s t−1 + b r )<label>(3)</label></formula><formula xml:id="formula_5">h t = tanh(V h .u t + W h .(s t−1 ⊗ r t ) + b h ) (4) s t = (1 − z t ) ⊗ h t + z t ⊗ s t−1<label>(5)</label></formula><p>Here, V, W and b are parameter matrices and vector and ⊗ represents element-wise multiplication. The above equations can be summarized as:</p><formula xml:id="formula_6">s t = GRU λ (s t−1 , u t ).</formula><p>Memory Representation: For each λ ∈ {a, b}, a memory representation M λ = [m 1 λ , ..., m K λ ] for hist λ is generated using a GRU. To grasp the temporal context, the K utterances in hist λ are framed as a sequence (starting from the oldest one) and fed to the GRU λ . At each timestep t ∈ <ref type="bibr">[1, K]</ref>, the GRU λ 's internal state s t (equation 5) forms the t th memory cell m t λ of memory representation M λ . Memory Input: This step takes the memory representation M λ and performs an attention mechanism on it, resulting in an attention vector p λ ∈ R K . First, the current utterance u i is embedded into a vector q i of dimension R d using a projection matrix B ∈ R d×d in . To find the relevance of each memory m t λ 's context with q i , a match between both is computed. We do this by taking an inner product as follows:</p><formula xml:id="formula_7">q i = B.u i<label>(6)</label></formula><formula xml:id="formula_8">p t λ = sof tmax(q T i .m t λ )<label>(7)</label></formula><p>Here, sof tmax(x i ) = e x i ∑ j e x j and attention vector p λ = {p t λ } is a probability distribution over the input memories M λ = {m t λ } for t ∈ <ref type="bibr">[1, K]</ref>. </p><formula xml:id="formula_9">o λ = t p t λ .(m t λ ) ′ = M ′ λ .p λ<label>(8)</label></formula><p>Thus, the output representation o λ contains weighted contextual summary accumulated from the memory.</p><p>Final Prediction: To generate the predictions for the current utterance u i , we combine the output representations of both persons: o a and o b with u i 's representation q i and perform an affine transformation using matrix W o . Softmax is applied to this final vector to get the emotion predictions,</p><formula xml:id="formula_10">ˆ y = sof tmax(W o .(q i + o a + o b ))<label>(9)</label></formula><p>Categorical cross-entropy is used as the loss:</p><formula xml:id="formula_11">Loss = −1 N N i=1 C j=1 y i,j log 2 (ˆ y i,j )<label>(10)</label></formula><p>Here, N denotes total utterances across all videos and C is the number of emotion categories. y i is the one-hot vector ground truth of i th utterance from the training set andˆyandˆ andˆy i,j is its predicted probability of belonging to class j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Multiple Layers</head><p>Many recent works on memory networks adopt a multiple hop scheme in their network. This repeated input and output cycle on the memories along with a soft attention module, leads to a refined representation of the memories ( <ref type="bibr" target="#b41">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b25">Kumar et al., 2016)</ref>. Motivated by these works, we extend our model to perform R hops on the memories. This is done by stacking the single hop layers (Section 4.2.1) as follows:</p><p>• At a particular hop r, the output memory of the previous hop M λ . This constraint of sharing parameters adjacently between layers is added for reduction in total parameters and ease of training.</p><p>• At every hop, the query utterance u i 's representation q i is updated as:</p><formula xml:id="formula_12">q (r+1) i = q (r) i + o (r) a + o (r) b<label>(11)</label></formula><formula xml:id="formula_13">o (r)</formula><p>λ is calculated as per equation 8 using M ′ λ (r) .</p><p>• After R hops, the final prediction is done using equation 9 as:</p><formula xml:id="formula_14">ˆ y = sof tmax(W o .(q (R+1) i )</formula><p>). Algorithm 1 summarizes the overall CMN network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Conversational Memory Network</head><p>1: procedure CMN(ui, hista, hist b , K, R) predict the emotion of ui 2: q</p><formula xml:id="formula_15">(1) i ← B.ui 3: M ′ λ (0) ← GRU (0) λ (hist λ ) 4: for r in [1,R] do Multi-hop memory I/O 5: M (r) λ ← M ′ λ (r−1) 6: M ′ λ (r) ← GRU (r) λ (hist λ ) 7: p λ ← sof tmax( (q r i ) T .M (r) λ ) Memory in 8: o (r) λ ← M ′ λ (r)</formula><p>.p λ Memory out 9: q of speakers is given multiple conversation scenarios which are grouped in a single session. All the conversations are segmented into utterances. Each utterance is annotated using the following emotion categories: anger, happiness, sadness, neutral, excitement, frustration, fear, surprise, and other. However, in our experiments, we consider the first four categories. This is done to compare our method with state-of-the-art frameworks <ref type="bibr">(Po- ria et al., 2017b;</ref><ref type="bibr" target="#b37">Rozgic et al., 2012</ref>). The dataset provides rich video and audio samples for all the utterances along with transcriptions. Apart from these emotional states, we also investigate the valence and arousal degrees of each utterance. IEMOCAP provides labels for both these attributes on a 5-point Likert scale. Following Aldeneh et al., 2017, we convert the attributes into 3 categories, namely, low (≤ 2), medium (&gt; 2 and &lt; 4) and high (≥ 4). The dataset configuration for the experiments is obtained from <ref type="bibr" target="#b31">Poria et al. (2017b)</ref>. The first 8 speakers (Session 1 -4) compose the training fold while the last session is used as the testing fold. Overall, the training and testing set comprises of 4290 utterances (120 conversational videos) and 1208 utterances (31 conversational videos), respectively. There is no speaker overlap in the training and testing set to make the model person-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Emotional Influence Patterns</head><p>In this section, we perform dataset exploration to check the existence of emotional influences. <ref type="figure">Figure  3a)</ref> presents the emotion sequence of two videos sampled from the dataset. Both videos show the presence of self and inter-speaker emotional influences. Visual exploration of videos from the dataset reveal significant existence of such instances in the conversations. To provide quantitative evidence of the emotional influence patterns, we curate a non exhaustive list of possible cases of influence. For all utterances in the dataset, we sample their histories by setting K = 5, i.e., five previous utterances (as per availability) from both speakers.</p><p>Cases 1 and 2 <ref type="figure">(Figure 3</ref>) represent scenarios when the emotion of current utterance is influenced by self or the other person respectively. In case 3, the utterance has relevant content in the histories that do not precede immediately. An effective attention mechanism provides the capability to capture this pattern. Finally case 4 presents the situation when the utterance is independent of the history. Such situations are indicative from the content of the utterance which often deviates from the previous topic of discussion or introduces a new information. <ref type="table">Table 1</ref> presents a statistical summary of these cases present in the dataset. From the table it can be seen that a large section of the dataset demonstrate these influence patterns. This provides motivation to explicitly model these patterns. We thus hypothesize that models that are able to capture these cases would have superior emotion inference capabilities.</p><p>This passive exploration is a label-based analysis which is performed as a sanity check. Needless to say, existence of some false positive patterns at the label level is imminent. On the other hand, our model CMN is content-based which enables it to mine intricate patterns from the utterance histories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Training Details</head><p>We use 10% of the training set as a held-out validation set for hyperparameter tuning. To optimize the parameters, we use Stochastic Gradient Descent (SGD) optimizer, starting with an initial learning  <ref type="table">Table 1</ref>: Percentage of occurrence of different cases in the dataset as mentioned in Section 5.2. All cases are analyzed with K = 5. Utterances whose history has atleast 3 similar emotion labels in either own history or the history of the other person, is counted in case 1 or 2, respectively. Case 3 is considered when the utterance's emotion is found in atleast 3 utterances which occur before the second past-utterance of each history. Case 4 is considered when no history has the emotion label of the current utterance.</p><p>rate (lr) of 0.01. An annealing approach halves the lr every 20 epochs and termination is decided using an early-stop measure with a patience of 12 by monitoring the validation loss. Gradient clipping is used for regularization with a norm set to 40. Hyperparameters are decided using a Random Search ( <ref type="bibr" target="#b4">Bergstra and Bengio, 2012</ref>). Based on validation performance, context window length K is set to be 40 and the number of hops R is fixed at 3 hops. If K previous utterances are unavailable, then null utterances are added at the beginning of the history sequence. The dimension size of the memory cells d is set as 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Baselines</head><p>We compare CMN with the following baselines:</p><p>SVM-ensemble: A strong context-free benchmark model which uses similar multimodal approach on an ensemble of trees. Each node represents binary support vector machines (SVM) ( <ref type="bibr" target="#b37">Rozgic et al., 2012</ref>).</p><p>bc-LSTM: A bi-directional LSTM equipped with hierarchical fusion, proposed by <ref type="bibr" target="#b31">Poria et al., 2017b</ref>. It is the present state-of-the-art method. The model uses context features from unimodal LSTMs and its concatenation is fed to a final LSTM for classification. For fair comparison in an end-toend learning paradigm, we remove the penultimate SVM of this model. The model doesn't accommodate inter-speaker dependencies.</p><p>Memn2n: The original memory network as proposed by <ref type="bibr" target="#b41">Sukhbaatar et al., 2015</ref>. Contrasting to CMN, the model generates the memory representations for each historical utterance using an embedding matrix B as used in equation 7, without sequential modeling. Thus for utterance u i , both memories are created as M λ using {m t λ = B.u t u t ∈ hist λ and t ∈ [1, K]} for λ ∈ {a, b}. CMN Self : In this baseline, we use only self history for classifying emotion of utterance u i . Thus, if u i is spoken by person P a , then only hist a is considered. Clearly, this variant is also incapable of modeling inter-speaker dependencies.</p><p>CMN N A : Single layer variant of the CMN with no attention module. Thus, its output o λ (equation 8) is generated using a uniform probability distribution p λ , i.e., {p t λ = 1 K } K t=1 .   <ref type="bibr" target="#b31">Poria et al., 2017b</ref>), where our model does explicitly well for the active emotions happiness and anger. This trend suggests that CMN is capable of capturing inter-speaker emotional influences which are often seen in the presence of such active emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The importance of sequential processing of the histories using a recurrent neural network (in our case, a GRU) is evidenced by the poorer performance of Memn2n with respect to CMN. This suggests that gathering contexts temporally through sequential processing is indeed a superior method over non-temporal memory representations. CMN self which uses only single history channel also provides lesser performance when compared to CMN. This signifies the role of inter-speaker influences that often moderate the emotions of the current utterance. Overall, predictions on valence and arousal levels also show similar results which reinforce our hypothesis of CMN's ability to model emotional dynamics.  Hyperparameters: <ref type="figure" target="#fig_4">Figure 4</ref> provides a summary of the performance trend of our model for different values of the hyperparameters K (context window length) and Q (number of hops). In the first graph, as K increases, more past-utterances are provided to the model as memories. The performance maintains a positive correlation with K. This trend supplements our intuition that the historical context acts as an essential resource to model emotional dynamics. Given enough history, the performance saturates. The second graph shows that multiple hops on the histories indeed lead to an improvement in performance. The attention-based filtering in each hop provides a refined context representation of the histories. Models with hops in the range of 3 − 10 outperform the single layer variant. However, each added hop contributes a new set of parameters for memory representation, leading to an increase in total parameters of the model and making it susceptible to overfitting. This effect is evidenced in the figure where higher hops lead to a dip in performance.</p><p>Multimodality:  While K is varied, Q is set to be 3. Similarly, K = 20 when Q varies.</p><p>You mustn't be serious my dear one, that's just what they want.</p><p>[hap] Person A Person B Whose they? <ref type="bibr">[hap]</ref> With the most perfect poise .</p><p>[hap]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Histories</head><p>Person A: All the futile mortals who try to make life unbearable.</p><p>[?]</p><p>To classify:</p><p>Hello? What? ... Wrong Number <ref type="bibr">[hap]</ref> Oh , it sent shivers up my spine.</p><p>[hap]</p><p>2.</p><p>7.</p><p>1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behave exquisitely [hap] 4.</head><p>Oh, what shall we do if they suddenly walk in on us? <ref type="bibr">[hap]</ref> 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>(a) Correct label: happiness</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Person A Person B</head><p>Well I'll come back, what, are you not willing to wait for me? <ref type="bibr">[ang]</ref> You don't have to do this. Its not gonna work out. We're not a complete family without you being here.</p><p>[sad] I am afraid when you leave you won't come back.</p><p>[sad]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Histories</head><p>Person A: We are not a complete family here, you don't understand.</p><p>[?]</p><p>To classify:</p><p>So you're leaving tomorrow.</p><p>[sad]</p><p>I have to do this. Do you think I want to miss seeing her grow? <ref type="bibr">[sad]</ref> I don't know what to say. I don't want to go but I don't have a choice. <ref type="bibr">[sad]</ref> 2.</p><p>4. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Past</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Increasing attention</head><p>Thanks that helps. I feel much better now. <ref type="bibr">[ang]</ref> 7.</p><p>(b) Correct label: anger <ref type="figure">Figure 5</ref>: Average attention vectors across 3 hops for both memories for a given test utterance.</p><p>this shift is the improved representational scheme of the textual modality. Text tends to have lesser noisy signals as opposed to audio-visual sources, thus providing better features in the joint representation. Overall, multimodal systems outperform the unimodal variants justifying the design of CMN as a multimodal system. <ref type="table" target="#tab_5">Table 3</ref> also showcases the superiority of CMN and its variants over bc-LSTM. The proposed model achieves better performance over the state of the art in all the unimodal and multimodal segments. This asserts the importance of the memorynetwork framework and its ability to effectively store context information.</p><p>Role of Attention: Attention module plays a vital role in memory refinement. This is also observed in <ref type="table" target="#tab_2">Table 2</ref>, where CMN N A provides inferior performance over CMN. With the uniform weight, all the memory cells in both memories M a and M b equally contribute to the output representation. This incorporates irrelevant information from the perspective of emotional context.</p><p>Case Study: We perform qualitative visualization of the attention module by applying it on the testing set. <ref type="figure">Figure 5a</ref> represents a conversation where both the speakers are in an excited and jolly mood. Person A, in particular, drives the dialogue with less influence from Person B. To classify the test utterance of A, the attention module of CMN successfully focuses on the utterances 1, 3, 5 which had triggered the speaker's positive mood in the video. This shows CMN's capacity to model speaker-based emotions. Also, at the textual level, utterances 3 and 6 do not seem to depict a happy mood. However, audio and visual sources provide contrasting evidence which helps CMN to correctly model them as utterances spoken with happiness. This shows the advantage of a multimodal system.</p><p>In <ref type="figure">Figure 5b</ref> we reiterate through the dialogue presented in <ref type="figure">Figure 1</ref>. As shown, Person A converses in a sad mood (utterances 1, 3, 5 in <ref type="figure">Fig 5b)</ref>, bounded by the grief of his wife's departure. But when he expresses his inhibitions, his wife B reacts in an angry and sarcastic manner (utterance 7). This ignites an emotional shift for A who then replies angrily. In this example, CMN is able to focus on utterance 7 spoken by B to anticipate A's test utterance to be an angry statement, thus showing its ability to model inter-speaker influences. However, there are cases where our model fails, e.g., in the absence of historical utterances as this forces attention to focus on null memories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we presented a deep neural framework that identifies emotions for utterances in dyadic conversational videos. Our results suggest that leveraging context information from utterance histories and representing them as memories indeed helps to better recognize emotions. Performing speaker-specific modeling and considering interspeaker influences also helps in capturing emotional dynamics. This work also showed the importance of attention mechanism in filtering relevant contextual information from utterance histories and, hence, paved the path to the development of more efficient and human-like dialogue systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall architecture of proposed model: CMN. First, multimodal representations are extracted for each utterance and then the previous K = 4 utterances by both persons are used to model the histories using GRUs. For each person, R + 1 different GRUs are used to represent M (r)</figDesc><graphic url="image-3.png" coords="5,324.72,105.03,172.39,175.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Memory Output:</head><label></label><figDesc>First a new set of memories are created using another GRU ′ λ to get new memory representation M ′ λ = {(m t λ ) ′ }. An output represen- tation o λ ∈ R d is then generated using the weighted sum of attention vector p λ and new memory M ′ λ as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>)</head><label></label><figDesc>is used as the input mem- ory of the current hop M (r) λ . Output memory of current r th hop is generated using a new GRU (r)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3: Each block represents an utterance and the blocks are ordered as per temporal occurrence. Color scheme identifies their corresponding emotions. The arrows denote emotional influence directions.</figDesc><graphic url="image-9.png" coords="6,307.28,62.80,218.26,169.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance trends of our model with different values of K (history length) and Q (number of hops). While K is varied, Q is set to be 3. Similarly, K = 20 when Q varies.</figDesc><graphic url="image-10.png" coords="8,423.10,648.34,100.41,60.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>1 i o 1 b weighted sum ∑ q 2 i Memory Input</head><label></label><figDesc></figDesc><table>Memory Output 

Conversation : 

Person B 

Person A 

Multimodal 
Feature Extractor 

D o yo u ha ve 
yo ur fo rm ? 

E xc us e M e. 

Le t m e se e th em . 

Is th er e a 
pr ob le m ? 

W ho to ld yo u to 
ge t in th is lin e? 

Yo u di d. 

O ka y. B ut I 
di dn 't te ll yo u to 
ge t in th is lin e if 
yo u ar e fil lin g ou t 
th is pa rt ic ul ar 
fo rm . 

W ha t? I am 
ge tt in g an ID . 
Th is is w hy I 
ca m e he re . 
Ye ah . 

GRU 
GRU 
GRU 
GRU 

GRU 
GRU 
GRU 
GRU 

Happy 
Sad 
Neutral 
Angry 

A V T 
A V T 
A V T 
A V T 

A V T 
A V T 
A V T 
A V T 

A 

V 

T 

: Audio 
: Video 
: Text 

A 
V 
T 

softmax 

q Memory I/O for a single hop 

Memory 
Input for Pa 

Memory 
Output for Pa 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 presents the performances of CMN and its variants along with the state-of-the-art mod-</head><label>2</label><figDesc></figDesc><table>Models 

Emotion Categories 
Valence 
Arousal 
hops history Happiness Sadness Neutral Anger 

WAA 
WAA 
UAR 
WAA 
UAR 

SVM-ensemble 1 
-

single 

72.40 
61.90 
58.10 73.10 
69.50 
-
-
-
-
bc-LSTM 2 
-

single 

74.21 
76.50 
66.31 75.68 
74.31 
64.3 62.3 70.1 45.0 

Memn2n 
1 

dual 

72.36 
76.16 
66.93 80.23 
74.17 
-
-
-
-
3 

dual 

75.03 
76.36 
66.45 81.59 
75.08 
65.3 64.0 71.5 45.6 
CMN Self 
3 

single 

77.14  † 
76.99 
66.99 87.26  † 76.54  † 65.5 64.0 72.1 47.1 
CMN N A 
1 

dual 

74.33 
76.93 
66.49 86.29  † 75.77 
65.6 64.2 71.6 46.3 
CMN 
3 

dual 

81.75  † 
77.73 
67.32 89.88  † 77.62  † 66.1 64.3 72.2 47.6 

1 (Rozgic et al., 2012), 2 (Poria et al., 2017b).  †: significantly better than bc-LSTM 1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of CMN and its variants with state-of-the-art models (Section 5.2.2). All results use multi-
modal features. We report scores using weighted accuracy (WAA) and unweighted recall (UAR). UAR is a popular 
metric that is used when dealing with imbalanced classes (Rosenberg, 2012). Results are an average of 10 runs 
with varied weight initializations. We assert significance when p &lt; 0.05 under McNemar's test. 

els. CMN succeeds over both neural (Poria et al., 
2017b) and SVM-based (Rozgic et al., 2012) meth-
ods by 3.3% and 8.12%, respectively. Improvement 
in performance is seen for all emotions over the 
ensemble-SVM based method. A similar trend is 
seen with bc-LSTM (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of CMN to all the baselines in 
different modalities. Weighted accuracy is used as the 
metric. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 summarizes</head><label>3</label><figDesc></figDesc><table>the perfor-
</table></figure>

			<note place="foot" n="1"> http://audeering.com/technology/ opensmile</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was supported in part by the National Natural Science Foundation of China under Grant no. 61472266 and by the National University of Singapore (Suzhou) Research Institute, 377 Lin Quan Street, Suzhou Industrial Park, Jiang Su, People's Republic of China, 215123.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dimitrios Dimitriadis, and Emily Mower Provost. 2017. Pooling acoustic and lexical features for the prediction of valence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zakaria</forename><surname>Aldeneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Khorram</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards an automatic classification of emotions in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICSLP</title>
		<imprint>
			<biblScope unit="page" from="699" to="702" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multimodal fusion for multimedia analysis: a survey. Multimedia systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradeep K Atrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulmotaleb</forename><forename type="middle">El</forename><surname>Anwar Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan S</forename><surname>Saddik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kankanhalli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="345" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time speech emotion and sentiment recognition for interactive dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Farhad Bin Siddique</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Ho Yin Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1042" to="1047" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analysis of emotion recognition using facial expressions, speech and multimodal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serdar</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><forename type="middle">Min</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="205" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sentiment analysis is a big suitcase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discovering conceptual primitives for sentiment analysis by means of context embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Emotion recognition through multiple modalities: face, body gesture, speech. Affect and emotion in human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ginevra</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Kessous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Caridakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="92" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semantic audiovisual data fusion for automatic emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Datcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emotion recognition using bimodal data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon Jm</forename><surname>Dragos¸datcudragos¸dragos¸datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Systems and Technologies</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="122" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting depression via social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Munmun De Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Counts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICWSM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognizing emotion in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Polzin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICSLP</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1970" to="1973" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A review and meta-analysis of multimodal affect detection systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K D&amp;apos;</forename><surname>Sidney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Facial expression and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">384</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emotion cycles: On the social influence of emotion in organizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Hareli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Rafaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research in organizational behavior</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="35" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Changing emotion dynamics: individual differences in the effect of anticipatory social stress on emotional inertia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Koval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kuppens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">256</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Emotional dynamics and strategizing processes: A study of strategic conversations in top team meetings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sally</forename><surname>Maitlis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Management Studies</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="202" to="234" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How emotions work: The social functions of emotional expression in negotiations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keltner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research in organizational behavior</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mirroring facial expressions and emotions in dyadic conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costanza</forename><surname>Navarretta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Grobelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maegaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Affective computing: from laughter to ieee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rosalind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="17" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">formation Fusion</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-level multiple attentions for contextual multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM 2017</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A deeper look into sarcastic tweets using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1601" to="1612" />
		</imprint>
	</monogr>
	<note>Devamanyu Hazarika, and Prateek Vij</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition using deep learning architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Hiranmayi Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Emotion regulation in romantic relationships: The cognitive consequences of concealing feelings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">A</forename><surname>Jane M Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Social and Personal Relationships</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="599" to="620" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Classifying skewed data: Importance weighting to optimize average recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2012</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ensemble of svm trees for multimodal emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankaranarayanan</forename><surname>Viktor Rozgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirin</forename><surname>Ananthakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">APSIPA ASC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Emotion, affect and conversation. The handbook of conversation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Ruusuvuori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="330" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The handbook of conversation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sidnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Stivers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">121</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Audio-visual based emotion recognition-a new approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end multimodal emotion recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Björn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE JSTSP</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Select-additive learning: Improving generalization in multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaksha</forename><surname>Meghawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="949" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint/>
	</monogr>
<note type="report_type">Bordes. 2014. Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Context-sensitive multimodal emotion recognition from speech and facial expression using bidirectional lstm modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Emotional state transition model based on stimulus and personality characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xiaolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Lun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">China Communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="146" to="155" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Liu Xin, and Wang Zhiliang</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Natural language based financial forecasting: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Textbased emotion transformation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Hong-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Guo</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Engineering &amp; Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Augmenting end-to-end dialog systems with commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subham</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02709</idno>
		<title level="m">Recent trends in deep learning based natural language processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
	<note>Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Emotional chatting machine: Emotional conversation generation with internal and external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01074</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
