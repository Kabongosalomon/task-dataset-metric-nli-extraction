<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphAIR: Graph Representation Learning with Neighborhood Aggregation and Interaction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenyu</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
						</author>
						<title level="a" type="main">GraphAIR: Graph Representation Learning with Neighborhood Aggregation and Interaction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>PATTERN RECOGNITION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Graph representation learning</term>
					<term>neighborhood aggregation</term>
					<term>graph neural networks</term>
					<term>neighborhood interaction</term>
					<term>node classification</term>
					<term>link prediction !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph representation learning is of paramount importance for a variety of graph analytical tasks, ranging from node classification to community detection. Recently, graph convolutional networks (GCNs) have been successfully applied for graph representation learning. These GCNs generate node representation by aggregating features from the neighborhoods, which follows the "neighborhood aggregation" scheme. In spite of having achieved promising performance on various tasks, existing GCN-based models have difficulty in well capturing complicated non-linearity of graph data. In this paper, we first theoretically prove that coefficients of the neighborhood interacting terms are relatively small in current models, which explains why GCNs barely outperforms linear models. Then, in order to better capture the complicated non-linearity of graph data, we present a novel GraphAIR framework which models the neighborhood interaction in addition to neighborhood aggregation. Comprehensive experiments conducted on benchmark tasks including node classification and link prediction using public datasets demonstrate the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>G RAPH representation learning aims to transform nodes on the graph into low-dimensional dense vectors whilst still preserving the attribute features of nodes and structure features of graphs. In recent years, there has been a surge of research interest in utilizing neural networks to handle graph-structured data. Among them, graph convolutional networks (GCNs) have been shown effective in graph representation learning. They can model complex attribute features and structure features of graphs and achieve the state-of-the-art performance on various tasks. The core of graph convolution is that nodes learn their representations by aggregating features from their neighbors, i.e. the "neighborhood aggregation" scheme. Recently, some graph convolutional models, which primarily differ in the neighborhood aggregation strategies, have been proposed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. For example, GCN <ref type="bibr" target="#b0">[1]</ref> can be seen as the approximation of aggregation on the first-order neighbors; GraphSAGE <ref type="bibr" target="#b4">[5]</ref> designs several aggregators for inductive learning, where unlabeled data does not appear in the training process; GAT <ref type="bibr" target="#b1">[2]</ref> introduces the attention mechanism to model influence of neighbors with learnable parameters.</p><p>From a historical perspective, machine learning research has gone through a long process of development, with one clear trend from simple and linear models to complex and non-linear models. For example, limitations of the linear support vector machine (SVM) motivated the development of non-linear and more expressive kernel-based SVM classifiers <ref type="bibr" target="#b5">[6]</ref>. Besides, similar trends can be observed in the realm of image processing as real-world data distribution is usually rather complex. For example, simple and linear image filters <ref type="bibr" target="#b6">[7]</ref> are gradually superseded by deep nonlinear convolutional neural networks (CNNs) <ref type="bibr" target="#b7">[8]</ref>. Driven by the significance of modeling complex and non-linear distributions of data, a question arises: are existing GCNs capable enough to model the complex and non-linear distributions of graphs? We find that most previous graph convolutional models (e.g., GCN and GAT) are usually shallow with only one or two non-linear activation function layers, which may restrict the model from well capturing the complicated nonlinearity of graph data.</p><p>In this paper, we first theoretically prove that the effect of non-linear activation functions in GCNs is to introduce the interaction terms of neighborhood features. We then show that coefficients of the neighborhood interacting terms are relatively small in current GCN-based models. To this end, we present a general framework named GraphAIR (Aggregation and InteRaction) <ref type="bibr" target="#b0">1</ref> . The key idea behind our approach is to explicitly model the neighborhood interaction in addition to neighborhood aggregation, which can better capture the complex and non-linear node features. WAs illustrated in <ref type="figure">Figure 1</ref>, GraphAIR consists of two parts, i.e. aggregation and interaction. The aggregation module constructs node representations by combining features from neighborhoods; the interaction module explicitly models neighborhood interactions through multiplication.</p><p>Nevertheless, several challenges exist in modeling the neighborhood interaction. Firstly, different nodes may have various numbers of adjacent neighbors, leading to different numbers of interaction pairs among neighbors. Thereby, defining a universal neighborhood interaction operator which is able to handle arbitrary numbers of interaction pairs is challenging. Secondly, it is preferable to propose a general plug-and-play interaction module instead of designing model-specific neighborhood interaction strategies for different GCN-based models.</p><p>To tackle the aforementioned challenges, we derive that the neighborhood interaction can be easily obtained through the multiplication of node embeddings. As a result, both of the neighborhood aggregation module and the neighborhood interaction module can be implemented by most existing graph convolutional layers.</p><p>In a nutshell, the main contributions of this paper are three-fold. Firstly, to best of our knowledge, it is the first work to analyze existing GCN-based methods from the perspective of capturing non-linearity of graph data. We then propose to explicitly model neighborhood interaction for capturing non-linearity of graph-structured data. Secondly, the proposed GraphAIR can easily integrate off-the-shelf graph convolutional models, which shows favorable generality. Thirdly, extensive experiments conducted on benchmark tasks of node classification and link prediction show that GraphAIR achieves the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Graph representation learning, which aims to learn lowdimensional representations that preserve useful topology and attributive information, plays an important role in many tasks, such as node classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, graph classification <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, clustering <ref type="bibr" target="#b13">[14]</ref>, time-series analysis <ref type="bibr" target="#b14">[15]</ref>, and recommendation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. There have been a lot of attempts in recent literature to employ neural networks for graph representation learning. Among them, graph convolutional neural networks (GCNs) receive a lot of research interests. GCN-based models generally follow the neighborhood aggregation scheme. To be specific, the model passes the input signals from neighborhoods through filters to aggregate information. Many approaches design different strategies to aggregate information from nodes' neighborhood. According to different strategies, these models can be roughly grouped into two categories, i.e. spectral-based approaches and spatial-based approaches.</p><p>One the one hand, spectral methods depend on the Laplacian eigenbasis to define parameterized filters. The first work <ref type="bibr" target="#b19">[20]</ref> introduce convolutional operations in the Fourier domain by computing the eigendecomposition of the graph Laplacian, which results in potentially heavy computational burden. Following its work, <ref type="bibr" target="#b20">[21]</ref> propose to approximate filters using Chebyshev expansion of the graph Laplacian. Then, graph convolutional neural networks (GCNs) <ref type="bibr" target="#b0">[1]</ref> have been widely applied for graph representation learning. The core of GCNs is the neighborhood aggregation scheme which generates node embedding by combining information from neighborhoods. Since GCN only captures local information, DGCN <ref type="bibr" target="#b2">[3]</ref> then proposes to construct an information matrix to encode global consistency.</p><p>On the other hand, the spatial approaches directly operate on spatially close neighbors. To enable parameter sharing of filters across neighbors of different sizes, <ref type="bibr" target="#b21">[22]</ref> first propose to learn weight matrices for different node degrees. MoNet <ref type="bibr" target="#b22">[23]</ref> proposes a spatial-domain model to provide a unified convolutional network on graphs. To compute node representations in an inductive manner, GraphSAGE <ref type="bibr" target="#b4">[5]</ref> samples fixed-size neighborhoods of nodes and performs aggregation over them. Similarly, <ref type="bibr" target="#b23">[24]</ref> select a fixed number of neighbors and enable the use of conventional convolutional operations on Euclidean spaces. Recently, GAT <ref type="bibr" target="#b1">[2]</ref> introduces attention mechanisms to graph neural networks, which computes hidden representations by attending over neighbors with a self-attention strategy. BASGCN <ref type="bibr" target="#b24">[25]</ref> transforms the arbitrary-sized graphs into fixed-sized backtrackless aligned grid structures, and performs a novel backtrackless spatial graph convolutional operation on the grid structures to extract multi-scale local-level vertex features.</p><p>Recently, some methods are proposed to focus on linearity and non-linearity of graphs respectively. On the one hand, simplified graph convolutional networks (SGCs) <ref type="bibr" target="#b25">[26]</ref> try to reduce the complexity and eliminate redundant computation of GCN by successively removing non-linear activation functions. SGC makes assumptions that nonlinearity between GCN layers is not critical to the model performance and the majority of the benefit is brought by the neighborhood aggregation scheme. While being more computationally efficient, SGC achieves comparable empirical performance to vanilla GCN.</p><p>There are other methods arguing that modeling nonlinear distributions of node features can bring improvements. For example, GraphSAGE-LSTM <ref type="bibr" target="#b4">[5]</ref> employs the long-short-term memory (LSTM) module to learn the complex relationships between the nodes. Empirically, GraphSAGE-LSTM outperforms other aggregation functions such as GraphSAGE-mean and GraphSAGE-GCN. Graph isomorphic networks (GIN) <ref type="bibr" target="#b26">[27]</ref> apply multilayer perceptrons (MLPs) in each graph convolutional layer, which is able to model complex non-linearity of graphs. Although theoretically it is well known that MLPs are universal approximators <ref type="bibr" target="#b27">[28]</ref>, there is no formal theorem giving instructions on how to asymptotically approximate the desired function ([29], p. 182; <ref type="bibr" target="#b29">[30]</ref>, p. 328). Different from GraphSAGE-LSTM and GIN, to best of our knowledge, our work is the first to point out that most existing GCNs may not well capture non-linearity of graph data and we demonstrate the effectiveness of explicitly modeling nonlinearity of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND AND PRELIMINARIES</head><p>In this section, we firstly introduce the notations used throughout the paper and then summarize some of the most common GCN models. Last, we briefly introduce residual learning which we employ in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>Let G = (A, X) be an undirected graph with n nodes, where A ∈ R n×n is the adjacency matrix, X ∈ R n×d is the feature attribute matrix, and x i ∈ R 1×d denotes the attribute of node i. Please kindly note that in this paper we primarily focus on undirected graphs, but our proposed method can be easily generalized to work with weighted or directed graphs. The mathematical notations used throughout this paper are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Aggregators in Graph Convolutional Models</head><p>As mentioned above, existing GCNs mainly differ in the neighborhood aggregation functions. The representative graph convolutional model such as GCN <ref type="bibr" target="#b0">[1]</ref> and GAT <ref type="bibr" target="#b1">[2]</ref> can be formulated as:</p><formula xml:id="formula_0">n (k) i = h (k) i W (k) ,<label>(1)</label></formula><formula xml:id="formula_1">h (k+1) i = σ   j∈Ni e ij n (k) j   ,<label>(2)</label></formula><p>where h (k) i ∈ R d k is the embedding of the i th node resulting from the k th graph convolutional layer, W (k) ∈ R d k ×d k+1 is a learnable weight matrix, e ij is a scalar which indicates the importance of node j's features to node i, and j e ij = 1. σ(·) is the activation function, e.g., ReLU(·) = max(0, ·) and N i is the set containing the first-order neighbors of node i as well as node i itself. To obtain the node embedding, a linear transformation is first conducted to project features to a new feature subspace. Then, the node embedding can be updated by weighted summation over the projected features of its neighbors, followed by a non-linear activation function.</p><p>Different models adopt different strategies to design the aggregators. For GCN, it uses a predefined weight matrix A =D − 1 2ÃD − 1 2 for summarization, whereÃ = A + I is the adjacency matrix with self-loops andD ii = jÃ ij . Here, entry a ij ofÂ is a predefined weight factor for weighted summarization over neighborhoods, i.e. e ij in Eq.(2). Unlike GCN, GAT makes use of the attention mechanism to explicitly learn e ij as follows:</p><formula xml:id="formula_2">α ij = g(n i , n j ), e ij = softmax(α ij ) = exp (α ij ) k∈Ni exp (α ik ) ,<label>(3)</label></formula><p>where g : R d ×R d → R is a self-attention function, which can be simply implemented as a feed-forward neural network. The implicit and insufficient neighborhood interaction involved in existing GCNs. It is seen from Eq. (2) that without the activation function, the node representation would depend linearly on the neighborhood features. Then, although mainstream models adopt non-linear activation functions, which is able to introduce the neighborhood interaction implicitly as a side effect, they still face challenges in learning the neighborhood interaction sufficiently. We take the sigmoid function s(t) = 1 1+e −t as an example and approximate it with Taylor polynomials. Note that mainstream GCN-based models (as summarized in <ref type="table" target="#tab_1">Table  2</ref>) use piecewise non-saturating activation functions, such as ReLU and LeakyReLU(x) = max(0.01x, x). These functions suppress negative values yet are still linear for positive values. Here we analyze the sigmoid function as it brings more non-linearity. Since the elements in the node embeddings are small 2 , the high-order interacting terms among the neighborhoods are small as well. Then, we just analyze the coefficients of high-order interacting terms, which is claimed in the following proposition. Proposition 1. When applying the sigmoid function s(t) on the result of the linear combination as formulated in Eq. <ref type="formula" target="#formula_1">(2)</ref>, the equivalent coefficient of high-order interacting terms of the neighborhood embeddings is at most 1 48 .</p><p>Proof. The sigmoid function s(t) can be approximated as Taylor polynomials at t 0 = 0:</p><formula xml:id="formula_3">s(t) ≈ P p=0 s (p) (0) p! t p = 1 2 + 1 4 t− 1 48 t 3 +· · ·+ s (P ) (0) P ! t P ,<label>(4)</label></formula><p>where P is the degree of the polynomial. The approximation error can be bounded using the Lagrange form of the remainder:</p><formula xml:id="formula_4">|R p (t)| ≤ |t| P +1 (P + 1)! M p , where |s P +1 (θ)| ≤ M p , θ ∈ (−t, t).<label>(5)</label></formula><p>Since the coefficient of the quadratic term is zero, we set P = 2 and analyze the contribution of high-order interacting terms. Then, replacing t with j∈Ni e ij n (k) j , Eq. (2) can be written as follows:</p><formula xml:id="formula_5">h (k+1) i = 1 2 + 1 4   j∈Ni e ij n (k) i   + M   j∈Ni k∈Ni l∈Ni e ij e ik e il n (k) j · n (k) k · n (k) l   ,<label>(6)</label></formula><p>where M is the bound of the reminder. To analyze its maximum value, we first get the third derivative of the sigmoid function:</p><formula xml:id="formula_6">s (3) (θ) = d 3 dθ 3 1 1 + e −θ = e −θ (1 + e −θ ) 2 − 6e −2θ (1 + e −θ ) 3 + 6e −3θ (1 + e −θ ) 4 .<label>(7)</label></formula><p>2. Most existing graph convolutional models, including GCN, Graph-SAGE, and GAT normalize the input and initialize the weights using Glorot initialization <ref type="bibr" target="#b30">[31]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation Description</head><p>A, A adjacency matrix, adjacency matrix with self-loops n the number of nodes d the dimension of the input feature N i the set of first-order neighbors of node i including itself n i transformed embeddings before aggregation W <ref type="bibr">(k)</ref> weight matrix in the k-th graph convolutional layer e ij importance weight of node j's feature to node i β ij interaction weight between node i and node j h i embedding of node i resulting from graph convolution h agg i embedding of node i resulting from neighborhood aggregation h ir i embedding of node i resulting from neighborhood interaction h air i embedding of node i combining neighborhood aggregation and interaction z i embedding of node i resulting from the output layer </p><formula xml:id="formula_7">h i = σ(W 1 deg(i) j∈Ni h j ) ReLU GraphSAGE-GCN [5] h i = σ(W avg j∈N sample ∩{i} (h j )) ReLU GAT [2] h i = σ( j∈Ni e ij h j ) ELU SGC [26] h i = W 1 deg(i) j∈Ni h j N.A.</formula><p>Then, making s (4) (θ) = 0, we can calculate its roots:</p><formula xml:id="formula_8">θ 1 = 0, θ 2 = log 5 + 2 √ 6 , θ 3 = log 5 − 2 √ 6 .<label>(8)</label></formula><p>Therefore, the corresponding extreme values of s (3) (θ) are − 1 8 , <ref type="bibr">1 24</ref> , and <ref type="bibr">1 24</ref> . It is obvious to see the maximum absolute value of s (3) (θ) is <ref type="bibr">1 8</ref> . Therefore,</p><formula xml:id="formula_9">M = |s (3) (θ)| 3! ≤ 1 48 ,<label>(9)</label></formula><p>which concludes the proof.</p><p>Remark. Proposition 1 states that the effect of non-linear activation functions in GCNs is to introduce the interaction terms of neighborhood features. The coefficients of the neighborhood interacting terms in current GCN-based models are relatively small, leading to a negligible contribution to node representations. Considering other kinds of non-linear activation functions, such as ReLU-like piecewise non-saturating activation function, they exhibit less nonlinearity than then sigmoid function; therefore, the coefficients of the neighborhood interacting terms in current GCN-based models will be smaller than 1 48 . As existing GCNs are usually shallow with only one or two non-linear layers to avoid oversmoothing and overfitting <ref type="bibr" target="#b31">[32]</ref>, nonlinearity of graph data cannot be learned sufficiently.</p><p>Moreover, we also conduct empirical experiments to compare the performance of existing representative GCNbased models by using other activation functions, including sigmoid and tanh. As shown in <ref type="table" target="#tab_2">Table 3</ref>, even with highly non-linear activation functions such as tanh and sigmoid, existing GCN-based methods perform even worse than their original models. We think the performance degradation is due to these saturating non-linear activation functions suffer from vanishing gradients and are much slower than nonsaturating activation function <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. The results in <ref type="table" target="#tab_2">Table 3</ref> demonstrates the inefficiency of leveraging activation functions to capture non-linearity of graph-structured data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Residual Learning</head><p>In this paper, we employ residual learning to combine the neighborhood aggregation and interaction. Residual learning <ref type="bibr" target="#b34">[35]</ref> is a widely-used building block for deep learning. Suppose h(x) is the true and desired mapping and x is the suboptimal representation which serves as the input feature to the residual module. Residual learning can be formulated as:</p><formula xml:id="formula_10">h(x) = f (x) + x,<label>(10)</label></formula><p>where f (·) is a residual function. Practically, we can apply a few non-linear layers to obtain the suboptimal representation x and some other non-linear layers to implement the residual function f . The essence of residual learning lies in the skip connection, through which the earlier representations are able to flow to later layers. The skip connection enables more direct reuse of the suboptimal representation and improves the information flow during forward and backward propagation <ref type="bibr" target="#b34">[35]</ref>, which makes the network easier to be optimized. Many approaches <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> have shown that residual learning helps break away from the local optimum and improving the performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE PROPOSED METHOD: GRAPHAIR</head><p>In this section, we firstly formulate the model of neighborhood interaction and then describe how the parameters of GraphAIR model can be learned. Finally, we summarize the overall model architecture and analyze the computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Modeling the Neighborhood Interaction with Residual Functions</head><p>As discussed in Section 3.2, the node representation resulting from the neighborhood aggregation scheme is less likely to well capture complicated non-linearity of graphs because they learn the neighborhood interaction implicitly and inefficiently. In this section, we describe the embedding generation algorithm of GraphAIR, which aims to incorporate the neighborhood interaction into node representations.</p><p>To begin with, a natural idea to model the quadratic terms of neighborhood interaction is formulated as:</p><formula xml:id="formula_11">h ir i = j∈Ni k∈Ni β jk n j n k ,<label>(11)</label></formula><p>where h ir i is the neighborhood interaction representation of node i, β jk denotes the coefficient of the quadratic term, and is the element-wise multiplication operator. However, it is infeasible to learn β jk in our case. For each node i, there are O(|N i | 2 ) coefficients to estimate, which exposes the risk of overfitting. To alleviate this problem, we simply assign β jk as the product of importance weights e ij and e ik . The simplification is reasonable with the following aspects. For node i, if e ij and e ik are large, then the neighbor nodes j and k should be considered as important factors for the representation of node i. Compared to other interacting terms, the interaction between node j and k are likely to provide more relevant information about node i. Consequently, β jk should be large. In contrast, if e ij and e ik are small, neighbor nodes j and k may have a slight impact on node i. Thus, the interacting coefficient should be small as well. Formally, we arrive at:</p><formula xml:id="formula_12">h ir i =   j∈Ni e ij n j     k∈Ni e ik n k   =   j∈Ni e ij h j W     k∈Ni e ik h k W   = h agg i h agg i ,<label>(12)</label></formula><p>where h agg i = j∈Ni e ij h j W denotes the representation resulting from neighborhood aggregation. <ref type="figure" target="#fig_2">Figure 2a</ref> illus-  trates this process. For brevity, we name this operation as self-InteRaction (self-IR)</p><p>In order to introduce more non-linearity to our model, we apply non-linear activation function on the two representations resulting from neighborhood aggregation and neighborhood interaction respectively. Since combining different non-linear layers by skip connection for capturing various information has been widely used in recent work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>, we combine these two representations by using a skip connection as follows.</p><formula xml:id="formula_13">h air i = σ(h agg i ) + σ(h ir i ).<label>(13)</label></formula><p>It should be noted that concatenation is also widely used and we found that skip connections by adding performs better for combining neighborhood aggregation and interaction. Please refer to Section 5.5 for additional experiments. However, although we adopt a skip connection here, we argue that we still cannot benefit from residual learning, where both of the suboptimal representation and the residual function are implemented by different non-linear layers. As formulated in Eqs. <ref type="bibr" target="#b11">(12,</ref><ref type="bibr" target="#b12">13)</ref>, the two representations resulting from neighborhood aggregation and interaction are based on the same weight matrix W , which means the variations of the two representations during the backpropagation process are highly correlated. According to <ref type="bibr" target="#b37">[38]</ref>, it is important to disentangle the factors of variation to the representations as only a few factors tend to change at a time. Therefore, to make use of residual learning which can ease the optimization, we introduce another weight matrix W ∈ R d k ×d k+1 to disentangle learning the neighborhood interaction from neighborhood aggregation. Formally, instead of Eq. (12), we use the following equation to learn the neighborhood interaction in our model:</p><formula xml:id="formula_14">h ir i =   j∈Ni e ij h j W     k∈Ni e ik h k W   = h agg i h agg i ,<label>(14)</label></formula><p>where the first term h agg i = j∈Ni e ij h j W denotes the representation resulting from neighborhood aggregation and the second termh agg i = k∈Ni e ik n k W provides the other half node representation for multiplication in the interaction process. h agg i is the input representation to the residual module and W is the learnable weight of the residual function. Note that both terms h agg i andh agg i can be implemented by existing graph convolutional layers. We illustrate Eq. (14) in <ref type="figure" target="#fig_2">Figure 2b</ref>. We also conduct ablation study in Section 5.4 to prove the effectiveness of Eq. <ref type="bibr" target="#b13">(14)</ref>.</p><p>To sum up, the neighborhood aggregation and interaction module shown in <ref type="figure">Figure 1</ref> can be combined by skip connections, as shown in <ref type="figure" target="#fig_2">Figure 2c</ref>. Thus, the proposed GraphAIR framework is compatible with most existing GCN-based models such as GCN <ref type="bibr" target="#b0">[1]</ref> and GAT <ref type="bibr" target="#b1">[2]</ref> and it provides a plug-and-play module for the neighborhood interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning the Parameters of GraphAIR</head><p>In this section, we introduce how to learn the parameters under the GraphAIR framework. As we aim to propose a general approach for graph representation learning, we can apply different kinds of graph-based loss function, such as the proximity ranking loss in link prediction tasks and the cross-entropy loss in node classification tasks. Without loss of generality, we take the task of node classification as an example.</p><p>To compute the probability that each node belongs to a certain class, existing GCN-based models usually employ one additional graph convolutional layer with a softmax classifier for prediction. Then, the output representation z i is formulated as: <ref type="bibr" target="#b14">(15)</ref> where g(·) is the prediction function, W (k+1) ∈ R d k ×|Y| , and |Y| is the number of classes. Then, the loss of node classification can be calculated as</p><formula xml:id="formula_15">z i = g h (k) i = softmax   j∈Ni e ij h (k) j W (k+1)   ,</formula><formula xml:id="formula_16">L = 1 n n i=1 L clf (z i , y i )</formula><p>where y i is the true label for node i and L clf is the crossentropy loss.</p><p>To obtain more accurate node embeddings h agg i and h agg i , we apply two auxiliary classifiers on h agg i andh agg i . Subsequently, the resulting representation h ir i for the neighborhood interaction will be more precise as well. Then, as formulated in Eq. (15), we apply one additional graph convolutional layer on each of h air i , h agg i , andh agg i to attain z air i , z agg i , andz agg i . Eventually, the overall objective function is the weighted sum of the three losses:</p><formula xml:id="formula_17">L total = 1 n n i=1 λ 1 L clf (z air i , y i ) + λ 2 L clf (z agg i , y i ) + λ 3 L clf (z agg i , y i ) = λ 1 L air + λ 2 L agg + λ 3L agg ,<label>(16)</label></formula><p>where λ 1 , λ 2 , and λ 3 are hyperparameters controlling weights of the three loss functions. For training, we minimize the total loss L total , while for inference, we only use z air i , since z agg i andz air i are to ensure h air i is accurate enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Architecture and Complexity Analysis</head><p>We suppose there are (K +1) layers in the underlying graph convolutional model, where the last layer is employed for node classification. For GraphAIR, we employ two separate and symmetric branches, each of which consists of K graph convolutional layers to obtain h for the sake of efficiency. Additionally, we employ three graph convolutional layers followed by softmax activation functions on h air i , h agg i , andh agg i . In summary, there will be (2K + 3) layers in GraphAIR.</p><p>The proposed GraphAIR model with K = 1 is illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. Each layer in GraphAIR has the same space and time complexity as the underlying model and the additional computation cost of GraphAIR is mainly introduced by the multiplication process for the neighborhood interaction. For the neighborhood interaction in Eq. <ref type="bibr" target="#b13">(14)</ref>, the cost is O(nd) where d is the embedding dimension. For each layer of the existing graph convolutional model such as GCN and GAT, it takes O(n 2 d) time to proceed Eq. (2). Therefore, the additional computation cost of neighborhood interaction is insignificant. That is to say, our proposed approach is as asymptotically efficient as the underlying graph convolutional model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>We extensively evaluate our proposed GraphAIR model on the node classification task and link prediction using five public datasets. Besides, we also conduct ablation studies on the neighborhood interaction module. For readers of interest, we include comparison of training time and all details of the experimental configurations in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We use five widely-used datasets to evaluate model performance on both transductive learning and inductive learning scenarios. Specifically, three citation networks (Cora, Citeseer, Pubmed) are used for tranductive node classification and link prediction, one knowledge graph (NELL) is used for transductive node classification, and one multi-graph molecular network (PPI) is for inductive node classification. We exactly follow the setup in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. The statistics of datasets used throughout the experiments are summarized in <ref type="table" target="#tab_3">Table 4</ref>. Citation networks. We build undirected citation networks from three datasets, where documents and citations are treated as nodes and edges respectively. We treat the bag-of-words of each document as the feature vector. Our goal is to predict the class of each document. Only twenty labels per class are used for training.</p><p>Knowledge graphs. The dataset collected from the knowledge base of Never Ending Language Learning (NELL) contains entities, relations, and text description. For every triplet (e 1 , r, e 2 ), where e 1 and e 2 are entities and r is the relationship between them, r will be assigned with two separate nodes r 1 and r 2 . Then, we add two edges between (e 1 , r 1 ) and (e 2 , r 2 ). For the knowledge graph, we conduct the entity classification. Similarly, we use bag-of-words as feature vectors. Only one label per class is used for training.</p><p>Molecular networks. We use the PPI (protein-protein interaction) network that consists of twenty-four (24) graphs corresponding to different human tissues. Each node contains fifty (50) features composed of positional gene sets, motif gene sets, and immunological signatures. We select twenty (20) graphs as the training set, two (2) for validation, and two (2) for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments on Node Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Baseline Methods</head><p>We comprehensively compare our method with various traditional random-walk-based algorithms and state-of-theart GCN-based methods. We closely follow the experimental setting of previous work; the performance of those baselines is reported as in their original papers. <ref type="bibr" target="#b2">3</ref> Transductive node classification. In the transductive setting, the baselines include skip-gram-based network embedding method DeepWalk <ref type="bibr" target="#b40">[41]</ref>, graph convolutional networks with higher-order Chebyshev filters (Planetoid) <ref type="bibr" target="#b38">[39]</ref>, graph convolution with one-hop neighbors (GCN) <ref type="bibr" target="#b0">[1]</ref>, and graph attention networks (GAT) <ref type="bibr" target="#b1">[2]</ref>. In addition, we further compare the performance of the proposed model with the recently proposed simplified graph convolutional networks (SGCs) <ref type="bibr" target="#b25">[26]</ref> which removes redundant non-linear activations. Also, we modify graph isomorphic networks (GINs) <ref type="bibr" target="#b26">[27]</ref> which utilize non-linear MLPs as the aggregation function for the node classification task. Note that since GIN was originally proposed for graph classification, we apply two GIN convolutional layers and remove the graph-level readout function for the transductive node classification task. Moreover, we include two recent methods Graph Markov Neural Networks (GMNN) <ref type="bibr" target="#b41">[42]</ref> and Deeper Graph Convolutional Networks (DAGNN) <ref type="bibr" target="#b42">[43]</ref>.</p><p>Inductive node classification. For inductive node classification, we mainly compare GraphAIR with inductive graph convolutional networks (GraphSAGE) <ref type="bibr" target="#b4">[5]</ref> and graph attention networks (GAT) <ref type="bibr" target="#b1">[2]</ref>. Note that GraphSAGE provides several variants of neighborhood aggregators: SAGEmean concatenates the features of the neighborhoods and the central node, SAGE-GCN takes the average over neighborhood feature vectors, SAGE-LSTM combines neighborhood features by using a LSTM model, and SAGE-pool uses an element-wise max-pooling operator to aggregate the neighborhood information nonlinearly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Experimental Configurations</head><p>We employ our GraphAIR framework on top of three representative models, including GCN, GraphSAGE, and GAT, which is denoted by AIR-GCN, AIR-SAGE, and AIR-GAT, respectively. Particularly, while GraphSAGE proposes several variants for neighborhood aggregation, among them only SAGE-GCN satisfies the coefficient normalization in Eq. <ref type="bibr" target="#b13">(14)</ref>. Therefore, we select SAGE-GCN as the base model for GraphAIR. For a fair comparison, we closely follow the same hyper-parameters setting as the underlying graph convolutional model, such as learning rate, dropout rate, weight decay factor, hidden dimensions, etc. Considering GIN is originally proposed for graph-level classification, the hidden dimensions are set to the same as GCN. In the experiment, we only tune the weights of three loss functions by grid search, where λ i ∈ [0.1, 0.2, . . . , 1.5], ∀i ∈ {1, 2, 3}.</p><p>For the transductive setting, we use the features of all data but only the labels of the training set are used for training. For the inductive setting, we train our model without the validation data and testing data. In addition, we report the average accuracy of 20 measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Results and Analysis</head><p>Transductive. We summarize the results of transductive node classification in <ref type="table" target="#tab_4">Table 5</ref>. Note that even we apply the sparse version implementation of GAT, it requires more than 64G memory on NELL dataset. Thus, the performance of GAT and AIR-GAT is not reported. From the tables, it is seen that GraphAIR achieves state-of-the-art performance over all datasets, which demonstrates the effectiveness of the proposed GraphAIR framework. SGC acquires comparable results to that of GCN, which corresponds to our conclusion in Proposition 1 that existing GCNs are not able to learn the nonlinearity of graph data sufficiently. For our proposed AIR-GCN, it outperforms its base model GCN by margins of 3.2%, 2.6%, 1.0%, and 2.5%. The same trends hold for AIR-GAT with its base model GAT as well. To sum up, the improvements demonstrate the effectiveness of modeling the non-linear distributions of nodes. In addition, another important observation is that, both AIR-GAT and AIR-GCN outperform the complex non-linear opponents such as GIN. Although MLPs are able to asymptotically approximate any complicated and non-linear functions theoretically, they tend to converge to undesired local minima in practice <ref type="bibr" target="#b43">[44]</ref>. The experimental results prove the rationality of explicitly introducing neighborhood interaction.</p><p>Inductive. The results of inductive learning are shown in <ref type="table" target="#tab_5">Table 6</ref>. AIR-SAGE-GCN outperforms its base model SAGE-GCN by 2.2%. Besides, we can clearly observe that AIR-GAT achieves the best performance. It is worth noting that the previous state-of-the-art method has already reached pretty high performance and the proposed AIR-GAT still acquires the improvement of 1.3% over the vanilla GAT. Besides, it is suggested that the proposed GraphAIR framework is also generalizable for multiple graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments on Link Prediction</head><p>In order to further verify our proposed framework is general for other graph representation learning tasks, we conduct experiments on link prediction additionally. We choose citation networks as benchmark datasets and compare against various state-of-the-art methods, including graph autoencoders (GAE) <ref type="bibr" target="#b39">[40]</ref> and variational graph autoencoders (VGAE) <ref type="bibr" target="#b39">[40]</ref>, as well as other baseline algorithms, including SC <ref type="bibr" target="#b44">[45]</ref> and DeepWalk <ref type="bibr" target="#b40">[41]</ref>. We employ our GraphAIR framework on the basis of GAE, which constructs the graph autoencoder with GCNs. The resulting model is denoted by AIR-GAE.</p><p>We report the performance in terms of area under the ROC curve (AUC) based on the performance of 20 runs. The mean performance and standard error are presented in <ref type="table" target="#tab_6">Table  7</ref>. It is shown from the table that the proposed AIR-GAE outperforms its vanilla opponents GAE and VGAE, which once again verifies the necessity to incorporate the neighborhood interaction to neighborhood aggregation. Please note that previous state-of-the-art methods have already obtained high enough performance on the Pubmed dataset and our method AIR-GAE pushes the boundary with absolute improvements of 2.8%, achieving 99.2% in terms of AUC. Also, it can be observed that the proposed method obtains much more obvious improvements, compared with the performance of node classification. We suspect that this is primarily because models for the link prediction task usually employ pairwise decoders for calculating the probability of the link between two nodes. For example, GAE and VGAE assume the probability that there exists an edge between two nodes is proportional to the dot product of the embeddings of these two nodes. Therefore, our approach, which explicitly models the neighborhood interaction through the multiplication of the embeddings of   two nodes, is inherently related to the link prediction task and obtains more improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Studies on the Neighborhood Interaction Module</head><p>As we analyzed in Section 3.3, the number of parameters in GraphAIR is almost two times than that of the underlying graph convolutional model. In this section, we conduct ablation studies to answer the following questions:</p><p>• Q1: How much improvement has the proposed neighborhood interaction module brought?</p><p>• Q2: Does the disentangled residual learning strategy bring sufficient improvements?</p><p>To answer Q1 and verify the effectiveness of GraphAIR is introduced by the proposed neighborhood interaction module rather than the larger number of parameters in the model, we remove the neighborhood interaction module of AIR-GCN. Then, the resulting model has exactly the To answer Q2, we employ only one branch of graph convolutional networks consisting (K +1) layers to produce the output representations. To obtain neighborhood interaction h ir , we directly make use of the self-interaction strategy described in Eq. (9) instead of Eq. <ref type="bibr" target="#b10">(11)</ref>. The resulting model is termed as self-IR-GCN.</p><p>For fair comparison, other experimental configurations are kept the same as AIR-GCN. The results of node classification are presented in <ref type="figure" target="#fig_5">Figure 4</ref>. It is seen from the figure that the proposed AIR-GCN evidently achieves the best performance and outperforms DP-GCN and self-IR-GCN. For Q1, we can observe that DP-GCN only obtains slightly better accuracy on Cora and Citeseer and almost the same performance as the vanilla GCN on Pubmed. It can be verified that the neighborhood interaction module mainly contributes to the performance improvement of the proposed AIR-GCN model. For Q2, it is seen that the performance of self-IR-GCN only gets slightly improved on three datasets, which demonstrates the rationality of modeling neighborhood interaction. However, disengaging the neighborhood interaction from neighborhood aggregation can bring more improvements. 8: Accuracy of node classification with different ways of combining node embeddings resulting from neighborhood aggregation and interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Cora Citeseer Pubmed NELL AIR-GCN 84.7% ± 0.1% 72.9% ± 0.1% 80.0% ± 0.1% 68.5% ± 0.2% AIR-GAT 84.5% ± 0.7% 73.5% ± 0.6% 80.0% ± 0.2% -AIR-GCN-concat 83.1% ± 0.1% 70.6% ± 0.1% 79.2% ± 0.1% 66.8% ± 0.1% AIR-GAT-concat 83.3% ± 0.8% 71.2% ± 0.6% 79.4% ± 0.3% - </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Discussions on Combining Neighborhood Aggregation and Interaction</head><p>In this section, we also examine different ways of combining node embeddings resulting from neighborhood aggregation and interaction. Apart from residual connections <ref type="bibr" target="#b34">[35]</ref>, we also conduct experiments that combine node embeddings from neighborhood aggregation and interaction using concatenation, where the models are denoted as AIR-GCNconcat and AIR-GAT-concat respectively. The results are summarized in <ref type="table">Table 8</ref>. It is seen that although using concatenation introduces no information loss, it may cause performance loss due to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Parameter Sensitivity Analysis</head><p>In this section, we attempt to investigate how the hyperparameters of loss functions impact the model performance. To this end, we conduct experiments on the Cora dataset with different combinations of parameters (λ 2 , λ 3 ), where λ 2 , λ 3 ∈ {0, 0.1, 0.2, . . . , 1.5}. We report the node classification performance in terms of accuracy and the corresponding results are shown in <ref type="figure" target="#fig_6">Figure 5</ref>. For other hyperparameters, we set λ 1 = 1.1 according to the performance on most datasets, which is also consistent with our previous experiments reported in Section 5.2 and Section 5.3. It can be seen from <ref type="figure" target="#fig_6">Figure 5</ref> that, when λ 3 equals to 0.4 or 0.5, the performance of the model stays stable. In other words, the hyperparameter λ 2 can be selected in a wide range, which means that the selection of hyperparameters in GraphAIR model does not impact the performance too much. Nevertheless, too small or too large hyperparameters (λ 2 , λ 3 ) greatly attenuate or accentuate the contribution of one branch, and thus lead to downgraded performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have firstly proved that existing mainstream GCN-based models have difficulty in well capturing the complicated non-linearity of graph data. Then, in order to better capture the complicated and non-linear distributions of nodes, we have proposed a novel GraphAIR framework that explicitly models the neighborhood interaction in addition to the neighborhood aggregation scheme. By employing residual learning strategy, we disentangle learning the neighborhood interaction from the neighborhood aggregation, which makes the optimization easier. The proposed GraphAIR is compatible with most existing graph convolutional models and it can provide a plug-and-play module for the neighborhood interaction. Finally, GraphAIR based on well-known models including GCN, GraphSAGE, and GAT have been thoroughly investigated through empirical evaluation. Extensive experiments on benchmark tasks including node classification and link prediction demonstrate the effectiveness of our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 Fig. 1 :</head><label>11</label><figDesc>A graphical illustration of the proposed GraphAIR model. The aggregation module sums up the neighborhood features; the interaction module models the pair-wise feature interaction among the neighborhoods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Overview of node aggregation and interaction in GraphAIR. "gconv" block indicates the general graph convolutional layer, which can be instantiated as GCN<ref type="bibr" target="#b0">[1]</ref>, GAT<ref type="bibr" target="#b1">[2]</ref>, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Detailed illustration of neighborhood aggregation and interaction in GraphAIR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>information from neighborhoods, here we conduct the neighborhood interaction only once by multiplying h agg i andh agg i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>The proposed GraphAIR framework. The "gconv" block indicates the general graph convolutional layer, which can be instantiated as GCN layers, GAT layers, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Accuracy of node classification in the ablation study. same parameters as AIR-GCN. As there are almost double parameters than vanilla GCN in the resulting model, we denote the resulting model as DP-GCN (Double-Parameter GCN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Accuracy of node classification on Cora with varying hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Notations used throughout this paper.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>Activation functions used in representation graph convolutional networks.</figDesc><table><row><cell>Model</cell><cell>Propagation rule</cell><cell>Activation function</cell></row><row><cell>GCN [1]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 :</head><label>3</label><figDesc>Node classification accuracy of representation GCN-based models with different activation functions.</figDesc><table><row><cell cols="5">Model (Activation function) Cora Citeseer Pubmed NELL</cell></row><row><cell>GCN (ReLU)</cell><cell>81.5%</cell><cell>70.3%</cell><cell>79.0%</cell><cell>66.0%</cell></row><row><cell>GCN (sigmoid)</cell><cell>79.4%</cell><cell>51.8%</cell><cell>77.3%</cell><cell>63.2%</cell></row><row><cell>GCN (tanh)</cell><cell>80.6%</cell><cell>71.2%</cell><cell>79.4%</cell><cell>66.4%</cell></row><row><cell>GAT (ELU)</cell><cell>83.0%</cell><cell>72.5%</cell><cell>79.0%</cell><cell>-</cell></row><row><cell>GAT (sigmoid)</cell><cell>36.0%</cell><cell>18.5%</cell><cell>54.4%</cell><cell>-</cell></row><row><cell>GAT (tanh)</cell><cell>82.6%</cell><cell>71.3%</cell><cell>77.5%</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 :</head><label>4</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Cora Citeseer Pubmed</cell><cell>NELL</cell><cell>PPI</cell></row><row><cell>Task</cell><cell></cell><cell></cell><cell cols="2">Transductive</cell><cell>Inductive</cell></row><row><cell>Type</cell><cell></cell><cell cols="2">Citation network</cell><cell cols="2">Knowledge graph Molecular</cell></row><row><cell># Vertices</cell><cell>2,708</cell><cell>3,327</cell><cell>19,717</cell><cell>65,755</cell><cell>56,944</cell></row><row><cell># Edges</cell><cell>5,429</cell><cell>4,732</cell><cell>44,338</cell><cell>266,144</cell><cell>818,716</cell></row><row><cell># Classes</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>210</cell><cell>121</cell></row><row><cell># Features</cell><cell>1,433</cell><cell>3,703</cell><cell>500</cell><cell>5,414</cell><cell>50</cell></row><row><cell># Training nodes</cell><cell>140</cell><cell>120</cell><cell>60</cell><cell>210</cell><cell>44,906</cell></row><row><cell># Test nodes</cell><cell>1,000</cell><cell>1,000</cell><cell>1,000</cell><cell>1,000</cell><cell>5,524</cell></row><row><cell># Validation nodes</cell><cell>500</cell><cell>500</cell><cell>500</cell><cell>500</cell><cell>6,514</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 :</head><label>5</label><figDesc>Accuracy of transductive node classification with the best performance highlighted in bold.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>NELL</cell></row><row><cell>DeepWalk</cell><cell>67.2%</cell><cell>43.2%</cell><cell>65.3%</cell><cell>58.1%</cell></row><row><cell>Planetoid</cell><cell>75.7%</cell><cell>64.7%</cell><cell>77.2%</cell><cell>61.9%</cell></row><row><cell>GIN+0</cell><cell>78.3%</cell><cell>62.9%</cell><cell>78.0%</cell><cell>65.5%</cell></row><row><cell>GIN+</cell><cell>76.6%</cell><cell>63.8%</cell><cell>75.5%</cell><cell>63.5%</cell></row><row><cell>GCN</cell><cell>81.5%</cell><cell>70.3%</cell><cell>79.0%</cell><cell>66.0%</cell></row><row><cell>SGC</cell><cell cols="4">81.0% ± 0.0% 71.9% ± 0.1% 78.9% ± 0.0% 65.4% ± 0.2%</cell></row><row><cell>GAT</cell><cell cols="3">83.0% ± 0.7% 72.5% ± 0.7% 79.0% ± 0.3%</cell><cell>-</cell></row><row><cell>GMNN</cell><cell>83.7%</cell><cell>72.9%</cell><cell>81.8%</cell><cell>68.0%</cell></row><row><cell>DAGNN</cell><cell cols="4">84.4% ± 0.5% 73.3% ± 0.6% 80.5% ± 0.5% 67.5% ± 0.3 %</cell></row><row><cell cols="5">AIR-GCN 84.7% ± 0.1% 72.9% ± 0.1% 80.0% ± 0.1% 68.5% ± 0.2%</cell></row><row><cell cols="4">AIR-GAT 84.5% ± 0.7% 73.5% ± 0.6% 80.0% ± 0.2%</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6 :</head><label>6</label><figDesc>Accuracy of inductive node classification with the best performance highlighted in bold.</figDesc><table><row><cell>Method</cell><cell>PPI</cell></row><row><cell>Random</cell><cell>39.6%</cell></row><row><cell>SAGE-GCN</cell><cell>55.6%</cell></row><row><cell>SAGE-mean</cell><cell>64.5%</cell></row><row><cell>SAGE-LSTM</cell><cell>66.8%</cell></row><row><cell>SAGE-pool</cell><cell>73.8%</cell></row><row><cell>GAT</cell><cell>97.3% ± 0.2%</cell></row><row><cell cols="2">AIR-SAGE-GCN 57.8% ± 0.1%</cell></row><row><cell>AIR-GAT</cell><cell>98.6% ± 0.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7 :95.4% ± 0.01% 95.0% ± 0.01% 99.2% ± 0.02%</head><label>7</label><figDesc>AUC of link prediction in citation networks with the best performance highlighted in bold. 6% ± 0.01% 80.5% ± 0.01% 84.2% ± 0.02% DeepWalk 83.1% ± 0.01% 80.5% ± 0.02% 84.4% ± 0.00% GAE 91.0% ± 0.02% 89.5% ± 0.04% 96.4% ± 0.00% VGAE 91.4% ± 0.01% 90.8% ± 0.02% 94.4% ± 0.02%AIR-GAE</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>SC</cell><cell>84.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. In experiments, we found that the results reported in<ref type="bibr" target="#b4">[5]</ref> after ten epochs did not converge to the best values. For a fair comparison with other models, we reuse its official implementation and report the results of the baselines after 200 epochs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Dual graph convolutional networks for graph-based semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WWW</publisher>
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="5453" to="5462" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Alvey Vision Conference</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="147" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4532" to="4539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Contrastive Learning with Adaptive Augmentation</title>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Graph Contrastive Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Graph Representation Learning and Beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Every document owns its structure: Inductive text classification via graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="334" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Quantum-Inspired Similarity Measure for the Analysis of Complete Weighted Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiview Clustering via Unified and View-Specific Embeddings Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5541" to="5553" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Entropic Dynamic Time Warping Kernels for Co-Evolving Financial Time Series Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Session-based Recommendation with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TAGNN: Target Attentive Graph Neural Networks for Session-based Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1921" to="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Personalized Graph Neural Networks with Attention Mechanism for Session-Aware Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dressing as a whole: Outfit compatibility learning based on node-wise graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="307" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="2224" to="2232" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in KDD</title>
		<imprint>
			<biblScope unit="page" from="1416" to="1424" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning Backtrackless Aligned-Spatial Graph Convolutional Networks for Graph Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?&quot; in ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Patterson</surname></persName>
		</author>
		<title level="m">Artificial Neural Networks: Theory and Applications</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fausett</surname></persName>
		</author>
		<title level="m">Fundamentals of Neural Networks: Architectures, Algorithms, and Applications</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIS-TATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in AAAI</title>
		<imprint>
			<biblScope unit="page" from="3538" to="3545" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ima-geNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Empirical Evaluation of Rectified Activations in Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="6389" to="6399" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-Attention Graph Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Deep Learning Workshop (NIPS 2016)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">GMNN -Graph Markov Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5241" to="5250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Towards Deeper Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv.org</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rojas</surname></persName>
		</author>
		<title level="m">Neural Networks: A Systematic Introduction</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Leveraging Social Media Networks for Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="478" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
