<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fully Convolutional Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook A.I. Research</orgName>
								<address>
									<settlement>Paris, Menlo Park</settlement>
									<region>New York</region>
									<country>France, USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CoML</orgName>
								<orgName type="institution" key="instit2">ENS/CNRS</orgName>
								<orgName type="institution" key="instit3">EHESS</orgName>
								<orgName type="institution" key="instit4">INRIA/PSL Research University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook A.I. Research</orgName>
								<address>
									<settlement>Paris, Menlo Park</settlement>
									<region>New York</region>
									<country>France, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook A.I. Research</orgName>
								<address>
									<settlement>Paris, Menlo Park</settlement>
									<region>New York</region>
									<country>France, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook A.I. Research</orgName>
								<address>
									<settlement>Paris, Menlo Park</settlement>
									<region>New York</region>
									<country>France, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook A.I. Research</orgName>
								<address>
									<settlement>Paris, Menlo Park</settlement>
									<region>New York</region>
									<country>France, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook A.I. Research</orgName>
								<address>
									<settlement>Paris, Menlo Park</settlement>
									<region>New York</region>
									<country>France, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fully Convolutional Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Speech recognition</term>
					<term>end-to-end</term>
					<term>convolutional</term>
					<term>language model</term>
					<term>waveform</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current state-of-the-art speech recognition systems build on recurrent neural networks for acoustic and/or language modeling, and rely on feature extraction pipelines to extract melfilterbanks or cepstral coefficients. In this paper we present an alternative approach based solely on convolutional neural networks, leveraging recent advances in acoustic models from the raw waveform and language modeling. This fully convolutional approach is trained end-to-end to predict characters from the raw waveform, removing the feature extraction step altogether. An external convolutional language model is used to decode words. On Wall Street Journal, our model matches the current state-of-the-art. On Librispeech, we report state-of-the-art performance among end-to-end models, including Deep Speech 2, that was trained with 12 times more acoustic data and significantly more linguistic data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent work on convolutional neural network architectures shows they are competitive with recurrent architectures even on tasks where modeling long-range dependencies is critical, such as language modeling <ref type="bibr" target="#b0">[1]</ref>, machine translation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and speech synthesis <ref type="bibr" target="#b3">[4]</ref>. In end-to-end speech recognition however, recurrent neural networks are still prevalent for acoustic and/or language modeling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>There is a history of using convolutional networks in speech recognition, but only as part of an otherwise more traditional pipeline. They were first introduced as TDNNs to predict phoneme classes <ref type="bibr" target="#b9">[10]</ref>, and later to generate HMM posteriorgrams <ref type="bibr" target="#b10">[11]</ref>. They have recently been used in end-to-end systems, but only in combination with recurrent layers <ref type="bibr" target="#b6">[7]</ref>, or ngram language models <ref type="bibr" target="#b11">[12]</ref>, or for phone recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Convolutional architectures are prevalent when learning from the raw waveform <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>, because they naturally model the computation of standard features such as melfilterbanks. Given the evidence that convolutional networks are also suitable on long-range dependency tasks, we expect them to be competitive at all levels of the speech recognition pipeline.</p><p>In this paper, we present a fully convolutional approach to end-to-end speech recognition. Building on recent advances in convolutional learnable front-ends for speech <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>, convolutional acoustic models <ref type="bibr" target="#b11">[12]</ref>, and convolutional language models <ref type="bibr" target="#b0">[1]</ref>, our model is a deep convolutional network that takes the raw waveform as input and is trained end-to-end to predict letters. Sentences are then predicted using beam-search decoding with a convolutional language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* Equal contribution</head><p>In addition to presenting the first application of convolutional language models to speech recognition, the main contribution of the paper is to show that fully convolutional architectures achieve state-of-the-art performance among end-to-end systems. Thus, our results challenge the prevalence of recurrent architectures for speech recognition, and they parallel the prior results on other application domains that convolutional architectures are on-par with recurrent ones.</p><p>More precisely, we perform experiments on the large vocabulary task of the Wall Street Journal dataset (WSJ) and on the 1000h Librispeech. Our overall pipeline improves the stateof-the-art of end-to-end systems on both datasets. In particular, we decrease by 2% (absolute) the Word Error Rate on the noisy test set of Librispeech compared to DeepSpeech 2 <ref type="bibr" target="#b6">[7]</ref> and the best sequence-to-sequence model <ref type="bibr" target="#b8">[9]</ref>. On clean speech, the improvement is about 0.5% on Librispeech compared to the best end-to-end systems; on WSJ, our results are competitive with the current state-of-the-art, a DNN-HMM system <ref type="bibr" target="#b18">[19]</ref>.</p><p>In particular, the detailed results show that the convolutional language model yields systematic and consistent improvement over a 4-gram language model for its better perplexity and larger receptive field. In addition, we complement the promising results of <ref type="bibr" target="#b17">[18]</ref> regarding the performance of learning the front-end of speech recognition systems: first, we show that learning the front-end yields substantial improvements on noisy speech compared to a mel-filterbanks front-end. Second, we show additional improvements on both WSJ and Librispeech by varying the number of filters in the learnable front-end, leading to a 1.5% absolute decrease in WER on the noisy test set of Librispeech. Our results are the first in which an end-to-end system trained on the raw waveform achieves state-of-the-art performance (among all end-to-end systems) on two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>Our approach, described in this section, is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolutional Front-end</head><p>Several proposals to learn the front-end of speech recognition systems have been made <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>. Following the comparison in <ref type="bibr" target="#b17">[18]</ref>, we consider their best architecture, called "scattering based" (hereafter refered to as learnable front-end). The learnable front-end contains first a convolution of width 2 that emulates the pre-emphasis step used in mel-filterbanks. It is followed by a complex convolution of width 25ms and k filters. After taking the squared absolute value, a low-pass filter of width 25ms and stride 10ms performs decimation. The frontend finally applies a log-compression and a per-channel meanvariance normalization (equivalent to an instance normalization layer <ref type="bibr" target="#b19">[20]</ref>). Following <ref type="bibr" target="#b17">[18]</ref>, the "pre-emphasis" convolution is initialized to [−0.97; 1], and then trained with the rest of the  network. The low-pass filter is kept constant to a squared Hanning window, and the complex convolutional layer is initialized randomly.</p><p>In addition to the k = 40 filters used by <ref type="bibr" target="#b17">[18]</ref>, we experiment with k = 80 filters. Notice that since the stride is the same as for mel-filterbanks, acoustic models on top of the learnable front-ends can also be applied to mel-filterbanks (simply modifying the number of input channels if k = 40).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Convolutional Acoustic Model</head><p>The acoustic model is a convolutional neural network with gated linear units <ref type="bibr" target="#b0">[1]</ref>, which is fed with the output of the learnable front-end. As in <ref type="bibr" target="#b11">[12]</ref>, the networks uses a growing number of channels, and dropout <ref type="bibr" target="#b20">[21]</ref> for regularization. These acoustic models are trained to predict letters directly with the Auto Segmentation Criterion (ASG) <ref type="bibr" target="#b21">[22]</ref>. The ASG criterion is similar to CTC <ref type="bibr" target="#b4">[5]</ref> except that it adds input-independent transition scores between letters. The depth, number of feature maps per layer, receptive field and amount of dropout of models on each dataset are adjusted individually based on the amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Convolutional Language Model</head><p>The convolutional language model (LM) is the GCNN-14B from <ref type="bibr" target="#b0">[1]</ref>, which achieved competitive results on language modeling benchmarks with similar size of vocabulary and training data to the ones used in our experiments. The network contains 14 convolutional residual blocks <ref type="bibr" target="#b22">[23]</ref>, and this deep architecture gives us a large enough receptive field. In each residual block, two 1 × 1 1-D convolutional layers are placed at the beginning and the end serving as bottlenecks for computational efficiency. Gated linear units are used as activation functions.</p><p>We use the language model to score candidate transcriptions in addition to the acoustic model in the beam search decoder described in the next section. Compared to n-gram LMs, convolutional LMs allow the decoder to look at longer context with better perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Beam-search decoder</head><p>We use the beam-search decoder presented in <ref type="bibr" target="#b11">[12]</ref> to generate word sequences given the output from our acoustic model. Given input X to the acoustic model, the decoder finds the word transcription W which maximizes:</p><formula xml:id="formula_0">AM (W |X)+α log P lm (W )+β|W |−γ|{i|πi = sil }|,<label>(1)</label></formula><p>where π is a path representing a valid sequence of letters for W and πi is the i-th letter in this sequence. The score of the acoustic model is computed based on the score of paths of letters (including silences) that are compatible with the output sequence. Denoting by Gasg the corresponding graph, the score of a sequence of words W given by the accoustic model is</p><formula xml:id="formula_1">AM (W ) = logadd π∈Gasg(W ) T t=1 f t π t + gπ t−1 ,π t ,<label>(2)</label></formula><p>where T is the number of output frames to be decoded, f t i is the log-probability of letter i for frame t and and gi,j is the transition score from letter i to letter j, respectively.</p><p>The other hyper-parameters α, β, γ ≥ 0 in (1) control the weight of the language model, the word insertion reward, and the silence insertion penalty. Additional hyper-parameters of the beam search are the beam size and the beam score. The latter is a global threshold on the score of a hypothesis to be included in the beam and is only used for efficiency.</p><p>To make decoding more efficient, at each time step, we merge all hypotheses getting into the same node in Gasg before sorting so as to better utilize the beam. Also, to reduce the number of forward passes through the LM, we cache the scores of unchanged LM states from previous time steps, and only forward batches of new hypotheses generated at current step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We evaluate our approach on the large vocabulary task of the Wall Street Journal (WSJ) dataset <ref type="bibr" target="#b23">[24]</ref>, which contains 80 hours of clean read speech, and Librispeech <ref type="bibr" target="#b24">[25]</ref>, which contains 1000 hours with separate train/dev/test splits for clean and noisy speech. Each dataset comes with official textual data to train language models, which contain 37 million tokens for WSJ, 800 million tokens for Librispeech. Our language models are trained separately for each dataset on the official text data only. These datasets were chosen to study the impact of the different components of our system at different scales of training data and in different recording conditions.</p><p>The models are evaluated in Word Error Rate (WER). Our experiments use the open source codes of wav2letter 1 for the acoustic model, and fairseq 2 for the language model. More details on the experimental setup are given below.</p><p>Baseline Our baseline for each dataset follows <ref type="bibr" target="#b11">[12]</ref>. It uses the same convolutional acoustic model as our approach but a mel-filterbanks front-end and a 4-gram language model. Training/test splits On WSJ, models are trained on si284. nov93dev is used for validation and nov92 for test. On Librispeech, we train on the concatenation of train-clean and trainother. The validation set is dev-clean when testing on testclean, and dev-other when testing on test-other.</p><p>Acoustic modeling The architecture for the convolutional acoustic model is the "high dropout" model from <ref type="bibr" target="#b11">[12]</ref> for Librispeech, which has 19 layers in addition to the front-end (melfilterbanks for the baseline, or the learnable front-end for our approach). On WSJ, we use the lighter version used in <ref type="bibr" target="#b17">[18]</ref>, which has 17 layers. Dropout is applied at each layer after the front-end, following <ref type="bibr" target="#b21">[22]</ref>. The learnable front-end uses 40 or 80 filters.</p><p>The acoustic models are trained following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>, using SGD with a decreasing learning rate, weight normalization and gradient clipping at 0.2 and a momentum of 0.9.</p><p>Language modeling As described in Section 2.3, we use the GCNN-14B model of <ref type="bibr" target="#b0">[1]</ref> with dropout at each convolutional and linear layer on both WSJ and Librispeech. The kernel size of the 1-D convolutional layer in the middle of each residual block is set to 5. In terms of the training vocabulary, we keep all the words (162K) in the WSJ training corpus, while only the most frequent 200K tokens out of 900K are kept for Librispeech. Language models are trained with Nesterov accelerated gradient <ref type="bibr" target="#b25">[26]</ref>. Following <ref type="bibr" target="#b0">[1]</ref>, we also use weight normalization and gradient clipping as regularization.</p><p>Hyperparameter tuning The parameters of the beam search (see Section 2.4) α, β and γ are tuned on the validation set with a beam size of 2500 and a beam score of 26 for computational efficiency. Once α, β, γ are chosen, the test WER together with the optimal number on the dev set are further pushed with a beam size of 3000 and a beam score of 50. <ref type="table" target="#tab_1">Table 1</ref> shows Word Error Rates (WER) on WSJ for the current state-of-the-art and our models. The current best model trained on this dataset is an HMM-based system which uses a combination of convolutional, recurrent and fully connected layers, as well as speaker adaptation, and reaches 3.5% WER on nov92. DeepSpeech 2 shows a WER of 3.6% but uses 150 times more training data for the acoustic model and huge text datasets for LM training. Finally, the state-of-the-art among end-to-end systems trained only on WSJ, and hence the most  comparable to our system, uses lattice-free MMI on augmented data (with speed perturbation) and gets 4.1% WER. Our baseline system, trained on mel-filterbanks, and decoded with a ngram language model has a 5.6% WER. Replacing the n-gram LM by a convolutional one reduces the WER to 4.1%, and puts our model on par with the current best end-to-end system. Replacing the speech features by a learnable front-end finally reduces the WER to 3.7% and then to 3.5% when doubling the number of learnable filters, improving over DeepSpeech 2 and matching the performance of the best HMM-DNN system. <ref type="table" target="#tab_3">Table 2</ref> reports WER on the Librispeech dataset. The CAPIO <ref type="bibr" target="#b27">[28]</ref> ensemble model combines the lattices from 8 individual HMM-DNN systems (using both convolutional and LSTM layers), and is the current state-of-the-art on Librispeech. CAPIO (single) is the best individual system, selected either on devclean or dev-other. The sequence-to-sequence baseline is an encoder-decoder with attention and a BPE-level <ref type="bibr" target="#b28">[29]</ref> LM, and currently the best end-to-end system on this dataset. We can observe that our fully convolutional model improves over CAPIO (Single) on the clean part, and is the current best end-to-end system on test-other with an improvement of 2.3% absolute. Our system also outperforms DeepSpeech 2 on both test sets by a significant margin. An interesting observation is the impact of each convolutional block. While replacing the 4-gram LM by a convolutional LM improves similarly on the clean and noisier parts, learning the speech front-end gives similar performance on the clean part but significantly improves the performance on noisier, harder utterances, a finding that is consistent with previous literature <ref type="bibr" target="#b15">[16]</ref>. Moreover, doubling the number of learnable filters improves the performance of our system on all development and test sets, which is consistent with our results on WSJ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Word Error Rate results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Wall Street Journal dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Librispeech dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of the convolutional language model</head><p>Since this paper uses convolutional language models for speech recognition systems for the first time, we present additional studies of the language model in isolation. These experiments use our best language model on Librispeech, and evaluations in WER are carried out using the baseline system trained on mel-filterbanks. The decoder parameters are tuned using the grid search described in Section 3. To save computational resources, in these experiments, we use a slightly suboptimal beam size/beam score fixed to 2500/30 respectively.   Perplexity and WER <ref type="figure" target="#fig_1">Figure 2</ref> shows the correlation between perplexity and WER as the training progresses. As perplexity decreases, the WER on both dev-clean and dev-other also decreases following the same trend. It illustrates that perplexity on the linguistic data is a good surrogate of the final performance of the speech recognition pipeline. Architectural choices or hyper-parameter tuning can thus be carried out mostly using perplexity alone.</p><p>Context and WER <ref type="table" target="#tab_5">Table 3</ref> reports WER obtained for different context sizes used in the LM. Looking at contexts ranging from 3 (comparable to the n-gram baseline) to 50 for our best language model, the WER decreases monotonically until a context size of about 20, and then almost stays still. We observe that the convolutional LM already improves on the n-gram model even with the same context size. Increasing the context gives a significant boost in performance, with the major gains obtained between a context of 3 to 9 (−1.9% absolute WER).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of the learnt front-ends</head><p>In order to understand qualitatively the advantages of a learnable front-end over mel-filterbanks, we compare the frequency scale learned by our systems trained on the waveform. To do so, we compute a power spectrum of each convolutional filter of the first layer. We then define as its center frequency, the frequency at which its power spectrum is maximal. <ref type="figure">Figure 3</ref> compares the mel-scale to the scales learnt by the front-ends of our best models on WSJ and Librispeech. quency, downsampling by a factor of 2 for the front-ends with 80 filters. We observe that all models converge to similar scales, regardless of the dataset they are trained on, or how many filters they have. These learnt scales show a logarithmic shape similar to a mel-scale, but they are significantly more biased towards the lower frequencies. Moreover, these learnt scales closely relate to those learned by previously proposed gammatone-based learnable front-ends (see <ref type="figure" target="#fig_1">Figure 2</ref> of <ref type="bibr" target="#b15">[16]</ref> and <ref type="figure">Figure 3</ref> of <ref type="bibr" target="#b16">[17]</ref>). Still, analyzing a learnable filter only by its center frequency does not inform on other important characteristics such as its localization in frequency. <ref type="figure">Figure 4</ref> shows a heatmap of the power of the mel filters along the frequency axis, and of the 40 filters learned on Librispeech, at convergence. We observe that even though the learnt filters show some artifacts in high frequencies, they are mostly localized around their center frequency, similarly to mel-filters.</p><p>Overall, the consistency of the scales learned by various front-ends, over several studies, and on many datasets, suggests strongly that the mel-scale is suboptimal for automatic speech recognition and that learnable front-ends are able to learn a more appropriate one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced the first fully convolutional pipeline for speech recognition, that can directly process the raw waveform and shows state-of-the art performance on Wall Street Journal and on Librispeech among end-to-end systems. This first attempt at exploiting convolutional language models in speech recognition improves significantly over a 4-gram language model on both datasets. Replacing mel-filterbanks by a learnable front-end gives additional gains in performance, that appear to be more prevalent on noisy data. This suggests learning the front-end is a promising avenue for speech recognition with challenging recording conditions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the fully convolutional architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Evolution of WER (%) on Librispeech with the perplexity of the language model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>The figure is obtained by sorting the filters according to their center fre-WSJ/40 FILTERS) LEARNT SCALE (LIBRI/40 FILTERS) LEARNT SCALE (WSJ/80 FILTERS) LEARNT SCALE (LIBRI/80 FILTERS) Center frequency of the front-end filters, for the melfilterbank baseline and the learnable front-ends. Power heatmap of the 40 mel-filters (left) and of the frequency response of the 40 convolutional filters learned from the raw waveform on Librispeech (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>WER (%) on the open vocabulary task of WSJ.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">dev93 nov92</cell></row><row><cell>E2E Lattice-free MMI [27]</cell><cell></cell><cell>-</cell><cell>4.1</cell></row><row><cell>(data augmentation)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN-DNN-BLSTM-HMM [19]</cell><cell></cell><cell>6.6</cell><cell>3.5</cell></row><row><cell cols="2">(speaker adaptation, 3k acoustic states)</cell><cell></cell><cell></cell></row><row><cell>DeepSpeech 2 [7]</cell><cell></cell><cell>5</cell><cell>3.6</cell></row><row><cell cols="2">(12k training hours AM, common crawl LM)</cell><cell></cell><cell></cell></row><row><cell>Front-end</cell><cell>LM</cell><cell></cell><cell></cell></row><row><cell>Mel-filterbanks</cell><cell>4-gram</cell><cell>9.5</cell><cell>5.6</cell></row><row><cell>Mel-filterbanks</cell><cell>ConvLM</cell><cell>7.5</cell><cell>4.1</cell></row><row><cell cols="2">Learnable front-end (40 filters) ConvLM</cell><cell>6.9</cell><cell>3.7</cell></row><row><cell cols="2">Learnable front-end (80 filters) ConvLM</cell><cell>6.8</cell><cell>3.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>WER (%) on Librispeech.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evolution of WER (%) on Librispeech with the context size of the language model.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/wav2letter 2 https://github.com/facebookresearch/fairseq</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocký</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Recurrent neural network based language model,&quot; in INTER-SPEECH</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Listen, attend and spell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1508.01211</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03294</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1533" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Letterbased speech recognition with gated convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno>abs/1712.09444</idno>
		<ptr target="http://arxiv.org/abs/1712.09444" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards end-to-end speech recognition with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning filterbanks from raw speech for phone recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5509" to="5513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks-based continuous speech recognition using raw speech signal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Palaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Doss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4295" to="4299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech acoustic modeling from raw multichannel waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning the speech front-end with raw waveform cldnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition from the raw waveform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep recurrent neural networks for acoustic modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01482</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Instance normalization: the missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Wav2letter: an endto-end convnet-based speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The design for the wall street journalbased csr corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Speech and Natural Language</title>
		<meeting>the workshop on Speech and Natural Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">End-to-end speech recognition using lattice-free mmi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sameti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The capio 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandrashekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
