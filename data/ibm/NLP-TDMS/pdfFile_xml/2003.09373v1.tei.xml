<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SER-FIQ: Unsupervised Estimation of Face Image Quality Based on Stochastic Embedding Robustness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Terhörst</surname></persName>
							<email>philipp.terhoerst@igd.fraunhofer.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niklas Kolf</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Darmstadt</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
							<email>naser.damer@igd.fraunhofer.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kirchbuchner</surname></persName>
							<email>florian.kirchbuchner@igd.fraunhofer.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
							<email>arjan.kuijper@igd.fraunhofer.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Institute for Computer Graphics Research IGD</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SER-FIQ: Unsupervised Estimation of Face Image Quality Based on Stochastic Embedding Robustness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Visualization of the proposed unsupervised face quality assessment concept. We propose using the robustness of an image representation as a quality clue. Our approach defines this robustness based on the embedding variations of random subnetworks of a given face recognition model. An image that produces small variations in the stochastic embeddings (bottom left), demonstrates high robustness (red areas on the right) and thus, high image quality. Contrary, an image that produces high variations in the stochastic embeddings (top left) coming from random subnetworks, indicates a low robustness (blue areas on the right). Therefore, it is considered as low quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Face image quality is an important factor to enable highperformance face recognition systems. Face quality assessment aims at estimating the suitability of a face image for recognition. Previous work proposed supervised solutions that require artificially or human labelled quality values. However, both labelling mechanisms are error-prone as they do not rely on a clear definition of quality and may not know the best characteristics for the utilized face recognition system. Avoiding the use of inaccurate quality labels, we proposed a novel concept to measure face quality based on an arbitrary face recognition model. By determining the embedding variations generated from random subnetworks of a face model, the robustness of a sample representation and thus, its quality is estimated. The experiments are conducted in a cross-database evaluation setting on three publicly available databases. We compare our proposed solution on two face embeddings against six state-of-the-art ap-proaches from academia and industry. The results show that our unsupervised solution outperforms all other approaches in the majority of the investigated scenarios. In contrast to previous works, the proposed solution shows a stable performance over all scenarios. Utilizing the deployed face recognition model for our face quality assessment methodology avoids the training phase completely and further outperforms all baseline approaches by a large margin. Our solution can be easily integrated into current face recognition systems and can be modified to other tasks beyond face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Face images are one of the most utilized biometric modalities <ref type="bibr" target="#b40">[41]</ref> due to its high level of public acceptance and since it does not require an active user-participation <ref type="bibr" target="#b38">[39]</ref>. Under controlled conditions, current face recognition systems are able to achieve highly accurate performances <ref type="bibr" target="#b13">[14]</ref>. However, some of the most relevant face recognition systems work under unconstrained environments and thus, have to deal with large variabilities that leads to significant degradation of the recognition accuracies <ref type="bibr" target="#b13">[14]</ref>. These variabilities include image acquisition conditions (such as illumination, background, blurriness, and low resolution), factors of the face (such as pose, occlusions and expressions) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref> and biases of the deployed face recognition system. Since these variabilities lead to significantly degraded recognition performances, the ability to deal with these factors needs to be addressed <ref type="bibr" target="#b18">[19]</ref>.</p><p>The performance of biometric recognition is driven by the quality of its samples <ref type="bibr" target="#b3">[4]</ref>. Biometric sample quality is defined as the utility of a sample for the purpose of recognition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b3">4]</ref>. The automatic prediction of face quality (prior to matching) is beneficial for many applications. It leads to a more robust enrolment for face recognition systems. In negative identification systems, it prevents an attacker from getting access to a system by providing a low quality face image. Furthermore, it enables quality-based fusion approaches when multiple images <ref type="bibr" target="#b5">[6]</ref> (e.g. from surveillance videos) or multiple biometric modalities are given.</p><p>Current solutions for face quality assessment require training data with quality labels coming from human perception or are derived from comparison scores. Such a quality measure is generally poorly defined. Humans may not know the best characteristics for the utilized face recognition system. On the other hand, automatic labelling based on comparison scores represents the relative performance of two samples and thus, one low-quality sample might negatively affect the quality labels of the other one.</p><p>In this work, we propose a novel unsupervised face quality assessment concept by investigating the robustness of stochastic embeddings. Our solution measures the quality of an image based on its robustness in the embedding space. Using the variations of embeddings extracted from random subnetworks of the utilized face recognition model, the representation robustness of the sample and thus, its quality is determined. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the working principle.</p><p>We evaluated the experiments on three publicly available databases in a cross-database evaluation setting. The comparison of our approach was done on two face recognition systems against six state-of-the-art solutions: three no-reference image quality metrics, two recent face quality assessment algorithms from previous work, and one commercial off-the-shelf (COTS) face quality assessment product from industry.</p><p>The results show that the proposed solution is able to outperform all state-of-the-art solutions in most investigated scenarios. While every baseline approach shows performance instabilities in at least two scenarios, our solution shows a consistently stable performance. When using the deployed face recognition model for the proposed face quality assessment methodology, our approach outperforms all baseline by a large margin. Contrarily to previous definitions of face quality assessment <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19]</ref> that states the face quality as a utility measure of a face image for an arbitrary face recognition model, our results show that it is highly beneficial to estimate the sample quality with regard to a specific (the deployed) face recognition model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Several standards have been proposed for insure face image quality by constraining the capture requirements, such as ISO/IEC 19794-5 <ref type="bibr" target="#b22">[23]</ref> and ICAO 9303 <ref type="bibr" target="#b21">[22]</ref>. In these standards, quality is divided into image-based qualities (such as pose, expression, illumination, occlusion) and subject-based quality measures (such as accessories). These mentioned standards influenced many face quality assessment approaches that have been proposed in the recent years. While the first solutions to face quality assessment focused on analytic image quality factors, current solutions make use of the advances in supervised learning.</p><p>Approaches based on analytic image quality factors define quality metrics for facial asymmetries <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10]</ref>, propose vertical edge density as a quality metric to capture pose variations <ref type="bibr" target="#b41">[42]</ref>, or measured in terms of luminance distortion in comparison to a known reference image <ref type="bibr" target="#b34">[35]</ref>. However, these approaches have to consider every possible factor manually, and since humans may not know the best characteristics for face recognition systems, more current research focus on learning-based approaches.</p><p>The transition to learning-based approaches include works that combine different analytical quality metrics with traditional machine learning approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>End-to-end learning approaches for face quality assessment were first presented in 2011. Aggarwal et al. <ref type="bibr" target="#b2">[3]</ref> proposed an approach for predicting the face recognition performance using a multi-dimensional scaling approach to map space characterization features to genuine scores. In <ref type="bibr" target="#b42">[43]</ref>, a patch-based probabilistic image quality approach was designed that works on 2D discrete cosine transform features and trains a Gaussian model on each patch. In 2015, a rank-based learning approach was proposed by Chen et al. <ref type="bibr" target="#b4">[5]</ref>. They define a linear quality assessment function with polynomial kernels and train weights based on a ranking loss. In <ref type="bibr" target="#b26">[27]</ref>, face images assessment was performed based on objective and relative face image qualities. While the objective quality metric refers to objective visual quality in terms of pose, alignment, blurriness, and brightness, the relative quality metric represents the degree of mismatch between training face images and a test face image. Best-Rowden and Jain <ref type="bibr" target="#b3">[4]</ref> proposed an automatic face quality prediction approach in 2018. They proposed two methods for quality assessment of face images based on (a) human assessments of face image quality and (b) quality values from similarity scores. Their approach is based on support vector machines applied to deeply learned representations. In 2019, Hernandez-Ortega et al. proposed Face-Qnet <ref type="bibr" target="#b18">[19]</ref>. This solution fine-tunes a face recognition neural network to predict face qualities in a regression task. Beside image quality estimation for face recognition, quality estimation has been also developed to predict soft-biometric decision reliability based on the investigated image <ref type="bibr" target="#b37">[38]</ref>.</p><p>All previous face image quality assessment solutions require training data with artificial or manually labelled quality values. Human labelled data might transfer human bias into the quality predictions and does not take into account the potential biases of the biometric system. Moreover, humans might not know the best quality factors for a specific face recognition system. Artificially labelled quality values are created by investigating the relative performance of a face recognition system (represented by comparison scores). Consequently, the score might be heavily biased by low-quality samples.</p><p>The solution presented in this paper is based on our hypothesis that representation robustness is better suited as a quality metric, since it provides a measure for the quality of a single sample independently of others and avoids the use of misleading quality labels for training. This metric can intrinsically capture image acquisition conditions and factors of the face that are relevant for the used face recognition system. Furthermore, it is not affected by human bias, but takes into account the bias and the decision patterns of the used face embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our approach</head><p>Face quality assessment aims at estimating the suitability of a face image for face recognition. The quality of a face image should indicate its expected recognition performance. In this work, we based our face image quality definition on the relative robustness of deeply learned embeddings of that image. Calculating the variations of embeddings coming from random subnetworks of a face recognition model, our solution defines the magnitude of these variations as a robustness measure, and thus, image quality. An illustration of this methodology is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sample-quality estimation</head><p>More formally, our proposed solution predicts the face quality Q(I) of a given face image I using a face recognition model M. The face recognition model have to be trained with dropout and aims at extracting embeddings that are well identity-separated. To make a robustness-based quality estimation of I, m = 100 stochastic embeddings are generated from the model M using stochastic forward passes with different dropout patterns. The choice for m is defined by the trade-off between time complexity and sta- bility of the quality measure as described in Section 3.2. Each stochastic forward pass applies a different dropout pattern (during prediction) producing a different subnetwork of M. Each of these subnetworks generates different stochastic face embeddings x s . These stochastic embeddings are collected in a set X(I) = {x s } s∈{1,2,...,m} . We define the face quality</p><formula xml:id="formula_0">q(X(I)) = 2 σ − 2 m 2 i&lt;j d(x i , x j ) ,<label>(1)</label></formula><p>of image I as the sigmoid of the negative mean euclidean distance d(x i , x j ) between all stochastic embeddings pairs (x i , x j ) ∈ X × X. The sigmoid function σ(·) ensures that q ∈ [0, 1]. Since Gal et al. <ref type="bibr" target="#b11">[12]</ref> proofed that applying dropout repetitively on a network approximates the uncertainty of a Gaussian process <ref type="bibr" target="#b32">[33]</ref>, the euclidean distance is a suitable choice for d(x i , x j ). A greater variation in the stochastic embedding set X indicate a low robustness of the representation and thus, a lower sample quality q. Lower variations in X indicate high robustness in the embedding space and is considered as a high sample quality q. The quality prediction strategy is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Properties</head><p>The aim of SER-FIQ is to estimate the face image quality from the perspective of utilisation in recognition tasks, which might be different than estimating the notion of image quality. An image that produces relatively stable identity-related embeddings despite various variations (here caused by dropout) is an image with high utilisation in a recognition task, given that the recognition network training aims at being robust against intra-identity variations. X ← empty list <ref type="bibr">3:</ref> for i ← 1, . . . , m do 4:</p><formula xml:id="formula_1">x i ← M.pred(I, dropout = T rue) 5: X = X.add(x i ) 6: Q ← q(X) 7:</formula><p>return Q Face recognition algorithms are trained with the aim of learning robust representations to increase inter-identity separability and decrease intra-identity separability. Assuming that a face recognition network is trained with dropout and the quality of a sample correlates with its embedding robustness, different subnetworks can be created from the basic model so that they possess different dropout patterns. The agreement between the subnetworks can be used to estimate the embedding robustness, and thus the quality. If the m subnetworks produce similar outputs (high agreement), the variations over these random subnetworks (the stochastic embedding set X) are low. Consequently, the robustness of this embedding, and thus the quality of the sample, is high. Conversely, if the m subnetworks produce dissimilar representations (low agreement), the variations over the random subnetworks are high. Therefore, the robustness in the embedding space is low and the quality of the sample can be considered low as well.</p><p>Our approach has only one parameter m, the number of stochastic forward passes. This parameter can be interpreted as the number of steps in a Monte-Carlo simulation and controls the stability of the quality predictions. A higher m leads to more stable quality estimates. Since the computational time t = O(m 2 ) of our method grows quadratically with m, it should not be chosen too high. However, our method can compensate for this issue and can easily run in real-time, since it is highly parallelizable and the computational effort can be greatly reduced by repeating the stochastic forward passes only through the last layer(s) of the network.</p><p>In contrast to previous work, our solution does not require quality labels for training. Furthermore, if the deployed face recognition system was trained with dropout, the same network can be used for determining the embedding robustness and therefore, the sample quality. By doing so the training phase can be completely avoided and the quality predictions further captures the decision patterns and bias of the utilized face recognition model. Therefore, we highly recommend utilizing the deployed face recognition model for the quality assessment task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup</head><p>Databases The face quality assessment experiments were conducted on three publicly available databases chosen to have variation in quality and to prove the generalization of our approach on multiple databases. The ColorFeret database <ref type="bibr" target="#b31">[32]</ref> consists of 14,126 high-resolution face images from 1,199 different individuals. The data possess a variety of face poses and facial expressions under wellcontrolled conditions. The Adience dataset <ref type="bibr" target="#b8">[9]</ref> consists of 26,580 images from over 2,284 different subjects under unconstrained imaging conditions. Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b20">[21]</ref> contains 13,233 face images from 5749 identities. For both datasets, large variations in illumination, location, focus, blurriness, pose, and occlusion are included.</p><p>Evaluation metrics To evaluate the face quality assessment performance, we follow the methodology by Grother et al. <ref type="bibr" target="#b15">[16]</ref> using error versus reject curves. These curves show a verification error-rate over the fraction of unconsidered face images. Based on the predicted quality values, these unconsidered images are these with the lowest predicted quality and the error rate is calculated on the remaining images. Error versus reject curves indicates good quality estimation when the verification error decreases consistently when increasing the ratio of unconsidered images. In contrast to error versus quality-threshold curves, this process allows to fairly compare different algorithms for face quality assessment, since it is independent of the range of quality predictions. The cruve was adapted in the approved ISO working item <ref type="bibr" target="#b24">[25]</ref> and used in the literature <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>The face verification error rates within the error versus reject curves are reported in terms of false non-match rate (FNMR) at fixed false match rate (FMR) and as equal error rate (EER). The EER equals the FMR at the threshold where FMR = 1−FNMR and is well known as a single-value indicator of the verification performance. These error rates are specified for biometric verification evaluation in the international standard <ref type="bibr" target="#b23">[24]</ref>. In our experiment, we report the face verification performance on three operating points to cover a wider range of potential applications. The face recognition performance is reported in terms of EER and FNMR at a FMR threshold of 0.01. The FNMR is also reported at 0.001 FMR threshold as recommended by the best practice guidelines for automated border control of Frontex <ref type="bibr" target="#b10">[11]</ref>.</p><p>Face recognition networks To get face embedding from a given face image, the image is aligned, scaled, and cropped. The preprocessed image is passed to a face recognition models to extract the embeddings. In this work, we use two face recognition models, FaceNet <ref type="bibr" target="#b33">[34]</ref> and Arc-Face <ref type="bibr" target="#b6">[7]</ref>. For FaceNet, the image is aligned, scaled, and cropped as described in <ref type="bibr" target="#b25">[26]</ref>. To extract the embeddings, a pretrained model 1 was used. For ArcFace, the image preprocessing was done as described in <ref type="bibr" target="#b16">[17]</ref> and a pretrained model 2 provided by the authors of ArcFace is used. Both models were trained on the MS1M database <ref type="bibr" target="#b17">[18]</ref>. The output size is 128 for FaceNet and 512 for ArcFace. The identity verification is performed by comparing two embeddings using cosine-similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On-top model preparation</head><p>To apply our quality assessment methodology, a recognition model that was trained with dropout <ref type="bibr" target="#b35">[36]</ref> is needed. Otherwise, a model containing dropout need to added on the top of the existing model. The direct way to apply our approach is to take a pretrained recognition model and repeat the stochastic forward passes only in the last layer(s) during prediction. This is even expected to reach a better performance than training a custom network, because the verification decision, as well as the quality estimation decision, is done in a shared embedding space.</p><p>To demonstrate that our solution can be applied to any arbitrary face recognition system, in our experiments we show both approaches: (a) training a small custom network on top of the deployed face recognition system, which we will refer to as SER-FIQ (on-top model), and (b) using the deployed model for the quality assessment, which we will refer to as SER-FIQ (same model).</p><p>The structure of SER-FIQ (on-top model) was optimized such that its produced embeddings achieve a similar EER on ColorFeret as that of the FaceNet embeddings. It consist of five layers with n emb /128/512/n emb /n ids dimensions. The two intermediate layers have 128 and 512 dimensions. The last layer has the dimension equal to the number of training identities n ids and is only needed during training. All layers contain dropout <ref type="bibr" target="#b35">[36]</ref> with the recommended dropout probability p d = 0.5 and a tanh activation. The training of the small custom network is done using the AdaDelta optimizer <ref type="bibr" target="#b43">[44]</ref> with a batchsize of 1024 over 100 epochs. Since the size of the in-and output layers (blue and green) of the networks differs dependent on the used face embeddings, a learning rate of α F N = 10 −1 was chosen for FaceNet and α AF = 10 −4 for the higher dimensional Ar-cFace embeddings. As the loss function, we used a simple binary cross-entropy loss on the classification of the training identities.</p><p>Investigations To investigate the generalization of face quality assessment performance, we conduct the experiments in a cross-database setting. The training is done on ColorFeret to make the models learn variations in a controlled environment. The testing is done on two unconstrained datasets, Adience and LFW. The embeddings used for the experiments are from the widely used FaceNet (2015) and recently published ArcFace (2019) models.</p><p>To put the experiments in a meaningful setting, we evaluated our approach in comparison to six baseline solutions. Three of these baselines are well-known no-reference image quality metrics from the computer vision community: Brisque <ref type="bibr" target="#b27">[28]</ref>, Niqe <ref type="bibr" target="#b28">[29]</ref>, Piqe <ref type="bibr" target="#b39">[40]</ref>. The other three baselines are state-of-the-art face quality assessment approaches from academia and industry. COTS <ref type="bibr" target="#b29">[30]</ref> is an off the shelf industry product from Neurotechnology. We further compare our method with the two recent approaches from academia: the face quality assessment approach presented by Best-Rowden and Jain <ref type="bibr" target="#b3">[4]</ref> (2018) and FaceQnet <ref type="bibr" target="#b18">[19]</ref> (2019). Training the solution presented by Best-Rowden was done on ColorFeret following the procedure described in <ref type="bibr" target="#b3">[4]</ref>. The generated labels come from cosine similarity scores using the same embeddings as in the evaluation scenario. For all other baselines, pretrained models are utilized.</p><p>Our proposed methodology is presented in two settings, the SER-FIQ (on-top model) and the SER-FIQ (same model). SER-FIQ (on-top model) demonstrates that our unsupervised method can be applied to any face recognition system. SER-FIQ (same model) make use of the deployed face recognition model for quality assessment, to show the effect of capture its decision patterns for face quality assessment. In the latter case, we apply the stochastic forward passes only between the last two layers of the deployed face recognition network.  <ref type="figure">Figure 3</ref>: Face quality distributions of the used databases: Adience, LFW, and ColorFeret. The quality predictions were done using the pretrained models FaceQnet <ref type="bibr" target="#b18">[19]</ref>, COTS <ref type="bibr" target="#b29">[30]</ref>, and the proposed SER-FIQ (same model) based on FaceNet and ArcFace.</p><p>Database face quality rating To justify the choices of the used databases, <ref type="figure">Figure 3</ref> shows the face quality distributions of the databases using quality estimates from four pretrained face quality assessment models. ColorFeret was captured under well-controlled conditions and generally shows very high qualities. However, it contains non-frontal head poses and for COTS and SER-FIQ (on FaceNet) <ref type="figure">(Figure 3a)</ref> this is considered as low image quality. Because of these controlled variations, we choose ColorFeret as the training database. Adience and LFW are unconstrained databases and for all quality measures, most face images are far away from perfect quality conditions. For this reason, we choose these databases for testing. The experiments are evaluated at three different operation points to investigate the face quality assessment performance over a wider spectrum of potential applications. Following the best practice guidelines for automated border control of the European Border and Coast Guard Agency Frontex <ref type="bibr" target="#b10">[11]</ref>, <ref type="figure" target="#fig_3">Figure 4</ref> shows the face quality assessment performance at a FMR of 0.001. <ref type="figure" target="#fig_6">Figure 6</ref> presents the same at a FMR of 0.01 and <ref type="figure">Figure 7</ref> shows the face quality assessment performance at the widely-used EER. Moreover Since the statements about each tested face quality assessment approach are very similar over all experiments, we will make a discussion over each approach separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>No-reference image quality approaches To understand the importance of different image quality measures for the task of face quality assessment, we evaluated three noreference quality metrics Brisque <ref type="bibr" target="#b27">[28]</ref>, Niqe <ref type="bibr" target="#b28">[29]</ref>, Piqe <ref type="bibr" target="#b39">[40]</ref> (all represented as dotted lines). While in some evaluation scenarios the verification error decrease when the proportion of neglected images (low quality) is increased, in most cases they lead to an increased verification error. This demonstrates that image quality alone is not suitable for generalized face quality estimation. Factors of the face (such as pose, occlusions, and expressions) and model biases are not covered by these algorithms and might play an important role for face quality assessment.</p><p>Best-Rowden The proposed approach from Best-Rowden and Jain <ref type="bibr" target="#b3">[4]</ref> works well in most scenarios and reaches a top-rank performance in some minor cases (e.g. LFW with FaceNet features). However, it shows instabilities that can lead to highly wrong quality predictions. This can be observed well on the Adience dataset using FaceNet embed-  <ref type="figure" target="#fig_6">Figure 6a and 6b</ref> show the results for FaceNet and ArcFace embeddings on Adience. <ref type="figure" target="#fig_6">Figure 6c and 6d</ref> show the same on LFW.</p><p>dings, see <ref type="figure" target="#fig_3">Figure 4a</ref> and 6a. These mispredictions might be explained by the ColorFeret training data that does not contain all important quality factors for a given face embedding. On the other hand, these quality factors are generally unknown and thus, training data should never be considered to be covering all factors.</p><p>FaceQnet FaceQnet <ref type="bibr" target="#b18">[19]</ref>, proposed by Hernandez-Ortega et al., shows a suitable face quality assessment behaviour in most cases. In comparison with other face quality assessment approaches, it only shows a mediocre performance. Although FaceQnet was trained on labels coming from the same FaceNet embeddings as in our evaluation setting, it often fails in predicting well-suited quality labels on these embeddings, e.g. in <ref type="figure" target="#fig_3">Figure 4c</ref> on LFW. Also on Adience (e.g. <ref type="figure" target="#fig_6">Figure 6a and 7a</ref>), the performance plot shows a Ushape that demonstrates that the algorithm can not distinguish well between medium and higher quality face images. Since the method is trained on the same features, these FaceNet-related instabilities might result from overfitting.</p><p>COTS The industry baseline COTS <ref type="bibr" target="#b29">[30]</ref> from Neurotechnology generally shows a good face quality assessment when the used face recognition system is based on FaceNet features. Specifically on LFW (see <ref type="figure" target="#fig_3">Figure 4c</ref>, 6c, and 7c) a small U-shape can be observed similar to FaceQnet. While it shows a good performance using FaceNet embeddings, the face quality predictions using the more recent ArcFace embeddings are of no significance (see <ref type="figure" target="#fig_3">Figure 4b, 4d, 6b, 6d, 7b, and 7d</ref>). Here, rejecting face images with low predicted face quality does not improve the face recognition performance. Since no information about the inner workflow is given, it can be assumed that their method is optimized to more traditional face embeddings, such as FaceNet. More recent embeddings, such as Arc-Face, are probably intrinsically robust to the quality factors that COTS is trained on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SER-FIQ (on-top model)</head><p>On the contrary to the discussed supervised methods, our proposed unsupervised solution that builds on training a small custom face recognition network shows a stable performance in all investigated scenarios <ref type="figure" target="#fig_3">(Figure 4, 6, and 7)</ref>. Furthermore, our solution is always close to the top performance and outperforms all baseline approaches in the majority of the scenarios, e.g. in <ref type="figure" target="#fig_3">Figure 4a</ref>, 4d, 6a, 6b, 6d, 7a, 7b, and 7d. Our method proved to be particularly effective in combination with recent ArcFace embeddings (see <ref type="figure" target="#fig_6">Figures 6b, 6d, 7b, and 7d)</ref>. The unsupervised nature of our solution seems to be a more accurate and more stable strategy.  <ref type="figure">Figure 7</ref>: The face verification performance given as EER for the predicted face quality values. The curves show the effectiveness of rejecting low-quality face images in terms of EER. <ref type="figure">Figure 7a</ref> and 7b show the results for FaceNet and ArcFace embeddings on Adience. <ref type="figure">Figure 7c</ref> and 7d show the some on LFW.</p><p>SER-FIQ (same model) Our method that avoids training by utilizing the deployed face recognition systems is build on the hypotheses that face quality assessment should aim at estimating the sample quality of a specific face recognition model. This way it adapts to the models' decision patterns and can predict the suitability of face sample more accurately. The effect of this adaptation can be seen clearly in nearly all evaluated cases (see <ref type="figure" target="#fig_3">Figure 4</ref>, 6, and 7). It outperforms all baseline approaches by a large margin and demonstrates an even stronger performance at small FMR (see <ref type="figure" target="#fig_3">Figures 4a, 4b, 4c</ref>, and 4d at the Frontex recommended FMR of 0.001). This demonstrates the benefit of focusing on the face quality assessment to a specific (the deployed) face recognition model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Face quality assessment aims at predicting the suitability of face images for face recognition systems. Previous works provided supervised models for this task based on inaccurate quality labels with only limited consideration of the decision patterns of the deployed face recognition system. In this work, we solved these two gaps by proposing a novel unsupervised face quality assessment methodology that is based on a face recognition model trained with dropout. Measuring the embeddings variations generated from ran-dom subnetworks of the face recognition model, the representation robustness of a sample and thus, the sample's quality is determined. To evaluate a generalized face quality assessment performance, the experiments were conducted using three publicly available databases in a cross-database evaluation setting. We compared our solution on two different face embeddings against six state-of-the-art approaches from academia and industry. The results showed that our proposed approach outperformed all other approaches in the majority of the investigated scenarios. It was the only solution that showed a consistently stable performance. By using the deployed face recognition model for verification and the proposed quality assessment methodology, we avoided the training phase completely and further outperformed all baseline approaches by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the proposed methodology: an input I is forwarded to different random subnetworks of the used face recognition model M. Each subnetwork produces a different stochastic embedding xs. The variations between these embeddings are calculated using pairwise-distances and define the quality of I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Stochastic Embedding Robustness (SER) Input: preprocessed input image I, NN-model M Output: quality value Q for input image I 1: procedure SER(I, M, m = 100) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) COTS (b) FaceQnet (c) SER-FIQ (on FaceNet) (d) SER-FIQ (on ArcFace)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Face verification performance for the predicted face quality values. The curves show the effectiveness of rejecting low-quality face images in terms of FNMR at a threshold of 0.001 FMR.Figure 4a and 4bshow the results for FaceNet and ArcFace embeddings on Adience.Figure 4c and 4dshow the same on LFW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Sample face images from Adience with the corresponding quality predictions from four face quality assessment methods. SER-FIQ refers to our same model approach based on ArcFace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, Figure 5 shows sample images with their corresponding qual-ity predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Face verification performance for the predicted face quality values. The curves show the effectiveness of rejecting low-quality face images in terms of FNMR at a threshold of 0.01 FMR.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/davidsandberg/facenet 2 https://github.com/deepinsight/insightface</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quality metrics for practical face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bourlai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)</title>
		<meeting>the 21st International Conference on Pattern Recognition (ICPR2012)</meeting>
		<imprint>
			<date type="published" when="2012-11" />
			<biblScope unit="page" from="3103" to="3107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Design and evaluation of photometric image quality measures for effective face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bourlai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Biometrics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="314" to="324" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting performance of face recognition systems: An image characterization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soma</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR Workshops</title>
		<meeting><address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="20" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning face image quality from human assessments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3064" to="3077" />
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face image quality assessment based on learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="94" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face and Facial Expression Recognition from Real World Videos -International Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timotheos</forename><surname>Samartzidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Nouak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>Qiang Ji, Thomas B. Moeslund, Gang Hua, and Kamal Nasrollahi</editor>
		<imprint>
			<biblScope unit="volume">8912</biblScope>
			<biblScope unit="page" from="85" to="98" />
			<date type="published" when="2014-08-24" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Personalized face reference from video: Key-face selection and feature-level fusion. Revised Selected Papers</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A bayesian model for predicting face recognition performance using image quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luuk</forename><forename type="middle">J</forename><surname>Veldhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spreeuwers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Biometrics</title>
		<meeting><address><addrLine>Clearwater, IJCB 2014, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-10-02" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2170" to="2179" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face image conformance to iso/icao standards in machine readable travel documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maltoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1204" to="1213" />
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Best practice technical guidelines for automated border control (abc) systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frontex</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<editor>Maria-Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Standardization of face image sample quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiufeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiren</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Biometrics</title>
		<editor>Seong-Whan Lee and Stan Z. Li</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="242" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ongoing face recognition vendor test (frvt) part 2: Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hanaoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NISTIR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">NIST Interagency/Internal Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Face recognition vendor test -face recognition quality assessment concept and goals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayee</forename><surname>Hanaoka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Performance of biometric quality measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elham</forename><surname>Tabassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="531" to="543" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stacked dense u-nets with dual transformers for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-09-03" />
			<biblScope unit="page">44</biblScope>
		</imprint>
		<respStmt>
			<orgName>Northumbria University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MS-Celeb-1M: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faceqnet: Quality assessment for face recognition based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Hernandez-Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Fiérrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Haraksim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Beslay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Biometrics</title>
		<meeting><address><addrLine>Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quality assessment of facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics Symposium: Special Session on Research at the Biometric Consortium Conference</title>
		<imprint>
			<date type="published" when="2006-09" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Machine Readable Travel Documents. Standard, International Civil Aviation Organization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Information technology -Biometric data interchange formats -Part 5: Face image data. Standard, International Organization for Standardization</title>
		<imprint>
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iso/Iec</surname></persName>
		</author>
		<title level="m">Information technology Biometric performance testing and reporting. Standard</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Performance evaluation of face image quality algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iso/Iec Awi 24357</surname></persName>
		</author>
		<imprint>
			<pubPlace>Standard</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face image assessment learned with objective and relative face image qualities for improved face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="4027" to="4031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Making a completely blind image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Neurotechnology. Neurotec Biometric SDK</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the existence of face quality measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Givens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Teli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2013-09" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The feret evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjoon</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image-quality-based adaptive face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sellahewa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Jassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="805" to="813" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Biometric sample quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elham</forename><surname>Tabassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Grother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Biometrics</title>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reliable age and gender estimation from face images: Stating the confidence of model predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Terhörst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Niklas</forename><surname>Kolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Zelch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kirchbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference on Biometrics Theory, Applications and Systems</title>
		<meeting><address><addrLine>Tampa, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-09-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the complex domain deep machine learning for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bipin Kumar Tripathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="382" to="396" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Blind image quality evaluation using perception based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Venkatanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Praneeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maruthi Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Bh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Medasani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 Twenty First National Conference on Communications (NCC)</title>
		<imprint>
			<date type="published" when="2015-02" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust face recognition via discriminative and common hybrid dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-She</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hou-Bing</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Assessing face image quality for smartphone based face recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wasnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 5th International Workshop on Biometrics and Forensics (IWBF)</title>
		<imprint>
			<date type="published" when="2017-04" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Patch-based probabilistic image quality assessment for face selection and improved video-based face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011 WORKSHOPS</title>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method. CoRR, abs/1212</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5701</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
