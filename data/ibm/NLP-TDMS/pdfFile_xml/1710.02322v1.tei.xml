<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human Pose Regression by Combining Indirect Part Detection and Contextual Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diogo</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
							<email>diogo.luvizon@ensea.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">ETIS Lab</orgName>
								<orgName type="laboratory" key="lab2">UMR 8051</orgName>
								<orgName type="institution" key="instit1">Université Paris Seine</orgName>
								<orgName type="institution" key="instit2">Université Cergy-Pontoise</orgName>
								<orgName type="institution" key="instit3">ENSEA</orgName>
								<address>
									<region>CNRS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Tabia</surname></persName>
							<email>hedi.tabia@ensea.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">ETIS Lab</orgName>
								<orgName type="laboratory" key="lab2">UMR 8051</orgName>
								<orgName type="institution" key="instit1">Université Paris Seine</orgName>
								<orgName type="institution" key="instit2">Université Cergy-Pontoise</orgName>
								<orgName type="institution" key="instit3">ENSEA</orgName>
								<address>
									<region>CNRS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
							<email>picard@ensea.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">ETIS Lab</orgName>
								<orgName type="laboratory" key="lab2">UMR 8051</orgName>
								<orgName type="institution" key="instit1">Université Paris Seine</orgName>
								<orgName type="institution" key="instit2">Université Cergy-Pontoise</orgName>
								<orgName type="institution" key="instit3">ENSEA</orgName>
								<address>
									<region>CNRS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Human Pose Regression by Combining Indirect Part Detection and Contextual Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose an end-to-end trainable regression approach for human pose estimation from still images. We use the proposed Soft-argmax function to convert feature maps directly to joint coordinates, resulting in a fully differentiable framework. Our method is able to learn heat maps representations indirectly, without additional steps of artificial ground truth generation. Consequently, contextual information can be included to the pose predictions in a seamless way. We evaluated our method on two very challenging datasets, the Leeds Sports Poses (LSP) and the MPII Human Pose datasets, reaching the best performance among all the existing regression methods and comparable results to the state-of-the-art detection based approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation from still images is a hard task since the human body is strongly articulated, some parts may not be visible due to occlusions or low quality images, and the visual appearance of body parts can change significantly from one pose to another. Classical methods use keypoint detectors to extract local information, which are combined to build pictorial structures <ref type="bibr" target="#b15">[16]</ref>. To handle difficult cases of occlusion or partial visualization, contextual information is usually needed to provide visual cues that can be extracted from a broad region around the part location <ref type="bibr" target="#b14">[15]</ref> or by interaction among detected parts <ref type="bibr" target="#b44">[45]</ref>. In general, pose estimation can be seen from two different perspectives, namely as a correlated part detection problem or as a regression problem. Detection based approaches commonly try to detect keypoints individually, which are aggregated in post-processing stages to form one pose prediction. In contrast, methods based on regression use a function to map directly input images to body joint positions.</p><p>In the last few years, pose estimation have gained attention with the breakthrough of deep Convolutional Neural Networks (CNN) <ref type="bibr" target="#b41">[42]</ref> alongside consistent computational power increase. This can be seen as the shift from classical approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25]</ref> to deep architectures. In many recent works from different domains, CNN based methods have overcome classical approaches by a large margin <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37]</ref>. A key benefit from CNN is that the full pipeline is differentiable, allowing end-to-end learning. In the context of human pose estimation, the first methods using deep neural networks tried to do regression directly by learning a non-linear mapping function from RGB images to joint coordinates <ref type="bibr" target="#b41">[42]</ref>. By contrast, the majority of the methods in the state of the art tackle pose estimation as a detection problem by predicting heat maps that corresponds to joint locations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5]</ref>. In such methods, the ground truth is artificially generated from joint positions, generally as a 2D Gaussian distribution centered on the joint location, while the context information is implicitly learned by the hidden convolutional layers.</p><p>Despite achieving state-of-the-art accuracy on 2D pose estimation, detection based approaches have some limitations. For example, such methods relies on additional steps to convert heat maps to joint positions, usually by applying the argmax function, which is not differentiable, breaking the learning chain on neural networks. Additionally, the precision of predicted keypoints is proportional to that of the heat map resolution, which leads the top ranked methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref> to high memory consumption and high computational requirements.</p><p>Moreover, regression based methods are conceptually more adapted to 2D and 3D scenarios and can be used indistinctly on both cases <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref>. However, the regression function map is sub-optimally learned, resulting in lower results if compared to detection based approaches. In this paper, we aim at solving this problem by bridging the gap between detection and regression based methods. We propose to replace the argmax function, used to convert heat maps into joint locations, by what we call the Soft-argmax function, which keeps the properties of specialized part detectors while being fully differentiable. With this solution, we are able to optimize our model from end-to-end using regression losses, i.e., from input RGB images to final (x, y) body joint coordinates.</p><p>The contributions of our work are the following: first, we present a human pose regression approach from still images based on the Soft-argmax function, resulting in an end-to-end trainable method which does not require artificial heat maps generation for training. Second, the proposed method can be trained using an insightful regression loss function, which is directly linked to the error distance between predicted and ground truth joint positions. Third, in the proposed architecture, contextual information is directly accessible and is easily aggregated to the final predictions. Finally, the accuracy reached by our method surpasses that of regression methods ans is close to that of state-of-theart detection methods, despite using a much smaller network. Some examples of our regressed poses are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Additionally, we provide our implementation of the proposed method in Python using the open source Keras library <ref type="bibr" target="#b9">[10]</ref>. <ref type="bibr" target="#b0">1</ref> The rest of this paper is divided as follows. In the next section, we present a review of the most relevant related work. The proposed method is presented in Section 3. In Section 4, we show the experimental evaluations, followed by our conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Several approaches for human pose estimation have been presented for both 2D <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref> and 3D <ref type="bibr" target="#b21">[22]</ref> scenarios, as well as for video sequences <ref type="bibr" target="#b28">[29]</ref>. Among classical methods, Pictorial Structures <ref type="bibr" target="#b1">[2]</ref> and poselet-based features <ref type="bibr" target="#b30">[31]</ref> have been widely used in the past. In this section, due to limited space, we focus on CNN based methods that are more related to our work. We briefly refer to the most recent works, splitting them as regression based and detection based approaches.</p><p>Regression based approaches. Some methods tackled pose estimation as a keypoint regression problem. One of the first regression approaches was proposed by Toshev and Szegedy <ref type="bibr" target="#b41">[42]</ref> as a holistic solution based on cascade regression for body part detection, where individual joint positions are recursively improved, taking the full frame as input. Pfister et al. <ref type="bibr" target="#b29">[30]</ref> proposed the Temporal Pose ConvNet to track upper body parts, and Carreira et al. <ref type="bibr" target="#b5">[6]</ref> proposed the Iterative Error Feedback by injecting the prediction error back to the input space, improving estimations recursively. Recently, Sun et al. <ref type="bibr" target="#b37">[38]</ref> proposed a structured bone based representation for human pose, which is statistically less variant than absolute joint positions and can be indistinctly used for both 2D and 3D representations. However, the <ref type="bibr" target="#b0">1</ref> The Python source code will be publicly available after acceptance at https://github.com/dluvizon/pose-regression. method requires converting pose data to the relative bone based format. Moreover, those results are all outperformed by detection based methods.</p><p>Detection based approaches. Pischulin et al. <ref type="bibr" target="#b32">[33]</ref> proposed DeepCut, a graph cutting algorithm that relies on body parts detected by DeepPose <ref type="bibr" target="#b41">[42]</ref>. This method has been improved in <ref type="bibr" target="#b20">[21]</ref> by replacing the previous CNN by a deep Residual Network (ResNet) <ref type="bibr" target="#b18">[19]</ref>, resulting in very competitive accuracy results, specially on multi-person detection. More recent methods have shown significant improvements on accuracy by using fully convolutional models to generate belief maps (or heat maps) for joint probabilities <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b12">13]</ref>. For example, Bulat et al. <ref type="bibr" target="#b4">[5]</ref> proposed a two-stages CNN for coarse and fine heat map regression using pre-trained models, and Gkioxari et al. <ref type="bibr" target="#b16">[17]</ref> presented a structured prediction method, where the prediction of each joint depends on the intermediate feature maps and the distribution probability of the previously pre-dicted joints. Following the tendency of deeper models with residual connections, Newell et al. <ref type="bibr" target="#b26">[27]</ref> proposed a stacked hourglass network with convolutions in multi-level features, allowing reevaluation of previous estimations due to a stacked block architecture with many intermediate supervisions. The part-based learning process can benefit from intermediate supervision because it acts as constraints on the lower level layers. As a result, the feature maps on higher levels tend to be cleaner. Recently, Chu et al. <ref type="bibr" target="#b12">[13]</ref> extended the stacked hourglass network by increasing the network complexity and using a CRF based attention map alongside intermediate supervisions to reinforce the task of learning structural information. These results provide to our knowledge state-of-the-art performance.</p><p>All the previous methods that are based on detection need additional steps on training to produce artificial ground truth from joint positions, which represent an additional processing stage and additional hype-parameters, since the ground truth heat maps have to be defined by hand. On evaluation, the inverse operation is required, i.e., heat maps have to be converted to joint positions, generally using the argmax function. Consequently, in order to achieve good precision, predicted heat maps need reasonable spacial resolution, which increases quadratically the computational cost and memory usage. In order to provide an alternative to heat maps based approaches, we present our framework in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>The proposed approach is an end-to-end trainable network which takes as input RGB images and outputs two vectors: the probability p n of joint n being in the image and the regressed joint coordinates y n = (x n , y n ), where n = {1, 2, . . . , N J } is the index of each joint and N J is the number of joints. In what follows, we first present the global architecture of our method, and then detail its most important parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network architecture</head><p>An overview of the proposed method is presented in <ref type="figure" target="#fig_3">Fig. 2</ref>. Our approach is based on a convolutional neural network essentially composed of three parts: one entry flow (Stem), Block-A and Block-B. The role of the stem is to provide basic feature extraction, while Block-A provides refined features and Block-B provides body-part and contextual activation maps. One sequence of Block-A and Block-B is used to build one prediction block, which output is used as intermediate supervision during training. The full network is composed by the stem and a sequence of K prediction blocks. The final prediction is the output of the K th prediction block. To predict the pose at each prediction block, we aggregate the 2D coordinates generated by applying Soft-argmax to the part-based and contextual maps that are output by Block-B. Similarly to recent approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b12">13]</ref>, on each prediction block we produce one estimation that is used as intermediate supervision, providing better accuracy and more stability to the learning process. As a convention, we use the generic term "heat map" to refer both to part-based and contextual feature maps, since these feature maps converge to heat maps like representations.</p><p>The proposed CNN model is partially based on Inception-v4 <ref type="bibr" target="#b38">[39]</ref> and on the Stacked Hourglass <ref type="bibr" target="#b26">[27]</ref> networks. We also inspired our model on the "extreme" inception (Xception) <ref type="bibr" target="#b8">[9]</ref> network, which relies on the premise that convolutions can be separated into spatial convolutions (individual for each channel) followed by a 1 × 1 convolution for cross-channel projection, resulting in significant reduction on the number of parameters and on computations. This idea is called depthwise separable convolution (Sep-Conv) and an optimized implementation is available on the TensorFlow framework.</p><p>In our network, the "Stem" is based on Inception-v4's stem followed by a SepConv layer in parallel with a shortcut layer, as presented in <ref type="figure">Fig. 3a</ref>. For Block-A, we use a similar architecture as the Stacked Hourglass replacing all the residual blocks by a residual separable convolution (Res-SepConv), as depicted in <ref type="figure">Fig. 3b</ref>. Additionally, our approach increased the results from <ref type="bibr" target="#b26">[27]</ref> with only three feature map resolutions, from 32 × where N c is the number of context maps for each joint. The produced heat maps are projected back to the feature space and reintroduced to the network flow by a 1 × 1 convolution. Similar techniques have been used by many previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b12">13]</ref>, resulting in significant gain of performance. From the generated heat maps, our method computes the predicted joint locations and joint probabilities in the regression block, which has no trainable parameters. The architecture of Block-B and the regression stage is shown in <ref type="figure" target="#fig_6">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed regression method</head><p>As presented in Section 2, traditional regression based methods use fully connected layers on feature maps and learn the regression mapping. However, this approach usually gives sub-optimal solutions. While state-of-the-art methods are overwhelmingly based on part detection, approaches based on regression have the advantages of providing directly the pose prediction as joint coordinates without additional steps or post-processing. In order to provide an     <ref type="figure">Figure 3</ref>: In the proposed network architecture, the Stem (a) is based on Inception-v4's stem <ref type="bibr" target="#b38">[39]</ref> followed by a separable convolution in parallel to a shortcut connection. In (b), we present the residual separable convolution (Res-SepConv), used to replace the residual block in the Stacked Hourglass <ref type="bibr" target="#b26">[27]</ref> model. If N f in is equal to N f out , the shortcut convolution is replaced by the identity mapping. alternative to detection based methods, we propose an efficient and fully differentiable way to convert heat maps directly to (x , y) coordinates, which we call Soft-argmax. Additionally, the Soft-argmax operation can be implemented as a CNN layer, as detailed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Soft-argmax layer</head><p>Let us define the Softmax operation on a single heat map h ∈ R W ×H as:</p><formula xml:id="formula_0">Φ(h i,j ) = e hi,j W k=1 H l=1 e h k,l ,<label>(1)</label></formula><p>where h i,j is the value of heat map h at location (i , j ), and W × H is the heat map size. Contrary to the more common cross-channel softmax, we use here a spatial softmax that  ensures each heat maps is normalized. Then, we define the Soft-argmax as follows:</p><formula xml:id="formula_1">Ψ d (h) = W i=1 H j=1 W i,j,d Φ(h i,j ),<label>(2)</label></formula><p>where d is a given component x or y, and W is a W ×H ×2 weight matrix corresponding to the coordinates (x , y). The matrix W can be expressed by its components W x and W y , which are 2D discrete normalized ramps, defined as follows:</p><formula xml:id="formula_2">W i,j,x = i W , W i,j,y = j H .<label>(3)</label></formula><p>Finally, given a heat map h, the regressed location of the predicted joint is given by</p><formula xml:id="formula_3">y = (Ψ x (h), Ψ y (h)) T .<label>(4)</label></formula><p>This Soft-argmax operation can be seen as a weighted average of points distributed on an uniform grid, with the weights being equal to the corresponding heat map.</p><p>In order to integrate the Soft-argmax layer into a deep network, we need its derivative with respect to h:</p><formula xml:id="formula_4">∂Ψ d (h i,j ) ∂h i,j = W i,j,d e hi,j ( W k=1 H l=1 e h k,l − e hi,j ) ( W k=1 H l=1 e h k,l ) 2 . (5)</formula><p>The Soft-argmax function can thus be integrated in a trainable framework by using back propagation and the chain rule on equation <ref type="bibr" target="#b4">(5)</ref>. Moreover, from equation <ref type="formula">(5)</ref>, we can see that the gradient is exponentially increasing for higher values, resulting in very discriminative response at the joint position.</p><p>The implementation of Soft-argmax can be easily done with recent frameworks, such as TensorFlow, just by concatenating a spatial softmax followed by one convolutional layer with 2 filters of size W × H , with fixed parameters according to equation <ref type="formula" target="#formula_2">(3)</ref>.</p><p>Unlike traditional argmax, Soft-argmax provides subpixel accuracy, allowing good precision even with very low resolution. Moreover, the Soft-argmax operation allows to learn very discriminative heat maps directly from the (x , y) joint coordinates without explicitly computing artificial ground truth. Samples of heat maps learned by our approach are shown in <ref type="figure" target="#fig_8">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Joint probability</head><p>Additionally to the joint locations, we estimate the joint probability p n , which corresponds to the probability of the n th joint being present in the image. The estimated joint probability is given by the sigmoid activation on the global max-pooling from heat map h n . Despite giving an additional piece of information, the joint probability does not depends on additional parameters and is computationally negligible, compared to the cost of convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Detection and context aggregation</head><p>Even if the correlation between some joints can be learned in the hidden convolutional layers, the joint regression approach is designed to locate body parts individually, resulting in low flexibility to learn from the context. For example, the same filters that give high response to images of a clean head, also must react positively to a hat or a pair of sunglasses. In order to provide multi-source information to the final prediction, we include in our framework specialized part-based heat maps and context heat maps, which are defined as H d = [h d 1 , . . . , h d N J ] and H c = [h c 1,1 , . . . , h c Nc,Nj ], respectively. Additionally, we define the joint probability related to each context map as p c i,n , where i = {1, . . . , N c } and n = {1, . . . , N j }.</p><p>Finally, the n th joint position from detection and contextual information aggregated is given by:</p><formula xml:id="formula_5">y n = αy d n + (1 − α) Nc i=1 p c i,n y c i,n Nc i=1 p c i,n ,<label>(6)</label></formula><p>where y d n = Soft-argmax(h d n ) is the predicted location from the n th part based heat map, y c i,n = Soft-argmax(h c i,n ) and p c i,n are respectively the location and the probability for the i th context heat map for joint n, and α is a hyper-parameter.</p><p>From equation <ref type="formula" target="#formula_5">(6)</ref> we can see that the final prediction is a combination of one specialized prediction and N c contextual predictions pondered by their probabilities. The contextual weighted contribution brings flexibility, allowing specific filters to be more responsive to particular patterns. This aggregation scheme within the learning stage is only possible because we have the joint probability and position directly available inside the network in a differentiable way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the proposed method on the very challenging MPII Human Pose <ref type="bibr" target="#b0">[1]</ref> and Leeds Sports Poses (LSP) <ref type="bibr" target="#b22">[23]</ref> datasets. The MPII dataset contains 25K images collected from YouTube videos, including around 28K annotated poses for training and 15K poses for testing. The annotated poses have 16 body joints, some of them are not present and others are occluded but can be predicted by the context. The LSP dataset is composed by 2000 annotated poses with up to 14 joint locations. The images were gethered from Flickr with sports people. The details about training the model and achieved accuracy results are given as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training</head><p>The proposed network was trained simultaneously on joints regression and joint probabilities. For joints regression, we use the elastic net loss function (L1 + L2):</p><formula xml:id="formula_6">L y = 1 N J N J n=1 y n −ŷ n 1 + y n −ŷ n 2 2 ,<label>(7)</label></formula><p>where y n andŷ n are respectively the ground truth and the predicted n th joint coordinates. In this case, we use directly the joint coordinates normalized to the interval [0, 1], where the top-left image corner corresponds to (0, 0), and the bottom-right image corner corresponds to <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b0">1)</ref>.</p><p>For joint probability estimation, we use the binary cross entropy loss function on the joint probability p: <ref type="bibr" target="#b7">(8)</ref> where p n andp n are respectively the ground truth and the predicted joint probability. We optimize the network using back propagation and the RMSProp optimizer, with batch size of 16 samples. For the MPII dataset, we train the network for 120 epochs. The learning rate begins at 10 −3 and decreases by a factor of 0.4 when accuracy on validation plateaus. We use the same validation split as proposed in <ref type="bibr" target="#b39">[40]</ref>. On the LSP dataset, we start from the model trained on MPII and fine-tuned it for more 70 epochs, beginning with learning rate 2 · 10 −5 and using the same decrease procedure. The full training of our network takes three days on the relatively outdated NVIDIA GPU Tesla K20 with 5GB of memory.</p><formula xml:id="formula_7">L p = 1 N J N J n=1 [(p n − 1) log (1 −p n ) − p n logp n ],</formula><p>Data augmentation. We used standard data augmentation on both MPII and LSP datasets. Input RGB images were cropped and centered on the main subject with a squared bounding box, keeping the people scale (when provided), then resized to 256 × 256 pixels. We perform random rotations (±40 • ) and random rescaling from 0.7 to 1.3 on MPII and from 0.85 to 1.25 on LSP to make the model more robust to image changes.</p><p>Parameters setup. The network model is defined according to <ref type="figure" target="#fig_3">Fig. 2</ref> and composed of eight prediction blocks (K = 8). We trained the network to regress 16 joints with 2 context maps for each joint (N j = 16, N c = 2). In the aggregation stage, we use α = 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>LSP dataset. We evaluate our method on the LSP dataset using two metrics, the "Percentage of Correct Parts" (PCP) and the "Probability of Correct Keypoint" (PCK) measures, as well as two different evaluation protocols, "Observer-Centric" (OC) and "Person-Centric" (PC), resulting in four different evaluation settings. Our results compared to the state-of-the-art results on the LSP dataset using OC annotations are present in <ref type="table" target="#tab_1">Table 1 (PCK measure)</ref> and <ref type="table" target="#tab_2">Table 2</ref> (PCP measure). In both cases, we overcome the best scores by a significant margin, specially with respect to the lower leg and the ankles, on which we increase the results of Pishchulin et al. <ref type="bibr" target="#b32">[33]</ref> by 6.3% and 4.6%, respectively. To the best of our knowledge, we are the sole regression method to report results using this evaluation settings.</p><p>Using the PC annotations on LSP, we achieved the best results among regression based approaches and the second general score, as presented in <ref type="table" target="#tab_3">Tables 3 and 4</ref>. On the PCK measure, we outperform the results reported by Carreira et   al. <ref type="bibr" target="#b5">[6]</ref> (CVPR 2016), which is the only regression method reported on this setup, by 18.0%. MPII dataset. On the MPII dataset, we evaluate our method using the "Single person" challenge <ref type="bibr" target="#b0">[1]</ref>. The scores were computed by the providers of the dataset, since the test labels are not publicly available. As shown in <ref type="table" target="#tab_5">Table 5</ref>, we reached a test score of 91.2%, which is only 0.7% lower then the best result using detection based method, and 4.8% higher than the second score using regression.</p><p>Taking into account the competitiveness of the MPII Human Pose challenge 2 , our score represents a very significant improvement over regression based approaches and a promising result compared to detection based methods. Moreover, our method is much simpler than the stacked hourglass network from Newell et al. <ref type="bibr" target="#b26">[27]</ref> or its extension from Chu et al. <ref type="bibr" target="#b12">[13]</ref>. Due to limited memory resources, we were not able to train these two models in our hardware. Despite that, we reach comparable results with a model that fits in much smaller GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>As suggested in Section 3.2.1, the proposed Soft-argmax function acts as a constrain on the regression approach, driving the network to learn part-based detectors indirectly. This effect provides the flexibility of regression based methods, which can be easily integrated to provide 2D pose estimation to other applications such as 3D pose estimation or action recognition, while preserving the performance of detection based methods. Some examples of part-based maps indirectly learned by our method are show in <ref type="figure" target="#fig_8">Fig. 5</ref>. As we can see, the responses are very well localized on the true location of the joints without explicitly requiring so.</p><p>Additionally to the part-based maps, the contextual maps give extra information to refine the predicted pose. In some cases, the contextual maps provide strong responses to regions around the joint location. In such cases, the aggregation scheme is able to refine the predicted joint position.  contextual aggregation are shown in <ref type="figure" target="#fig_9">Fig. 6</ref>. The contextual maps are able to increase the precision of the predictions by providing complementary information, as we can see for the right elbows of the poses in <ref type="figure" target="#fig_9">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we presented a new regression method for human pose estimation from still images. The method is based on the Soft-argmax operation, a differentiable operation that can be integrated in a deep convolutional network to learn part-based detection maps indirectly, resulting in a significant improvement over the state-of-the-art scores from regression methods and very competitive re-  sults compared to detection based approaches. Additionally, we demonstrate that contextual information can be seamless integrated into our framework by using additional context maps and joint probabilities. As a future work, other methods could be build up to our approach to provide 3D pose estimation or human action recognition from pose in a fully differentiable way.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Test samples from the Leeds Sports Poses (LSP) dataset. Input image (a), the predicted part-based maps encoded as RGB image for visualizasion (b), and the regressed pose (c). Corresponding human limbs have the same colors in all images. This figure is better seen in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>32 to 8 × 8, instead of the original five resolutions, from 64 × 64 to 4 × 4. At each prediction stage, Block-B is used to transform input feature maps into M d part-based detection maps (H d ) and M c context maps (H c ), resulting in M = M d +M c heat maps. For the specific problem of pose estimation, M d corresponds to the number of joints N J , and M c = N c N J ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed approach for pose regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Network architecture of Block-B and an overview of the regression stage. The input is projected into M heat maps (M d + M c ) which are then used for pose regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>On the other hand, if the contextual map response is weak, the context reflects in very few changes on the pose. Some examples of predicted poses and visual contributions from 2 MPII Leader Board: http://human-pose.mpi-inf.mpg.de</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Indirectly learned part-based heat maps from our method. All the joints encoded to RGB are shown in the first image (top-left corner) and the final pose is shown in the last image (bottom-right corner). On each column, the intermediate images correspond to the predicted heat maps before (left) and after (right) the Softmax normalization. The presented heat maps correspond to right ankle, right hip, right wrist, right shoulder, upper neck, head top, left knee, and left wrist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Samples of context maps aggregated to refine predicted pose. Input image (a), part-based detection maps (b), predicted pose without context (c), two different context maps (d) and (e), and the final pose with aggregated predictions (f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on LSP test samples using the PCK measure at 0.2 with OC annotations. Method Head Sho. Elb. Wri. Hip Knee Ank. PCK Detection based methods Kiefel and Gehler [24] 83.5 73.7 55.9 36.2 73.7 70.5 66.9 65.8 Ramakrishna et al. [35] 84.9 77.8 61.4 47.2 73.6 69.1 68.8 69.0 Pishchulin et al.</figDesc><table><row><cell>[32]</cell><cell>87.5 77.6 61.4 47.6 79.0 75.2 68.4 71.0</cell></row><row><cell>Ouyang et al. [28]</cell><cell>86.5 78.2 61.7 49.3 76.9 70.0 67.6 70.0</cell></row><row><cell>Chen and Yuille [7]</cell><cell>91.5 84.7 70.3 63.2 82.7 78.1 72.0 77.5</cell></row><row><cell>Yang et al. [44]</cell><cell>90.6 89.1 80.3 73.5 85.5 82.8 68.8 81.5</cell></row><row><cell>Chu et al. [12]</cell><cell>93.7 87.2 78.2 73.8 88.2 83.0 80.9 83.6</cell></row><row><cell>Pishchulin et al. [33]</cell><cell>97.4 92.0 83.8 79.0 93.1 88.3 83.7 88.2</cell></row><row><cell></cell><cell>Regression based method</cell></row><row><cell>Our method</cell><cell>97.4 93.8 86.8 82.3 93.7 90.9 88.3 90.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on LSP test samples using the PCP measure with OC annotations.</figDesc><table><row><cell>Method</cell><cell cols="6">Torso Upper Lower Upper Fore-Head PCP</cell></row><row><cell></cell><cell></cell><cell>leg</cell><cell>leg</cell><cell>arm</cell><cell>arm</cell><cell></cell></row><row><cell></cell><cell cols="3">Detection based methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kiefel and Gehler [24]</cell><cell>84.3</cell><cell>74.5</cell><cell>67.6</cell><cell>54.1</cell><cell>28.3</cell><cell>78.3 61.2</cell></row><row><cell>Pishchulin et al. [31]</cell><cell>87.4</cell><cell>75.7</cell><cell>68.0</cell><cell>54.4</cell><cell>33.7</cell><cell>77.4 62.8</cell></row><row><cell cols="2">Ramakrishna et al. [35] 88.1</cell><cell>79.0</cell><cell>73.6</cell><cell>62.8</cell><cell>39.5</cell><cell>80.4 67.8</cell></row><row><cell>Ouyang et al. [28]</cell><cell>88.6</cell><cell>77.8</cell><cell>71.9</cell><cell>61.9</cell><cell>45.4</cell><cell>84.3 68.7</cell></row><row><cell>Pishchulin et al. [32]</cell><cell>88.7</cell><cell>78.9</cell><cell>73.2</cell><cell>61.8</cell><cell>45.0</cell><cell>85.1 69.2</cell></row><row><cell>Chen and Yuille [7]</cell><cell>92.7</cell><cell>82.9</cell><cell>77.0</cell><cell>69.2</cell><cell>55.4</cell><cell>87.8 75.0</cell></row><row><cell>Yang et al. [44]</cell><cell>96.5</cell><cell>88.7</cell><cell>81.7</cell><cell>78.8</cell><cell>66.7</cell><cell>83.1 81.1</cell></row><row><cell>Chu et al. [12]</cell><cell>95.4</cell><cell>87.6</cell><cell>83.2</cell><cell>76.9</cell><cell>65.2</cell><cell>89.6 81.1</cell></row><row><cell>Pishchulin et al. [33]</cell><cell>96.0</cell><cell>91.0</cell><cell>83.5</cell><cell>82.8</cell><cell>71.8</cell><cell>96.2 85.0</cell></row><row><cell></cell><cell cols="4">Regression based method</cell><cell></cell><cell></cell></row><row><cell>Our method</cell><cell>98.2</cell><cell>93.8</cell><cell>89.8</cell><cell>85.8</cell><cell>75.5</cell><cell>96.0 88.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on LSP test samples using the PCK measure at 0.2 with PC annotations.</figDesc><table><row><cell>Method</cell><cell>Head Sho. Elb. Wri. Hip Knee Ank. PCK</cell></row><row><cell></cell><cell>Detection based methods</cell></row><row><cell>Pishchulin et al. [32]</cell><cell>87.2 56.7 46.7 38.0 61.0 57.5 52.7 57.1</cell></row><row><cell>Chen and Yuille [7]</cell><cell>91.8 78.2 71.8 65.5 73.3 70.2 63.4 73.4</cell></row><row><cell>Fan et al. [15]</cell><cell>92.4 75.2 65.3 64.0 75.7 68.3 70.4 73.0</cell></row><row><cell>Tompson et al. [41]</cell><cell>90.6 79.2 67.9 63.4 69.5 71.0 64.2 72.3</cell></row><row><cell>Yang et al. [44]</cell><cell>90.6 78.1 73.8 68.8 74.8 69.9 58.9 73.6</cell></row><row><cell>Rafi et al. [34]</cell><cell>95.8 86.2 79.3 75.0 86.6 83.8 79.8 83.8</cell></row><row><cell>Yu et al. [46]</cell><cell>87.2 88.2 82.4 76.3 91.4 85.8 78.7 84.3</cell></row><row><cell>Belag. and Ziss. [4]</cell><cell>95.2 89.0 81.5 77.0 83.7 87.0 82.8 85.2</cell></row><row><cell>Lifshitz et al. [26]</cell><cell>96.8 89.0 82.7 79.1 90.9 86.0 82.5 86.7</cell></row><row><cell>Pishchulin et al. [33]</cell><cell>97.0 91.0 83.8 78.1 91.0 86.7 82.0 87.1</cell></row><row><cell cols="2">Insafutdinov et al. [21] 97.4 92.7 87.5 84.4 91.5 89.9 87.2 90.1</cell></row><row><cell>Wei et al. [43]</cell><cell>97.8 92.5 87.0 83.9 91.5 90.8 89.9 90.5</cell></row><row><cell>Bulat and Tzimi. [5]</cell><cell>97.2 92.1 88.1 85.2 92.2 91.4 88.7 90.7</cell></row><row><cell>Chu et al. [13]</cell><cell>98.1 93.7 89.3 86.9 93.4 94.0 92.5 92.6</cell></row><row><cell></cell><cell>Regression based methods</cell></row><row><cell>Carreira et al. [6]</cell><cell>90.5 81.8 65.8 59.8 81.6 70.6 62.0 73.1</cell></row><row><cell>Our method</cell><cell>97.5 93.3 87.6 84.6 92.8 92.0 90.0 91.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on LSP test samples using the PCP measure with PC annotations.</figDesc><table><row><cell>Method</cell><cell cols="6">Torso Upper Lower Upper Fore-Head PCP</cell></row><row><cell></cell><cell></cell><cell>leg</cell><cell>leg</cell><cell>arm</cell><cell>arm</cell><cell></cell></row><row><cell></cell><cell cols="4">Detection based methods</cell><cell></cell><cell></cell></row><row><cell>Pishchulin et al. [32]</cell><cell>88.7</cell><cell>63.6</cell><cell>58.4</cell><cell>46.0</cell><cell>35.2</cell><cell>85.1 58.0</cell></row><row><cell>Tompson et al. [41]</cell><cell>90.3</cell><cell>70.4</cell><cell>61.1</cell><cell>63.0</cell><cell>51.2</cell><cell>83.7 66.6</cell></row><row><cell>Fan et al. [15]</cell><cell>95.4</cell><cell>77.7</cell><cell>69.8</cell><cell>62.8</cell><cell>49.1</cell><cell>86.6 70.1</cell></row><row><cell>Chen and Yuille [7]</cell><cell>96.0</cell><cell>77.2</cell><cell>72.2</cell><cell>69.7</cell><cell>58.1</cell><cell>85.6 73.6</cell></row><row><cell>Yang et al. [44]</cell><cell>95.6</cell><cell>78.5</cell><cell>71.8</cell><cell>72.2</cell><cell>61.8</cell><cell>83.9 74.8</cell></row><row><cell>Rafi et al. [34]</cell><cell>97.6</cell><cell>87.3</cell><cell>80.2</cell><cell>76.8</cell><cell>66.2</cell><cell>93.3 81.2</cell></row><row><cell>Belag. and Ziss. [4]</cell><cell>96.0</cell><cell>86.7</cell><cell>82.2</cell><cell>79.4</cell><cell>69.4</cell><cell>89.4 82.1</cell></row><row><cell>Yu et al. [46]</cell><cell>98.0</cell><cell>93.1</cell><cell>88.1</cell><cell>82.9</cell><cell>72.6</cell><cell>83.0 85.4</cell></row><row><cell>Lifshitz et al. [26]</cell><cell>97.3</cell><cell>88.8</cell><cell>84.4</cell><cell>80.6</cell><cell>71.4</cell><cell>94.8 84.3</cell></row><row><cell>Pishchulin et al. [33]</cell><cell>97.0</cell><cell>88.8</cell><cell>82.0</cell><cell>82.4</cell><cell>71.8</cell><cell>95.8 84.3</cell></row><row><cell cols="2">Insafutdinov et al. [21] 97.0</cell><cell>90.6</cell><cell>86.9</cell><cell>86.1</cell><cell>79.5</cell><cell>95.4 87.8</cell></row><row><cell>Wei et al. [43]</cell><cell>98.0</cell><cell>92.2</cell><cell>89.1</cell><cell>85.8</cell><cell>77.9</cell><cell>95.0 88.3</cell></row><row><cell>Bulat and Tzimi. [5]</cell><cell>97.7</cell><cell>92.4</cell><cell>89.3</cell><cell>86.7</cell><cell>79.7</cell><cell>95.2 88.9</cell></row><row><cell>Chu et al. [13]</cell><cell>98.4</cell><cell>95.0</cell><cell>92.8</cell><cell>88.5</cell><cell>81.2</cell><cell>95.7 90.9</cell></row><row><cell></cell><cell cols="4">Regression based methods</cell><cell></cell><cell></cell></row><row><cell>Carreira et al. [6]</cell><cell>95.3</cell><cell>81.8</cell><cell>73.3</cell><cell>66.7</cell><cell>51.0</cell><cell>84.4 72.5</cell></row><row><cell>Our method</cell><cell>98.2</cell><cell>93.6</cell><cell>91.0</cell><cell>86.6</cell><cell>78.2</cell><cell>96.8 89.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison results with state-of-the-art methods on the MPII dataset on testing, using PCKh measure with threshold as 0.5 of the head segment length. Detection based methods are shown on top and regression based methods on bottom.MethodHead Shoulder Elbow Wrist Hip Knee Ankle Total Detection based methods Pishchulin et al.</figDesc><table><row><cell>[32]</cell><cell>74.3</cell><cell>49.0</cell><cell>40.8</cell><cell cols="3">34.1 36.5 34.4</cell><cell>35.2</cell><cell>44.1</cell></row><row><cell>Tompson et al. [41]</cell><cell>95.8</cell><cell>90.3</cell><cell>80.5</cell><cell cols="3">74.3 77.6 69.7</cell><cell>62.8</cell><cell>79.6</cell></row><row><cell>Tompson et al. [40]</cell><cell>96.1</cell><cell>91.9</cell><cell>83.9</cell><cell cols="3">77.8 80.9 72.3</cell><cell>64.8</cell><cell>82.0</cell></row><row><cell>Hu and Ramanan [20]</cell><cell>95.0</cell><cell>91.6</cell><cell>83.0</cell><cell cols="3">76.6 81.9 74.5</cell><cell>69.5</cell><cell>82.4</cell></row><row><cell>Pishchulin et al. [33]</cell><cell>94.1</cell><cell>90.2</cell><cell>83.4</cell><cell cols="3">77.3 82.6 75.7</cell><cell>68.6</cell><cell>82.4</cell></row><row><cell>Lifshitz et al. [26]</cell><cell>97.8</cell><cell>93.3</cell><cell>85.7</cell><cell cols="3">80.4 85.3 76.6</cell><cell>70.2</cell><cell>85.0</cell></row><row><cell>Gkioxary et al. [17]</cell><cell>96.2</cell><cell>93.1</cell><cell>86.7</cell><cell cols="3">82.1 85.2 81.4</cell><cell>74.1</cell><cell>86.1</cell></row><row><cell>Rafi et al. [34]</cell><cell>97.2</cell><cell>93.9</cell><cell>86.4</cell><cell cols="3">81.3 86.8 80.6</cell><cell>73.4</cell><cell>86.3</cell></row><row><cell cols="2">Belagiannis and Zisserman [4] 97.7</cell><cell>95.0</cell><cell>88.2</cell><cell cols="3">83.0 87.9 82.6</cell><cell>78.4</cell><cell>88.1</cell></row><row><cell>Insafutdinov et al. [21]</cell><cell>96.8</cell><cell>95.2</cell><cell>89.3</cell><cell cols="3">84.4 88.4 83.4</cell><cell>78.0</cell><cell>88.5</cell></row><row><cell>Wei et al. [43]</cell><cell>97.8</cell><cell>95.0</cell><cell>88.7</cell><cell cols="3">84.0 88.4 82.8</cell><cell>79.4</cell><cell>88.5</cell></row><row><cell>Bulat and Tzimiropoulos [5]</cell><cell>97.9</cell><cell>95.1</cell><cell>89.9</cell><cell cols="3">85.3 89.4 85.7</cell><cell>81.7</cell><cell>89.7</cell></row><row><cell>Newell et al. [27]</cell><cell>98.2</cell><cell>96.3</cell><cell>91.2</cell><cell cols="3">87.1 90.1 87.4</cell><cell>83.6</cell><cell>90.9</cell></row><row><cell>Chu et al. [13]</cell><cell>98.5</cell><cell>96.3</cell><cell>91.9</cell><cell cols="3">88.1 90.6 88.0</cell><cell>85.0</cell><cell>91.5</cell></row><row><cell>Chou et al. [11]</cell><cell>98.2</cell><cell>96.8</cell><cell>92.2</cell><cell cols="3">88.0 91.3 89.1</cell><cell>84.9</cell><cell>91.8</cell></row><row><cell>Chen et al. [8]</cell><cell>98.1</cell><cell>96.5</cell><cell>92.5</cell><cell cols="3">88.5 90.2 89.6</cell><cell>86.0</cell><cell>91.9</cell></row><row><cell></cell><cell cols="4">Regression based methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rogez et al. [36]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.2</cell></row><row><cell>Carreira et al. [6]</cell><cell>95.7</cell><cell>91.7</cell><cell>81.7</cell><cell cols="3">72.4 82.8 73.2</cell><cell>66.4</cell><cell>81.3</cell></row><row><cell>Sun et al. [38]</cell><cell>97.5</cell><cell>94.3</cell><cell>87.0</cell><cell cols="3">81.2 86.5 78.5</cell><cell>75.4</cell><cell>86.4</cell></row><row><cell>Our method</cell><cell>98.1</cell><cell>96.6</cell><cell>92.0</cell><cell cols="3">87.5 90.6 88.0</cell><cell>82.7</cell><cell>91.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was partially supported by the Brazilian National Council for Scientific and Technological Development (CNPq) -Grant 233342/2014-1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2D Human Pose Estimation: New Benchmark and State of the Art Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust optimization for deep regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="2830" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Recurrent human pose estimation. CoRR, abs/1605.02914</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation via Convolutional Part Heatmap Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1705.00389</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno>abs/1610.02357</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Self adversarial training for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1707.02439</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07432</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human Pose Estimation Using Body Parts Dependent Joint Regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="3041" to="3048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<title level="m">Chained Predictions Using Convolutional Neural Networks. European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with convolutional latent-variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="2220" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Human Pose Estimation with Fields of Parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="331" to="346" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Human Pose Estimation Using Deep Consensus Voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="246" to="260" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="2337" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for efficient pose estimation in gesture videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Poselet Conditioned Pictorial Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3487" to="3494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pose Machines: Articulated Pose Estimation via Inference Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-Classification-Regression for Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07432</idno>
		<title level="m">Compositional human pose regression</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno>abs/1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient object localization using Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DeepPose: Human Pose Estimation via Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recognizing proxemics in personal photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="3522" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep Deformation Network for Object Landmark Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="52" to="70" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
