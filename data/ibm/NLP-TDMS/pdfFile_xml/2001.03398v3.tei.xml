<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DSGN: Deep Stereo Geometry Network for 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
							<email>ylchen@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
							<email>sliu@smartmore.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
							<email>xiaoyong@smartmore.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DSGN: Deep Stereo Geometry Network for 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most state-of-the-art 3D object detectors heavily rely on LiDAR sensors because there is a large performance gap between image-based and LiDAR-based methods. It is caused by the way to form representation for the prediction in 3D scenarios. Our method, called Deep Stereo Geometry Network (DSGN), significantly reduces this gap by detecting 3D objects on a differentiable volumetric representation -3D geometric volume, which effectively encodes 3D geometric structure for 3D regular space. With this representation, we learn depth information and semantic cues simultaneously. For the first time, we provide a simple and effective one-stage stereo-based 3D detection pipeline that jointly estimates the depth and detects 3D objects in an end-to-end learning manner. Our approach outperforms previous stereo-based 3D detectors (about 10 higher in terms of AP) and even achieves comparable performance with several LiDAR-based methods on the KITTI 3D object detection leaderboard. Our code is publicly available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D scene understanding is a challenging task in 3D perception, which serves as a basic component for autonomous driving and robotics. Due to the great capability of LiDAR sensors to accurately retrieve 3D information, we witness fast progress on 3D object detection. Various 3D object detectors were proposed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b61">60,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b9">10]</ref> to exploit LiDAR point cloud representation. The limitation of LiDAR is on the relatively sparse resolution of data with several laser beams and on the high price of the devices.</p><p>In comparison, video cameras are cheaper and are with much denser resolutions. The way to compute scene depth on stereo images is to consider disparity via stereo correspondence estimation. Albeit recently several 3D detectors based on either monocular <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b50">50]</ref> or stereo <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b58">58]</ref> setting push the limit of image-based 3D object detection, the accuracy is still left far behind compared with the LiDAR-based approaches. Challenges One of the greatest challenges for imagebased approaches is to give appropriate and effective representation for predicting 3D objects. Most recent work <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b1">2]</ref> divides this task into two sub ones, i.e., depth prediction and object detection. Camera projection is a process that maps 3D world into a 2D image. One 3D feature in different object poses causes local appearance changes, making it hard for a 2D network to extract stable 3D information.</p><p>Another line of solutions <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b32">32]</ref> generate intermediate point cloud followed by a LiDAR-based 3D object detector. This 3D representation is less effective since the transformation is non-differentiable and incorporates several independent networks. Besides, the point cloud faces the challenge of object artifacts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b58">58]</ref> that limits the detection accuracy of the following 3D object detector.</p><p>Our Solution In this paper, we propose a stereo-based end-to-end 3D object detection pipeline ( <ref type="figure" target="#fig_0">Figure 1</ref>) -Deep Stereo Geometry Network (DSGN), which relies on space transformation from 2D features to an effective 3D structure, called 3D geometric volume (3DGV).</p><p>The insight behind 3DGV lies in the approach to construct the 3D volume that encodes 3D geometry. 3D geometric volume is defined in 3D world space, transformed from a plane-sweep volume (PSV) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> constructed in the camera frustum. The pixel-correspondence constraint can be well learned in PSV, while 3D features for real-world objects can be learned in 3DGV. The volume construction is fully differentiable and thus can be jointly optimized for learning of both stereo matching and object detection.</p><p>This volumetric representation has two key advantages. First, it is easy to impose the pixel-correspondence constraint and encode full depth information into 3D real-world volume. Second, it provides 3D representation with geometry information that makes it possible to learn 3D geometric features for real-world objects. As far as we know, there was no study yet to explicitly investigate the way of encoding 3D geometry into an image-based detection network. Our contribution is summarized as follows.</p><p>• To bridge the gap between 2D image and 3D space, we establish stereo correspondence in a plane-sweep volume and then transform it to 3D geometric volume for capability to encode both 3D geometry and semantic cues for prediction in 3D regular space.</p><p>• We design an end-to-end pipeline for extracting pixellevel features for stereo matching and high-level features for object recognition. The proposed network jointly estimates scene depth and detects 3D objects in 3D world, enabling many practical applications.</p><p>• Without bells and whistles, our simple and fullydifferentiable network outperforms all other stereobased 3D object detectors (10 points higher in terms of AP) on the official KITTI leaderboard <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly review recent work on stereo matching and multi-view stereo. Then we survey 3D object detection based on LiDAR, monocular images, and stereo images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stereo Matching</head><p>In the field of stereo matching on binocular images, methods of <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b60">59,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b48">48]</ref> process the left and right images by a Siamese network and construct a 3D cost volume to compute the matching cost. Correlation-based cost volume is applied in recent work <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b44">44]</ref>. GC-Net <ref type="bibr" target="#b21">[22]</ref> forms a concatenationbased cost volume and applies 3D convolution to regress disparity estimates. Recent PSMNet <ref type="bibr" target="#b3">[4]</ref> further improves the accuracy by introducing pyramid pooling module and stacks hourglass modules <ref type="bibr" target="#b34">[34]</ref>. State-of-the-art methods already achieved less than 2% 3-pixel error on KITTI 2015 stereo benchmark.</p><p>Multi-View Stereo Methods of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref> reconstruct 3D objects in a multi-view stereo setting <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>MVSNet <ref type="bibr" target="#b56">[56]</ref> constructs plane-sweep volumes upon a camera frustum to generate the depth map for each view. Point-MVSNet <ref type="bibr" target="#b4">[5]</ref> instead intermediately transforms the plane-sweep volume to point cloud representation to save computation. Kar et al. <ref type="bibr" target="#b20">[21]</ref> proposed the differentiable projection and unprojection operation on multi-view images.</p><p>LiDAR-based 3D Detection LiDAR sensors are very powerful, proven by several leading 3D detectors. Generally two types of architectures, i.e., voxel-based approaches <ref type="bibr" target="#b61">[60,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b9">10]</ref> and point-based approaches <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b51">51]</ref>, were proposed to process point cloud.</p><p>Image-based 3D Detection Another line of detection is based on images. Regardless of monocular-or stereo-based setting, methods can be classified into two types according to intermediate representation existence.</p><p>3D detector with depth predictor: the solution relies on 2D image detectors and depth information extraction from monocular or stereo images. Stereo R-CNN Stereo <ref type="bibr" target="#b27">[27]</ref> formulates 3D detection into multiple branches/stages to explicitly resolve several constraints. We note that the keypoint constraint may be hard to generalize to other categories like Pedestrian, and the dense alignment for stereo matching directly operating raw RGB images may be vulnerable to occlusion.</p><p>MonoGRNet M ono <ref type="bibr" target="#b38">[38]</ref> consists of four subnetworks for progressive 3D localization and directly learning 3D information based solely on semantic cues. MonoDIS Mono <ref type="bibr" target="#b42">[42]</ref> disentangles the loss for 2D and 3D detection. It achieves both tasks in an end-to-end manner. M3D-RPN Mono <ref type="bibr" target="#b1">[2]</ref> applies multiple 2D convolutions of non-shared weights to learn location-specific features for joint prediction of 2D and 3D boxes. Triangulation Stereo <ref type="bibr" target="#b39">[39]</ref> directly learns offset from predefined 3D anchors on bird's eye view and establishes object correspondence on RoI-level features. Due to low resolutions, pixel correspondence is not fully exploited.</p><p>3D representation based 3D Detector: 3DOP Stereo <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> generates point cloud by stereo and encodes the prior knowledge and depth in an energy function. Several methods <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b32">32]</ref> transform the depth map to Pseudo-LiDAR (point cloud) intermediately followed by another independent network. This pipeline yields large improvement over previous methods. OFT-Net Mono <ref type="bibr" target="#b40">[40]</ref> maps image feature into an orthographic bird's eye view representation and detects 3D objects on bird's eye view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this section, we first explore the proper representation for 3D space and motivate our network design. Based on the discussion, we present our complete 3D detection pipeline under a binocular image pair setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>Due to perspective, objects appear smaller with the increase of distance, which makes it possible to roughly estimate the depth according to the relative scale of objects sizes and the context. However, 3D objects of the same category may still have various sizes and orientations. It   greatly increases the difficulty to make accurate prediction.</p><p>Besides, the visual effect of foreshortening causes that nearby 3D objects are not scaled evenly in images. A regular cuboid car appears like an irregular frustum. These two problems impose major challenges for 2D neural networks to model the relationship between 2D imaging and real 3D objects <ref type="bibr" target="#b27">[27]</ref>. Thus, instead of relying on 2D representation, by reversing the process of projection, an intermediate 3D representation provides a more promising way for 3D object understanding. The following two representations can be typically used in 3D world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point-based Representation</head><p>Current state-of-the-art pipelines <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b32">32]</ref> generate intermediate 3D structure of point cloud by depth prediction approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22]</ref> and apply LiDAR-based 3D object detectors. The main possible weakness is that it involves several independent networks and potentially loses information during intermediate transformation, making the 3D structure (such as cost volume) boiled down to point cloud.</p><p>This representation often encounters streaking artifacts near object edges <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b58">58]</ref>. Besides, the network is hard to be differentiated for multi-object scenes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Voxel-based Representation Volumetric representation, as another way of 3D representation, is investigated less intensively. OFT-Net mono <ref type="bibr" target="#b40">[40]</ref> directly maps the image feature to the 3D voxel grid and then collapses it to the feature on bird's eye view. However, this transformation keeps the 2D representation for this view and does not explicitly encode the 3D geometry of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Advantage</head><p>The key to establishment of an effective 3D representation relies on the ability to encode accurate 3D geometric information of the 3D space. A stereo camera provides an explicit pixel-correspondence constraint for computing depth. Aiming to design a unified network to exploit this constraint, we explore deep architectures capable of extracting both pixel-level features for stereo correspon-dence and high-level features for semantic cues.</p><p>On the other hand, the pixel-correspondence constraint is supposedly imposed along the projection ray through each pixel where the depth is considered to be definite. To this end, we create an intermediate plane-sweep volume from a binocular image pair to learn stereo correspondence constraint in camera frustum and then transform it to a 3D volume in 3D space. In this 3D volume with 3D geometric information lifted from the plane-sweep volume, we are able to well learn 3D features for real-world objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deep Stereo Geometry Network</head><p>In this subsection, we describe our overall pipeline -Deep Stereo Geometry Network (DSGN) as shown in <ref type="figure">Figure</ref> 2. Taking the input of a binocular image pair (I L , I R ), we extract features by a Siamese network and construct a plane-sweep volume (PSV). The pixel-correspondence is learned on this volume. By differentiable warping, we transform PSV to a 3D geometric volume (3DGV) to establish 3D geometry in 3D world space. Then the following 3D neural network on the 3D volume learns necessary structure for 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Image Feature Extraction</head><p>Networks for stereo matching <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref> and object recognition <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b43">43]</ref> have different architecture designs for their respective tasks. To ensure reasonable accuracy of stereo matching, we adopt the main design of PSMNet <ref type="bibr" target="#b3">[4]</ref>.</p><p>Because the detection network requires a discriminative feature based on high-level semantic features and large context information, we modify the network for grasping more high-level information. Besides, the following 3D CNN for cost volume aggregation takes much more computation, which gives us room to modify the 2D feature extractor without introducing extra heavy computation overhead in the overall network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Architecture Details</head><p>Here we use the notations conv 1, conv 2, ..., conv 5 following <ref type="bibr" target="#b15">[16]</ref>. The key modification for 2D feature extractor is as follows.</p><p>• Shift more computation from conv 3 to conv 4 and conv 5, i.e., changing the numbers of basic blocks of conv 2 to conv 5 from {3, 16, 3, 3} to {3, 6, 12, 4}.</p><p>• The SPP module used in PSMNet concatenates the output layers of conv 4 and conv 5.</p><p>• The output channel number of convolutions in conv 1 is 64 instead of 32 and the output channel number of a basic residual block is 192 instead of 128.</p><p>Full details of our 2D feature extraction network are included in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Constructing 3D Geometric Volume</head><p>To learn 3D convolutional features in 3D regular space, we first create a 3D geometric volume (3DGV) by warping a plane-sweep volume to 3D regular space. Without loss of generality, we discretize the region of interest in 3D world space to a 3D voxel occupancy grid of size (W V , H V , D V ) along the right, down and front directions in camera view.</p><formula xml:id="formula_0">W V , H V , D V denote the width, height and length of the grid, respectively. Each voxel is of size (v w , v h , v d ).</formula><p>Plane-Sweep Volume In binocular vision, an image pair (I L , I R ) is used to construct a disparity-based cost volume for computing matching cost, which matches a pixel i in the left image I L to the correspondence in the right image I R horizontally shifted by an integral disparity value d. The depth is inversely proportional to disparity.</p><p>It is thus hard to distinguish among distant objects due to the similar disparity values <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b58">58]</ref>. For example, objects 40-meter and 39-meter away have almost no difference (&lt; 0.25pix) on disparity on KITTI benchmark <ref type="bibr" target="#b13">[14]</ref>.</p><p>In a different way to construct the cost volume, we follow the classic plane sweeping approach <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b56">56]</ref> to construct a plane-sweep volume by concatenating the left image feature F L and the reprojected right image feature F R−&gt;L at equally spaced depth interval, which avoids imbalanced mapping of features to 3D space.</p><p>The coordinate of PSV is represented by (u, v, d), where (u, v) represents (u, v)-pixel in the image and it adds another axis orthogonal to the image plane for depth. We call the space of (u, v, d) grid camera frustum space. The depth candidates d i are uniformly sampled along the depth dimension with interval v d following the pre-defined 3D grid. Concatenation-based volume enables the network to learn semantic features for object recognition.</p><p>We apply 3D convolution to this volume and finally get a matching cost volume for all depth. To ease computation, we apply only one 3D hourglass module, contrary to the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>World Space</head><p>Camera Frustum Space</p><formula xml:id="formula_1">Plane-Sweep Volume 3D Geometric Volume L/R Camera World Space −1</formula><p>Image Plane <ref type="figure">Figure 3</ref>. Illustration of volume transformation. The image is captured at the image plane (red solid line). PSV is constructed by projecting images at equally spaced depth (blue dotted lines) in left camera frustum, which is shown in the 3D world space (left) and camera frustum space (middle). Car is shown to be distorted in the middle. Mapping by the camera intrinsic matrix K, PSV is warped to 3DGV, which restores the car.</p><p>three used in PSMNet <ref type="bibr" target="#b3">[4]</ref>. We note that the resulting performance degradation can be compensated in the following detection network since the overall network is differentiable.</p><p>3D Geometric Volume With known camera internal parameters, we transform the last feature map of PSV before computing matching cost from camera frustum space</p><formula xml:id="formula_2">(u, v, d) to 3D world space (x, y, z) by reversing 3D pro- jection with   x y z   =   1/f x 0 −c u /f x 0 1/f y −c v /f y 0 0 1     ud vd d   (1)</formula><p>where f x , f y are the horizontal and vertical focal lengths. This transformation is fully-differentiable and saves computation by eliminating background outside the pre-defined grid, such as the sky. It can be implemented by warp operation with trilinear interpolation. <ref type="figure">Figure 3</ref> illustrates the transformation process. The common pixel-correspondence constraint (red dotted lines) is imposed in camera frustum while object recognition is learned in regular 3D world space (Euclidean space). There obviously is difference in these two representations.</p><p>In the last feature map of plane-sweep volume, a lowcost voxel (u, v, d) means the high probability of object existing at depth d along the ray through the focal point and image point (u, v). With the transformation to regular 3D world space, the feature of low cost suggests that this voxel is occupied in the front surface of the scene, which can serve as a feature for 3D geometric structure. Thus it is possible for the following 3D network to learn 3D object features on this volume.</p><p>This operation is fundamentally different from differentiable unprojection <ref type="bibr" target="#b20">[21]</ref>, which directly lifts the image feature from 2D image frame to 3D world by bilinear interpolation. Our goal is to lift geometric information from cost volume to 3D world grid. We make pixel-correspondence constraint easy to be imposed along the projection ray.</p><p>The contemporary work <ref type="bibr" target="#b58">[58]</ref> applies a similar idea to construct depth-cost-volume like plane-sweep volume. Differently, we aim to avoid imbalanced warping from planesweep volume to 3D geometric volume, and deal with the streaking artifact problem. Besides, our transformation keeps the distribution of depth instead of deducting it to a depth map. Our strategy intriguingly avoids object artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Depth Regression on Plane-Sweep Cost Volume</head><p>To compute the matching cost on the plane-sweep volume, we reduce the final feature map of plane-sweep volume by two 3D convolutions to get 1D cost volume (called planesweep cost volume). Soft arg-min operation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b60">59]</ref> is applied to compute the expectation for all depth candidates with probability σ(−c d ) aŝ</p><formula xml:id="formula_3">d = d∈{zmin,zmin+v d ,...,zmax} d × σ(−c d )<label>(2)</label></formula><p>where the depth candidates are uniformly sampled within pre-defined grid [z min , z max ] with interval v d . The softmax function encourages the model to pick a single depth plane per pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">3D Object Detector on 3D Geometric Volume</head><p>Motivated by recent one-stage 2D detector FCOS <ref type="bibr" target="#b46">[46]</ref>, we extend the idea of centerness branch in our pipeline and design a distance-based strategy to assign targets for the real world. Because objects of the same category are of similar size in 3D scene, we still keep the design of anchors. Let V ∈ R W ×H×D×C be the feature map for 3DGV of size (W, H, D) and denote the channels as C. Considering the scenario of autonomous driving, we gradually downsample along the height dimension and finally get the feature map F of size (W, H) for bird's eye view. The network architecture is included in the supplementary material.</p><p>For each location (x, z) in F, several anchors of different orientations and sizes are placed. Anchors A and ground-truth boxes G are represented by the location, prior size and orientation, i.e., (</p><formula xml:id="formula_4">x A , y A , z A , h A , w A , l A , θ A ) and (x G , y G , z G , h G , w G , l G , θ G ).</formula><p>Our network regresses from anchor and gets the final prediction</p><formula xml:id="formula_5">(h A e δh , w A e δw , l A e δl , x A + δx, y A + δy, z A + δz, θ A + π/N θ tanh(δθ)),</formula><p>where N θ denotes the number of anchor orientations and δ· is the learned offset for each parameter.</p><p>Distance-based Target Assignment Taking object orientation into consideration, we propose distance-based target assignment. The distance is defined as the distance of 8 corners between anchor and ground-truth boxes as</p><formula xml:id="formula_6">distance(A, G) = 1 8 8 i=1 (x Ai − x Gi ) 2 + (z Ai − z Gi ) 2 )</formula><p>In order to balance the ratio of positive and negative samples, we let the anchors with top N nearest distance to ground-truth as positive samples, where N = γ × k and k is the number of voxels inside ground-truth box on bird's eye view. γ adjusts the number of positive samples. Our centerness is defined as the exponent of the negative normalized distance of eight corners as</p><formula xml:id="formula_7">centerness(A, G) = e −norm(distance(A,G)) ,<label>(3)</label></formula><p>where norm denotes min-max normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-task Training</head><p>Our network with stereo matching network and 3D object detector is trained in an end-to-end fashion. We train the overall 3D object detector with a multi-task loss as</p><formula xml:id="formula_8">Loss = L depth + L cls + L reg + L centerness . (4)</formula><p>For the loss of depth regression, we adopt smooth L 1 loss <ref type="bibr" target="#b21">[22]</ref> in this branch as</p><formula xml:id="formula_9">L depth = 1 N D N D i=1 smooth L1 d i −d i ,<label>(5)</label></formula><p>where N D is the number of pixels with ground-truth depth (obtained from the sparse LiDAR sensor).</p><p>For the loss of classification, focal loss <ref type="bibr" target="#b31">[31]</ref> is adopted in our network to deal with the class imbalance problem in 3D world as</p><formula xml:id="formula_10">L cls = 1 N pos (x,z)∈F Focal Loss(p A (x,z) , p G (x,z) ),<label>(6)</label></formula><p>where N pos denotes the number of positive samples. Binary cross-entropy (BCE) loss is used for centerness. For the loss of 3D bounding box regression, smooth L 1 Loss is used for the regression of bounding boxes as</p><formula xml:id="formula_11">L reg = 1 N pos (x,z)∈Fpos centerness(A, G)× smooth L1 (l1 distance(A, G))<label>(7)</label></formula><p>where F pos denotes all positive samples on bird's eye view. We try two different regression targets with and without jointly learning all parameters.</p><p>• Separably optimizing box parameters.</p><p>The regression loss is directly applied to the offset of (x, y, z, h, w, l, θ).</p><p>• Jointly optimizing box corners. For jointly optimizing box parameters, the loss is made on the average L1 distance of eight box corners between predicted boxes from 3D anchors and ground-truth boxes following that of <ref type="bibr" target="#b35">[35]</ref>.</p><p>In our experiments, we use the second regression target for Car and the first regression target for Pedestrian and Cyclist. Because it is hard for even human to accurately predict or annotate the orientation of objects like Pedestrian from an image, other parameter estimation under joint optimization can be affected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets Our approach is evaluated on the popular KITTI 3D object detection dataset <ref type="bibr" target="#b13">[14]</ref>, which is union of 7, 481 stereo image-pairs and point clouds for training and 7, 518 for testing. The ground-truth depth maps are generated from point clouds following <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b58">58]</ref>. The training data has annotation for Car, Pedestrian and Cyclist. The KITTI leaderboard limits the access to submission to the server for evaluating test set. Thus, following the protocol in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b47">47]</ref>, the training data is divided into a training set (3,712 images) and a validation set (3,769 images). All ablation studies are conducted on the split. For the submission of our approach, our model is trained from scratch on the 7K training data only.</p><p>Evaluation Metric KITTI has three levels of difficulty setting of easy, moderate (main index) and hard, according to the occlusion/truncation and the size of an object in the 2D image. All methods are evaluated for three levels of difficulty under different IoU criteria per class , i.e., IoU ≥ 0.7 for Car and IoU ≥ 0.5 for Pedestrian and Cyclist for 2D, bird's eye view and 3D detection.</p><p>Following most image-based 3D object detection setting <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b42">42]</ref>, the ablation experiments are conducted on Car. We also report the results of Pedestrian and Cyclist for reference in the supplementary file. KITTI benchmark recently changes evaluation where AP calculation uses 40 recall positions instead of the 11 recall positions proposed in the original Pascal VOC benchmark. Thus, we show the main test results following the official KITTI leaderboard. We generate the validation results using the original evaluation code for fair comparison with other approaches in ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation</head><p>Training Details By default, models are trained on 4 NVIDIA Tesla V100 (32G) GPUs with batch-size 4 -that is, each GPU holds one pair of stereo images of size 384 × 1248. We apply ADAM <ref type="bibr" target="#b23">[23]</ref> optimizer with initial learning rate 0.001. We train our network for 50 epochs and the learning rate is decreased by 10 at 50-th epoch. The overall training time is about 17 hours. The data augmentation used is horizontal flipping only.</p><p>Following other approaches <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b61">60,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b9">10]</ref>, another network is trained for Pedestrian and Cyclist, we first pre-train the network with all training images for the stereo network and then apply fine-tune with 3D box annotation for both branches because only about 1/3 images have annotations of these two objects. Implementation Details For constructing plane-sweep volume, the image feature map is shrunk to 32D and downsampled by 4 for both left and right images. Then by reprojection and concatenation, we construct the volume of shape (W I /4, H I /4, D I /4, 64), where the image size is (W I = 1248, H I = 384) and the number of depth is D I = 192. It is followed by one 3D hourglass module <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">34]</ref> and extra 3D convolutions to get the matching cost volume of shape (W I /4, H I /4, D I /4, 1). Then interpolation is used to upsample this volume to fit the image size.</p><p>To construct 3D geometric volume, We discretize the region in range Other implementation details and the network architecture are included in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>We give comparison with state-of-the-art 3D detectors in <ref type="table" target="#tab_1">Tables 1 and 2</ref>. Without bells and whistles, our approach outperforms all other image-based methods on 3D and BEV object detection. We note that Pseudo-LiDARs <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b58">58]</ref> is with pre-trained PSMNet <ref type="bibr" target="#b3">[4]</ref> on a large-scale synthetic scene flow dataset <ref type="bibr" target="#b33">[33]</ref> (with 30,000+ pairs of stereo images and dense disparity maps) for stereo matching. Stereo R-CNN <ref type="bibr" target="#b27">[27]</ref> uses ImageNet pre-trained ResNet-101 as backbone and has input images of resolution 600 × 2000.</p><p>Differently, our model is trained from scratch only on these 7K training data with input of resolution 384 × 1248. Also, Pseudo-LiDARs <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b58">58]</ref> approaches apply two independent networks including several LiDAR-based detectors, while ours is just one unified network.</p><p>DSGN without explicitly learning 2D boxes surpasses those applying strong 2D detectors based on ResNet-101 <ref type="bibr" target="#b27">[27]</ref> or DenseNet-121 <ref type="bibr" target="#b1">[2]</ref>. It naturally achieves duplicate removal by non-maximum suppression (NMS) in 3D space, which coincides with the common belief that there is no collision between regular objects.</p><p>More intriguingly, as shown in <ref type="table" target="#tab_1">Table 1</ref>, DSGN even achieves comparable performance on BEV detection and better performance on 3D detection on KITTI easy regime with MV3D <ref type="bibr" target="#b8">[9]</ref> (with LiDAR input only) -a classic LiDAR-based 3D object detector, for the first time. This result demonstrates a promising future application at least in the scenario of low-speed autonomous driving.</p><p>The above comparison manifests the effectiveness of 3D geometric volume, which serves as a link between 2D im-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Ablation study of 3D Volume Construction</head><p>One of the main obstacles to construct an effective 3D geometric representation is the appropriate way of learning 3D geometry. We therefore investigate the effect of following three key components to construct a 3D volume.</p><p>Input Data Monocular-based 3D volume only has the potential to learn the correspondence between 2D and 3D feature, while stereo-based 3D volume can learn extra 2D fea-ture correspondence for pixel-correspondence constraint.</p><p>Constructing 3D Volume One straightforward solution to construct 3D volume is by directly projecting the image feature to 3D voxel grid <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">40]</ref> (denoted as IMG→3DV). Another solution in <ref type="figure">Figure 3</ref> transforms plane-sweep volume or disparity-based cost volume to 3D volume, which provides a natural way to impose pixel-correspondence constraint along the projection ray in camera frustum (denoted as IMG→(PS)CV→3DV).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervising Depth</head><p>Supervised with or without the point cloud data, the network learns the depth explicitly or implicitly. One way is to supervise the voxel occupancy of 3D grid by ground-truth point cloud using binary cross-entropy loss. The second is to supervise depth on the plane-sweep cost volume as explained in Section 3.3. For fair comparison, the models IMG→3DV and IMG→(PS)CV→3DV have the same parameters by adding the same 3D hourglass module for the model IMG→3DV. In addition, several important facts can be revealed from  <ref type="table">Table 3</ref>. Ablation study of depth encoded approaches. "PSCV" and "3DV" with "Supervision" header represent that the constraint is imposed in (plane-sweep) cost volume and 3D volume, respectively. The results are evaluated in moderate level. <ref type="table">Table 3</ref> and are explained in the following.</p><p>Supervision of point cloud is important. The approaches under the supervision of the LiDAR point cloud consistently perform better than those without supervision, which demonstrates the importance of 3D geometry for imagebased approaches.</p><p>Stereo-based approaches work much better than monocular ones under supervision. The discrepancy between stereo and monocular approaches indicates that direct learning of 3D geometry from semantic cues is a quite difficult problem. In contrast, image-based approaches without supervision make these two lines yield similar performance, which indicates that supervision only by 3D bounding boxes is insufficient for learning of 3D geometry.</p><p>Plane-sweep volume is a more suitable representation for 3D structure. Plane-sweep cost volume (54.27 AP) performs better than disparity-based cost volume (45.89 AP). It shows that balanced feature mapping is important during the transformation to 3D volume.</p><p>Plane-sweep volume, as an intermediate encoder, more effectively contains depth information. The inconsistency between IMG→PSCV→3DV and IMG→3DV manifests that plane-sweep volume as the intermediate representation can effectively help learning of depth information. The observation explains that the soft arg-min operation encourages the model to pick a single depth plane per pixel along the projection ray, which shares the same spirit as the assumption that only one depth-value is true for each pixel. Another reason can be that PSCV and 3DV have different matching densities -PSCV intermediately imposes the dense pixel correspondence over all image pixels. In contrast, only the left-right pixel pairs through the voxel centers are matched on 3DV. From above comparison of volume construction, we observe that the three key facts affect the performance of computation pipelines. The understanding and recognition of how to construct a suitable 3D volume is still at the very early stage. More study is expected to reach comprehensive understanding of the volume construction from the multi-  <ref type="table">Table 4</ref>. Influence on depth estimation, evaluated on KITTI val images. PSMNet-PSV* is a variant of PSMNet <ref type="bibr" target="#b3">[4]</ref>, which uses one 3D hourglass module instead of three of them for refinement considering limited memory space and takes the plane-sweep approach to construct cost volume.</p><p>view images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Influence on Stereo Matching</head><p>We conduct experiments for investigating the influence of depth estimation, which is evaluated on KITTI val set following <ref type="bibr" target="#b47">[47]</ref>. The average and median value of absolute depth estimation errors within the pre-defined range of [z min , z max ] is shown in <ref type="table">Table 4</ref>. A natural baseline for our approach is PSMNet-PSV* modified from PSMNet <ref type="bibr" target="#b3">[4]</ref> whose 2D feature extractor takes 0.041s while ours takes 0.113s. Trained with depth estimation branch only, DSGN performs slightly better than PSMNet-PSV* with the same training pipeline in depth estimation. For joint training of both tasks, both approaches suffer from larger and similar depth error (0.5586 meter for DSGN vs. 0.5606 meter for PSMNet-PSV*). Differently, DSGN outperforms the alternatives by 7.86 AP on 3D object detection and 6.34 AP on BEV detection. The comparison indicates that our 2D network extracts better high-level semantic features for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a new 3D object detector on binocular images. It shows that an end-to-end stereo-based 3D object detection is feasible and effective. Our unified network encodes 3D geometry via transforming the plane-sweep volume to a 3D geometric one. Thus, it is able to learn highquality geometric structure features for 3D objects on the 3D volume. The joint training lets the network learn both pixeland high-level features for the important tasks of stereo correspondence and 3D object detection.</p><p>Without bells and whistles, our one-stage approach outperforms other image-based approaches and even achieves comparable performance with a few LiDAR-based approaches on 3D object detection. The ablation study investigates several key components for training 3D volume in <ref type="table">Table 3</ref>. Although the improvement is clear and explained, our understanding of how the 3D volume transformation works will be further explored in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. More Experiments</head><p>Correlation between stereo (depth) and 3D object detection accuracy. Following KITTI stereo metric, we consider a pixel as being correctly estimated if its depth error is less than an outlier thresh. The percentage of non-outlier pixels inside the object box is deemed as depth estimation precision.</p><p>With several outlier threshes (0.1, 0.3, 0.5, 1.0, 2.0 meters), scatter plots of stereo and detection precision are drawn. We observed that when outlier thresh is 0.3 meter, the strongest linear correlation is yielded.  <ref type="table">Table 5</ref>. Pearson's correlation coefficients (PCC) for a set of outlier threshes between depth estimation precision and detection accuracy. Outlier thresh= 0.3m yields the strongest linear correlation. <ref type="figure" target="#fig_5">Figure 4</ref> shows the scatter plots with the outlier thresh 0.3m and 0.1m. Quite a few predictions get over 0.7 detected precision within a certain range of depth estimation error. This reveals that the end-to-end network gives rise to its ability to detect 3D objects even with a larger depth estimation error. The following 3D detector enables compensation for the stereo depth estimation error by 3D location regression with back-propagation.  Relationship between distance and detection accuracy. <ref type="figure" target="#fig_6">Figure 5</ref> illustrates, as distance increases, all detection accuracy indicators have a shrinking tendency. The average accuracy maintains 80%+ within 25 meters. In all indicators, 3D AP decreases the fastest, followed by BEV AP and last 2D AP. This observation suggests that the 3D detection accuracy is determined by the BEV location precision beyond 20 meters. Influence of different features for 3D geometry. We discuss the efficiency of different geometric representations for volumetric structure. Most depth prediction approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref> apply the strategy of "depth classification" (such as cost volume) instead of "depth regression". Thus, we have several choices for encoding the depth information of cost volume into a 3D volume. The intuitive one is to use a 3D voxel occupancy (denoted by "Occupancy"). An advanced version is by keeping the probability of voxel occupancy (denoted by "Probability"). They both have explicit meaning for 3D geometry and can be easily visualized. Another one is by using the last feature map for cost volume as geometric embedding for 3D volume (denoted by "Last Features").  <ref type="table">Table 6</ref>. Ablation study on 3D geometric representation. "Occupancy" indicates only using binary feature for 3D volume. It is 1 for voxel of minimum cost along the projection ray and is 0 otherwise. "Probability" denotes keeping the probability of voxel occupancy instead of quantizing it to 0 or 1. "Last Features" represents transforming the last features of cost volume to 3D volume. <ref type="table">Table 6</ref> reveals the performance gap between "Occupancy" / "Probability". "Last Features" indicates the latent feature embedding (64D) that enables the network to extract more 3D latent geometric information and even semantic cues than the explicit voxel occupancy. It aids learning of 3D structure.</p><formula xml:id="formula_12">Outlier thresh &gt; 2m &gt; 1m &gt; 0.5m &gt; 0.3m &gt; 0.</formula><p>Technical details We explore several technical details used in DSGN and discuss their importance in the pipeline in <ref type="table">Table 7</ref>. Joint optimization of bounding box regression improves accuracy (+4.80 AP) than the separable optimiza-  <ref type="table">Table 7</ref>. Ablation study evaluated in moderate level. "JOINT" indicates using joint optimization instead of separable optimization for bounding boxes regression. "IMG" denotes concatenating the mapped left image feature to 3DGV for retrieving more 2D semantics. "ATT (Attention)" represents concatenating the mapped image feature weighted by the corresponding depth probability. "Depth" indicates warping the final matching cost volume to 3DGV. "HG (Hourglass)" represents applying an hourglass module in 3DGV. "Flip" means using random horizontal flipping augmentation.</p><p>tion. The intermediate 3D volume representation enables the network to naturally retrieve image feature for more 2D semantic cues. However, 3DGV cannot directly benefit from the concatenation of mapped 32D image features and warped predicted cost volume. Instead, the image feature weighted by the depth probability achieves +1.01 AP gain. Further, Involving more computation by an extra hourglass module on 3D object detector and flip augmentation, DSGN finally achieves 54.27 AP on 3D object detection.</p><p>Pedestrian and Cyclist detection. The main challenges for detecting Pedestrian and Cyclist are the limited data (about only 1/3 of images are annotated) and the difficulty to estimate their poses in an image even for human. As a result, most image-based approaches yield poor performance or are not validated on Pedestrian and Cyclist. Since the evaluation metric is changed on the official KITTI leaderboard, We only report the available results from original papers and the KITTI leaderboard.</p><p>Experimental results in <ref type="table" target="#tab_8">Table 8</ref> shows that our approach achieves better results on Pedestrian but worse ones on Cyclist compared with PL: F-PointNet. We note that PL: F-PointNet used Scene Flow dataset <ref type="bibr" target="#b33">[33]</ref> to pre-train the stereo matching network. Besides, PL: F-PointNet achieves the best result on Pedestrian and the model PL: AVOD works best on Car and Cyclist. <ref type="table" target="#tab_9">Table 9</ref> shows the submitted results on the official KITTI leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. More Implementation Details</head><p>Network Architecture. We show the full network architecture in <ref type="table" target="#tab_1">Table 10</ref>, including the networks for 2D feature extraction, constructing plane-sweep volume and 3D geometric volume, stereo matching and 3D object detection. Implementation Details of 3D Object Detector. Given the feature map F on bird's eye view, we put four anchors of different orientation angles (0, π/2, π, 3π/2) on all locations of F. The box sizes of pre-defined anchors used for respectively Car, Pedestrian, Cyclist are (h A = 1.56, w A = 1.6, l A = 3.9), (h A = 1.73, w A = 0.6, l A = 0.8), and (h A = 1.73, w A = 0.6, l A = 1.76).</p><p>The horizontal coordinate (x A , z A ) of each anchor lies on the center of each grid in bird's eye view and its center along the vertical direction locates on y A = 0.825 for Car and y A = 0.74 for Pedestrian and Cyclist. We set γ = 1 for Car and γ = 5 for Pedestrian and Cyclist for balancing the positive and negative samples. The classification head of 3D object detector is initialized following RetinaNet <ref type="bibr" target="#b31">[31]</ref>. NMS with IoU threshold 0.6 is applied to filter out the predicted boxes on bird's eye view.</p><p>Implementation Details of Differentiable Warping from PSV to 3DGV. Let U ∈ R H I ×W I ×D I ×C be the last feature map of PSV, where C is the channel size of features. We first construct a pre-defined 3D volume ∈ R H V ×W V ×D V ×3 to store the center coordinate (x, y, z) of each voxel in 3D space (Section 4.1). Then we get the projected pixel coordinate (u, v) by multiplying the projection matrix. z is directly concatenated to pixel coordinate to get (u, v, z) in camera frustum space.</p><p>As a result, we get a coordinate volume ∈ R H V ×W V ×D V ×3 , which stores the mapped coordinates in camera frustum space. By trilinear interpolation, we fetch the corresponding feature of U at the projected coordinates to construct the 3D volume V ∈ R H V ×W V ×D V ×C , i.e., 3D geometric volume. We ignore the projected coordinates outside the image by setting these voxel features to 0. In backward operations, the gradient is passed and computed using the same coordinate volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Future Work</head><p>More further studies on stereo-based 3D object detection are recommended here. The Gap with state-of-the-art LiDAR-based approaches. Although our approach achieves comparable performance with some LiDAR-based approaches on 3D object detection, there remains a large gap with state-ofthe-art LiDAR-based approaches <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b52">52]</ref>. Besides, an obvious problem is the accuracy gap on bird's eye view (BEV) detection. As shown in the table of main results, there is almost 12 AP gap on the moderate and hard level in BEV detection.</p><p>One possible solution is high-resolution stereo matching <ref type="bibr" target="#b53">[53]</ref>, which can help obtain more accurate depth information to increase the robustness for highly occluded, truncated and far objects.  3D Volume Construction. <ref type="table">Table 3</ref> shows basic comparison of volume construction in DSGN. We expect a more in-depth analysis of the volume construction from multiview or binocular images, which serves as an essential component design for 3D object understanding. Besides, the effectiveness of 3D volume construction methods still requires more investigation since it needs to balance and provide both depth information and semantic information. Computation Bottleneck. The computation bottleneck of DSGN locates on the computation of 3D convolutions for computing cost volume. Recent stereo matching work <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b48">48]</ref> focused on accelerating the computation of cost volume. Another significant aspect of constructing cost volume is that current cost volume <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4]</ref> is designed for regressing disparity but not depth. Further research might explore more efficient feature encoding for the plane-sweep cost volume. Network Architecture Design. There is a trade-off between stereo matching network and 3D detection network for balancing the feature extraction of pixel-and high-level features, which can be conducted by recent popular Network Architecture Search (NAS). Application on Low-speed Scenario.</p><p>Our approach shows comparable performance with the LiDAR-based approach on 3D and BEV detection in the close range in the KITTI easy set. Most importantly, it is affordable even with one strong GPU Tesla V100 ($11,458 (USD)) compared with the price of a 64-beam LiDAR $75,000 <ref type="bibr" target="#b58">[58]</ref>. It is a promising application of image-based autonomous driving system for low-speed scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Qualitative Results</head><p>We provide a video demo 1 for visualization of our approach, which shows both the detected 3D boxes on front view and bird's eye view. The ground-truth LiDAR point cloud is shown on bird's eye view. The detection results are obtained by DSGN trained on KITTI training split only. The unit of the depth map is meter.</p><p>Some noise observed in the predicted depth map is mainly caused by the implementation details. (1) Noise in the near and far part: 3D volumes are constructed in <ref type="bibr">[2, 40.4]</ref> (meters). (2) Noise and large white zone in the higher region (&gt;3m): The stereo branch is trained with a sparse GT depth map (64 lines around [-1,3 (meters) along the gravitational z-axis, captured by a 64-ray LiDAR). Plane-Sweep Volume</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv3Ds</head><p>Conv3Ds</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv3Ds</head><p>Bird's Eye View </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>DSGN jointly estimates depth and detects 3D objects from a stereo image pair. It intermediately generates a planesweep volume and 3D geometric volume to represent 3D structure in two different 3D space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Shared</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Overview of Deep Stereo Geometry Network (DSGN). The whole neural network consists of four components. (a) A 2D image feature extractor for capture of both pixeland high-level feature. (b) Constructing the plane-sweep volume and 3D geometric volume. (c) Depth Estimation on the plane-sweep volume. (d) 3D object detection on 3D geometric volume.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>−30.4, 30.4] × [−1, 3] × [2, 40.4] (meters) to a 3D voxel occupancy grid of size (W V = 300, H V = 20, D V = 192) along the right (X), down (Y ) and front (Z) directions in camera's view. 3D geometric volume is formed by warping the last feature map of PSV. Each voxel is a cube of size (0.2, 0.2, 0.2) (meter).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>BEV Detection precision versus depth precision for all predicted boxes for Car on KITTI val set. Only TPs with IoU&gt; 0.01 and score &gt; 0.1 are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Detection accuracy versus distance. We separate the range [0, 40] (meters) into 8 intervals, each with 5 meters. All evaluations are conducted within each interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of main results on KITTI test set (official KITTI leaderboard). The results are evaluated using new evaluation metric on the KITTI leaderboard. Several methods undergoing old evaluation are not available on the leaderboard. PL/PL++* uses extra Scene Flow dataset to pre-train the stereo matching network and Stereo R-CNN* uses ImageNet pre-trained model.</figDesc><table><row><cell>Modality</cell><cell>Method</cell><cell cols="3">3D Detection AP (%) Easy Moderate Hard</cell><cell cols="3">BEV Detection AP (%) Easy Moderate Hard</cell><cell cols="3">2D Detection AP (%) Easy Moderate Hard</cell></row><row><cell>LiDAR</cell><cell>MV3D (LiDAR) [9]</cell><cell>68.35</cell><cell>54.54</cell><cell>49.16</cell><cell>86.49</cell><cell>78.98</cell><cell>72.23</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>OFT-Net [40]</cell><cell>1.61</cell><cell>1.32</cell><cell>1.00</cell><cell>7.16</cell><cell>5.69</cell><cell>4.61</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mono</cell><cell>MonoGRNet [38] M3D-RPN [2]</cell><cell>9.61 14.76</cell><cell>5.74 9.71</cell><cell>4.25 7.42</cell><cell>18.19 21.02</cell><cell>11.17 13.67</cell><cell>8.73 10.23</cell><cell>88.65 89.04</cell><cell>77.94 85.08</cell><cell>63.31 69.26</cell></row><row><cell></cell><cell>AM3D [32]</cell><cell>16.50</cell><cell>10.74</cell><cell>9.52</cell><cell>25.03</cell><cell>17.32</cell><cell>14.91</cell><cell>92.55</cell><cell>88.71</cell><cell>77.78</cell></row><row><cell></cell><cell>3DOP [7]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>93.04</cell><cell>88.64</cell><cell>79.10</cell></row><row><cell>Stereo</cell><cell>Stereo R-CNN* [27] PL: AVOD* [47]</cell><cell>47.58 54.53</cell><cell>30.23 34.05</cell><cell>23.72 28.25</cell><cell>61.92 67.30</cell><cell>41.31 45.00</cell><cell>33.42 38.40</cell><cell>93.98 85.40</cell><cell>85.98 67.79</cell><cell>71.25 58.50</cell></row><row><cell></cell><cell cols="2">PL++: P-RCNN* [58] 61.11</cell><cell>42.43</cell><cell>36.99</cell><cell>78.31</cell><cell>58.01</cell><cell>51.25</cell><cell>94.46</cell><cell>82.90</cell><cell>75.45</cell></row><row><cell></cell><cell>DSGN (Ours)</cell><cell>73.50</cell><cell>52.18</cell><cell>45.14</cell><cell>82.90</cell><cell>65.05</cell><cell>56.60</cell><cell>95.53</cell><cell>86.43</cell><cell>78.75</cell></row><row><cell>Modality</cell><cell>Method</cell><cell cols="3">3D Detection AP (%) Easy Moderate Hard</cell><cell cols="3">BEV Detection AP (%) Easy Moderate Hard</cell><cell cols="3">2D Detection AP (%) Easy Moderate Hard</cell></row><row><cell>LiDAR</cell><cell>MV3D (LiDAR) [9]</cell><cell>71.29</cell><cell>56.60</cell><cell>55.30</cell><cell>86.18</cell><cell>77.32</cell><cell>76.33</cell><cell>88.41</cell><cell>87.76</cell><cell>79.90</cell></row><row><cell></cell><cell>OFT-Net [40]</cell><cell>4.07</cell><cell>3.27</cell><cell>3.29</cell><cell>11.06</cell><cell>8.79</cell><cell>8.91</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mono</cell><cell>MonoGRNet [38] M3D-RPN [2]</cell><cell>13.88 20.27</cell><cell>10.19 17.06</cell><cell>7.62 15.21</cell><cell>43.75 25.94</cell><cell>28.39 21.18</cell><cell>23.87 17.90</cell><cell>-90.24</cell><cell>78.14 83.67</cell><cell>-67.69</cell></row><row><cell></cell><cell>AM3D [32]</cell><cell>32.23</cell><cell>21.09</cell><cell>17.26</cell><cell>43.75</cell><cell>28.39</cell><cell>23.87</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>MLF [50]</cell><cell>-</cell><cell>9.80</cell><cell>-</cell><cell>-</cell><cell>19.54</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>3DOP [7]</cell><cell>6.55</cell><cell>5.07</cell><cell>4.10</cell><cell>12.63</cell><cell>9.49</cell><cell>7.59</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Triangulation [39]</cell><cell>18.15</cell><cell>14.26</cell><cell>13.72</cell><cell>29.22</cell><cell>21.88</cell><cell>18.83</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Stereo R-CNN* [27]</cell><cell>54.1</cell><cell>36.7</cell><cell>31.1</cell><cell>68.5</cell><cell>48.3</cell><cell>41.5</cell><cell>98.73</cell><cell>88.48</cell><cell>71.26</cell></row><row><cell>Stereo</cell><cell>PL: F-PointNet* [47]</cell><cell>59.4</cell><cell>39.8</cell><cell>33.5</cell><cell>72.8</cell><cell>51.8</cell><cell>33.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PL: AVOD* [47]</cell><cell>61.9</cell><cell>45.3</cell><cell>39.0</cell><cell>74.9</cell><cell>56.8</cell><cell>49.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PL++: AVOD* [58]</cell><cell>63.2</cell><cell>46.8</cell><cell>39.8</cell><cell>77.0</cell><cell>63.7</cell><cell>56.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PL++: PIXOR* [58]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.7</cell><cell>61.1</cell><cell>54.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PL++: P-RCNN* [58]</cell><cell>67.9</cell><cell>50.1</cell><cell>45.3</cell><cell>82.0</cell><cell>64.0</cell><cell>57.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DSGN (Ours)</cell><cell>72.31</cell><cell>54.27</cell><cell>47.71</cell><cell>83.24</cell><cell>63.91</cell><cell>57.83</cell><cell>89.25</cell><cell>83.59</cell><cell>78.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of main results on KITTI val set. As described in Section 4, we use original KITTI evaluation metric here. PL/PL++* uses extra Scene Flow dataset to pre-train the stereo matching network and Stereo R-CNN* uses ImageNet pre-trained model. ages and 3D space by combining the depth information and semantic feature.</figDesc><table><row><cell>Inference Time On a NVIDIA Tesla V100 GPU, the in-</cell></row><row><cell>ference time of DSGN for one image pair is 0.682s on av-</cell></row><row><cell>erage, where 2D feature extraction for left and right images</cell></row><row><cell>takes 0.113s, constructing the plane-sweep volume and 3D</cell></row><row><cell>geometric volume takes 0.285s, and 3D object detection on</cell></row><row><cell>3D geometric volume takes 0.284s. The computation bot-</cell></row><row><cell>tleneck of DSGN lies on 3D convolution layers.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>JOINT IMG ATT Depth HG Flip AP 3D /AP BEV /AP 2D 40.71 / 53.71 / 76.11 45.51 / 56.65 / 78.31 44.79 / 56.24 / 81.58 46.52 / 57.44 / 82.41 45.79 / 56.89 / 78.49 51.73 / 61.74 / 83.6 54.27 / 63.91 / 83.59</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Comparison of results for Pedestrian and Cyclist on KITTI val set. PL: F-PointNet* uses extra Scene Flow dataset to pretrain the stereo matching network.</figDesc><table><row><cell>Modality</cell><cell>Method</cell><cell cols="3">3D Detection AP (%) Easy Moderate Hard</cell><cell cols="3">BEV Detection AP (%) Easy Moderate Hard</cell><cell cols="3">2D Detection AP (%) Easy Moderate Hard</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pedestrian</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mono</cell><cell>M3D-RPN [2]</cell><cell>-</cell><cell>11.09</cell><cell>-</cell><cell>-</cell><cell>11.53</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Stereo</cell><cell>PL: F-PointNet* [47] DSGN (ours)</cell><cell>33.8 40.16</cell><cell>27.4 33.85</cell><cell>24.0 29.43</cell><cell>41.3 47.92</cell><cell>34.9 41.15</cell><cell>30.1 36.08</cell><cell>-59.06</cell><cell>-54.00</cell><cell>-49.65</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cyclist</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mono</cell><cell>M3D-RPN [2]</cell><cell>-</cell><cell>2.81</cell><cell>-</cell><cell>-</cell><cell>3.61</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Stereo</cell><cell>PL: F-PointNet* [47] DSGN (ours)</cell><cell>41.3 37.87</cell><cell>25.2 24.27</cell><cell>24.9 23.15</cell><cell>47.6 41.86</cell><cell>29.9 25.98</cell><cell>27.0 24.87</cell><cell>-49.38</cell><cell>-33.97</cell><cell>-32.40</cell></row><row><cell>Modality</cell><cell>Method</cell><cell cols="3">3D Detection AP (%) Easy Moderate Hard</cell><cell cols="3">BEV Detection AP (%) Easy Moderate Hard</cell><cell cols="3">2D Detection AP (%) Easy Moderate Hard</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pedestrian</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mono</cell><cell>M3D-RPN [2]</cell><cell>4.92</cell><cell>3.48</cell><cell>2.94</cell><cell>5.56</cell><cell>4.05</cell><cell>3.29</cell><cell>56.64</cell><cell>41.46</cell><cell>37.31</cell></row><row><cell>Stereo</cell><cell>RT3DStereo [24] DSGN (ours)</cell><cell>3.28 20.53</cell><cell>2.45 15.55</cell><cell>2.35 14.15</cell><cell>4.72 26.61</cell><cell>3.65 20.75</cell><cell>3.00 18.86</cell><cell>41.12 49.28</cell><cell>29.30 39.93</cell><cell>25.25 38.13</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cyclist</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mono</cell><cell>M3D-RPN [2]</cell><cell>0.94</cell><cell>0.65</cell><cell>0.47</cell><cell>1.25</cell><cell>0.81</cell><cell>0.78</cell><cell>61.54</cell><cell>41.54</cell><cell>35.23</cell></row><row><cell>Stereo</cell><cell>RT3DStereo [24] DSGN (ours)</cell><cell>5.29 27.76</cell><cell>3.37 18.17</cell><cell>2.57 16.21</cell><cell>7.03 31.23</cell><cell>4.10 21.04</cell><cell>3.88 18.93</cell><cell>19.58 49.10</cell><cell>12.96 35.15</cell><cell>11.47 31.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Comparison of results for Pedestrian and Cyclist on KITTI test set (official KITTI Leaderboard).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc></figDesc><table /><note>Full network architecture of DSGN. The color of the table highlights different components.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.youtube.com/watch?v=u6mQW89wBbo</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large-scale data for multiple-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Rasmus Ramsbøl Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders Bjorholm</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="153" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Point-based multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9775" to="9784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A space-sweep approach to true multiimage matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="358" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wukui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepmvs: Learning multi-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Han</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2821" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dpsnet: end-to-end deep plane sweep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Gon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Xiaoming Liu, and Daniel Morris. Depth coefficients for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Saif Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Surfacenet: An end-to-end 3d neural network for multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2307" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Realtime 3D Object Detection for Automated Driving Using Stereo Vision and Semantic Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Knigshof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><forename type="middle">Ole</forename><surname>Salscheider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Conf. Intelligent Transportation Systems, Auckland</title>
		<meeting>IEEE Intl. Conf. Intelligent Transportation Systems, Auckland</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
		<respStmt>
			<orgName>NZ</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rui Hu, and Raquel Urtasun. Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2811" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8851" to="8858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Triangulation learning network: from monocular to stereo 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Edgestereo: A context integrated residual pyramid network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Anytime stereo image depth estimation on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5893" to="5900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection with pseudo-lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hierarchical deep stereo matching on highresolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Manela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Happold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Segstereo: Exploiting semantic information for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="636" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="767" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6044" to="6053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Geoff Pleiss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<imprint>
			<pubPlace>Bharath Hariharan, Mark Campbell, and Kilian</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ga-net: Guided aggregation net for endto-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
