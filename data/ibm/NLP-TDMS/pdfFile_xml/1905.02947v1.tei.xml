<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<email>sporia@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">NTU</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">CIC</orgName>
								<orgName type="institution">Instituto Politécnico Nacional</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<email>mihalcea@umich.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cs.cmu.edu</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion is intrinsic to humans and consequently emotion understanding is a key part of human-like artificial intelligence (AI). Emotion recognition in conversation (ERC) is becoming increasingly popular as a new research frontier in natural language processing (NLP) due to its ability to mine opinions from the plethora of publicly available conversational data in platforms such as Facebook, Youtube, Reddit, Twitter, and others. Moreover, it has potential applications in health-care systems (as a tool for psychological analysis), education (understanding student frustration) and more. Additionally, ERC is also extremely important for generating emotion-aware dialogues that require an understanding of the user's emotions. Catering to these needs calls for effective and scalable conversational emotion-recognition algorithms. However, it is a strenuous problem to solve because of several research challenges. In this paper, we discuss these challenges and shed light on the recent research in this field. We also describe the drawbacks of these approaches and discuss the reasons why they fail to successfully overcome the research challenges in ERC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Emotion is often defined as an individual's mental state associated with thoughts, feelings and behaviour. Stoics like Cicero organized emotions into four categories -metus (fear), aegritudo (pain), libido (lust) and laetitia (pleasure). Later, evolutionary theory of emotion were initiated in the late 19th century by Charles Darwin <ref type="bibr" target="#b0">[1]</ref>. He hypothesized that emotions evolved through natural selection and, hence, have crossculturally universal counterparts. In recent times, Plutchik <ref type="bibr" target="#b1">[2]</ref> categorized emotion into eight primary types, visualized by the wheel of emotions ( <ref type="figure" target="#fig_2">Fig. 4)</ref>. Further, Ekman <ref type="bibr" target="#b2">[3]</ref> argued the correlation between emotion and facial expression.</p><p>Natural language is often indicative of one's emotion. Hence, emotion recognition has been enjoying popularity in the field of NLP <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, due to its widespread applications in opinion mining, recommender systems, health-care, and so on. Strapparava and Mihalcea <ref type="bibr" target="#b5">[6]</ref> addressed the task of emotion detection on news headlines. A number of emotion lexicons <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> have been developed to tackle the textual emotion recognition problem.</p><p>Only in the past few years has emotion recognition in conversation (ERC) gained attention from the NLP community <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> due to the increase of public availability of conversational data. ERC can be used to analyze conversations that take place on social media. It can also aid in analyzing conversations in real times, which can be instrumental in legal trials, interviews, e-health services and more.</p><p>Unlike vanilla emotion recognition of sentences/utterances, ERC ideally requires context modeling of the individual utterances. This context can be attributed to the preceding utterances, and relies on the temporal sequence of utterances. Compared to the recently published works on ERC <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, both lexicon-based <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref> and modern deep learning-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> vanilla emotion recognition approaches fail to work well on ERC datasets as these works ignore the conversation specific factors such as the presence of contextual cues, the temporality in speakers' turns, or speaker-specific information. <ref type="figure">Fig. 5a</ref> and <ref type="figure">Fig. 5b</ref> show an example where the same utterance changes its meaning depending on its preceding utterance.</p><p>Task definition -Given the transcript of a conversation along with speaker information of each constituent utterance, the ERC task aims to identify the emotion of each utterance from several pre-defined emotions. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates one such conversation between two people, where each utterance is labeled by the underlying emotion. Formally, given the input sequence of N number of utterances [(u 1 , p 1 ), (u 2 , p 2 ), . . . , (u N , p N )], where each utterance u i = [u i,1 , u i,2 , . . . , u i,T ] consists of T words u i,j and spoken by party p i , the task is to predict the emotion label e i of each utterance u i .</p><p>Controlling variables in conversations -Conversations are broadly categorized into two categories: task oriented and chit-chat (also called as non-task oriented). Both kinds of conversation are governed by different factors or pragmatics <ref type="bibr" target="#b14">[15]</ref>, such as topic, interlocutors' personality, argumentation logic, viewpoint, intent <ref type="bibr" target="#b15">[16]</ref>, and so on. <ref type="figure">Fig. 1</ref> depicts how these factors play out in a dyadic conversation. Firstly, topic (T opic) and interlocutor personality (P * ) always influence the conversation, irrespective of the time. A speaker makes up his/her mind (S t * ) about the reply (U t * ) based on the contextual preceding utterances (U &lt;t * ) from both speaker and listener, the previous utterance being the most important one since it usually makes the largest change in the joint task model S t+1 (for task-oriented conversations) or the speaker's emotional state (for chit-chat). Delving deeper, the pragmatic features, as explained by Hovy <ref type="bibr" target="#b14">[15]</ref>, like argumentation logic, interlocutor viewpoint, inter-personal relationship and dependency, situational awareness are encoded in speaker state (S t * ). Intent (I t * ) of the speaker is decided based on previous intent I t−2 * and speaker state S t * , as the interlocutor may change his/her intent based on the opponent's utterance and current situation. Then, the speaker formulates appropriate emotion E t * for the response based on the state S t * and intent I t * . Finally, the response U t * is produced based on the speaker state S t * , intent I t * and emotion E t * . We surmise that considering these factors would help representing the argument and discourse structure of the conversation, which leads to improved conversation understanding, including emotion recognition.</p><formula xml:id="formula_0">B S t A U t+1 B U t A P B P A Topic I t+1 B I t A U t−1 B U &lt; t A,B U &lt; t−1 A,B Person A Person B t t + 1 E t A E t+1 B I t−2 A I t−1 B Fig</formula><p>Early computational work on dialogue focused mostly on task-oriented cases, in which the overall conversational intent and step-by-step sub-goals played a large part <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Cohen and Levesque <ref type="bibr" target="#b18">[19]</ref> developed a model and logic to represent intentions and their connections to utterances, whose operators explicate the treatment of beliefs about the interlocutor's beliefs and vice versa, recursively. Emotion however played no role in this line of research. In more recent work, chatbots and chit-chat dialogue have become more prominent, in part due to the use of distributed (such as embedding) representations that do not readily support logical inference.</p><p>On conversational setting, K. D'Mello et al. <ref type="bibr" target="#b19">[20]</ref> and Yang et al. <ref type="bibr" target="#b21">[21]</ref> worked with small datasets with three and four emotion labels, respectively. This was followed by Phan et al. <ref type="bibr" target="#b22">[22]</ref>, where emotion detection on conversation transcript was attempted. Recently, several works <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref> have devised deep learning-based techniques for ERC. These works are crucial as we surmise an instrumental role of ERC in emotion-aware a.k.a. affective dialogue generation which has fallen within the topic of "text generation under pragmatics constriants" as proposed by Hovy <ref type="bibr" target="#b14">[15]</ref>. <ref type="figure">Fig. 3</ref> illustrates one such conversation between a human (user) and a medical chatbot (healthassistant). The assistant responds with emotion based on the user's input. Depending on whether the user suffered an injury earlier or not, the health-assistant responds with excitement (evoking urgency) or happiness (evoking relief).</p><p>As ERC is a new research field, outlining research challenges, available datasets, and benchmarks can potentially aid future research on ERC. In this paper, we aim to serve this purpose by discussing various factors that contribute to the emotion dynamics in a conversation. We surmise that this paper will not only help the researchers to better understand the challenges and recent works on ERC but also show possible future research directions. The rest of the paper is organized as follows: Section II presents the key research challenges; Section III and IV cover the datasets and recent progress in this field; finally Section V concludes the paper. Well I guess you aren't trying hard enough.</p><p>[ neutral ]</p><p>Its been three years. I have tried everything.</p><p>[ frustrated ]</p><p>Maybe you're not smart enough.</p><p>[ neutral ]</p><p>Just go out and keep trying. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RESEARCH CHALLENGES</head><p>Recent works on ERC, e.g., DialogueRNN <ref type="bibr" target="#b10">[11]</ref> or ICON <ref type="bibr" target="#b23">[23]</ref>, strive to address several key research challenges that make the task of ERC difficult to solve: a) Categorization of emotions: Emotion is defined using two type of models -categorical and dimensional. Categorical model classifies emotion into a fixed number of discrete categories. In contrast, dimensional model describes emotion as a point in a continuous multi-dimensional space.</p><p>In the categorical front, Plutchik <ref type="bibr" target="#b1">[2]</ref>'s wheel of emotions ( <ref type="figure" target="#fig_2">Fig. 4)</ref>  Ekman <ref type="bibr" target="#b2">[3]</ref> concludes six basic emotions -anger, disgust, fear, happiness, sadness and surprise.</p><p>Most dimensional categorization models <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref> adopt two dimensions -valence and arousal. Valence represents the degree of emotional positivity, and arousal represents the intensity of the emotion. In contrast with the categorical models, dimensional models map emotion into a continuous spectrum rather than hard categories. This enables easy and intuitive comparison of two emotional states using vector operations, whereas comparison is non-trivial for categorical models. As there are multiple categorization and dimensional taxonomies available, it is challenging to select one particular model for annotation. Choosing a simple categorization model e.g., Ekman's model has a major drawback as these models are unable to ground complex emotions. On the other hand, complex emotion models such as Plutchik's model make it very difficult for the annotators to discriminate between the related emotions, e.g., discerning anger from rage. Complex emotion models also increase the risk of obtaining a lower inter-annotator agreement.</p><p>The popular ERC dataset IEMOCAP <ref type="bibr" target="#b28">[27]</ref> adopted both categorical and dimensional models. However, newer ERC datasets like DailyDialogue <ref type="bibr" target="#b29">[28]</ref> have employed only categorical model due to its more intuitive nature. Most of the available datasets for emotion recognition in conversation adopted simple taxonomies, which are slight variants of Ekman's model. Each emotional utterance in the EmoContext dataset is labeled with one of the following emotions: happiness, sadness and anger. The majority of the utterances in EmoContext do not elicit any of these three emotions and are annotated with an extra label: others. Naturally, the inter-annotator agreement for the EmoContext dataset is higher due to its simplistic emotion taxonomy. However, the short context length and simple emotion taxonomy make ERC on this dataset less challenging. b) Basis of emotion annotation: Annotation with emotion labels is challenging as the label depends on the annotators perspective. Self-assessment by the interlocutors in a conversation is arguably the best way to annotate utterances. However, in practice it is unfeasible as real-time tagging of unscripted conversations will impact the conversation flow. Post-conversation self-annotation could be an option, but it has not been done yet.</p><p>As such, many ERC datasets <ref type="bibr" target="#b28">[27]</ref> are scripted and annotated by a group of people uninvolved with the script and conversation. The annotators are given the context of the utterances as prior knowledge for accurate annotation. Often pre-existing transcripts are annotated for quick turn-around, as in EmotionLines <ref type="bibr" target="#b9">[10]</ref>.</p><p>The annotators also need to be aware of the interlocutors perspective for situation-aware annotation. For example, the emotion behind the utterance "Lehman Brothers' stock is plummeting!!" depends on whether the speaker benefits from the crash. The annotators should be aware of the nature of association between the speaker and Lehman Brothers for accurate labeling. c) Conversational context modeling: Context is at the core of the NLP research. According to several recent studies <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b31">30]</ref>, contextual sentence and word embeddings can improve the performance of the state-of-the-art NLP systems by a significant margin.</p><p>The notion of context can vary from problem to problem. For example, while calculating word representations, the surrounding words carry contextual information. Likewise, to classify a sentence in a document, other neighboring sentences are considered as its context. In Poria et al. <ref type="bibr" target="#b32">[31]</ref>, surrounding utterances are treated as context and they experimentally show that contextual evidence indeed aids in classification.</p><p>Similarly in conversational emotion-detection, to determine the emotion of an utterance at time t, the preceding utterances at time &lt; t can be considered as its context. However, computing this context representation often exhibits major difficulties due to emotional dynamics. Emotional dynamics of conversations consists of two important aspects: self and inter-personal dependencies <ref type="bibr" target="#b33">[32]</ref>. selfdependency, also known as emotional inertia, deals with the aspect of emotional influence that speakers have on themselves during conversations <ref type="bibr" target="#b34">[33]</ref>. On the other hand, inter-personal dependencies relate to the emotional influences that the counterparts induce into a speaker. Conversely, during the course of a dialogue, speakers also tend to mirror their counterparts to build rapport <ref type="bibr" target="#b35">[34]</ref>. This phenomenon is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Here, P a is frustrated over her long term unemployment and seeks encouragement (u 1 , u 3 ). P b , however, is pre-occupied and replies sarcastically (u 4 ). This enrages P a to appropriate an angry response (u 6 ). In this dialogue, emotional inertia is evident in P b who does not deviate from his nonchalant behavior. P a , however, gets emotionally influenced by P b . Modeling self and inter-personal relationship and dependencies may also depend on the topic of the conversation as well as various other factors like argument structure, interlocutors' personality, intents, viewpoints on the conversation, attitude towards each other etc.. Hence, analyzing all these factors are key for a true self and inter-personal dependency modeling that can lead to enriched context understanding.</p><p>The contextual information can come from both local and distant conversational history. While the importance of local context is more obvious, as stated in recent works, distant context often plays a less important role in ERC. Distant contextual information is useful mostly in the scenarios when a speaker refers to earlier utterances spoken by any of the speakers in the conversational history.</p><p>The usefulness of context is more prevalent in classifying short utterances, like "yeah", "okay", "no", that can express different emotions depending on the context and discourse of the dialogue. The examples in <ref type="figure">Fig. 5a</ref> and <ref type="figure">Fig. 5b</ref> explain this phenomenon. The emotions expressed by the same utterance "Yeah" in both these examples differ from each other and can only be inferred from the context.</p><p>Finding contextualized conversational utterance representations is an active area of research. Leveraging such contextual clues is a difficult task. Memory networks, RNNs, and attention mechanisms have been used in previous works e.g., HRLCE or DialogueRNN, to grasp information from the context. d) Speaker specific modeling: Individuals have their own subtle way of expressing emotions. For instance, some individuals are more sarcastic than others. For such cases, the usage of certain words would vary depending on if they are being sarcastic. Let's consider this example, P a ∶ "The order has been cancelled.", P b ∶ "This is great!". If P b is a sarcastic person, then his response would express negative emotion to the order being canceled through the word great. On the other hand, P b 's response, great, could be taken literally if the canceled order is beneficial to P b (perhaps P b cannot afford the product he ordered). Since, necessary background information is often missing from the conversations, speaker profiling based on preceding utterances often yields improved results. e) Listener specific modeling: During a conversation, the listeners make up their mind about the speaker's utterance as it's spoken. However, there is no textual data on the listener's reaction to the speaker. A model must resort to visual modality to model the listener's facial expression to capture the listener's reaction. However, according to DialogueRNN, capturing listener reaction does not yield any improvement as the listener's subsequent utterance carries their reaction. Moreover, of the listener never speaks in a conversation, his/her reaction remains irrelevant. Nonetheless, listener modeling can be useful in the scenarios where continuous emotion recognition of every moment of the conversation is necessary, like audience reaction during a political speech, as opposed to emotion recognition of each utterance.</p><p>f) Presence of emotion shift: Due to emotional inertia, participants in a conversation tend to stick a particular emotional state, unless some external stimuli, usually the other participants, invoke a change. This is illustrated in <ref type="figure">Fig. 6</ref>, where Joey changes his emotion from neutral to anger due to the last utterance of Chandler, which was unexpected and rather shocking to Joey. This is a hard problem to solve, as the state-of-the-art ERC model, DialogueRNN is more accurate in emotion detection for the utterances without emotional shift or when the shift is to a similar emotion (e.g., from fear to sad). It is equally interesting and challenging to track the development, shift and dynamics of the participants' opinions on the topic of discussion in the conversation. g) Fine-grained emotion recognition: Fine-grained emotion recognition aims at recognizing emotion expressed on the explicit and implicit topics. It involves a deeper understanding of the topic of the conversation, interlocutor opinion and stand. For example, in <ref type="figure" target="#fig_5">Fig. 7</ref>, while both persons take a supportive stand for the government's bill, they use completely opposite emotions to express it. It is not possible for a vanilla emotion recognizer to understand the positive emotion of both the interlocutors on the aspect of government's bill. Only by interpreting Person 2's frustration about the opposition's protest against the bill can a classifier infer Person 2's support for the bill. On the other hand, even though Person 1 does not explicitly express his/her opinion on the opposition, from the discourse of the conversation, it can be inferred that Person 1 holds a negative opinion on the opposition. h) Multiparty conversation: In a multiparty conversation, more than two participants are involved. Naturally, emotion recognition in such conversations is more challenging in comparison with dyadic conversations due to the difficulty in tracking individual speaker states and handling co-references.</p><p>i) Presence of sarcasm: Sarcasm is a linguistic tool that uses irony to express contempt. An ERC system incapable of detecting sarcasm mostly fails to predict emotion of the sarcastic utterances correctly. Sarcasm detection in a conversation largely depends on the context and discourse of the conversation. For example, the utterance "The part where Obama signed it" can only be detected as sarcastic if we look at the previous utterance "What part of this would be   unconstitutional?". Sarcastic nature is also person dependent, which again warrants speaker profiling in the conversation. j) Emotion reasoning: The ability to reason is necessary for any explainable AI system. In the context of ERC, it is often desired to understand the cause of an expressed emotion by a speaker. As an example, we can refer to <ref type="figure" target="#fig_1">Fig. 2</ref>. An ideal ERC system, with the ability of emotion reasoning, should perceive the reason for P erson A 's anger, expressed in u 6 of <ref type="figure" target="#fig_1">Fig. 2</ref>. It is evident upon observation that this anger is caused by the persistent nonchalant behavior of P erson B . Readers should not conflate emotion reasoning with context modeling, which we discuss earlier in this section. Unlike context modeling, emotion reasoning does not only find the contextual utterances in conversational history that triggers the emotion of an utterance, but also determines the function of those contextual utterances on the target utterance. In <ref type="figure" target="#fig_1">Fig. 2</ref>, it is the indifference of P erson B , reflected by u 4 and u 5 , that makes P erson A angry. Similarly, in <ref type="figure">Fig. 6</ref>, Joey expresses anger once he ascertains Chandler's deception in the previous utterance. It is hard to define a taxonomy or tagset for emotion reasoning. At present, there is no available dataset which contains such rich annotations. Building such dataset would enable future dialogue systems to frame meaningful argumentation logic and discourse structure, taking one step closer to human-like conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASETS</head><p>In the last few years, emotion recognition in conversation has gained major research interest, mainly because of its potential application in dialogue systems to generate emotionaware and empathetic dialogues <ref type="bibr" target="#b11">[12]</ref>. The primary goal of ERC task is to label each utterance in the conversation with an emotion label. In this section, we discuss the publicly available ERC datasets as well as the shortcomings of these datasets. There are a few publicly available datasets for ERC -IEMO-CAP <ref type="bibr" target="#b28">[27]</ref>, SEMAINE <ref type="bibr" target="#b36">[35]</ref>, Emotionlines <ref type="bibr" target="#b9">[10]</ref>, MELD <ref type="bibr" target="#b37">[36]</ref>, DailyDialog <ref type="bibr" target="#b29">[28]</ref> and EmoContext <ref type="bibr" target="#b38">[37]</ref>. A detailed comparison of these datasets is drawn in <ref type="table" target="#tab_3">Table II</ref>. Out of these five datasets, IEMOCAP, SEMAINE and MELD are multimodal (containing acoustic, visual and textual information) and the remaining two are textual. Apart from SEMAINE dataset, rest of the  <ref type="table" target="#tab_3">Neutral  85572  6436  6530  1708  -Happiness/Joy  12885  2308  1710  648  4669  Surprise  1823  1636  1658  --Sadness  1150  1002  498  1084  5838  Anger  1022  1607  772  1103  5954  Disgust  353  361  338  --Fear  74  358</ref> 255  <ref type="bibr" target="#b0">1]</ref>) and power ([0, ∞)). We also show the emotion label distribution of these datasets in <ref type="table" target="#tab_3">Table I</ref>. In EmoContext dataset, an emotion label is assigned to only the last utterance of each dialogue. None of these datasets can be used for emotion reasoning as they lack necessary annotation details required for the reasoning task. Readers should also note that, all these datasets do not contain fine-grained and topic level emotion annotation.</p><formula xml:id="formula_1">- - Frustrated - - - 1849 - Excited - - - 1041 - Other - - - - 21960</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RECENT ADVANCES</head><p>In this section we give a brief introduction to the recent work on this topic. We also compare the approaches and report their drawbacks.</p><p>As depicted in <ref type="figure">Fig. 1</ref>, recognizing emotion of an utterance in a conversation primarily depends on these following three factors: 1) the utterance itself and its context defined by the interlocutors' preceding utterances in the conversation, as well as intent and the topic of the conversation, 2) the speaker's state comprising variables like personality and argumentation logic and, 3) emotions expressed in the preceding utterances.</p><p>Although, IEMOCAP and SEMAINE have been developed almost a decade ago, most of the works that used these two datasets did not consider the aforementioned factors.</p><p>a) Benchmarks and their drawbacks: Based on these factors, a number of approaches to address the ERC problem have been proposed recently. Conversational memory network (CMN), proposed by Hazarika et al. <ref type="bibr" target="#b39">[38]</ref> for dyadic dialogues, is one of the first ERC approaches that utilizes distinct memories for each speaker for speaker-specific context modeling. Later, Hazarika et al. <ref type="bibr" target="#b23">[23]</ref> improved upon this approach with interactive conversational memory network (ICON), which interconnects these memories to model self and inter-speaker emotional influence. None of these two methods actually exploit the speaker information of the target utterance for classification. This makes the model blind to speaker-specific nuances. Recently, Yeh et al. <ref type="bibr" target="#b8">[9]</ref> proposed an ERC method called Interaction-aware Attention Network (IANN) by leveraging inter-speaker relation modeling. Similar to ICON and CMN, IANN utilises distinct memories for each speaker. DialogueRNN <ref type="bibr" target="#b10">[11]</ref> aims to solve this issue by considering the speaker information of the target utterance and, further, modeling self and inter-speaker emotional influence with a hierarchical multi-stage RNN with attention mechanism. On both IEMOCAP and SEMAINE datasets, DialogueRNN outperformed <ref type="table" target="#tab_3">(Table III and Table IV</ref>) the other two approaches.</p><p>All of these models affirm that contextual history, modeling self and inter-speaker influence are beneficial to ERC (shown in <ref type="figure" target="#fig_6">Fig. 8 and Fig. 9</ref>). Further, DialogueRNN shows that the nearby utterances are generally more context rich and ERC performance improves when the future utterances, at time &gt; t, are available. This is indicated by <ref type="figure">Fig. 9</ref>, where DialogueRNN uses both past and future utterances as context with roughly the same frequency. Also, the distant utterances are used less frequently than the nearby utterances. On the other hand, CMN and ICON do not use future utterances as context at all. However, for real-time applications, systems cannot rely on future utterances. In such cases, CMN, ICON and DialogueRNN with fixed context window would be befitting.</p><p>All these networks, namely CMN, ICON, IANN and Di-alogueRNN, perform poorly on the utterances with emotion shift. In particular, the cases where the emotion of the target utterance differs from the previous utterance, DialogueRNN could only correctly predict 47.5% instances. This stands less as compared to the 69.2% success-rate that it achieves at the regions of no emotional-shift.</p><p>Among these three approaches, only DialogueRNN is capable of handling multiparty conversations on large scale. However, on the multiparty conversational dataset MELD, only a little performance improvement (shown in <ref type="table" target="#tab_9">Table V)</ref> is observed by DialogueRNN compared to bc-LSTM which depicts a future research direction on multiparty ERC. ICON and CMN are designed to detect emotions in dyadic dialogues. Adapting ICON and CMN to apply on multiparty conversational dataset MELD can cause scalability issue in situations when number speakers participating in a conversation in the test data is more than the training data.</p><p>Due to the sequential nature of the utterances in conversations, RNNs are used for context generation in the aforemen- <ref type="table" target="#tab_3">utterances  train  val  test  train  val  test  IEMOCAP  120  31  5810  1623  SEMAINE  63  32  4368  1430  EmotionLines  720  80  200  10561 1178  2764  MELD  1039  114  280  9989  1109  2610</ref>       <ref type="bibr" target="#b23">[23]</ref> as in our experiment, we disregard their contextual feature extraction and pre-processing part. and More details can be found in Majumder et al. <ref type="bibr" target="#b10">[11]</ref>. tioned models. However, there is ample room for improvement, as the RNN-based context representation methods perform poorly in grasping long distant contextual information.</p><formula xml:id="formula_2">Dataset # dialogues #</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEMOCAP</head><p>Recently, two shared tasks -EmotionX (co-located with SocialNLP workshop) and EmoContext 1 (co-located with Semeval 2019) have been organized to address the ERC problem. EmoContext shared task has garnered more than 500 participants, affirming the growing popularity of this research 1 https://www.humanizing-ai.com/emocontext.html field. Compared to other datasets, EmoContext dataset <ref type="bibr" target="#b38">[37]</ref> has very short conversations consisting only three utterances where the goal is to label the 3rd utterance as shown in <ref type="figure">Fig. 10</ref>. Emotion labels of the previous utterances are not present in the EmoContext dataset. The key works <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b38">37]</ref> on this dataset have mainly leveraged on context modeling using bc-LSTM architecture <ref type="bibr" target="#b32">[31]</ref> that encapsulates the temporal order of the utterances using an LSTM. A common trend can be noticed in these works, where traditional word embeddings, such as Glove <ref type="bibr" target="#b41">[40]</ref>, are combined with contextualized word   embeddings, such as ELMo <ref type="bibr" target="#b30">[29]</ref> to improve the performance. In <ref type="figure" target="#fig_8">Fig. 11</ref>, we depict the HRLCE framework, proposed by Huang et al. <ref type="bibr" target="#b40">[39]</ref>, that comprises of an utterance encoder and a context encoder that takes input from the utterance encoder. To represent each utterance, HRLCE utilizes ELMo <ref type="bibr" target="#b30">[29]</ref>, Glove <ref type="bibr" target="#b41">[40]</ref> and Deepmoji <ref type="bibr" target="#b42">[41]</ref>. The context encoder in HRLCE  adapts the bc-LSTM framework followed by a multi-head attention layer. Huang et al. <ref type="bibr" target="#b40">[39]</ref> applied HRLCE framework only on the EmoContext dataset. However, HRLCE can be easily adapted to apply on other ERC datasets. It should be noted that none of the works on the EmoContext dataset utilize speaker information. In fact, in our experiments, we found that DialogueRNN, which makes use of the speaker information, performs similar <ref type="table" target="#tab_3">(Table VI)</ref> to Bae et al. <ref type="bibr" target="#b24">[24]</ref>, Huang et al. <ref type="bibr" target="#b40">[39]</ref> and Chatterjee et al. <ref type="bibr" target="#b38">[37]</ref> on EmoContext dataset. One possible reason for this could be the presence of very short context history in the dataset that renders speaker information inconsequential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Emotion recognition in conversation has been gaining popularity among NLP researchers. In this work, we summarize the recent advances in ERC and highlight several key research challenges associated with this research area. Further, we point out how current work has partly addressed these challenges, while also presenting some shortcomings. Overall, we surmise that an effective emotion-shift recognition model and context encoder can yield significant performance improvement over chit-chat dialogue, and even improve some aspects of task-oriented dialogue. Moreover, challenges like topic-level speaker-specific emotion recognition, ERC on multiparty conversations, and conversational sarcasm detection can form new research directions. Additionally, fine-grained speaker-specific continuous emotion recognition may become of interest for the purpose of tracking emotions during long monologue. We believe that addressing each of the challenges outlined in this paper will not only enhance AI-enabled conversation understanding, but also improve the performance of dialogue systems by catering to affective information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>I</head><label></label><figDesc>don't think I can do this anymore. [ frustrated ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>[ neutral ] I am smart enough. I am really good at what I do. I just don't know how to make someone else see that.[anger] An abridged dialogue from the IEMOCAP dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Plutchik's wheel of emotion<ref type="bibr" target="#b1">[2]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Positive emotion expressed by the utterance Yeah.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Role of context in emotion recognition in conversation. Emotion shift of speakers in a dialogue in comparison with speaker's previous emotion. Red and blue colors are used to show the emotion shift of Joey and Chandler respectively..It is so frustrating to see the oppositions protesting against it.Person1Person2 I fully support this bill brought by the government.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fine-grained emotion understanding: an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Comparison of attention scores over utterance history of CMN and DialogueRNN. Higher attention value signifies more important contextual information. Note: Figure taken from Majumder et al. [11].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Histogram of ∆t = distance between the target utterance and its context utterance based on DialogueRNN's attention scores. Note: Figure taken from Majumder et al. [11]. I hate my girlfriend You got a girlfriend?! Yes An example of a 3-turn conversation extracted from Emo-Context dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 :</head><label>11</label><figDesc>HRLCE framework applied on the EmoContext dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>defines eight discrete primary emotion types, each of which has finer related subtypes. On the other hand,</figDesc><table><row><cell>User</cell><cell>Health Assistant</cell></row><row><cell>My head is aching</cell><cell></cell></row><row><cell>Frustrated</cell><cell></cell></row><row><cell></cell><cell>I'm sorry to hear that!</cell></row><row><cell></cell><cell>Sad</cell></row><row><cell></cell><cell>Did you suffer a head injury</cell></row><row><cell></cell><cell>earlier?</cell></row><row><cell></cell><cell>Neutral</cell></row><row><cell>No</cell><cell>Yes</cell></row><row><cell>Neutral</cell><cell>Neutral</cell></row><row><cell>Pleased to hear that Happy</cell><cell>Please, immediately see a nearby doctor Excited</cell></row><row><cell>Did you consume alcohol recently? Neutral</cell><cell>You might be concussed!</cell></row></table><note>Sad Fig. 3: Illustration of an affective conversation where the emotion depends on the context.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Label distribution statistics in different Emotion Recognition datasets.</figDesc><table><row><cell>datasets contains categorical emotion labels. In contrast, each</cell></row><row><cell>utterance of SEMAINE dataset is annotated with four real</cell></row><row><cell>valued affective attributes: valence ([−1, 1]), arousal ([−1, 1]),</cell></row><row><cell>expectancy ([−1,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II :</head><label>II</label><figDesc>Comparison among IEMOCAP, SEMAINE, EmotionLines, MELD and DailyDialog datasets.</figDesc><table><row><cell>Emotion</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>fru</cell><cell>neu</cell><cell>neu</cell><cell></cell></row><row><cell>CMN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell></row><row><cell>Turns</cell><cell>31</cell><cell>32</cell><cell>33</cell><cell>34</cell><cell>35</cell><cell>36</cell><cell>37</cell><cell>38</cell><cell>39</cell><cell>40</cell><cell>41</cell><cell>42</cell><cell>43</cell><cell></cell></row><row><cell>Speaker</cell><cell>PB</cell><cell>PB</cell><cell>PA</cell><cell>PB</cell><cell>PA</cell><cell>PB</cell><cell>PA</cell><cell>PB</cell><cell>PA</cell><cell>PB</cell><cell>PA</cell><cell>PB</cell><cell>PA</cell><cell>0.4</cell></row><row><cell>DialogueRNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2 0.0</cell></row><row><cell></cell><cell>Turn</cell><cell></cell><cell cols="2">Utterance</cell><cell cols="2">Emotion</cell><cell>Turn</cell><cell></cell><cell></cell><cell>Utterance</cell><cell></cell><cell></cell><cell>Emotion</cell><cell></cell></row><row><cell></cell><cell>41</cell><cell cols="3">PA: Because I paid it on time.</cell><cell>fru</cell><cell></cell><cell>42</cell><cell cols="5">PB: Yeah, since it was a computer glitch … you should just call back</cell><cell>neu</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Turn</cell><cell></cell><cell></cell><cell cols="2">Utterance</cell><cell></cell><cell></cell><cell cols="4">Emotion DialogueRNN CMN</cell><cell></cell></row><row><cell cols="3">Test Utterance:</cell><cell>44</cell><cell cols="6">PA: But if I call back, I have to go through this whole rigmarole again.</cell><cell>fru</cell><cell></cell><cell>fru</cell><cell>neu</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>29.86 57.14 53.83 34.33 40.14 61.17 52.44 46.15 50.09 62.99 55.75 48.92 48.18 memnet 25.72 33.53 55.53 61.77 58.12 52.84 59.32 55.39 51.50 58.30 67.20 59.00 55.72 55.10 bc-LSTM 29.17 34.43 57.14 60.87 54.17 51.81 57.06 56.73 51.17 57.95 67.19 58.92 55.21 54.95 bc-LSTM+Att 30.56 35.63 56.73 62.90 57.55 53.00 59.41 59.24 52.84 58.85 65.88 59.41 56.32 56.19 CMN 25.00 30.38 55.92 62.41 52.86 52.39 61.76 59.83 55.52 60.25 71.13 60.69 56.56 56.13 ICON 22.22 29.91 58.78 64.57 62.76 57.38 64.71 63.04 58.86 63.42 67.19 60.81 59.09 58.54 DialogueRNN 25.69 33.18 75.10 78.80 58.59 59.21 64.71 65.28 80.27 71.86 61.15 58.91 63.40 62.75</figDesc><table><row><cell></cell><cell>Happy</cell><cell>Sad</cell><cell>Neutral</cell><cell>Angry</cell><cell>Excited</cell><cell>Frustrated</cell><cell>Average(w)</cell></row><row><cell></cell><cell>Acc. F1</cell><cell>Acc. F1</cell><cell>Acc. F1</cell><cell>Acc. F1</cell><cell>Acc. F1</cell><cell>Acc. F1</cell><cell>Acc. F1</cell></row><row><cell>CNN</cell><cell>27.77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE III :</head><label>III</label><figDesc>Comparison between DialogueRNN and baseline methods on IEMOCAP dataset; bold font denotes the best performances. Average(w) = Weighted average. ICON results differ from the original paper</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison between DialogueRNN and baseline methods on SEMAINE dataset;Acc. = Accuracy, M AE = Mean Absolute Error, r = Pearson correlation coefficient; bold font denotes the best performances. More details can be found in Majumder et al.<ref type="bibr" target="#b10">[11]</ref>.</figDesc><table><row><cell>Modality</cell><cell cols="3">anger disgust fear</cell><cell cols="4">Emotions joy neutral sadness surprise w-avg.</cell></row><row><cell>CNN</cell><cell>34.49</cell><cell>8.22</cell><cell cols="2">3.74 49.39 74.88</cell><cell>21.05</cell><cell>45.45</cell><cell>55.02</cell></row><row><cell>bc-LSTM</cell><cell cols="4">42.06 21.69 7.75 54.31 71.63</cell><cell>26.92</cell><cell>48.15</cell><cell>56.44</cell></row><row><cell cols="2">DialogueRNN 40.59</cell><cell>2.04</cell><cell cols="2">8.93 50.27 75.75</cell><cell>24.19</cell><cell>49.38</cell><cell>57.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>Test-set F-score results of bc-LSTM and DialogueRNN for emotion classification in MELD. Note: w-avg denotes weightedaverage. text-CNN: CNN applied on text, contextual information were not used.</figDesc><table><row><cell>Distance between test</cell></row><row><cell>utterance and context</cell></row><row><cell>utterance with:</cell></row><row><cell>Highest attention</cell></row><row><cell>2 nd highest attention</cell></row><row><cell>8 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>HRLCE and DialogueRNN on the EmoContext dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The expression of the emotions in man and animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Darwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prodger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A psychoevolutionary theory of emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Plutchik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Information</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="529" to="553" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Facial expression and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">384</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Decision support with text-based emotion recognition: Deep learning for affective computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kratzwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feuerriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Prendinger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06397</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotion recognition on twitter: comparative study and training a unison model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Colneriĉ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1621474.1621487" />
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations, ser. SemEval &apos;07</title>
		<meeting>the 4th International Workshop on Semantic Evaluations, ser. SemEval &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
	<note>Semeval-2007 task 14: Affective text</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wordnet affect: an affective extension of wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valitutti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lrec</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text</title>
		<meeting>the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="26" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An interactionaware attention network for speech emotion recognition in spoken dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6685" to="6689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Emotionlines: An emotion corpus of multi-party conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Ku</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08379</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dialoguernn: An attentive rnn for emotion detection in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00405</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emotional chatting machine: Emotional conversation generation with internal and external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emotion recognition from text using semantic labels and separable mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on Asian language information processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="183" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emotion recognition from text based on automatically generated rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shaheen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>El-Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elbassuoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Data Mining Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating natural language under pragmatic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pragmatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="689" to="719" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clarifying intentions in dialogue: A corpus study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Schlöder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Computational Semantics</title>
		<meeting>the 11th International Conference on Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention, intentions, and the structure of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Sidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="204" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Planning english sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Appelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Appelt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speech acts and rationality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 23rd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1985" />
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Predicting affective states expressed through an emote-aloud procedure from autotutor&apos;s mixed-initiative dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Craig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graesser</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>I</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">J. Artificial Intelligence in Education</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="2006-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emotion classification using web blog corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/WIC/ACM International Conference on Web Intelligence (WI&apos;07)</title>
		<imprint>
			<date type="published" when="2007-11" />
			<biblScope unit="page" from="275" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple emotions detection in conversation transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/Y16-2006" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>the 30th Pacific Asia Conference on Language, Information and Computation<address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Icon: Interactive conversational memory network for multimodal emotion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Snu_ids at semeval-2019 task 3: Addressing training-test class distribution mismatch in conversational classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02163</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1161" to="1178" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="292" />
			<date type="published" when="1996-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/BF02686918</idno>
		<ptr target="https://doi.org/10.1007/BF02686918" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Iemocap: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language resources and evaluation</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dailydialog: A manually labelled multi-turn dialogue dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03957</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How emotions work: The social functions of emotional expression in negotiations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research in organizational behavior</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Emotional inertia and psychological maladjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuppens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Sheeber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="984" to="991" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mirroring facial expressions and emotions in dyadic conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Navarretta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grobelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maegaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schroder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Meld: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02508</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding emotions in text using deep learning and big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Chinnakotla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="309" to="317" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ana at semeval-2019 task 3: Contextual emotion detection in conversations through hierarchical lstms and bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaïane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00132</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00524</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
