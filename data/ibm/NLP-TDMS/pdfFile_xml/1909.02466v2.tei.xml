<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FreeAnchor: Learning to Match Anchors for Visual Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
							<email>zhangxiaosong18@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
							<email>qxye@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FreeAnchor: Learning to Match Anchors for Visual Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern CNN-based object detectors assign anchors for ground-truth objects under the restriction of object-anchor Intersection-over-Unit (IoU). In this study, we propose a learning-to-match approach to break IoU restriction, allowing objects to match anchors in a flexible manner. Our approach, referred to as FreeAnchor, updates hand-crafted anchor assignment to "free" anchor matching by formulating detector training as a maximum likelihood estimation (MLE) procedure. FreeAnchor targets at learning features which best explain a class of objects in terms of both classification and localization. FreeAnchor is implemented by optimizing detection customized likelihood and can be fused with CNN-based detectors in a plug-and-play manner. Experiments on COCO demonstrate that FreeAnchor consistently outperforms the counterparts with significant margins 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past few years we have witnessed the success of convolution neural network (CNN) for visual object detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. To represent objects with various appearance, aspect ratios, and spatial layouts with limited convolution features, most CNN-based detectors leverage anchor boxes at multiple scales and aspect ratios as reference points for object localization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. By assigning each object to a single or multiple anchors, features can be determined and two fundamental procedures, classification and localization (i.e., bounding box regression), are carried out.</p><p>Anchor-based detectors leverage spatial alignment, i.e., Intersection over Unit (IoU) between objects and anchors, as the criterion for anchor assignment. Each assigned anchor independently supervises network learning for object prediction, based upon the intuition that the anchors aligned with object bounding boxes are most appropriate for object classification and localization. However, we argue that such intuition is implausible and the hand-crafted IoU criterion is not the best choice.</p><p>On the one hand, for objects of acentric features, e.g., slender objects, the most representative features are not close to object centers. A spatially aligned anchor might correspond to fewer representative features, which deteriorate classification and localization capabilities. On the other hand, it is infeasible to match proper anchors/features for objects using IoU when multiple objects come together.</p><p>It is hard to design a generic rule which can optimally match anchors/features with objects of various geometric layouts. The widely used hand-crafted assignment could fail when facing acentric, slender, and/or crowded objects. A learning-based approach requires to be explored to solve this problem in a systematic way, which is the focus of this study.</p><p>We propose a learning-to-match approach for object detection, and target at discarding hand-crafted anchor assignment while optimizing learning procedures of visual object detection from three specific aspects. First, to achieve a high recall rate, the detector is required to guarantee that for each object at least one anchor's prediction is close to the ground-truth. Second, in order to achieve high detection precision, the detector needs to classify anchors with poor localization (large bounding box regression error) into background. Third, the predictions of anchors should be compatible with the non-maximum suppression (NMS) procedure, i.e., the higher the classification score is, the more accurate the localization is. Otherwise, an anchor with accurate localization but low classification score could be suppressed when using the NMS process.</p><p>To fulfill these objectives, we formulate object-anchor matching as a maximum likelihood estimation (MLE) procedure <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, which selects the most representative anchor from a "bag" of anchors for each object. We define the likelihood probability of each anchor bag as the largest anchor confidence within it. Maximizing the likelihood probability guarantees that there exists at least one anchor, which has high confidence for both object classification and localization. Meanwhile, most anchors, which have large classification or localization error, are classified as background. During training, the likelihood probability is converted into a loss function, which then drives CNN-based detector training and object-anchor matching.</p><p>The contributions of this work are concluded as follows:</p><p>• We formulate detector training as an MLE procedure and update hand-crafted anchor assignment to free anchor matching. The proposed approach breaks the IoU restriction, allowing objects to flexibly select anchors under the principle of maximum likelihood.</p><p>• We define a detection customized likelihood, and implement joint optimization of object classification and localization in an end-to-end mechanism. Maximizing the likelihood drives network learning to match optimal anchors and guarantees the comparability of with the NMS procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object detection requires generating a set of bounding boxes along with their classification labels associated with objects in an image. However, it is not trivial for a CNN-based detector to directly predict an order-less set of arbitrary cardinals. One widely-used workaround is to introduce anchors, which employs a divide-and-conquer process to match objects with features. This approach has been successfully demonstrated in Faster R-CNN <ref type="bibr" target="#b2">[3]</ref>, SSD <ref type="bibr" target="#b4">[5]</ref>, FPN <ref type="bibr" target="#b5">[6]</ref>, RetinaNet <ref type="bibr" target="#b6">[7]</ref>, DSSD <ref type="bibr" target="#b9">[10]</ref> and YOLOv2 <ref type="bibr" target="#b10">[11]</ref>. In these detectors, dense anchors need to be configured over convolutional feature maps so that features extracted from anchors can match object windows and the bounding box regression can be well initialized. Anchors are then assigned to objects or backgrounds by thresholding their IoUs with ground-truth bounding boxes <ref type="bibr" target="#b2">[3]</ref>.</p><p>Although effective, these approaches are restricted by heuristics that spatially aligned anchors are compatible for both object classification and localization. For objects of acentric features, however, the detector could miss the best anchors and features.</p><p>To break this limitation imposed by pre-assigned anchors, recent anchor-free approaches employ pixel-level supervision <ref type="bibr" target="#b11">[12]</ref> and center-ness bounding box regression <ref type="bibr" target="#b12">[13]</ref>. CornerNet <ref type="bibr" target="#b13">[14]</ref> and CenterNet <ref type="bibr" target="#b14">[15]</ref> replace bounding box supervision with key-point supervision. MetaAnchor <ref type="bibr" target="#b15">[16]</ref> approach learns to produce anchors from the arbitrary customized prior boxes with a sub-network.</p><p>GuidedAnchoring <ref type="bibr" target="#b16">[17]</ref> leverages semantic features to guide the prediction of anchors while replacing dense anchors with predicted anchors. IoU-Net <ref type="bibr" target="#b17">[18]</ref> incorporates IoU-guided NMS, which helps eliminating the suppression failure caused by the misleading classification confidences.</p><p>Existing approaches have taken a step towards learnable anchor customization. Nevertheless, to the best of our knowledge, there still lacks a systematic approach to model the correspondence between anchors and objects during detector training, which inhibits the optimization of feature selection and feature learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Approach</head><p>To model the correspondence between objects and anchors, we propose to formulate detector training as an MLE procedure. We then define the detection customized likelihood, which simultaneously facilitates object classification and localization. During detector training, we convert detection customized likelihood into detection customized loss and jointly optimizing object classification, object localization, and object-anchor matching in an end-to-end mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Detector Training as Maximum Likelihood Estimation</head><p>Let's begin with a CNN-based one-stage detector <ref type="bibr" target="#b6">[7]</ref>. Given an input image I, the ground-truth annotations are denoted as B, where a ground-truth box b i ∈ B is made up of a class label b cls i and a location b loc i . During the forward propagation procedure of the network, each anchor a j ∈ A obtains a class prediction a cls j ∈ R k after the Sigmoid activation, and a location prediction a loc j = {x, y, w, h} after the bounding box regression. k denotes the number of object classes.</p><p>During training, hand-crafted criterion based on IoU is used to assign anchors for objects, <ref type="figure" target="#fig_0">Fig. 1</ref>, and a matrix C ij ∈ {0, 1} is defined to indicate whether object b i matches anchor a j or not. When the IoU of b i and a j is greater than a threshold, b i matches a j and C ij = 1. Otherwise, C ij = 0. Specially, when multiple objects' IoU are greater than this threshold, the object of the largest IoU will successfully match this anchor, which guarantees that each anchor is matched by a single object at most, i.e.,</p><formula xml:id="formula_0">i C ij ∈ {0, 1}, ∀a j ∈ A. By defining A + ⊆ A as {a j | i C ij = 1} and A − ⊆ A as {a j | i C ij = 0}</formula><p>, the loss function L(θ) of the detector is written as follows:</p><formula xml:id="formula_1">L(θ) = aj ∈A+ bi∈B C ij L cls ij (θ) + β aj ∈A+ bi∈B C ij L loc ij (θ) + aj ∈A− L bg j (θ),<label>(1)</label></formula><p>where θ denotes the network parameters to be learned. L cls</p><formula xml:id="formula_2">ij (θ) = BCE(a cls j , b cls i , θ), L loc ij (θ) = SmoothL1(a loc j , b loc i , θ)</formula><p>and L bg j (θ) = BCE(a cls j , 0, θ) respectively denote the Binary Cross Entropy loss (BCE) for classification and the SmoothL1 loss defined for localization <ref type="bibr" target="#b1">[2]</ref>. β is a regularization factor and "bg" indicates "background".</p><p>From the MLE perspective, the training loss L(θ) is converted into a likelihood probability, as follows:</p><formula xml:id="formula_3">P(θ) = e −L(θ) = aj ∈A+ bi∈B C ij e −L cls ij (θ) aj ∈A+ bi∈B C ij e −βL loc ij (θ) aj ∈A− e −L bg j (θ) = aj ∈A+ bi∈B C ij P cls ij (θ) aj ∈A+ bi∈B C ij P loc ij (θ) aj ∈A− P bg j (θ),<label>(2)</label></formula><p>where P cls ij (θ) and P bg j (θ) denote classification confidence and P loc ij (θ) denotes localization confidence. Minimizing the loss function L(θ) defined in Eq. 1 is equal to maximizing the likelihood probability P(θ) defined in Eq. 2.</p><p>Eq. 2 strictly considers the optimization of classification and localization of anchors from the MLE perspective. However, it unfortunately ignores how to learn the matching matrix C ij . Existing CNN-based detectors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref> solve this problem by empirically assigning anchors using the IoU criterion, <ref type="figure" target="#fig_0">Fig. 1</ref>, but ignoring the optimization of object-anchor matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Detection Customized Likelihood</head><p>To achieve the optimization of object-anchor matching, we extend the CNN-based detection framework by introducing detection customized likelihood. Such likelihood intends to incorporate the objectives of recall and precision while guaranteeing the compatibility with NMS.</p><p>To implement the likelihood, we first construct a bag of candidate anchors for each object b i by selecting (n) top-ranked anchors A i ⊂ A in terms of their IoU with the object. We then learns to match the best anchor while maximizing the detection customized likelihood.</p><p>To optimize the recall rate, for each object b i ∈ B we requires to guarantee that there exists at least one anchor a j ∈ A i , whose prediction (a cls j and a loc j ) is close to the ground-truth. The objective function can be derived from the first two terms of Eq. 2, as follows:</p><formula xml:id="formula_4">P recall (θ) = i max aj ∈Ai P cls ij (θ)P loc ij (θ) .<label>(3)</label></formula><p>To achieve increased detection precision, detectors need to classify the anchors of poor localization into the background class. This is fulfilled by optimizing the following objective function:</p><formula xml:id="formula_5">P precision (θ) = j 1 − P {a j ∈ A − }(1 − P bg j (θ)) ,<label>(4)</label></formula><p>where P {a j ∈ A − } = 1 − max i P {a j → b i } is the probability that a j misses all objects and P {a j → b i } denotes the probability that anchor a j correctly predicts object b i .</p><p>To be compatible with the NMS procedure, P {a j → b i } should have the following three properties:</p><p>(1) P {a j → b i } is a monotonically increasing function of the IoU between a loc j and b i , IoU loc ij . (2) When IoU loc ij is smaller than a threshold t, P {a j → b i } is close to 0. (3) For an object b i , there exists one and only one a j satisfying P {a j → b i } = 1. These properties can be satisfied with a saturated linear function, as</p><formula xml:id="formula_6">Saturated linear(x, t 1 , t 2 ) =        0, x ≤ t 1 x − t 1 t 2 − t 1 , t 1 &lt; x &lt; t 2 , 1, x ≥ t 2</formula><p>which is shown in <ref type="figure">Fig. 2</ref>, and we have P {a j → b i } = Saturated linear IoU loc ij , t, max j (IoU loc ij ) . Implementing the definitions provided above, the detection customized likelihood is defined as follows: which incorporates the objectives of recall, precision and compatibility with NMS. By optimizing this likelihood, we simultaneously maximize the probability of recall P recall (θ) and precision P precision (θ) and then achieve free object-anchor matching during detector training.</p><formula xml:id="formula_7">P (θ) = P recall (θ) × P precision (θ) = i max aj ∈Ai (P cls ij (θ)P loc ij (θ)) × j 1 − P {a j ∈ A − }(1 − P bg j (θ)) ,<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Anchor Matching Mechanism</head><p>To implement this learning-to-match approach in a CNN-based detector, the detection customized likelihood defined by Eq. 5 is converted to a detection customized loss function, as follows:</p><formula xml:id="formula_8">L (θ) = − log P (θ) = − i log max aj ∈Ai (P cls ij (θ)P loc ij (θ)) − j log 1 − P {a j ∈ A − }(1 − P bg j (θ)) ,<label>(6)</label></formula><p>where the max function is used to select the best anchor for each object. During training, a single anchor is selected from a bag of anchors A i , which is then used to update the network parameter θ.</p><p>At early training epochs, the confidence of all anchors is small for randomly initialized network parameters. The anchor with the highest confidence is not suitable for detector training. We therefore propose using the Mean-max function, defined as:</p><formula xml:id="formula_9">Mean-max(X) = xj ∈X x j 1 − x j xj ∈X 1 1 − x j ,</formula><p>which is used to select anchors. When training is insufficient, the Mean-max function, as shown in <ref type="figure">Fig. 3</ref>, will be close to the mean function, which means almost all anchors in bag are used for training. Along with training, the confidence of some anchors increases and the Mean-max function moves closer to the max function. When sufficient training has taken place, a single best anchor can be selected from a bag of anchors to match each object.</p><p>Replacing the max function in Eq. 6 with Mean-max, adding balance factor w 1 w 2 , and applying focal loss <ref type="bibr" target="#b6">[7]</ref> to the second term of Eq. 6, the detection customized loss function of an FreeAnchor detector is concluded, as follows:</p><formula xml:id="formula_10">L (θ) = −w 1 i log Mean-max(X i ) + w 2 j F L P {a j ∈ A − }(1 − P bg j (θ)) ,<label>(7)</label></formula><p>where X i = {P cls ij (θ)P loc ij (θ)| a j ∈ A i } is a likelihood set corresponding to the anchor bag A i . By using the parameters α and γ from focal loss <ref type="bibr" target="#b6">[7]</ref>, we set w 1 = α ||B|| , w 2 = 1−α n||B|| , and F L(x) = −x γ log (1 − x).</p><p>With the detection customized loss defined, we implement the training procedure as Algorithm 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forward propagation:</head><p>Predict class a cls j and location a loc j for each anchor a j ∈ A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Anchor bag construction: A i ← Select n top-ranked anchors a j in terms of their IoU with b i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Loss calculation: Calculate L (θ) with Eq. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Backward propagation: θ t+1 = θ t − λ∇ θ t L (θ t ) using a stochastic gradient descent algorithm. 7: end for 8: return θ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present the implementation of an FreeAnchor detector to appraise the effect of the proposed learning-to-match approach. We also compare the FreeAnchor detector with the counterpart and the state-of-the-art approaches. Experiments were carried out on COCO 2017 <ref type="bibr" target="#b18">[19]</ref>, which contains ∼118k images for training, 5k for validation (val) and ∼20k for testing without provided annotations (test-dev). Detectors were trained on COCO training set, and evaluated on the val set. Final results were reported on the test-dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>FreeAnchor is implemented upon a state-of-the-art one-stage detector, RetinaNet <ref type="bibr" target="#b6">[7]</ref>, using ResNet <ref type="bibr" target="#b19">[20]</ref> and ResNeXt <ref type="bibr" target="#b20">[21]</ref> as the backbone networks. By simply replacing the loss defined in RetinaNet with the proposed detection customized loss, Eq. 7, we updated the RetinaNet detector to an FreeAnchor detector. For the last convolutional layer of the classification subnet, we set the bias initialization to b = − log ((1 − ρ)/ρ) with ρ = 0.02. Training used synchronized SGD over 8 Tesla V100 GPUs with a total of 16 images per mini-batch (2 images per GPU). Unless otherwise specified, all models were trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Effect</head><p>Learning-to-match: The proposed learning-to-match approach can select proper anchors to represent the object of interest, <ref type="figure">Fig. 4</ref>. As analyzed in the introduction section, hand-crafted anchor assignment often fails in two situations: Firstly, slender objects with acentric features; and secondly when multiple    objects are provided in crowded scenes. FreeAnchor effectively alleviated these two problems. Over slender objects, FreeAnchor significantly outperformed the RetinaNet baseline, <ref type="figure" target="#fig_5">Fig. 5</ref>. For other square objects FreeAnchor reported comparable performance with RetinaNet. The reason for this is that the learning-to-match procedure drives network activating at least one anchor within each object's anchor bag in order to predict correct category and location. The anchor is not necessary spatially aligned with the object, but has the most representative features for object classification and localization.</p><p>We further compared the performance of RetinaNet and FreeAnchor in scenarios of various crowdedness, <ref type="figure" target="#fig_6">Fig. 6</ref>. As the number of objects in each image increased, the FreeAnchor's advantage over RetinaNet became more and more obvious. This demonstrated that our approach, with the learning-to-match mechanism, can select more suitable anchors to objects in crowded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compatibility with NMS:</head><p>To assess the compatibility of anchors' predictions with NMS, we defined the NMS Recall (NR τ ) as the ratio of the recall rates after and before NMS for a given IoU thresholds τ . Following the COCO-style AP metric <ref type="bibr" target="#b18">[19]</ref>, NR was defined as the averaged NR τ when τ changes from 0.50 to 0.90 with an interval of 0.05, <ref type="table" target="#tab_1">Table 1</ref>. We compared RetinaNet and FreeAnchor in terms of their NR τ . It can be seen that FreeAnchor reported higher NR τ , which means higher compatibility with NMS. This validated that the detection customized likelihood, defined in Section 3.2, can drive joint optimization of classification and localization. Focal loss parameter: FreeAnchor introduced a bag of anchors to replace independent anchors and therefore faced more serious sample imbalance. To handle the imbalance, we experimented the parameters in Focal Loss <ref type="bibr" target="#b6">[7]</ref> as α in {0.25, 0.5, 0.75} and γ in {1.5 , 2.0, 2.5}, and set α = 0.5 and γ = 2.0.</p><p>Loss regularization factor β: The regularization factor β in Eq. 1, which balances the loss of classification and localization, was experimentally validated to be 0.75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Detection Performance</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, FreeAnchor was compared with the RetinaNet baseline. FreeAnchor consistently improved the AP up to ∼3.0%, which is a significant margin in terms of the challenging object detection task. Note that the performance gain was achieved with negligible cost of training time. FreeAnchor was compared with state-of-the-art one-stage detectors in <ref type="table" target="#tab_3">Table 3</ref>, used scale jitter and 2× longer training than the same model from <ref type="table" target="#tab_2">Table 2</ref>. It outperformed the baseline RetinaNet <ref type="bibr" target="#b6">[7]</ref> and the anchor-free approaches including FoveaBox <ref type="bibr" target="#b21">[22]</ref>, FSAF <ref type="bibr" target="#b22">[23]</ref>, FCOS <ref type="bibr" target="#b12">[13]</ref> and CornerNet <ref type="bibr" target="#b13">[14]</ref>. With a litter ResNeXt-64x4d-101 backbone network and fewer training iterations, FreeAnchor was comparable with CenterNet in AP (44.9% vs. 44.9%) and reported higher AP 50 , which is a more widely used metric in many applications.</p><p>"FreeAnchor*" refers to extending the scale range from [640, 800] to [480, 960], achieving 46.0% AP. "FreeAnchor**" further utilized multi-scale testing over scales {480, 640, 800, 960, 1120, 1280}, and increased AP up to 47.3%, which outperformed most state-of-the-art detectors with the same backbone network. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of hand-crafted anchor assignment (top) and FreeAnchor (bottom). FreeAnchor allows each object to flexibly match the best anchor from a "bag" of anchors during detector training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>9 Figure 2 :Figure 3 :</head><label>923</label><figDesc>Saturated linear function. Mean-max function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Algorithm 1</head><label>41</label><figDesc>Comparison of learning-to-match anchors (left) with hand-crafted anchor assignment (right) for the "laptop" object. Red dots denote anchor centers. Darker (redder) dots denote higher confidence to be matched. For clarity, we select 16 anchors of aspect-ratio 1:1 from all 50 anchors for illustration.(Best viewed in color) Detector training with FreeAnchor. Input: I: Input image. B: A set of ground-truth bounding boxes b i . A: A set of anchors a j in image. n: Hyper-parameter about anchor bag size . Output: θ: Detection network parameters. 1: θ ← initialize network parameters. 2: for i=1:MaxIter do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Performance comparison on square and slender objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Performance comparison on object crowdedness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of NMS recall (%) on COCO val set. We evaluated anchor bag sizes in {40, 50, 60, 100} and observed that the bag size 50 reported the best performance.Background IoU threshold t: A threshold was used in P {a j → b i } during training. We tried background IoU thresholds in {0.5, 0.6, 0.7} and validated that 0.6 worked best.</figDesc><table><row><cell>backbone</cell><cell>detector</cell><cell cols="5">NR NR 50 NR 60 NR 70 NR 80 NR 90</cell></row><row><cell>ResNet-50</cell><cell cols="2">RetinaNet [7] FreeAnchor (ours) 83.8 99.2 81.8 98.3</cell><cell>95.7 97.5</cell><cell>87.0 89.5</cell><cell>71.8 74.3</cell><cell>51.3 53.1</cell></row><row><cell cols="2">4.3 Parameter Setting</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Anchor bag size n:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of FreeAnchor and RetinaNet (baseline).</figDesc><table><row><cell>Backbone</cell><cell>Detector</cell><cell>Training time</cell><cell>AP AP 50 AP 75 AP S AP M AP L</cell></row><row><cell>ResNet-50</cell><cell>RetinaNet [7] FreeAnchor (ours)</cell><cell>5.02h 5.27h</cell><cell>35.7 55.0 38.5 18.9 38.9 46.3 38.7 57.3 41.6 20.2 41.3 50.1</cell></row><row><cell>ResNet-101</cell><cell>RetinaNet [7] FreeAnchor (ours)</cell><cell>6.96h 7.26h</cell><cell>37.8 57.5 40.8 20.2 41.1 49.2 40.9 59.9 43.8 21.7 43.8 53.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison with state-of-the-art one-stage detectors. Hourglass-104 500k 40.6 56.4 43.2 19.1 42.8 54.3 CenterNet [15] Hourglass-104 480k 44.9 62.4 48.1 25.6 47.4 57.4</figDesc><table><row><cell>Detector</cell><cell>Backbone</cell><cell>Iter. AP AP 50 AP 75 AP S AP M AP L</cell></row><row><cell>RetinaNet [7]</cell><cell>ResNet-101</cell><cell>135k 39.1 59.1 42.3 21.8 42.7 50.2</cell></row><row><cell cols="2">FoveaBox [22] ResNet-101</cell><cell>135k 40.6 60.1 43.5 23.3 45.2 54.5</cell></row><row><cell>FSAF [23]</cell><cell>ResNet-101</cell><cell>135k 40.9 61.5 44.0 24.0 44.2 51.3</cell></row><row><cell>FCOS [13]</cell><cell>ResNet-101</cell><cell>180k 41.5 60.7 45.0 24.4 44.8 51.6</cell></row><row><cell>RetinaNet [7]</cell><cell>ResNeXt-101</cell><cell>135k 40.8 61.1 44.1 24.1 44.2 51.2</cell></row><row><cell cols="2">FoveaBox [22] ResNeXt-101</cell><cell>135k 42.1 61.9 45.2 24.9 46.8 55.6</cell></row><row><cell>FSAF [23]</cell><cell>ResNeXt-101</cell><cell>135k 42.9 63.8 46.3 26.6 46.2 52.7</cell></row><row><cell>FCOS [13]</cell><cell>ResNeXt-101</cell><cell>180k 43.2 62.8 46.6 26.5 46.2 53.3</cell></row><row><cell>CornerNet [14] FreeAnchor</cell><cell>ResNet-101</cell><cell>180k 43.1 62.2 46.4 24.5 46.1 54.8</cell></row><row><cell>FreeAnchor</cell><cell>ResNeXt-101</cell><cell>180k 44.9 64.3 48.5 26.8 48.3 55.9</cell></row><row><cell>FreeAnchor*</cell><cell>ResNeXt-101</cell><cell>180k 46.0 65.6 49.8 27.8 49.5 57.7</cell></row><row><cell>FreeAnchor**</cell><cell>ResNeXt-101</cell><cell>180k 47.3 66.3 51.5 30.6 50.4 59.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed an elegant and effective approach, referred to as FreeAnchor, for visual object detection. FreeAnchor updated the hand-crafted anchor assignment to "free" object-anchor correspondence by formulating detector training as a maximum likelihood estimation (MLE) procedure. With FreeAnchor implemented, we significantly improved the performance of object detection, in striking contrast with the baseline detector. The underlying reality is that the MLE procedure with the detection customized likelihood facilitates learning convolutional features that best explain a class of objects. This provides a fresh insight for the visual object detection problem. Acnkowledgement. This work was supported in part by the NSFC under Grant 61836012, 61671427, and 61771447 and Post Doctoral Innovative Talent Support Program under Grant 119103S304.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="570" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the mathematical foundations of theoretical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page" from="309" to="368" />
			<date type="published" when="1922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">EAST: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01355</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="765" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Centernet: Object detection with keypoint triplets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Metaanchor: Learning to detect objects with customized anchors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="320" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2965" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
