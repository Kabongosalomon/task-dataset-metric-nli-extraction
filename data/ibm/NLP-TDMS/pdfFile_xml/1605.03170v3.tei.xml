<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Germany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Germany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Germany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Germany</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Germany</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. Evaluation is done on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation 1 .</p><p>1 Models and code available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human pose estimation has recently made dramatic progress in particular on standard benchmarks for single person pose estimation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. This progress has been facilitated by the use of deep learning-based architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and by the availability of large-scale datasets such as "MPII Human Pose" <ref type="bibr" target="#b1">[2]</ref>. In order to make further progress on the challenging task of multi-person pose estimation we carefully design and evaluate several key-ingredients for human pose estimation.</p><p>The first ingredient we consider is the generation of body part hypotheses. Essentially all prominent pose estimation methods include a component that detects body parts or estimates their position. While early work used classifiers such as SVMs and AdaBoost <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, modern approaches build on different flavors of deep learning-based architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. The second key ingredient are pairwise terms between body part hypotheses that help grouping those into valid human pose configurations. In earlier models such pairwise terms were essential for good performance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Recent methods seem to profit less from such pairwise terms due to stronger unaries <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Image-conditioned pairwise terms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> however have the promise to allow for better grouping. Last but not least, inference time is always a key consideration for pose estimation models. Often, model complexity has to be treated for speed and thus many models do not consider all spatial relations that would be beneficial for best performance.</p><p>In this paper we contribute to all three aspects and thereby significantly push the state of the art in multi-person pose estimation. We use a general optimization framework introduced in our previous work <ref type="bibr" target="#b9">[10]</ref> as a test bed for all three key ingredients proposed in this paper, as it allows to easily replace and combine different components. Our contributions are three-fold, leading to a novel multi-person pose estimation approach that is deeper, stronger, and faster compared to the state of the art <ref type="bibr" target="#b9">[10]</ref>:</p><p>-"deeper": we propose strong body part detectors based on recent advances in deep learning <ref type="bibr" target="#b11">[12]</ref> that -taken alone -already allow to obtain competitive performance on pose estimation benchmarks. -"stronger": we introduce novel image-conditioned pairwise terms between body parts that allow to push performance in the challenging case of multipeople pose estimation. -"faster": we demonstrate that using our image-conditioned pairwise along with very good part detection candidates in a fully-connected model dramatically reduces the run-time by 2-3 orders of magnitude. Finally, we introduce a novel incremental optimization method to achieve a further 4x run-time reduction while improving human pose estimation accuracy.</p><p>We evaluate our approach on two single-person and two multi-person pose estimation benchmarks and report the best results in each case. Sample multiperson pose estimation predictions by the proposed approach are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Related work. Articulated human pose estimation has been traditionally formulated as a structured prediction task that requires an inference step combining local observations of body joints with spatial constraints. Various formulations have been proposed based on tree <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6]</ref> and non-tree models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. The goal of the inference process has been to refine observations from local part detectors into coherent estimates of body configurations. Models of this type have been increasingly superseded by strong body part detectors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, which has been reinforced by the development of strong image representations based on convolutional networks. Recent work aimed to incorporate convolutional detectors into part-based models <ref type="bibr" target="#b8">[9]</ref> or design stronger detectors by combining the detector output with location-based features <ref type="bibr" target="#b20">[21]</ref>. Specifically, as we suggest in <ref type="bibr" target="#b9">[10]</ref>, in the presence of strong detectors spatial reasoning results in diminishing returns because most contextual information can be incorporated directly in the detector. In this work we elevate the task to a new level of complexity by addressing images with multiple potentially overlapping people. This results in a more complex structured prediction problem with a variable number of outputs. In this setting we observe a large boost from conducting inference on top of state-of-the-art part detectors.</p><p>Combining spatial models with convnets allows to increase the receptive field that is used for inferring body joint locations. For example <ref type="bibr" target="#b10">[11]</ref> iteratively trains a cascade of convolutional parts detectors, each detector taking the scoremap of all parts from the previous stage. This effectively increases the depth of the network and the receptive field is comparable to the entire person. With the recent developments in object detection newer architectures are composed of a large number of layers and the receptive field is large automatically. In this paper, we introduce a detector based on the recently proposed deep residual networks <ref type="bibr" target="#b11">[12]</ref>. This allows us to train a detector with a large receptive field <ref type="bibr" target="#b10">[11]</ref> and to incorporate intermediate supervision.</p><p>The use of purely geometric pairwise terms is suboptimal as they do not take local image evidence into account and only penalize deviation from the expected joint location. Due to the inherent articulation of body parts the expected location can only approximately guide the inference. While this can be sufficient when people are relatively distant from each other, for closely positioned people more discriminative pairwise costs are essential. Two prior works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> have introduced image-dependent pairwise terms between connected body parts. While <ref type="bibr" target="#b6">[7]</ref> uses an intermediate representation based on poselets our pairwise terms are conditioned directly on the image. <ref type="bibr" target="#b8">[9]</ref> clusters relative positions of adjacent joints into T = 11 clusters, and assigns different labels to the part depending on which cluster it falls to. Subsequently a CNN is trained to predict this extended set of classes and later an SVM is used to select the maximum scoring joint pair relation.</p><p>Single person pose estimation has advanced considerably, but the setting is simplified. Here we focus on the more challenging problem of multi-person pose estimation. Previous work has addressed this problem as sequence of person detection and pose estimation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. <ref type="bibr" target="#b21">[22]</ref> use a detector for initialization and reasoning across people, but rely on simple geometric body part relationships and only reason about person-person occlusions. <ref type="bibr" target="#b23">[24]</ref> focus on single partially occluded people, and handle multi-person scenes akin to <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr" target="#b9">[10]</ref> we propose to jointly detect and estimate configurations, but rely on simple pairwise terms only, which limits the performance and, as we show, results in prohibitive inference time to fully explore the search space. Here, we innovate on multiple fronts both in terms of speed and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DeepCut Recap</head><p>This section summarizes DeepCut <ref type="bibr" target="#b9">[10]</ref> and how unary and pairwise terms are used in this approach. DeepCut is a state-of-the-art approach to multi-person pose estimation based on integer linear programming (ILP) that jointly estimates poses of all people present in an image by minimizing a joint objective. This objective aims to jointly partition and label an initial pool of body part candidates into consistent sets of body-part configurations corresponding to distinct people. We use DeepCut as a general optimization framework that allows to easily replace and combine different components.</p><p>Specifically, DeepCut starts from a set D of body part candidates, i.e. putative detections of body parts in a given image, and a set C of body part classes, e.g., head, shoulder, knee. The set D of part candidates is typically generated by body part detectors and each candidate d ∈ D has a unary score for every body part class c ∈ C. Based on these unary scores DeepCut associates a cost or reward α dc ∈ R to be paid by all feasible solutions of the pose estimation problem for which the body part candidate d is a body part of class c.</p><p>Additionally, for every pair of distinct body part candidates d, d ∈ D and every two body part classes c, c ∈ C, the pairwise term is used to generate a cost or reward β dd cc ∈ R to be paid by all feasible solutions of the pose estimation problem for which the body part d, classified as c, and the body part d , classified as c , belong to the same person.</p><p>With respect to these sets and costs, the pose estimation problem is cast as an ILP in two classes of 01-variables: Variables x : D × C → {0, 1} indicate by x dc = 1 that body part candidate d is of body part class c. If, for a d ∈ D and all c ∈ C, x dc = 0, the body part candidate d is suppressed. Variables y : D 2 → {0, 1} indicate by y dd = 1 that body part candidates d and d belong to the same person. Additional variables and constraints described in <ref type="bibr" target="#b9">[10]</ref> link the variables x and y to the costs and ensure that feasible solutions (x, y) well-define a selection and classification of body part candidates as body part classes as well as a clustering of body part candidates into distinct people.</p><p>The DeepCut ILP is hard and hard to approximate, as it generalizes the minimum cost multicut or correlation clustering problem which is APX-hard <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. Using the branch-and-cut algorithm <ref type="bibr" target="#b9">[10]</ref> to compute constant-factor approximative feasible solutions of instances of the DeepCut ILP is not necessarily practical. In Sec. 5 we propose an incremental optimization approach that uses branch-and-cut algorithm to incrementally solve several instances of ILP, which results into 4-5x run-time reduction with increased pose estimation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Part Detectors</head><p>As argued before, strong part detectors are an essential ingredient of modern pose estimation methods. We propose and evaluate a deep fully-convolutional human body part detection model drawing on powerful recent ideas from semantic segmentation, object classification <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b11">12]</ref> and human pose estimation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>Architecture. We build on the recent advances in object classification and adapt the extremely deep Residual Network (ResNet) <ref type="bibr" target="#b11">[12]</ref> for human body part detection. This model achieved excellent results on the recent ImageNet Object Classification Challenge and specifically tackles the problem of vanishing gradients by passing the state through identity layers and modeling residual functions. Our best performing body part detection model has 152 layers (c.f. Sec. 3.2) which is in line with the findings of <ref type="bibr" target="#b11">[12]</ref>. Stride. Adapting ResNet for the sliding window-based body part detection is not straight forward: converting ResNet to the fully convolutional mode leads to a 32 px stride which is too coarse for precise part localization. In <ref type="bibr" target="#b9">[10]</ref> we show that using a stride of 8 px leads to good part detection results. Typically, spatial resolution can be recovered by either introducing up-sampling deconvolutional layers <ref type="bibr" target="#b26">[27]</ref>, or blowing up the convolutional filters using the hole algorithm <ref type="bibr" target="#b27">[28]</ref>. The latter has shown to perform better on the task of semantic segmentation. However, using the hole algorithm to recover the spatial resolution of ResNet is infeasible due to memory constraints. For instance, the 22 residual blocks in the conv4 bank of ResNet-101 constitute the major part of the network and running it at stride 8 px does not fit the net into GPU memory 2 . We thus employ a hybrid approach. First, we remove the final classification as well as average pooling layer. Then, we decrease the stride of the first convolutional layers of the conv5 bank from 2 px to 1 px to prevent down-sampling. Next, we add holes to all 3x3 convolutions in conv5 to preserve their receptive field. This reduces the stride of the full CNN to 16 px. Finally, we add deconvolutional layers for 2x up-sampling and connect the final output to the output of the conv3 bank. Receptive field size. A large receptive field size allows to incorporate context when predicting locations of individual body parts. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref> argue about the importance of large receptive fields and propose a complex hierarchical architecture predicting parts at multiple resolution levels. The extreme depth of ResNet allows for a very large receptive field (on the order of 1000 px compared to VGG's 400 px <ref type="bibr" target="#b3">[4]</ref>) without the need of introducing complex hierarchical architectures. We empirically find that re-scaling the original image such that an upright standing person is 340 px high leads to best performance. Intermediate supervision. Providing additional supervision addresses the problem of vanishing gradients in deep neural networks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11]</ref>. In addition to that, <ref type="bibr" target="#b10">[11]</ref> reports that using part scoremaps produced at intermediate stages as inputs for subsequent stages helps to encode spatial relations between parts, while <ref type="bibr" target="#b30">[31]</ref> use spatial fusion layers that learn an implicit spatial model. ResNets address the first problem by introducing identity connections and learning residual functions. To address the second concern, we make a slightly different choice: we add part loss layers inside the conv4 bank of ResNet. We argue that it is not strictly necessary to use scoremaps as inputs for the subsequent stages. The activations from such intermediate predictions are different only up to a linear transformation and contain all information about part presence that is available at that stage of the network. In Sec. 3.2 we empirically show a consistent improvement of part detection performance when including intermediate supervision.</p><p>Loss functions. We use sigmoid activations and cross entropy loss function during training <ref type="bibr" target="#b9">[10]</ref>. We perform location refinement by predicting offsets from the locations on the scoremap grid to the ground truth joint locations <ref type="bibr" target="#b9">[10]</ref>. Training. We use the publicly available ResNet implementation (Caffe) and initialize from the ImageNet-pre-trained models. We train networks with SGD for 1M iterations, starting with the learning rate lr=0.001 for 10k, then lr=0.002 for 420k, lr=0.0002 for 300k and lr=0.0001 for 300k. This corresponds to roughly 17 epochs of the MPII <ref type="bibr" target="#b1">[2]</ref> train set. Finetuning from ImageNet takes two days on a single GPU. Batch normalization <ref type="bibr" target="#b31">[32]</ref> worsens performance, as the batch size of 1 in fully convolutional training is not enough to provide a reliable estimate of activation statistics. During training we switch off collection of statistics and use the mean and variance that were gathered on the ImageNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation of Part Detectors</head><p>Datasets. We use three public datasets: "Leeds Sports Poses" (LSP) <ref type="bibr" target="#b0">[1]</ref> (personcentric (PC) annotations); "LSP Extended" (LSPET) <ref type="bibr" target="#b14">[15]</ref>; "MPII Human Pose" ("Single Person") <ref type="bibr" target="#b1">[2]</ref> consisting of 19185 training and 7247 testing poses. To evaluate on LSP we train part detectors on the union of MPII, LSPET and LSP training sets. To evaluate on MPII Single Person we train on MPII only. Evaluation measures. We use the standard "Percentage of Correct Keypoints (PCK)" evaluation metric <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b7">8]</ref> and evaluation scripts from the web page of <ref type="bibr" target="#b1">[2]</ref>. In addition to PCK at fixed threshold, we report "Area under Curve" (AUC) computed for the entire range of PCK thresholds. Results on LSP. The results are shown in Tab. 1. ResNet-50 with 8 px stride achieves 87.8% PCK and 63.7% AUC. Increasing the stride size to 16 px and up-sampling the scoremaps by 2x to compensate for the loss on resolution slightly drops the performance to 87.2% PCK. This is expected as up-sampling cannot fully compensate for the information loss due to a larger stride. Larger stride minimizes memory requirements, which allows for training a deeper ResNet-152. The latter significantly increases the performance (89.1 vs. 87.2% PCK, 65.1 vs. 63.1% AUC), as it has larger model capacity. Introducing intermediate supervision further improves the performance to 90.1% PCK and 66.1% AUC, as it constraints the network to learn useful representations in the early stages and uses them in later stages for spatial disambiguation of parts.   <ref type="bibr" target="#b9">[10]</ref> (+5.9% PCK h , +4.2% AUC), which again underlines the importance of using extremely deep model. The proposed approach performs on par with the best know result by Wei et al. <ref type="bibr" target="#b10">[11]</ref> (88.5 vs. 88.5% PCK h ) for the maximum distance threshold, while slightly loosing when using the entire range of thresholds (60.8 vs. 61.4% AUC). We envision that extending the proposed approach to incorporate multiple scales as in <ref type="bibr" target="#b10">[11]</ref> should improve the performance. The model trained on the union of MPII, LSPET and LSP training sets achieves 88.3% PCK h and 60.7% AUC. The fact that the same model achieves similar performance on both LSP and MPII benchmarks demonstrates the generality of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Image-Conditioned Pairwise Terms</head><p>As discussed in Sec. 3, a large receptive field for the CNN-based part detectors allows to accurately predict the presence of a body part at a given location. However, it also contains enough evidence to reason about locations of other parts in the vicinity. We draw on this insight and propose to also use deep networks to make pairwise part-to-part predictions. They are subsequently used  to compute the pairwise probabilities and show significant improvements for multi-person pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model</head><p>Our approach is inspired by the body part location refinement described in Sec. 3. In addition to predicting offsets for the current joint, we directly regress from the current location to the relative positions of all other joints. For each scoremap location k = (x k , y k ) that is marked positive w.r.t the joint c ∈ C and for each remaining joint c ∈ C \ c, we define a relative position of c w.r.t. c as a tuple t k cc = (x c − x k , y c − x k ). We add an extra layer that predicts relative position o k cc and train it with a smooth L 1 loss function. We thus perform joint training of body part detectors (cross-entropy loss), location regression (L 1 loss) and pairwise regression (L 1 loss) by linearly combining all three loss functions. The targets t are normalized to have zero mean and unit variance over the training set. Results of such predictions are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>We then use these predictions to compute pairwise costs β dd cc . For any pair of detections (d, d ) ( <ref type="figure" target="#fig_2">Fig. 3</ref>) and for any pair of joints (c, c ) we define the following quantities: locations l d , l d of detections d and d respectively; the offset prediction o d cc from c to c at location d (solid red) coming from the CNN and similarly the offset prediction o d c c (solid turquoise). We then compute the offset between the two predictions:ô dd = l d − l d (marked in dashed red). The degree to which the prediction o d cc agrees with the actual offsetô dd tells how likely d, d are of classes c, c respectively and belong to the same person. We measure this by computing the distance between the two offsets </p><formula xml:id="formula_0">∆ f = ô dd − o d<label>cc</label></formula><formula xml:id="formula_1">f dd cc = (∆ f , θ f , ∆ b , θ b , exp(−∆ f ), . . . , exp(−θ b )).</formula><p>We then use the features f dd cc and define logistic model:</p><formula xml:id="formula_2">p(z dd cc = 1|f dd cc , ω cc ) = 1 1 + exp(− ω cc , f dd cc )</formula><p>.</p><p>where K = (|C| × (|C| + 1))/2 parameters ω cc are estimated using ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sampling Detections</head><p>Location refinement NMS. DeepCut samples the set of detections D from the scoremap by applying non-maximum suppression (NMS). Here, we utilize location refinement and correct grid locations with the predicted offsets before applying NMS. This pulls detections that belong to a particular body joint towards its true location thereby increasing the density of detections around that location, which allows to distribute the detection candidates in a better way. Splitting of part detections. DeepCut ILP solves the clustering problem by labeling each detection d with a single part class c and assigning it to a particular cluster that corresponds to a distinct person. However, it may happen that the same spatial location is occupied by more than one body joint, and therefore, its corresponding detection can only be labeled with one of the respecting classes. A naive solution is to replace a detection with n detections for each part class, which would result in a prohibitive increase in the number of detections. We simply split a detection d into several if more than one part has unary probability that is higher than a chosen threshold s (in our case s = 0.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of Pairwise</head><p>Datasets and evaluation measure. We evaluate on the challenging public "MPII Human Pose" ("Multi-Person") benchmark <ref type="bibr">[</ref>  MPII Multi-Person Val. We report major results on the full testing set, and on the subset of 288 images for the direct comparison to <ref type="bibr" target="#b9">[10]</ref>. We follow the official evaluation protocol 3 and evaluate on groups using provided rough group location and scale. In more detail, we localize each group by cropping around the group using the provided information and use resulting crops as input to multi-person pose estimation. The AP measure <ref type="bibr" target="#b9">[10]</ref> evaluating consistent body part detections is used for performance comparison. Additionally, we report median running time per frame measured in seconds <ref type="bibr" target="#b3">4</ref> . Evaluation of unaries and pairwise. The results are shown in Tab. 3. Baseline DeepCut achieves 33.3% AP. Using the proposed pairwise significantly improves performance achieving 47.7% AP. This clearly shows the advantages of using image-conditioned pairwise to disambiguate the body part assignment for multiple overlapping individuals. Remarkably, the proposed pairwise dramatically reduce the run-time by two orders of magnitude (1987 vs. 259220 s/frame). This underlines the argument that using strong pairwise in the fully-connected model allows to significantly speed-up the inference. Using additionally the proposed part detectors further boosts the performance (52.3 vs. 47.7% AP), which can be attributed to better quality part hypotheses. Run-time is again almost halved, which clearly shows the importance of obtaining high-quality part detection candidates for more accurate and faster inference. Performing location refinement before NMS slightly improves the performance, but also reduces the run-time by 2x: this allows to increase the density of detections at the most probable body part locations and thus suppresses more detections around the most confident ones, which leads to better distribution of part detection candidates and reduces confusion generated by the near-by detections. Overall, we observe significant performance improvement and dramatic reduction in run-time by the proposed DeeperCut compared to the baseline DeepCut.</p><p>Ablation study of pairwise. An ablation study of the proposed image-conditioned pairwise is performed in Tab. 4. Regressing from both joints onto the opposite joint's location and including angles achieves the best performance of 52.6% AP and the minimum run-time of 578 s/frame. Regressing from a single joint only slightly reduces the performance to 51.1% AP, but significantly increases run-time by 4x: these pairwise are less robust compared to the bi-directional, which confuses the inference. Removing the angles from the pairwise features also decreases the performance (51.3 vs. 52.6% AP) and doubles run-time, as it removes the information about body part orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Incremental Optimization</head><p>Solving one instance of the DeepCut ILP for all body part candidates detected for an image, as suggested in <ref type="bibr" target="#b9">[10]</ref> and summarized in Sec. 2, is elegant in theory but disadvantageous in practice: Firstly, the time it takes to compute constant-factor approximative feasible solution by the branch-and-cut algorithm <ref type="bibr" target="#b9">[10]</ref> can be exponential in the number of body part candidates in the worst case. In practice, this limits the number of candidates that can be processed by this algorithm. Due to this limitation, it does happen that body parts and, for images showing many persons, entire persons are missed, simply because they are not contained in the set of candidates.</p><p>Secondly, solving one instance of the optimization problem for the entire image means that no distinction is made between part classes detected reliably, e.g. head and shoulders, and part classes detected less reliably, e.g. wrists, elbows and ankles. Therefore, it happens that unreliable detections corrupt the solution.</p><p>To address both problems, we solve not one instance of the DeepCut ILP but several, starting with only those body part classes that are detected most reliably and only then considering body part classes that are detected less reliably. Concretely, we study two variants of this incremental optimization approach which are defined in Tab. 5. Specifically, the procedure works as follows:</p><p>For each subset of body part classes defined in Tab. 5, an instance of the DeepCut ILP is set up and a constant-factor approximative feasible solution computed using the branch-and-cut algorithm. This feasible solution selects, labels and clusters a subset of part candidates, namely of those part classes that are considered in this instance. For the next instance, each cluster of body part candidates of the same class from the previous instance becomes just one part candidate whose class is fixed. Thus, the next instance is an optimization problem for selecting, labeling and clustering body parts that have not been determined by previous instances. Overall, this allows us to start with more part candidates consistently and thus improve the pose estimation result significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation of Incremental Optimization</head><p>Results are shown in Tab. 6. Single stage optimization with |D| = 100 part detection candidates achieves 52.6% AP (best from Tab. 3). More aggressive NMS with radius of 24 px improves the performance (54.5 vs. 52.6% AP), as it allows to better distribute detection candidates. Increasing |D| to 150 slightly improves the performance by +0.6% AP, but significantly increases run-time   (1041 vs. 596 s/frame). We found |D| = 150 to be maximum total number of detection candidates (11 per part) for which optimization runs in a reasonable time.</p><p>Incremental optimization of 2-stage inference slightly improves the performance (56.5 vs. 55.1% AP) as it allows for a larger number of detection candidates per body part <ref type="bibr" target="#b19">(20)</ref> and leverages typically more confident predictions of the upper body parts in the first stage before solving for the entire body. Most importantly, it halves the median run-time from 1041 to 483 s/frame. Incremental optimization of 3-stage inference again almost halves the run-time to 271 s/frame while noticeably improving the human pose estimation performance for all body parts but elbows achieving 57.6% AP. These results clearly demonstrate the advantages of the proposed incremental optimization. Splitting the detection candidates that simultaneously belong to multiple body parts with high confidence slightly improves the performance to 58.7% AP. This helps to overcome the limitation that each detection candidate can be assigned to a single body part and improves on cases where two body parts overlap thus sharing the same detection candidate. We also compare the obtained results to DeepCut in Tab. 6 (last row). The proposed DeeperCut outperforms baseline DeepCut (58.7 vs. 33.3% AP) by almost doubling the performance, while run-time is reduced dramatically by 3 orders of magnitude from the infeasible 259220 s/frame to affordable 270 s/frame. This comparison clearly demonstrates the power of the proposed approach and dramatic effects of better unary, pairwise and optimization on the overall pose estimation performance and run-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison to the State of the Art</head><p>We compare to others on MPII Multi-Person Test and WAF <ref type="bibr" target="#b21">[22]</ref> datasets.</p><p>Results on MPII Multi-Person. For direct comparison with DeepCut we evaluate on the same subset of 288 testing images as in <ref type="bibr" target="#b9">[10]</ref>. Additionally, we Remarkably, the run-time is reduced from 57995 to 230 s/frame, which is an improvement by two orders of magnitude. Both results underline the importance of strong image-conditioned pairwise terms and incremental optimization to maximize multi-person pose estimation performance at the reduced run-time. A similar trend is observed on the full set: 3-stage optimization improves over a single stage optimization (59.4 vs. 54.7% AP). We observe that the performance on the entire testing set is over 10% AP lower compared to the subset and runtime is doubled. This implies that the subset of 288 images is easier compared to the full testing set. We envision that performance differences between DeeperCut and DeepCut on the entire set will be at least as large as when compared on the subset. We also compare to a strong two-stage baseline: first each person is pre-localized by applying the state-of-the-art detector <ref type="bibr" target="#b36">[37]</ref> following by NMS and retaining rectangles with scores at least 0.8; then pose estimation for each rectangle is performed using DeeperCut unary only. Being significantly faster (1 s/frame) this approach reaches 51.0% AP vs. 59.4% AP by DeeperCut, which clearly shows the power of joint reasoning by the proposed approach.</p><p>Excluding out-of-group predictions. Qualitative analysis of group crops generated by our cropping procedure reveals that crops often include people from other groups due to excessive crop sizes. DeeperCut is being penalized for correctly predicting poses of such individuals outside of each group: in AP-based evaluation such predictions are treated as false positives thus significanly decreasing the AP performance. In order to address this issue we exclude such predictions in the following way: we extend provided bounding box aroud each group's center by a constant padding (37 pixels in the scale-normalized image), and use the extended bounding box to filter out all predicted poses whose centers of mass fall outside of the bounding box. This significantly improves DeeperCut results (70.0 vs. 59.4% AP), as well as the results of the baseline two-stage approach (59.7 vs. 51.0% AP). Proposed DeeperCut significantly improves over the strong two-stage baseline (70.0 vs. 59.7% AP), which is in agreement with our observations reported above.   DeeperCut and the baseline DeepCut are not as pronounced compared to MPII Multi-Person dataset. This is due to the fact that actual differences are washed out by the peculiarities of the mPCP evaluation measure: mPCP assumes that people are pre-detected and human pose estimation performance is evaluated only for people whose upper body detections match the ground truth. Thus, a pose estimation method is not penalized for generating multiple body pose predictions, since the only pose prediction is considered whose upper body bounding box best matches the ground truth. We thus re-evaluate the competing approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref> using the more realistic AP evaluation measure 5 . The results are shown in Tab. 9.</p><p>DeeperCut significantly improves over DeepCut (82.0 vs. 76.2% AP). The largest boost in performance is achieved for head (+16.0% AP) and wrists (+5.2% AP):</p><p>DeeperCut follows incremental optimization strategy by first solving for the most reliable body parts, such as head and shoulders, and then using the obtained solution to improve estimation of harder body parts, such as wrists. Most notably, run-time is dramatically reduced by 3 orders of magnitude from 22000 to 13 s/frame. These results clearly show the advantages of the proposed approach when evaluated in the real-world detection setting. The proposed DeeperCut also outperforms <ref type="bibr" target="#b23">[24]</ref> by a large margin. The performance difference is much more pronounced compared to using mPCP evaluation measure: in contrast to mPCP, AP penalizes multiple body pose predictions of the same person. We envision that better NMS strategies are likely to improve the AP performance of <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we significantly advanced the state of the art in articulated multiperson human pose estimation. To that end we carefully re-designed and thor-oughly evaluated several key ingredients. First, drawing on the recent advances in deep learning we proposed strong extremely deep body part detectors that -taken alone -already allow to obtain state of the art performance on standard pose estimation benchmarks. Second, we introduce novel image-conditioned pairwise terms between body parts that allow to significantly push the performance in the challenging case of multi-people pose estimation, and dramatically reduce the run-time of the inference in the fully-connected spatial model. Third, we introduced a novel incremental optimization strategy to further reduce the run-time and improve human pose estimation accuracy. Overall, the proposed improvements allowed to almost double the pose estimation accuracy in the challenging multi-person case while reducing the run-time by 3 orders of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Qualitative Evaluation on MPII Multi-Person Dataset</head><p>We perform qualitative analysis of the proposed DeeperCut approach on MPII Multi-Person dataset. First, we visualize novel image-conditioned pairwise terms. Then, we demonstrate successful and failure cases of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Image-Conditioned Pairwise</head><p>In order to visually analyze the proposed pairwise terms for a particular body part c, we proceed as follows. First, we randomly select a person in the image. Next, for each body part c = c we fix location of c at its ground truth location (to separate the effects of possible misdetection) and predict the location of c using the learned regressor. Then, we compute the pairwise probability p(cc ) 6 for every possible location of c in the image. <ref type="figure">Fig. 4</ref>, rows 1-5, shows the probability of c =right knee anywhere in the image given the fixed location of other body parts.</p><p>It can be seen that individual pairwise scoremaps have shape of cone extending towards the correct location, but are visually quite fuzzy. In order to visualize the effects of the pairwise predictions interacting in the fully-connected spatial model, we combine individual scoremaps by multiplying them. Combined scoremap is shown in <ref type="figure">Fig. 4</ref>, row 6. Clearly, interplay of individual pairwise scoremaps produces a strong evidence for a single right knee of a selected individual. This is in contrast to multi-modal unary scoremaps <ref type="figure" target="#fig_7">(Fig. 4, row 7)</ref> that show a strong response of any knee in the image. This clearly shows that the proposed pairwise terms can be successfully used to filter out body part locations belonging to multiple people thus effectively disambiguating between individuals. <ref type="figure">Fig. 5</ref> shows more examples of combined pairwise scoremaps for each body part of randomly selected individuals, and unary scoremaps. It can be seen that although combined pairwise scoremaps are more fuzzy compared to the unary scoremaps, they allow to filter out the body part detections of other individuals when predicting the pose of the person in question. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Examples of Successful and Failure Cases</head><p>In <ref type="figure" target="#fig_5">Fig. 6</ref> we include additional examples of successful pose estimation results by DeeperCut. The proposed approach correctly resolves cases with only subset of body parts of a person visible in the image. For example, in the image 4 it correctly outputs only visible parts for the person shown in red. DeeperCut is also able to correctly assemble body parts even for rare body articulations as in the image 25. Remarkably, the proposed approach also correctly handles cases of strong partial occlusions. For example, in the image 1 body parts are correctly associated to subjects shown in blue, cyan and magenta, even though only small portion of the cyan subject is visible in the image. We illustrate and analyze the failure cases of DeeperCut in <ref type="figure" target="#fig_7">Fig. 7</ref>. The included examples further illustrate the difficulty of the task of jointly estimating body articulations of multiple people. We identify several prominent failure modes, and include examples for each mode. First row shows examples of cases when DeeperCut generates a body configuration by merging body parts of several people.</p><p>In these examples the proposed pairwise terms failed to disambiguate between people due to their close proximity in the image. For example, in the image 7 DeeperCut groups left and right limbs of the dancing pair. In the resulting configuration the positions of upper limbs are geometrically consistent with each other, but are not consistent with respect to their appearance. Modeling consistency in appearance between left and right extremities should help to mitigate this type of errors and we will aim to address this in the future work. Another type of errors shown at the bottom of <ref type="figure" target="#fig_7">Fig. 7</ref> corresponds to cases when DeeperCut outputs spurious body configurations that can not be assigned to any ground-truth annotation. This happens either when body parts of the same person are grouped into several distinct clusters, or when a consistent body configuration is formed from detections in background. Both of these cases are penalized by the AP evaluation measure. In <ref type="figure" target="#fig_7">Fig. 7</ref> we also visualize examples of failures due to confusion between similarly looking left and right limbs as in the image 10 and occasional failures of pose estimation on rare body configurations as in image 14.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Sample multi-person pose estimation results by the proposed DeeperCut.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Visualizations of regression predictions. Top: from left shoulder to the right shoulder (green), right hip (red), left elbow (light blue), right ankle (purple) and top of the head (dark blue). Bottom: from right knee to the right hip (green), right ankle (red), left knee (dark blue), left ankle (light blue) and top of the head (purple). Longer-range predictions, such as e.g. shoulder -ankle may be less accurate for harder poses (top row, images 2 and 3) compared to the nearby predictions. However, they provide enough information to constrain the search space in the fully-connected spatial model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Visualization of features extracted to score the pairwise. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 ,</head><label>2</label><figDesc>and the absolute angle θ f = | (ô dd , o d cc )| where f stands for forward direction, i.e from d to d . Similarly, we incorporate the prediction o d c c in the backwards direction by computing ∆ b = ô d d − o d c c 2 and θ b = | (ô d d , o d c c )|. Finally, we define a feature vector by augmenting features with exponential terms:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>pFig. 4 .Fig. 5 .</head><label>45</label><figDesc>(r knee | r ankle) p(r knee | r hip) p(r knee | l ankle) p(r knee | l knee) p(r knee | l hip) pairwise per part p(r knee | r wrist) p(r knee | r elbow) p(r knee | r shoulder) p(r knee | l wrist) p(r knee | l elbow) p(r knee | l shoulder) p(r knee | chin) p(r knee | top head) Visualizations of the pairwise probabilities when the right knee of the middle person is used as target and fixed locations of other body parts as source. See text for explanation. Visualizations of all unary scoremaps (rows 2, 4 and 6) and all combined pairwise scoremaps (rows 1, 3 and 5). Pairwise scoremaps are visualized for a randomly selected individual to avoid clutter: for each target body part individual predictions from all other body parts are obtained and combined the same way as in Fig. 4. Color-coding corresponds to different body parts. Multiple scoremaps are overlaid on the same image for visualization purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Examples of successful pose estimation results obtained with our DeeperCut model on the MPII Multi-Person dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Failure cases of our DeeperCut model on the MPII Multi-Person dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>SettingHead Sho Elb Wri Hip Knee Ank PCK AUCResNet-50 (8 px) 96.9 90.3 85.0 81.5 88.6 87.3 84.8 87.8 63.7 ResNet-50 (16 px + 2x up-sample) 96.7 89.8 84.6 80.4 89.3 86.4 82.8 87.2 63.1 ResNet-101 (16 px + 2x up-sample) 96.9 91.2 85.8 82.6 90.9 90.2 85.9 89.1 64.6 ResNet-152 (16 px + 2x up-sample) 97.4 91.7 85.7 82.4 90.1 89.2 86.9 89.1 65.1 + intermediate supervision 97.4 92.7 87.5 84.4 91.5 89.9 87.2 90.1 66.1 DeepCut [10] 97.0 91.0 83.8 78.1 91.0 86.7 82.0 87.1 63.5 Wei et al. [11] 97.8 92.5 87.0 83.9 91.5 90.8 89.9 90.5 65.4 Pose estimation results (PCK) on LSP (PC) dataset.</figDesc><table><row><cell>Tompson et al. [8]</cell><cell>90.6 79.2 67.9 63.4 69.5 71.0 64.2 72.3 47.3</cell></row><row><cell>Chen&amp;Yuille [9]</cell><cell>91.8 78.2 71.8 65.5 73.3 70.2 63.4 73.4 40.1</cell></row><row><cell>Fan et al. [35]</cell><cell>92.4 75.2 65.3 64.0 75.7 68.3 70.4 73.0 43.2</cell></row><row><cell>Setting</cell><cell>Head Sho Elb Wri Hip Knee Ank PCK h AUC</cell></row><row><cell>ResNet-152</cell><cell>96.3 94.1 88.6 83.9 87.2 82.9 77.8 87.8 60.0</cell></row><row><cell cols="2">+ intermediate supervision 96.8 95.2 89.3 84.4 88.4 83.4 78.0 88.5 60.8</cell></row><row><cell>DeepCut [10]</cell><cell>94.1 90.2 83.4 77.3 82.6 75.7 68.6 82.4 56.5</cell></row><row><cell>Tompson et al. [8]</cell><cell>95.8 90.3 80.5 74.3 77.6 69.7 62.8 79.6 51.8</cell></row><row><cell>Carreira et al. [36]</cell><cell>95.7 91.7 81.7 72.4 82.8 73.2 66.4 81.3 49.1</cell></row><row><cell>Tompson et al. [20]</cell><cell>96.1 91.9 83.9 77.8 80.9 72.3 64.8 82.0 54.9</cell></row><row><cell>Wei et al. [11]</cell><cell>97.8 95.0 88.7 84.0 88.4 82.8 79.4 88.5 61.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Pose estimation results (PCK h ) on MPII Single Person. The results are compared to the state of the art in Tab. 1. Our best model significantly outperforms DeepCut [10] (90.1% PCK vs. 87.1% PCK), as it relies on deeper detection architectures. Our model performs on par with the recent approach of Wei et al. [11] (90.1 vs. 90.5% PCK, 66.1 vs. 65.4 AUC). This is interesting, as they use a much more complex multi-scale multi-stage architecture. Results on MPII Single Person. The results are shown in Tab. 2. ResNet-152 achieves 87.8% PCK h and 60.0% AUC, while intermediate supervision slightly improves the performance further to 88.5% PCK h and 60.8% AUC. Comparing the results to the state of the art we observe significant improvement over DeepCut</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>59.8 53.1 44.4 50.0 46.4 39.5 52.3 1171 + location refinement before NMS 70.3 61.6 52.1 43.7 50.6 47.0 40.6 52.6 578</figDesc><table><row><cell>Unary</cell><cell>Pairwise</cell><cell cols="2">Head Sho Elb Wri Hip Knee Ank AP time [s/frame]</cell></row><row><cell>DeepCut [10]</cell><cell>DeepCut [10]</cell><cell>50.1 44.1 33.5 26.5 33.0 28.5 14.4 33.3</cell><cell>259220</cell></row><row><cell>DeepCut [10]</cell><cell>this work</cell><cell>68.3 58.3 47.4 38.9 45.2 41.8 31.2 47.7</cell><cell>1987</cell></row><row><cell>this work</cell><cell>this work</cell><cell>70.9</cell><cell></cell></row></table><note>2] consisting of 3844 training and 1758 testing groups of multiple overlapping people in highly articulated poses with a variable number of parts. We perform all intermediate experiments on a validation set of 200 images sampled uniformly at random and refer to it as</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Effects of proposed pairwise and unaries on the pose estimation performance (AP) on MPII Multi-Person Val. Effects of different versions of the pairwise terms on the pose estimation performance (AP) on MPII Multi-Person Val.</figDesc><table><row><cell>Setting</cell><cell cols="2">Head Sho Elb Wri Hip Knee Ank AP time [s/frame]</cell></row><row><cell cols="2">bi-directional + angle 70.3 61.6 52.1 43.7 50.6 47.0 40.6 52.6</cell><cell>578</cell></row><row><cell cols="2">uni-directional + angle 69.3 58.4 51.8 44.2 50.4 44.7 36.3 51.1</cell><cell>2140</cell></row><row><cell>bi-directional</cell><cell>68.8 58.3 51.0 42.7 51.1 46.5 38.7 51.3</cell><cell>914</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>As the run-time of the DeepCut branch-and-cut algorithm limits the number of part candidates that can be processed in practice, we split the set of part classes into subsets, coarsely and finely, and solve the pose estimation problem incrementally.</figDesc><table><row><cell>Setting</cell><cell cols="2">Head Sho Elb Wri Hip Knee Ank AP time [s/frame]</cell></row><row><cell cols="2">1-stage optimize, 100 det, nms 1x 70.3 61.6 52.1 43.7 50.6 47.0 40.6 52.6</cell><cell>578</cell></row><row><cell cols="2">1-stage optimize, 100 det, nms 2x 71.3 64.1 55.8 44.1 53.8 48.7 41.3 54.5</cell><cell>596</cell></row><row><cell cols="2">1-stage optimize, 150 det, nms 2x 74.1 65.6 56.0 44.3 54.4 49.2 39.8 55.1</cell><cell>1041</cell></row><row><cell>2-stage optimize</cell><cell>75.9 66.8 58.8 46.1 54.1 48.7 42.4 56.5</cell><cell>483</cell></row><row><cell>3-stage optimize</cell><cell>78.3 69.3 58.4 47.5 55.1 49.6 42.5 57.6</cell><cell>271</cell></row><row><cell>+ split detections</cell><cell>78.5 70.5 59.7 48.7 55.4 50.6 44.4 58.7</cell><cell>270</cell></row><row><cell>DeepCut [10]</cell><cell>50.1 44.1 33.5 26.5 33.0 28.5 14.4 33.3</cell><cell>259220</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Performance</figDesc><table /><note>(AP) of different hierarchical versions of DeeperCut on MPII Multi-Person Val.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Pose estimation results (AP) on MPII Multi-Person.provide the results on the entire testing set. Results are shown in Tab. 7. DeeperCut without incremental optimization already outperforms DeepCut by a large margin (66.2 vs. 54.1% AP). Using 3-stage incremental optimization further improves the performance to 69.7% AP improving by a dramatic 16.5% AP over the baseline.</figDesc><table><row><cell>Setting</cell><cell cols="2">Head Sho Elb Wri Hip Knee Ank AP time [s/frame]</cell></row><row><cell></cell><cell>subset of 288 images as in [10]</cell><cell></cell></row><row><cell>DeeperCut (1-stage)</cell><cell>83.3 79.4 66.1 57.9 63.5 60.5 49.9 66.2</cell><cell>1333</cell></row><row><cell>DeeperCut</cell><cell>87.5 82.8 70.2 61.6 66.0 60.6 56.5 69.7</cell><cell>230</cell></row><row><cell>DeepCut [10]</cell><cell>73.4 71.8 57.9 39.9 56.7 44.0 32.0 54.1</cell><cell>57995</cell></row><row><cell></cell><cell>full set</cell><cell></cell></row><row><cell>DeeperCut (1-stage)</cell><cell>73.7 65.4 54.9 45.2 52.3 47.8 40.7 54.7</cell><cell>2785</cell></row><row><cell>DeeperCut</cell><cell>79.1 72.2 59.7 50.0 56.0 51.0 44.6 59.4</cell><cell>485</cell></row><row><cell cols="2">Faster R-CNN [37] + unary 64.9 62.9 53.4 44.1 50.7 43.1 35.2 51.0</cell><cell>1</cell></row><row><cell></cell><cell>full set, excluding out-of-group predictions</cell><cell></cell></row><row><cell>DeeperCut</cell><cell>89.4 84.5 70.4 59.3 68.9 62.7 54.6 70.0</cell><cell>485</cell></row><row><cell cols="2">Faster R-CNN [37] + unary 75.1 73.6 62.7 51.0 61.1 52.6 42.2 59.7</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Pose estimation results (mPCP) on WAF dataset.</figDesc><table><row><cell>Setting</cell><cell cols="2">Head Sho Elb Wri AP time [s/frame]</cell></row><row><cell>DeeperCut</cell><cell>92.6 81.1 75.7 78.8 82.0</cell><cell>13</cell></row><row><cell>DeepCut [10]</cell><cell>76.6 80.8 73.7 73.6 76.2</cell><cell>22000</cell></row><row><cell cols="2">Chen&amp;Yuille [24] 83.3 56.1 46.3 35.5 55.3</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Pose estimation results (AP) on WAF dataset.Results on WAF. Results using the official evaluation protocol<ref type="bibr" target="#b21">[22]</ref> assuming mPCP and AOP evaluation measures and considering detection bounding boxes provided by<ref type="bibr" target="#b21">[22]</ref> are shown in Tab. 8. DeeperCut achieves the best result improving over the state of the art DeepCut (86.3 vs. 84.7% mPCP, 88.1 vs. 86.5% AOP). Noticeable improvements are observed both for upper (+2.3% mPCP) and lower (+2.4% mPCP) arms. However, overall performance differences between</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use NVIDIA Tesla K40 GPU with 12 GB RAM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://human-pose.mpi-inf.mpg.de/#evaluation 4 Run-time is measured on a single core Intel Xeon 2.70GHz</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We used publicly-available pose predictions of<ref type="bibr" target="#b23">[24]</ref> for all people in WAF dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We intentionally simplify the notation compared to Eq. (1).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<idno>BMVC&apos;10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>CVPR&apos;14</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks. In: NIPS&apos;12</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Discriminative appearance models for pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>CVPR&apos;13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;14</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno>NIPS&apos;14</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;16</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Convolutional pose machines. In: CVPR&apos;16</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CVPR&apos;16</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to parse images of articulated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;06</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Global pose estimation using non-tree models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<idno>CVPR&apos;09</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning Effective Human Pose Estimation from Inaccurate Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<idno>CVPR&apos;11</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improved human parsing with a full relational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<idno>ECCV&apos;10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Beyond physical connections: Tree models in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno>CVPR&apos;13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>ICCV&apos;13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Articulated pose estimation using discriminative armlet classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>CVPR&apos;13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<idno>CVPR&apos;15</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno>ECCV&apos;14</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">We are family: Joint pose estimation of multiple persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>ECCV&apos;10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CVPR&apos;13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Parsing occluded people by flexible compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno>CVPR&apos;15</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<title level="m">Correlation clustering. ML&apos;04</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Correlation clustering in general weighted graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Demaine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>CVPR&apos;15</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>ICLR&apos;15</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno>AISTATS&apos;15</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>ICCV&apos;15</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<idno>CVPR&apos;13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CVPR&apos;14</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>CVPR&apos;16</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;15</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Parsing occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno>CVPR&apos;14</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
