<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Training for Large Neural Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<postCode>365 AI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<postCode>365 AI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<postCode>365 AI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<postCode>365 AI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<postCode>365 AI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
							<email>hoifung@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<postCode>365 AI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<postCode>365 AI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<postCode>365 AI</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Training for Large Neural Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pretraining can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and taskspecific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generalization and robustness are two fundamental considerations in assessing machine learning methods. Ideally, a learned model should perform well on unseen test examples and withstand adversarial attacks. In natural language processing (NLP), pretraining neural language models on unlabeled text has proven very effective to improve generalization performance for a variety of downstream tasks, as exemplified by BERT <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref> and other transformer-based models <ref type="bibr" target="#b15">(Liu et al., 2019c;</ref><ref type="bibr" target="#b23">Radford et al., 2018;</ref><ref type="bibr" target="#b6">Clark et al., 2020;</ref><ref type="bibr">Dong et al., 2019;</ref><ref type="bibr" target="#b0">Bao et al., 2020)</ref>. However, these models may still suffer catastrophic failures in adversarial scenarios <ref type="bibr" target="#b21">(Nie et al., 2019;</ref><ref type="bibr">Hsieh et al., 2019)</ref>. For example, <ref type="bibr">Jin et al. (2019)</ref> show that classification accuracy on a Yelp dataset drops from 95.6% on standard test to 6.8% on robust test for a BERT model.</p><p>Adversarial training <ref type="bibr" target="#b16">(Madry et al., 2017;</ref><ref type="bibr">Goodfellow et al., 2014)</ref> has been well studied in computer vision, but past work shows that it often hurts generalization <ref type="bibr" target="#b25">(Raghunathan et al., 2019;</ref><ref type="bibr" target="#b17">Min et al., 2020)</ref>. In NLP, there is growing interest in adversarial training, but existing work typically focuses on assessing the impact on generalization <ref type="bibr" target="#b46">(Zhu et al., 2019;</ref><ref type="bibr" target="#b5">Jiang et al., 2019;</ref>. Moreover, adversarial training is generally limited to task-specific fine-tuning 1 . See <ref type="bibr">Minaee et al. (2020a)</ref> for a recent survey.</p><p>In this paper, we present the first comprehensive study on adversarial pre-training, and show that it can improve both generalization and robustness for a wide range of NLP tasks. We propose a unifying algorithm ALUM (Adversarial training for large neural LangUage Models), which augments the standard training objective with an additional term that maximizes the adversarial loss via applying perturbation in the embedding space. ALUM is generally applicable to pre-training and fine-tuning, on top of any Transformer-based language models.</p><p>We conduct a comprehensive evaluation on various NLP tasks across multiple benchmark datasets, including GLUE, SQuAD v1.1/v2.0, SNLI, Sci-Tail for assessing model generalization, and ANLI, HELLSWAG, SWAG, Adversarial SQuAD for assessing model robustness. Experimental results show that by conducting adversarial pre-training, ALUM attains significant improvements, often outperforming previous state of the art by a large margin. This is true even for the extremely well-trained RoBERTa model, where continual pre-training without adversarial training fails to attain any gain.</p><p>Remarkably, in addition to improving generalization, we find that adversarial pre-training also substantially improves robustness, as exemplified by the resulting large gains in adversarial datasets such as ANLI, Adversarial-SQuAD, HELLASWAG, which significantly reduces the gap between standard errors and robust errors for popular models like BERT and RoBERTa. This suggests that adversarial training on unlabeled data can provide a promising direction to reconcile the apparent conflict between generalization and robustness as observed in prior work <ref type="bibr" target="#b25">(Raghunathan et al., 2019;</ref><ref type="bibr" target="#b17">Min et al., 2020)</ref>. We also show that adversarial pre-training can be combined with adversarial finetuning, resulting in extra gains.</p><p>Our contributions are summarized as follows:</p><p>• We propose ALUM, a general algorithm to incorporate adversarial training for pre-training and fine-tuning large neural language models.</p><p>• We conduct a comprehensive evaluation on a wide range of NLP tasks and assess the impact of adversarial training in pre-training from scratch, continual pre-training, task-specific fine-tuning, and their combinations.</p><p>• We obtain significant improvements over prior state of the art, including extremely welltrained models such as RoBERTa, in both generalization and robustness.</p><p>• To facilitate research, we will release our code and pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>In this section, we give a quick overview of language model pre-training, using BERT <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref> as a running example for transformerbased neural language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Input Representation</head><p>We assume that the input consists of text spans (typically sentences) separated by a special token <ref type="bibr">[SEP ]</ref>. To address the problem of out-ofvocabulary words, tokens are divided into subword units, using Byte-Pair Encoding (BPE) <ref type="bibr" target="#b28">(Sennrich et al., 2015)</ref> or its variants <ref type="bibr">(Kudo and Richardson, 2018)</ref>, which generates a fixed-size subword vocabulary to compactly represent words in training text corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Architecture</head><p>Following recent pre-training methods <ref type="bibr" target="#b9">(Devlin et al., 2018;</ref><ref type="bibr" target="#b15">Liu et al., 2019c)</ref>, we use transformerbased models <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref> to leverage a multi-head attention mechanism, which have demonstrated superiority in parallel computation and modeling long-range dependencies, compared to recurrent neural networks such as LSTM <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref>. The input is first passed to a lexical encoder, which combines a token embedding, a (token) position embedding and a segment embedding (i.e., which text span the token belongs to) by element-wise summation. The embedding layer is then passed to multiple layers of transformer modules to generate the contextual representation <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self Supervision</head><p>A key innovation in BERT <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref> is the use of Masked Language Model (MLM) for self-supervised pre-training. Instead of predicting the next token based on the preceding tokens, as in traditional generative language models, MLM randomly replaces a subset of tokens by a special token (e.g., [M ASK]), and asks the model to predict them. Essentially, it is a cloze task <ref type="bibr" target="#b35">(Taylor, 1953)</ref>, where the training objective is the crossentropy loss between the original tokens and the predicted ones. In BERT and RoBERTa, 15% of the input tokens are chosen, among which a random 80% are replaced by [M ASK], 10% are left unchanged and 10% are randomly replaced by a token from the vocabulary. In our experiments, instead of using a fixed masked rate of 15%, we gradually increase it from 5% to 25% with 5% increment for every 20% of training epochs, as we find this makes pre-training more stable. Additionally, BERT also uses Next Sentence Prediction (NSP), which is a binary classification task that for a given sentence pair determines whether one sentence follows the other in the original text. There have debates on how much NSP helps <ref type="bibr" target="#b15">(Liu et al., 2019c)</ref>. But we include it in our experiments for a fair comparison with BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALUM (Adversarial training for large neural LangUage Models)</head><p>In this section, we first present a unifying view of standard training objectives and prior approaches to adversarial training. We then present ALUM, which is a general adversarial training algorithm applicable to pre-training and fine-tuning, on top of any transformer-based neural language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Standard Training Objectives</head><p>Both pre-training and fine-tuning can be viewed as minimizing the standard error on training data, with the training objectives derived from selfsupervision (MLM and NSP in pre-training) or direct supervision (labeled examples in task-specific fine-tuning), respectively. Specifically, the training algorithms seek to learn a function f (x; θ) : x → C, parametrized by θ. In MLM, C is the vocabulary, and f (x; θ) tries to predict the masked token y. In fine-tuning, C is the task-specific label set, and f (x; θ) is the classifier. Given a training dataset D of input-output pairs (x, y) and the loss function l(., .) (e.g., cross entropy), f (x; θ) is trained to minimize the empirical risk:</p><p>min</p><formula xml:id="formula_0">θ E (x,y)∼D [l(f (x; θ), y)]<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial Training</head><p>Pre-training a large neural language model such as BERT has proven effective to improve generalization performance in task-specific fine-tuning <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref>. However, such models can still suffer catastrophic loss in adversarial scenarios <ref type="bibr" target="#b21">(Nie et al., 2019;</ref><ref type="bibr">Hsieh et al., 2019;</ref><ref type="bibr" target="#b16">Madry et al., 2017;</ref><ref type="bibr">Jin et al., 2019)</ref>, with attacks as simple as replacing a few words in input sentences while preserving the semantics. To improve model robustness and withstand adversarial attacks, adversarial training has been proposed and studied extensively, predominantly in computer vision literature <ref type="bibr">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b16">Madry et al., 2017)</ref>. The key idea is to modify the training objective by applying small perturbation to input images that maximize the adversarial loss:</p><formula xml:id="formula_1">min θ E (x,y)∼D [max δ l(f (x + δ; θ), y)] (2)</formula><p>where the inner maximization can be solved by running a number of projected gradient descent steps <ref type="bibr" target="#b16">(Madry et al., 2017)</ref>.</p><p>While adversarial training has been successful in mitigating adversarial attacks, past work often encounters an apparent conflict between generalization and robustness <ref type="bibr" target="#b25">(Raghunathan et al., 2019</ref><ref type="bibr" target="#b24">(Raghunathan et al., , 2020</ref><ref type="bibr" target="#b17">Min et al., 2020)</ref>, as adversarial training could hurt generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The ALUM Algorithm</head><p>In NLP, applying adversarial training is not straightforward, since the input are discrete elements (token or subword sequences), but there have been some recent successes <ref type="bibr" target="#b46">(Zhu et al., 2019;</ref><ref type="bibr" target="#b5">Jiang et al., 2019;</ref><ref type="bibr">Minaee et al., 2020b)</ref>. However, aside from , there has not been any prior work on adversarial pre-training, and  only applied adversarial training to generative language modeling using LSTM.</p><p>ALUM is applicable to both pre-training and fine-tuning. It builds on several key ideas that have proven useful in prior work. First, instead of applying perturbation to the input text directly, one would perturb the embedding space. Namely, x is the sub-word embedding in f (x; θ) <ref type="bibr" target="#b5">(Jiang et al., 2019;</ref><ref type="bibr" target="#b46">Zhu et al., 2019)</ref>.</p><p>Second, instead of adopting the adversarial training objective of Eq. 2, as in <ref type="bibr" target="#b46">Zhu et al. (2019)</ref> and most other approaches, we follow Jiang et al.</p><p>(2019) to regularize the standard objective using virtual adversarial training <ref type="bibr" target="#b20">(Miyato et al., 2018)</ref>:</p><formula xml:id="formula_2">min θ E (x,y)∼D [l(f (x; θ), y)+ α max δ l(f (x + δ; θ), f (x; θ))]<label>(3)</label></formula><p>Effectively, the adversarial term favors label smoothness in the embedding neighborhood, and α is a hyperparameter that controls the trade-off between standard errors and robust errors. We found that virtual adversarial training is superior to conventional adversarial training, especially when labels might be noisy. E.g., BERT pretraining uses the masked words as self-supervised labels, but in many cases, they could be replaced by other words to form completely legitimate text. Empirically, we verified that this is indeed the case, as pre-training benefits from larger α. We set α = 10 for pre-training, and α = 1 for fine-tuning in all our experiments.</p><p>Compared to standard training, adversarial training is rather expensive due to the inner maximization. <ref type="bibr" target="#b46">Zhu et al. (2019)</ref> adopted the free adversarial training idea in <ref type="bibr" target="#b29">Shafahi et al. (2019)</ref> for acceleration, by reusing the backward pass for gradient computation to carry out the inner ascent step Algorithm 1 ALUM Input: T : the total number of iterations, X = {(x 1 , y 1 ), ..., (x n , y n )}: the dataset, f (x; θ): the machine learning model parametrized by θ, σ 2 : the variance of the random initialization of perturbation δ, : perturbation bound, K: the number of iterations for perturbation estimation, η: the step size for updating perturbation, τ : the global learning rate, α: the smoothing proportion of adversarial training in the augmented learning objective, Π: the projection operation. 1: for t = 1, .., T do 2:</p><formula xml:id="formula_3">for (x, y) ∈ X do 3: δ ∼ N (0, σ 2 I) 4: for m = 1, .., K do 5: g adv ← ∇ δ l(f (x; θ), f (x + δ; θ)) 6: δ ← Π δ ∞≤ (δ + ηg adv ) 7: end for 8: g θ ← ∇ θ l(f (x; θ), y) +α∇ θ l(f (x; θ), f (x + δ; θ)) 9: θ ← θ − τ g θ 10:</formula><p>end for 11: end for Output: θ and outer descent step simultaneously. Inspired by ERNIE  and other continual pre-training approaches, we instead adopt a curriculum learning approach: first train the model using the standard objective (1); and then continue the training with virtual adversarial training (3).</p><p>Jiang et al. <ref type="formula" target="#formula_0">(2019)</ref> also incorporated a momentum term using the Bregman proximate point method, which can be quite costly in training time. We found that our curriculum learning approach largely rendered this unnecessary and simplified our algorithm without using this term.</p><p>Algorithm 1 shows the details of ALUM. Line 4-6 run K projected gradient steps to find the perturbation δ that maximizes the adversarial loss (violation of local smoothness). Note that a larger K leads to better approximation <ref type="bibr" target="#b16">(Madry et al., 2017;</ref><ref type="bibr" target="#b22">Qin et al., 2019)</ref>, but it is more expensive. To attain a good trade-off between speed and performance, we set K = 1 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Generalization vs. Robustness</head><p>Empirically, we found that by applying adversarial pre-training using ALUM, we were able to improve both generalization and robustness for a wide range of NLP tasks, as seen in Section 4. This is very interesting as prior work often finds that adversarial training hurts generalization, even with theoretical justification <ref type="bibr" target="#b25">(Raghunathan et al., 2019</ref><ref type="bibr" target="#b24">(Raghunathan et al., , 2020</ref><ref type="bibr" target="#b17">Min et al., 2020)</ref>.</p><p>We hypothesize that adversarial pre-training might be the key for reconciling this apparent incongruence, as prior work on the conflict between generalization and robustness generally focuses on the supervised learning setting. Interestingly, some nascent results in reconciling the two also leverage unlabeled data, such as self-training <ref type="bibr" target="#b24">(Raghunathan et al., 2020)</ref>. Additionally, we hypothesize that by perturbing the embedding space rather than the input space, adversarial training in NLP might inadvertently bias toward on-manifold perturbation than regular perturbation, which helps generalization <ref type="bibr" target="#b33">(Stutz et al., 2019)</ref>. We leave the theoretical analysis of all these connections to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present a comprehensive study of adversarial training on large neural language models. We show that ALUM substantially improves both generalization and robustness in a wide range of NLP tasks, for both the standard BERT model and the extremely well-trained RoBERTa model. We also show that ALUM can be applied to adversarial pre-training and fine-tuning alike and attain further gain by combining the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Pre-training: For BERT pre-training, we use Wikipedia (English Wikipedia dump 2 ; 13GB). For continual pre-training of RoBERTa, we use Wikipedia (13GB), OPENWEBTEXT (public Reddit content (Gokaslan and Cohen); 38GB), STO-RIES (a subset of CommonCrawl (Trinh and Le, 2018); 31GB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NLP application benchmarks:</head><p>To assess the impact of adversarial training on generalization, we use standard benchmarks such as GLUE <ref type="bibr" target="#b39">(Wang et al., 2018)</ref> and SQuAD (v1.1 and v2.0) <ref type="bibr" target="#b27">(Rajpurkar et al., 2016</ref><ref type="bibr" target="#b26">(Rajpurkar et al., , 2018</ref>, as well as three named entity recognition (NER) tasks in the biomedical domain. To evaluate the impact of adversarial training on robustness, we use ANLI <ref type="bibr" target="#b21">(Nie et al., 2019)</ref>, Adversarial SQuAD (Jia and Liang, 2017), and HELLASWAG <ref type="bibr">(Hampel, 1974)</ref>. To assess the combination of adversarial pre-training and fine-tuning, we follow Jiang et al. (2019) and use MNLI <ref type="bibr" target="#b42">(Williams et al., 2018</ref>) (from GLUE), ANLI, SWAG <ref type="bibr" target="#b44">(Zellers et al., 2018)</ref>, <ref type="bibr">SNLI (Bowman et al., 2015)</ref>, <ref type="bibr">SciTail (Khot et al., 2018)</ref>. These benchmarks cover a wide range of NLP tasks such as named entity recognition, textual entailment, and machine reading comprehension, spanning classification, ranking, and regression. For details, see Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We perform three types of adversarial training in our experiments: pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning.</p><p>We pre-train BERT models from scratch using Wikipedia 3 . The training code is based on Megatron, implemented in PyTorch <ref type="bibr">(Shoeybi et al., 2019) 4</ref> . We use ADAM (Kingma and Ba, 2014) for the optimizer with a standard learning rate schedule that increases linearly from zero to the peak rate of 1 × 10 −4 in first one percent of steps, and then decays linearly to zero in the remaining 99% of steps. Following <ref type="bibr" target="#b9">Devlin et al. (2018)</ref>, training is done for one million steps with batch size of 256. We set the perturbation size = 1 × 10 −5 , the step size η = 1 × 10 −3 , and the variance for initializing perturbation σ = 1 × 10 −5 . We set α = 10 for heightened regularization in virtual adversarial training, and set K = 1 for training efficiency (i.e., one projected gradient step). The training takes 10 days on one DGX-2 machine with 16 V100 GPUs.</p><p>For continual pre-training of RoBERTa <ref type="bibr" target="#b15">(Liu et al., 2019c)</ref>, we use RoBERTa's default training parameters, except a smaller learning rate (4 × 10 −5 ), and run for 100K training steps with a batch size of 256 on the union of Wikipedia, OPEN-WEBTEXT, and STORIES (total size 82GB). The code is based on FairSeq 5 . The training takes 7 days on two DGX-2 machines.</p><p>For fine-tuning with or without adversarial training, we use the MT-DNN open-sourced toolkit <ref type="bibr" target="#b12">(Liu et al., , 2015</ref> 6 . We follow <ref type="bibr" target="#b5">Jiang et al. (2019)</ref> for head-to-head comparison, which uses ADAM (Kingma and Ba, 2014) and RADAM <ref type="bibr" target="#b11">(Liu et al., 2019a</ref>) as our optimizers, with peak learning rates of {5 × 10 −6 , 8 × 10 −6 , 1 × 10 −5 , 2 × 10 −5 }, and batch sizes of 16, 32 or 64, depending on the tasks. <ref type="bibr">3</ref> BookCorpus is no longer publicly available. 4 https://github.com/NVIDIA/Megatron-LM 5 https://github.com/pytorch/fairseq 6 https://github.com/namisan/mt-dnn The dropout rate is set to 0.1 for all the task-specific layers, except 0.3 for MNLI and 0.05 for CoLA. To avoid gradient exploding, the gradient is clipped to keep the norm within 1. All the texts are tokenized using WordPiece and chopped to spans up to 512 tokens. We conduct fine-tuning for up to 10 epochs and pick the best model using the dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Improving Generalization</head><p>In this subsection, we study the impact of adversarial pre-training on generalization, by comparing the performance of pre-trained models in various downstream applications. First, we study the scenario of pre-training from scratch, by comparing three BERT models:</p><p>• BERT BASE is the standard BERT base model trained using the same setting as Devlin et al.</p><p>(2018) (i.e., 1M steps with a batch size of 256).</p><p>• BERT+ BASE is similar to BERT BASE , except that it is trained with 1.6M steps, which takes roughly the same amount of time as that of adversarial pre-training (see ALUM BERT-BASE below).</p><p>• ALUM BERT-BASE is a BERT model trained using the same setting as BERT BASE , except that ALUM is used in the last 500K steps. Each adversarial training step takes approximately 1.5 times longer than a step in standard training 7 .   We also assess the impact of adversarial pretraining in the biomedical domain, which is substantially different from the Wikipedia corpus used in pre-training. <ref type="table" target="#tab_2">Table 2</ref> shows the results on standard biomedical name entity recognition (NER) datasets: BC2GM <ref type="bibr" target="#b31">(Smith et al., 2008)</ref>, NCBI <ref type="bibr" target="#b10">(Dogan et al., 2014)</ref>, JNLPBA <ref type="bibr" target="#b7">(Collier and Kim, 2004)</ref>. Interestingly, ALUM still outperforms the standard BERT model on all three tasks, even though the application domain is substantially different from the pre-training one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Next, we assess the impact of adversarial training in the continual pre-training setting. We use our pre-training dataset (Wikipedia, OPENWEBTEXT,  STORIES; 82GB) 8 , and run 100K steps in all our continual pre-training experiments. We choose the RoBERTa models as the baseline, which use the same neural model as BERT, but were pre-trained on an order of magnitude more text (160GB vs 13GB). They are the state-of-the-art pre-trained language models, outperforming the standard BERT models in many NLP tasks. RoBERTa models are extremely well-trained. Standard continual pre-training fails to attain any gains in downstream applications such as MNLI <ref type="bibr" target="#b42">(Williams et al., 2018)</ref> and SST <ref type="bibr" target="#b32">(Socher et al., 2013)</ref> from GLUE <ref type="bibr" target="#b39">(Wang et al., 2018)</ref>, as shown in <ref type="table" target="#tab_4">Table 3</ref>. On the other hand, ALUM is able to attain further gain from continual pretraining of RoBERTa, as shown in <ref type="table" target="#tab_6">Table 4</ref>. E.g., ALUM ROBERTA-BASE outperforms RoBERTa BASE by +0.5%, and ALUM ROBERTA-LARGE outperforms RoBERTa LARGE by +0.7% on the MNLI development set. This is rather remarkable, as by contrast standard continual pre-training is unable to attain any gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Improving Robustness</head><p>In this subsection, we assess the impact of adversarial pre-training on the model's robustness against adversarial attacks, using three standard adversarial NLP benchmarks: ANLI <ref type="bibr" target="#b21">(Nie et al., 2019)</ref>, HELLASWAG <ref type="bibr" target="#b45">(Zellers et al., 2019) and</ref><ref type="bibr">adversarial SQuAD (Jia and</ref><ref type="bibr">Liang, 2017)</ref>. On ANLI, we follow the experimental setting of <ref type="bibr" target="#b21">Nie et al. (2019)</ref> to enable a head-to-head comparison, which combines four datasets (ANLI, MNLI, SNLI and FEVER <ref type="bibr" target="#b36">(Thorne et al., 2018)</ref>) for fine-tuning.</p><p>Adversarial pre-training substantially improves model robustness, as shown in <ref type="table" target="#tab_7">Table 5</ref> and Table 6. In all three adversarial datasets, ALUM consistently outperformed the standard pre-training counterparts, for BERT and RoBERTa alike. For    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Combining Adversarial Pre-Training and Fine-tuning</head><p>Adversarial training has been shown to be effective in task-specific fine-tuning <ref type="bibr" target="#b5">(Jiang et al., 2019;</ref><ref type="bibr" target="#b46">Zhu et al., 2019)</ref>. In this subsection, we explore combining adversarial pre-training with adversarial fine-tuning. Specifically, we use RoBERTa LARGE as the base model, and compare it with ALUM ROBERTA-LARGE , which uses adversarial continual pre-training but standard fine-tuning, and ALUM RoBERTA-LARGE-SMART , which uses adversarial training in both continual pre-training and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev Test SNLI Dataset (Accuracy%) GPT <ref type="bibr" target="#b23">(Radford et al., 2018)</ref> -89.9 BERT LARGE 91.7 91.0 MT-DNN LARGE <ref type="bibr" target="#b13">(Liu et al., 2019b)</ref>  SciTail Dataset (Accuracy%) GPT <ref type="bibr" target="#b23">(Radford et al., 2018)</ref> -88.3 BERT LARGE <ref type="bibr" target="#b13">(Liu et al., 2019b)</ref> 95.7 94.4 MT-DNN LARGE <ref type="bibr" target="#b13">(Liu et al., 2019b)</ref>   fine-tuning. <ref type="figure" target="#fig_0">Figure 2</ref> shows the results on the development sets of MNLI and ANLI, two rep-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev Test SWAG Dataset (Accuracy%) GPT <ref type="bibr" target="#b23">(Radford et al., 2018)</ref> -78.0 BERT LARGE <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref> -86.3 Human <ref type="bibr" target="#b44">(Zellers et al., 2018)</ref> 88.0 88.0 RoBERTa LARGE <ref type="bibr" target="#b15">(Liu et al., 2019c)</ref> -89.9 ALUM ROBERTA-LARGE 90.7 91.0 ALUM ROBERTA-LARGE-SMART 91.2 -HELLASWAG Dataset (Accuracy%) GPT <ref type="bibr" target="#b45">(Zellers et al., 2019)</ref> 41.9 41.7 BERT LARGE <ref type="bibr" target="#b45">(Zellers et al., 2019)</ref> 46.7 47.3 RoBERTa LARGE <ref type="bibr" target="#b15">(Liu et al., 2019c)</ref> -85.2 ALUM ROBERTA-LARGE 86.2 85.6 ALUM ROBERTA-LARGE-SMART 86.9 -Human 95.7 95.6 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose ALUM, a general adversarial training algorithm, and present the first comprehensive study of adversarial training in large neural language models. We show that adversarial pretraining can significantly improves both generalization and robustness, which provides a promising direction for reconciling their conflicts as observed in prior work. ALUM substantially improved accuracy for BERT and RoBERTa in a wide range of NLP tasks, and can be combined with adversarial fine-tuning for further gain.</p><p>Future directions include: further study on the role of adversarial pre-training in improving generalization and robustness; speed up adversarial training; apply ALUM to other domains. A NLP Application Benchmarks</p><p>• GLUE. The General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding (NLU) tasks. As shown in <ref type="table">Table 9</ref>, it includes question answering <ref type="bibr" target="#b27">(Rajpurkar et al., 2016)</ref>, linguistic acceptability <ref type="bibr" target="#b41">(Warstadt et al., 2018)</ref>, sentiment analysis <ref type="bibr" target="#b32">(Socher et al., 2013)</ref>, text similarity <ref type="bibr" target="#b4">(Cer et al., 2017)</ref>, paraphrase detection <ref type="bibr">(Dolan and Brockett, 2005)</ref>, and natural language inference (NLI) <ref type="bibr" target="#b1">Bar-Haim et al., 2006;</ref><ref type="bibr">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b2">Bentivogli et al., 2009;</ref><ref type="bibr">Levesque et al., 2012;</ref><ref type="bibr" target="#b42">Williams et al., 2018)</ref>. The diversity of the tasks makes GLUE very suitable for evaluating the generalization and robustness of NLU models.</p><p>• SNLI. The Stanford Natural Language Inference (SNLI) dataset contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotated <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref>. This is the most widely used entailment dataset for NLI.</p><p>• SciTail. This is a textual entailment dataset derived from a science question answering (SciQ) dataset <ref type="bibr">(Khot et al., 2018)</ref>. The task involves assessing whether a given premise entails a given hypothesis. In contrast to other entailment datasets mentioned previously, the hypotheses in SciTail are created from science questions while the corresponding answer candidates and premises come from relevant web sentences retrieved from a large corpus. As a result, these sentences are linguistically challenging and the lexical similarity of premise and hypothesis is often high, thus making SciTail particularly difficult.</p><p>• ANLI. The Adversarial Natural Language Inference (ANLI, <ref type="bibr" target="#b21">Nie et al. (2019)</ref>) is a new largescale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. Specifically, the instances are chosen to be difficult for the state-of-the-art models such as BERT and RoBERTa.</p><p>• SWAG. It is a large-scale adversarial dataset for the task of grounded commonsense inference, which unifies natural language inference and physically grounded reasoning <ref type="bibr" target="#b44">(Zellers et al., 2018)</ref>. SWAG consists of 113k multiple choice questions about grounded situations.</p><p>• HELLASWAG. It is similar to SWAG but more challenging <ref type="bibr" target="#b45">(Zellers et al., 2019)</ref>. For each query in HELLASWAG, it also has 4 choices and the goal is to find the best choice among them.</p><p>• SQuAD v1.1/v2.0. Stanford Question Answering Dataset (SQuAD) v1.1 and v2.0 <ref type="bibr" target="#b27">(Rajpurkar et al., 2016</ref><ref type="bibr" target="#b26">(Rajpurkar et al., , 2018</ref> are popular machine reading comprehension benchmarks. Their passages come from approximately 500 Wikipedia articles and the questions and answers are obtained by crowdsourcing. The SQuAD v2.0 dataset includes unanswerable questions about the same paragraphs.</p><p>• BC2GM. The Gene Mention Task at the Biocreative II workshop <ref type="bibr" target="#b31">(Smith et al., 2008)</ref> provides an annotated dataset for gene name entity recognition.</p><p>• NCBI. The NCBI disease corpus <ref type="bibr" target="#b10">(Dogan et al., 2014)</ref> contains annotations of disease mentions from a collection of PubMed abstracts.</p><p>• JNLPBA. JNLBA is a biomedical entity recognition shared task <ref type="bibr" target="#b7">(Collier and Kim, 2004)</ref>. It is one of the largest datasets covering a large fraction of major taxonomies in molecular biology.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Combining adversarial pre-training and finetuning attaining the best results on the development sets of MNLI and ANLI, two representative GLUE tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>compares these pre-trained models on</cell></row><row><cell>three standard benchmarks (SQuAD v1.1 (Ra-</cell></row><row><cell>jpurkar et al., 2016) and v2.0 (Rajpurkar et al.,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Comparison of standard and adversarial pre-</cell></row><row><cell>training on biomedical NER. Scores are entity-level F1.</cell></row><row><cell>2018), and MNLI from GLUE (Wang et al., 2018)),</cell></row><row><cell>using the same standard fine-tuning setting (with-</cell></row><row><cell>out adversarial training). The standard BERT mod-</cell></row><row><cell>els trained using only the Wikipedia data attain sim-</cell></row><row><cell>ilar results as in Devlin et al. (2018), thus provide</cell></row><row><cell>a good baseline for comparison. ALUM BERT-BASE</cell></row><row><cell>consistently outperforms the standard BERT mod-</cell></row><row><cell>els across all the datasets, even adjusting for the</cell></row><row><cell>slightly longer trainng time. E.g., on SQuAD v1.1,</cell></row><row><cell>ALUM BERT-BASE gains 2.3% points in F1 over</cell></row><row><cell>BERT BASE and 1.2% points over BERT+ BASE . Fig-</cell></row><row><cell>ure 1 shows ALUM at work on the development</cell></row><row><cell>set of MNLI. Once adversarial training is applied</cell></row><row><cell>in the middle (after first 500K steps), ALUM starts</cell></row><row><cell>outperforming BERT and the gap is widening.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: RoBERTa is an extremlly well-trained model:</cell></row><row><cell>standard continual pre-training without adversarial</cell></row><row><cell>training fails to improve generalization performance in</cell></row><row><cell>downstream tasks. (Scores are accuracy.)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of standard and adversarial pre-training on the GLUE development set. Results for ALUM ROBERTA-BASE and ALUM ROBERTA-LARGE are averaged over five runs. Results of RoBERTa BASE and RoBERTa LARGE are taken from Liu et al. (2019c).</figDesc><table><row><cell>Method</cell><cell>R1</cell><cell>R2</cell><cell>Dev</cell><cell>R3</cell><cell>All</cell><cell>R1</cell><cell>R2</cell><cell>Test</cell><cell>R3</cell><cell>All</cell></row><row><cell cols="6">MNLI + SNLI + ANLI + FEVER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT BASE</cell><cell cols="10">55.7 46.3 43.4 48.2 55.1 45.0 43.1 47.4</cell></row><row><cell>BERT+ BASE</cell><cell cols="10">57.5 47.3 43.0 48.9 57.7 43.7 43.0 47.8</cell></row><row><cell>ALUM BERT-BASE</cell><cell cols="10">62.0 48.6 48.1 52.6 61.3 45.9 44.3 50.1</cell></row><row><cell>BERT LARGE (Nie et al., 2019)</cell><cell cols="5">57.4 48.3 43.5 49.3</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>44.2</cell></row><row><cell>XLNet LARGE (Nie et al., 2019)</cell><cell cols="5">67.6 50.7 48.3 55.1</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>52.0</cell></row><row><cell cols="6">RoBERTa LARGE (Nie et al., 2019) 73.8 48.9 44.4 53.7</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>49.7</cell></row><row><cell>ALUM ROBERTA-LARGE</cell><cell cols="10">73.3 53.4 48.2 57.7 72.3 52.1 48.4 57.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of standard and adversarial pre-training on the adversarial dataset ANLI. R1, R2 and R3 are rounds with increasing difficulty. Note that<ref type="bibr" target="#b21">Nie et al. (2019)</ref> did not represent results for individual rounds, as signified by "-".</figDesc><table><row><cell></cell><cell cols="2">Adversarial SQuAD</cell><cell cols="2">HELLASWAG</cell></row><row><cell>Method</cell><cell cols="2">AddSent AddOneSent</cell><cell>Dev</cell><cell>Test</cell></row><row><cell></cell><cell>EM/F1</cell><cell>EM/F1</cell><cell cols="2">Accuracy Accuracy</cell></row><row><cell>BERT BASE</cell><cell>48.9/54.0</cell><cell>59.0/64.8</cell><cell>39.5</cell><cell>-</cell></row><row><cell>BERT+ BASE</cell><cell>50.1/56.2</cell><cell>60.5/65.7</cell><cell>40.3</cell><cell>-</cell></row><row><cell>ALUM BERT-BASE</cell><cell>54.6/60.4</cell><cell>63.2/69.8</cell><cell>44.0</cell><cell>-</cell></row><row><cell>RoBERTa LARGE</cell><cell>72.3/66.0</cell><cell>79.3/72.9</cell><cell>85.0</cell><cell>85.2</cell></row><row><cell cols="2">ALUM ROBERTA-LARGE 75.5/69.4</cell><cell>81.4/75.0</cell><cell>86.2</cell><cell>85.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of standard and adversarial pre-training on adversarial datasets Adversarial SQuAD and HEL-LASWAG. The test result on HELLASWAG is taken from the official leaderboard: rowanzellers.com/hellaswag; we couldn't get results for BERT base models as the organizers restrict the number of submissions.</figDesc><table><row><cell>example, on ANLI, ALUM ROBERTA-LARGE gains</cell></row><row><cell>7.3% points in test accuracy over RoBERTa LARGE ,</cell></row><row><cell>outperforms XLNet (Yang et al., 2019) by 5.0%</cell></row><row><cell>points, creating a new state-of-the-art result. The</cell></row><row><cell>gains on Adversarial SQuAD and HELLASWAG</cell></row><row><cell>are equally significant. For example, for Ad-</cell></row><row><cell>versarial SQuAD, ALUM BERT-BASE outperforms</cell></row><row><cell>BERT BASE by +6.4% F1 in the AddSent set-</cell></row><row><cell>ting and +5.0% F1 in the AddOneSent setting.</cell></row><row><cell>Against RoBERTa LARGE , ALUM ROBERTA-LARGE</cell></row><row><cell>gains +3.4% F1 in AddSent and +2.1% F1 in Ad-</cell></row><row><cell>dOneSent.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Combining adversarial pre-training and finetuning attains the best results on SNLI and SciTail.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Combining adversarial pre-training and fine-</cell></row><row><cell>tuning attains the best results on SWAG and HEL-</cell></row><row><cell>LASWAG.</cell></row><row><cell>resentative GLUE tasks. Combining adversarial</cell></row><row><cell>pre-training and fine-tuning attains the best results,</cell></row><row><cell>and substantially outperforms RoBERTa LARGE .</cell></row><row><cell>E.g., on ANLI, ALUM RoBERTa-SMART outperforms</cell></row><row><cell>ALUM ROBERTA-LARGE by +1.1% points in accu-</cell></row><row><cell>racy, and outperforms RoBERTa LARGE by +5.1%</cell></row><row><cell>points. On SNLI, SciTail, SWAG, and HEL-</cell></row><row><cell>LASWAG, we observe similar gains by combining</cell></row><row><cell>adversarial pre-training and fine-tuning, attaining</cell></row><row><cell>new state-of-the-art results on these tasks. See ta-</cell></row><row><cell>ble 7 and 8.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2019. Is bert really robust? natural language attack on text classification and entailment. arXiv preprint arXiv:1907.11932. Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. SciTail: A textual entailment dataset from science question answering. In AAAI.</figDesc><table><row><cell>Diederik Kingma and Jimmy Ba. 2014. Adam: A</cell><cell></cell></row><row><cell>method for stochastic optimization. arXiv preprint</cell><cell></cell></row><row><cell>arXiv:1412.6980.</cell><cell></cell></row><row><cell>Taku Kudo and John Richardson. 2018. Sentencepiece:</cell><cell></cell></row><row><cell>A simple and language independent subword tok-</cell><cell></cell></row><row><cell>enizer and detokenizer for neural text processing.</cell><cell></cell></row><row><cell>arXiv preprint arXiv:1808.06226.</cell><cell></cell></row><row><cell>Hector Levesque, Ernest Davis, and Leora Morgen-stern. 2012. The winograd schema challenge. In Thirteenth International Conference on the Princi-ples of Knowledge Representation and Reasoning.</cell><cell cols="2">William B Dolan and Chris Brockett. 2005. Automati-cally constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).</cell></row><row><cell></cell><cell cols="2">Li Dong, Nan Yang, Wenhui Wang, Furu Wei,</cell></row><row><cell></cell><cell cols="2">Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming</cell></row><row><cell></cell><cell cols="2">Zhou, and Hsiao-Wuen Hon. 2019.</cell><cell>Unified</cell></row><row><cell></cell><cell cols="2">language model pre-training for natural language</cell></row><row><cell></cell><cell>understanding and generation.</cell><cell>arXiv preprint</cell></row><row><cell></cell><cell>arXiv:1905.03197.</cell></row><row><cell></cell><cell cols="2">Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,</cell></row><row><cell></cell><cell cols="2">and Bill Dolan. 2007. The third PASCAL recogniz-</cell></row><row><cell></cell><cell cols="2">ing textual entailment challenge. In Proceedings of</cell></row><row><cell></cell><cell cols="2">the ACL-PASCAL Workshop on Textual Entailment</cell></row><row><cell></cell><cell cols="2">and Paraphrasing, pages 1-9, Prague. Association</cell></row><row><cell></cell><cell>for Computational Linguistics.</cell></row><row><cell></cell><cell cols="2">Aaron Gokaslan and Vanya Cohen. Openwebtext cor-</cell></row><row><cell></cell><cell>pus.</cell></row><row><cell></cell><cell cols="2">Ian J Goodfellow, Jonathon Shlens, and Christian</cell></row><row><cell></cell><cell cols="2">Szegedy. 2014. Explaining and harnessing adversar-</cell></row><row><cell></cell><cell cols="2">ial examples. arXiv preprint arXiv:1412.6572.</cell></row><row><cell></cell><cell cols="2">Frank R Hampel. 1974. The influence curve and its</cell></row><row><cell></cell><cell cols="2">role in robust estimation. Journal of the american</cell></row><row><cell></cell><cell cols="2">statistical association, 69(346):383-393.</cell></row><row><cell></cell><cell cols="2">Sepp Hochreiter and Jürgen Schmidhuber. 1997.</cell></row><row><cell></cell><cell cols="2">Long short-term memory. Neural computation,</cell></row><row><cell></cell><cell>9(8):1735-1780.</cell></row><row><cell></cell><cell cols="2">Yu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei</cell></row><row><cell></cell><cell cols="2">Wei, Wen-Lian Hsu, and Cho-Jui Hsieh. 2019. On</cell></row><row><cell></cell><cell cols="2">the robustness of self-attentive models. In Proceed-</cell></row><row><cell></cell><cell cols="2">ings of the 57th Annual Meeting of the Association</cell></row><row><cell></cell><cell cols="2">for Computational Linguistics, pages 1520-1529.</cell></row><row><cell></cell><cell cols="2">Robin Jia and Percy Liang. 2017. Adversarial exam-</cell></row><row><cell></cell><cell cols="2">ples for evaluating reading comprehension systems.</cell></row><row><cell></cell><cell>arXiv preprint arXiv:1707.07328.</cell></row><row><cell></cell><cell cols="2">Haoming Jiang, Pengcheng He, Weizhu Chen, Xi-</cell></row><row><cell></cell><cell cols="2">aodong Liu, Jianfeng Gao, and Tuo Zhao. 2019.</cell></row><row><cell></cell><cell cols="2">Smart: Robust and efficient fine-tuning for pre-</cell></row><row><cell></cell><cell cols="2">trained natural language models through princi-</cell></row><row><cell></cell><cell>pled regularized optimization.</cell><cell>arXiv preprint</cell></row><row><cell></cell><cell>arXiv:1911.03437.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A notable exception is, but it only applied adversarial training to generative language modeling.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://dumps.wikimedia.org/enwiki/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">With K=1 in Algorithm 1, ALUM requires two more forward passes and one more backward pass compared to standard training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">This is a subset of the data (160GB) used in RoBERTa pre-training.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Haoming Jiang, Tuo Zhao, Zhe Gan, Keivn Duh, Yangfeng Ji, Greg Yang, Pengchuan Zhang, Lei Zhang, Furu Wei, Li Dong, Masayuki Asahara, and Lis Pereira for valuable discussions and comments, Microsoft Research Technology Engineering team for setting up GPU machines.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>Task  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12804</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The second PASCAL recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Text Analysis Conference</title>
		<meeting>Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust neural machine translation with doubly adversarial inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1425</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4324" to="4333" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ELECTRA: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Introduction to the bio-entity recognition task at JNLPBA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP)</title>
		<meeting>the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP)<address><addrLine>Geneva, Switzerland. COLING</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
		<idno type="DOI">10.1007/11736790_9</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW&apos;05</title>
		<meeting>the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW&apos;05<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
	<note type="report_type">Heidelberg</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ncbi disease corpus: A resource for disease name recognition and concept normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rezarta</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2013.12.006</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="912" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Awa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07972</idno>
		<title level="m">The microsoft toolkit of multitask deep neural networks for natural language understanding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<title level="m">Towards deep learning models resistant to adversarial attacks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The curious case of adversarially robust models: More data can help, double descend, or hurt generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Karbasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11080</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03705</idno>
		<title level="m">Narjes Nikzad, Meysam Chenaghlu, and Jianfeng Gao. 2020a. Deep learning based text classification: A comprehensive review</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03705</idno>
		<title level="m">Narjes Nikzad, Meysam Chenaghlu, and Jianfeng Gao. 2020b. Deep learning based text classification: a comprehensive review</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14599</idno>
		<title level="m">Adversarial nli: A new benchmark for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongli</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02610</idno>
		<title level="m">Adversarial robustness through local linearization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Understanding and mitigating the tradeoff between robustness and accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanny</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10716</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanny</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06032</idno>
		<title level="m">Adversarial training can hurt generalization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<title level="m">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12843</idno>
		<title level="m">Adversarial training for free! arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using gpu model parallelism</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Overview of biocreative ii gene mention recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorraine</forename><surname>Tanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Ju</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Fang</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nan</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Struble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Povinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Baumgartner</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wilbur</surname></persName>
		</author>
		<idno type="DOI">10.1186/gb-2008-9-s2-s2</idno>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Suppl 2:S2</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disentangling adversarial robustness and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6976" to="6987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ernie 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12412</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">âȂIJcloze procedureâȂİ: A new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism quarterly</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="433" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Christos Christodoulopoulos, and Arpit Mittal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05355</idno>
	</analytic>
	<monogr>
		<title level="m">Fever: a large-scale dataset for fact extraction and verification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving neural language modeling via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6555" to="6565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05326</idno>
		<title level="m">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11764</idno>
		<title level="m">Freelb: Enhanced adversarial training for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
