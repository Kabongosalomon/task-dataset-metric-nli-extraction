<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Rolls-Royce@NTU Corporate Lab</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Analyzing people&apos;s opinions and sentiments towards certain aspects is an important task of natural language understanding. In this paper, we propose a novel solution to targeted aspect-based sentiment analysis, which tackles the challenges of both aspect-based sentiment analysis and targeted sentiment analysis by exploiting commonsense knowledge. We augment the long short-term memory (LSTM) network with a hierarchical attention mechanism consisting of a target-level attention and a sentence-level attention. Commonsense knowledge of sentiment-related concepts is incorporated into the end-to-end training of a deep neural network for sentiment classification. In order to tightly integrate the common-sense knowledge into the recurrent encoder, we propose an extension of LSTM, termed Sentic LSTM. We conduct experiments on two publicly released datasets, which show that the combination of the proposed attention architecture and Sen-tic LSTM can outperform state-of-the-art methods in targeted aspect sentiment tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In recent years, sentiment analysis ( <ref type="bibr" target="#b4">Cambria et al. 2017a</ref>) has become increasingly popular for processing social media data on online communities, blogs, wikis, microblogging platforms, and other online collaborative media. Sentiment analysis is a branch of affective computing research ( ) that aims to classify text into either positive or negative, but sometimes also neutral ( <ref type="bibr" target="#b6">Chaturvedi et al. 2017</ref>). Most of the literature is on English language but recently an increasing number of publications is tackling the multilinguality issue ( <ref type="bibr" target="#b14">Lo et al. 2017)</ref>.</p><p>While most works approach it as a simple categorization problem, sentiment analysis is actually a suitcase research problem ( <ref type="bibr" target="#b5">Cambria et al. 2017b</ref>) that requires tackling many natural language processing (NLP) tasks, including named entity recognition <ref type="bibr" target="#b15">(Ma, Cambria, and Gao 2016)</ref>, word polarity disambiguation ( <ref type="bibr" target="#b35">Xia et al. 2015</ref>), personality recognition ( <ref type="bibr" target="#b16">Majumder et al. 2017</ref>), sarcasm detection ( , and aspect extraction. The last one, in particular, is an extremely important subtask that, if ignored, can consistently reduce the accuracy of sentiment classification in the presence of multiple opinion targets.</p><p>Copyright Â© 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>Hence, aspect-based sentiment analysis (ABSA) ( <ref type="bibr" target="#b20">Pontiki et al. 2014;</ref><ref type="bibr" target="#b25">Poria, Cambria, and Gelbukh 2016)</ref> extends the typical setting of sentiment analysis with a more realistic assumption that polarity is associated with specific aspects (or product features) rather than the whole text unit. For example, in the sentence "The design of the space is good but the service is horrible", the sentiment expressed towards the two aspects ("space" and "service") is completely opposite. Through aggregating sentiment analysis with aspects, ABSA allows the model to produce a fine-grained understanding of people's opinion towards a particular product.</p><p>Targeted (or target-dependent) sentiment classification ( <ref type="bibr" target="#b8">Dong et al. 2014;</ref><ref type="bibr" target="#b34">Wang et al. 2017</ref>), instead, resolves the sentiment polarity of a given target in its context, assuming that a sentence might express different opinions towards different targeted entities. For instance, in the sentence "I just log on my <ref type="bibr">[facebook]</ref>. <ref type="bibr">[Transformers]</ref> is boring", the sentiment expressed towards <ref type="bibr">[Transformers]</ref> is negative, while there is no clear sentiment for <ref type="bibr">[facebook]</ref>. Recently, targeted ABSA ( <ref type="bibr" target="#b28">Saeidi et al. 2016</ref>) has attempted to tackle the challenges of both ABSA and targeted sentiment analysis. The task is to jointly detect the aspect category and resolve the polarity of aspects with respect to a given target.</p><p>Deep learning methods <ref type="bibr" target="#b19">(Nguyen and Shirai 2015;</ref><ref type="bibr" target="#b33">Wang et al. 2016;</ref><ref type="bibr" target="#b34">Wang et al. 2017</ref>) have achieved great accuracy when applied to ABSA and targeted sentiment analysis. Especially, neural sequential models, such as long short-term memory (LSTM) networks (Hochreiter and Schmidhuber 1997), are of growing interest for their capacity of representing sequential information. Moreover, most of these sequence-based methods incorporate the attention mechanism, which has its root in the alignment model of machine translation (Bahdanau, Cho, and Bengio 2014). Such mechanism takes an external memory and representations of a sequence as input and produces a probability distribution quantifying the concerns in each position of the sequence.</p><p>Despite these advances in sentiment analysis, we identify three problems remaining unsolved in current state-of-theart methods. Firstly, a given target might consist of multiple instances (mentions of the same target) or multiple words in a sentence, existing research assumes all instances are of equal importance and simply computes an average vector The Thirty-Second AAAI Conference on Artificial Intelligence <ref type="bibr">(AAAI-18)</ref> over such instances. This oversimplification conflicts with the fact that one or more instances of the target are often more tightly tied with sentiment than others. Secondly, hierarchical attention exploited by existing methods only implicitly models the process of inferring the sentiment-bearing words related to the given target and aspect as black-box. Last but not least, existing research falls short in effectively incorporating into the deep neural network external knowledge, e.g., affective or commonsense knowledge, that could directly contribute to the identification of aspects and sentiment polarity. Without any constraints, moreover, the global attention model might tend to encode task-irrelevant information. To address these problems, our method simultaneously learns a target-specific instance attention as well as a global attention. In particular, our contribution is three-fold: 1. We propose a hierarchical attention model that explicitly attends to first the targets and then the whole sentence; 2. We extend the classic LSTM cell with components accounting for integration with external knowledge; 3. We incorporate affective commonsense knowledge into a deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In this section, we survey multiple research areas related to the proposed framework, namely: ABSA, targeted sentiment analysis, targeted ABSA, and finally works on incorporating external knowledge into deep neural models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aspect-Based Sentiment Analysis</head><p>ABSA is the task of classifying sentiment polarity with respect to a set of aspects. The biggest challenge faced by ABSA is how to effectively represent the aspect-specific sentiment information of the whole sentence. Early works on ABSA have mainly relied on feature-engineering to characterize sentences ( <ref type="bibr" target="#b32">Wagner et al. 2014;</ref><ref type="bibr">Kiritchenko et al. 2014</ref>). Motivated by the success of deep learning in representation learning, many recent works ( <ref type="bibr" target="#b8">Dong et al. 2014;</ref><ref type="bibr" target="#b13">Lakkaraju, Socher, and Manning 2014;</ref><ref type="bibr">Nguyen and Shi- rai 2015;</ref><ref type="bibr" target="#b33">Wang et al. 2016</ref>) utilize deep neural networks to generate sentence embeddings (dense vector representation of sentences) which are then fed to a classifier as a lowdimensional feature vector. Moreover, the representation can be enhanced by using the attention mechanism ( <ref type="bibr" target="#b33">Wang et al. 2016)</ref>, which is typically a multi-layer neural network taking as input the word sequence and aspects. For each word of the sentence, the attention vector quantifies its sentiment salience as well as the relevance to the given aspect. The resulting sentiment representation benefits from the attention mechanism for it overcomes the shortcoming of recurrent neural networks (RNNs), which suffer from information loss when only one single output (e.g., the output at the end of the sequence) is used by the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeted Sentiment Analysis</head><p>Targeted sentiment analysis aims to analyze sentiment with respect to targeted entities in the sentence. It is thus critical for targeted sentiment analysis methods, e.g., the targetdependent LSTM (TDLSTM) and target connection LSTM (TCLSTM) ( , to model the interaction between sentiment targets and the whole sentence. In order to obtain the target-dependent sentence representation, TDL-STM directly uses the hidden outputs of a bidirectional-LSTM sentence encoders panning the target mentions, while TCLSTM extends TDLSTM by concatenating each input word vector with a target vector. Similar to ABSA, attention models are also applicable to targeted sentiment analysis. Rather than using a single level of attention, deep memory networks (Tang, Qin, and Liu 2016) and recurrent attention models <ref type="bibr" target="#b7">(Chen et al. 2017</ref>) have achieved superior performance by learning a deep attention over the singlelevel attention, as multiple passes (or hops) over the input sequence could refine the attended words again and again to find the most important words. All existing approaches have either ignored the problem of multiple target instances (or words) or simply used an averaging vector over target expressions (Tang, Qin, and Liu 2016; <ref type="bibr" target="#b34">Wang et al. 2017</ref>). Unlike such approaches, our method weights each target word with an attention weight so that a given target is represented by its most informative components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeted Aspect-Based Sentiment Analysis</head><p>Two baseline systems ( <ref type="bibr" target="#b28">Saeidi et al. 2016</ref>) are proposed together with SentiHood: a feature-based logistic regression model and a LSTM-based model. The feature-based logistic regression model uses feature templates including n-grams tokens and POS tags extracted from the context of instances. The LSTM baseline can be seen as an adaptation of TDL-STM that simply uses the hidden outputs at the position of target instances assuming that all target instances are equally important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating External Knowledge</head><p>External knowledge base has been typically used as a source of features <ref type="figure">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>In this section, we describe the proposed attention-based neural architecture in detail: we first proposed the task definition of targeted ABSA, followed by an overview of the whole neural architecture; afterwards, we describe instance attention and global attention model; lastly, we describe the proposed knowledge-embedded extension of LSTM cell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Definition</head><p>A sentence s consists of a sequence of words. Similar to ( <ref type="bibr" target="#b34">Wang et al. 2017</ref>), we consider all mentions of the same target as a single target. A target t composed of m words in sentence s, denoted as T = {t 1 , t 2 , â¯, t i , â¯, t m } with t i referring to the position of ith word in the target expression, the task of targeted ABSA can be divided into two subtasks. Firstly, it resolves the aspect categories of t belonging to a predefined set. Secondly, it classifies the sentiment polarity with respect to each aspect category associated with t. For example, the sentence "I live in [West London] for years. I like it and it is safe to live in much of <ref type="bibr">[west London]</ref>. Except <ref type="bibr">[Brent]</ref> maybe. " contains two targets, <ref type="bibr">[W estLondon]</ref> and <ref type="bibr">[Brent]</ref>. Our objective is to detect the aspects and classify the sentiment polarity. The desired output for <ref type="bibr">[W estLondon]</ref> is ['general':positive; 'safety':positive], while output for <ref type="bibr">[Brent]</ref> should be ['general':negative; 'safety':negative].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>In this section, we provide an overview of the proposed method. Our neural architecture consists of two components: the sequence encoder and a hierarchical attention component. <ref type="figure" target="#fig_1">Fig. 1</ref> illustrates how the neural architecture works. Given a sentence s = {w 1 , w 2 , â¯, w L }, a look-up operation is first performed to convert input words into word embeddings {v <ref type="bibr">w1</ref> , v <ref type="bibr">w2</ref> , â¯, v w L }. The sequence encoder, which is based on a bidirectional LSTM, transforms the word embeddings into a sequence of hidden outputs. The attention component is built on top of the hidden outputs. The target-level attention takes as input the hidden outputs at the positions of target expression (highlighted in brown) and computes a selfattention vector over these words.</p><p>The output of target-level attention component is a representation of the target. Afterwards, the target representation together with the aspect embeddings is used for computing a sentence-level attention transforming the whole sentence into a vector. The sentence-level attention component returns one sentence vector for each aspect and target pair. The aspect-based sentence vector is then fed into the corresponding multi-class (e.g., None, Neural, Negative, and Positive for a 4-class setting; or None, Negative, and Positive for a 3-class setting) classifier to resolve the sentiment polarity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long Short-Term Memory Network</head><p>The sentence is encoded using an extension of RNN ( <ref type="bibr">Schus- ter and Paliwal 1997)</ref>, termed LSTM (Hochreiter and Schmidhuber 1997), which was firstly introduced by <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber 1997)</ref> to solve the vanishing and exploding gradient problem faced by the vanilla RNN. A typical LSTM cell contains three gates: forget gate, input gate and output gate. These gates determine the information to flow in and flow out at the current time step. The mathematical representations of the cell are as follows:</p><formula xml:id="formula_0">f i = Ï(W f [x i , h iâ1 ] + b f ) I i = Ï(W I [x i , h iâ1 ] + b I ) Ì C i = tanh(W C [x i , h iâ1 ] + b C ) C i = f i * C iâ1 + I i * Ì C i o i = Ï(W o [x i , h iâ1 ] + b o ) h i = o i * tanh(C i ) (1)</formula><p>where f i , I i and o i are the forget gate, input gate and output gate, respectively. W f , W I , W o , b f , b I and b o are the weight matrix and bias scalar for each gate. C i is the cell state and h i is the hidden output. A single LSTM typically encodes the sequence from only one direction. However, two LSTMs can also be stacked to be used as a bidirectional encoder, referred to as bidirectional LSTM. For a sentence s = {w 1 , w 2 , â¯, w L }, bidirectional LSTM produces a sequence of hidden outputs,</p><formula xml:id="formula_1">H = [h 1 , h 2 ...h L ] = â¡ â¢ â¢ â¢ â£ ñ®½ â h 1 ñ®½ â h 2 â¯ ñ®½ â h L â ñ®½ h 1 â ñ®½ h 2 â¯ â ñ®½ h L â¤ â¥ â¥ â¥ â¦</formula><p>where each element of H is a concatenation of the corresponding hidden outputs of both forward and backward LSTM cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target-Level Attention</head><p>Based on the attention mechanism, we calculate an attention vector for a target expression. A target might consist of a consecutive or non-consecutive sequence of words, denoted as T = {t 1 , t 2 , â¯, t m }, where t i is the location of an individual word in a target expression. The hidden outputs corresponding to T is denoted as</p><formula xml:id="formula_2">H â² = {h t1 , h t2 , â¯, h tm }.</formula><p>We compute the vector representation of a target t as</p><formula xml:id="formula_3">v t = H â² Î± = â j Î± j h tj<label>(2)</label></formula><p>where the target attention vector Î± = {Î± 1 , Î± 2 , â¯, Î± m } is distributed over target word sequence T . The attention vector Î± is a self-attention vector that takes nothing but the hidden output itself as input. The attention vector Î± of target expression is computed by feeding the hidden output into a bi-layer perceptron, as shown in Equation 3.</p><formula xml:id="formula_4">Î± = sof tmax(W (2) a tanh(W (1) a H â² ))<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">W (1) a â R dmÃd h and W (2) a</formula><p>â R 1Ãdm are parameters of the attention component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-Level Attention Model</head><p>Following the target-level attention, our model learns a target-and-aspect-specific sentence attention over all the words of a sentence. Given a sentence s of length L, the hidden outputs are denoted as</p><formula xml:id="formula_6">H = [h 1 , h 2 , â¯, h L ].</formula><p>An attention model computes a linear combination of the hidden vectors into a single vector, i.e.,</p><formula xml:id="formula_7">v a s,t = HÎ² = â i Î² i h i<label>(4)</label></formula><p>where the vector Î² = [Î² 1 , Î² 2 , â¯, Î² L ] is called the sentencelevel attention vector. Each element Î² i encodes the salience of the word w i in the sentence s with respect to the aspect a and target T . Existing research on targeted sentiment analysis or ABSA mostly uses targets or aspect terms as queries. At first, each h i is transformed to a d m dimensional vector by a multi-layer neural network with a tanh activation function, followed by a dense softmax layer to generate a probability distribution over the words in sentence s, i.e., </p><formula xml:id="formula_8">Î² a = sof tmax(v T a tanh(W m (H â² â v t )))<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Commonsense Knowledge</head><p>In order to improve the accuracy of sentiment classification, we use commonsense knowledge as our knowledge source to be embedded into the sequence encoder. In particular, we use SenticNet ( <ref type="bibr" target="#b3">Cambria et al. 2016</ref>), a commonsense knowledge base that contains 50,000 concepts associated with a rich set of affective properties <ref type="table" target="#tab_1">(Table 1)</ref>. These affective properties provide not only concept-level representation but also semantic links to the aspects and their sentiment. For example, the concept 'rotten fish' has property "KindOffood" that directly relates with aspects such as 'restaurant' or 'food quality', but also emotions, e.g., 'joy', which can support polarity detection <ref type="figure" target="#fig_2">(Fig. 2)</ref>.</p><p>However, the high dimensionality of SenticNet hinders it from being used in deep neural models. AffectiveSpace ( <ref type="bibr" target="#b2">Cambria et al. 2015</ref>) has been built to map the concepts of SenticNet to continuous low-dimensional embeddings without losing the semantic and affective relatedness of the original space. Based on this new space of concepts, we embed concept-level information into deep neural sequential models to better classify both aspects and sentiment in natural language text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentic LSTM</head><p>In order to leverage SenticNet's affective commonsense knowledge efficiently, we propose an affective extension of LSTM, termed Sentic LSTM. It is reasonable to assume that SenticNet concepts contain information complementary to the textual word sequence as, by definition, commonsense knowledge is about concepts that are usually taken for granted and, hence, absent from text. Sentic LSTM aims to entitle the concepts with two important roles: 1) assisting with the filtering of information flowing from one time step to the next and 2) providing complementary information to the memory cell. At each time step i, we assume that a set of knowledge concept candidates can be triggered and mapped to a d c dimensional space. We denote the set of K concepts as {Î¼ i,1 , Î¼ i,2 , â¯, Î¼ i,K }. First, we combine the candidate embeddings into a single vector as follows:</p><formula xml:id="formula_9">Î¼ i = 1 K â j Î¼ i,j<label>(6)</label></formula><p>As we realized that there are only up to 4 extracted concepts for each time step, we simply use the average vector (although a more sophisticated attention model can also be easily employed to replace the averaging function).</p><formula xml:id="formula_10">f i = Ï(W f [x i , h iâ1 , Î¼ i ] + b f ) I i = Ï(W I [x i , h iâ1 , Î¼ i ] + b I ) Ì C i = tanh(W C [x i , h iâ1 ] + b C ) C i = f i * C iâ1 + I i * Ì C i o i = Ï(W o [x i , h iâ1 , Î¼ i ] + b o ) o c i = Ï(W co [x i , h iâ1 , Î¼ i ] + b co ) h i = o i * tanh(C i ) + o c i * tanh(W c Î¼ i )<label>(7)</label></formula><p>Our affective extension of LSTM is illustrated in Equation 7. At first, we assume that affective concepts are meaningful cues to control the information of token-level information. For example, a multi-word concept 'rotten fish' might indicate that the word 'rotten' is a sentiment-related modifier of its next word 'fish' and, hence, less information should be filtered out at next time step. We thus add knowledge concepts to the forget, input, and output gate of standard LSTM to help filtering the information. The presence of affective concepts in the input gate is expected to prevent the memory cell from being affected by input tokens conflicting with pre-existing knowledge. Similarly, the output gate uses such knowledge to filter out irrelevant information stored in the memory.</p><p>Another important feature of Sentic LSTM is based on the assumption that the information from the concept-level output is complementary to the token level. Therefore, we extended the regular LSTM with an additional knowledge output gate o c i to output concept-level knowledge complementary to the token-level memory. Since AffectiveSpace is learned independently, we leverage a transformation matrix W c â R d h ÃdÎ¼ to map it to the same space as the memory outputs. In other words, o c i models the relative contributions of token level and concept level.</p><p>Moreover, we notice that o c i * tanh(W c Î¼ i ) actually resembles the functionality of the sentinel vector used by <ref type="bibr" target="#b37">(Yang and Mitchell 2017)</ref>, which allows the model to choose whether to use affective knowledge or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction and Parameter Learning</head><p>The objective to train our classier is defined as minimizing the sum of the cross-entropy losses of prediction on each target-aspect pair, i.e.,</p><formula xml:id="formula_11">L s = 1 |D| â sâD â tâs â aâA log p a c,t</formula><p>where A is the set of predefined aspects, and p a c,t is the probability of the gold-standard polarity class c given target t with respect to a sentiment category a, which is defined by a softmax function,</p><formula xml:id="formula_12">p a c,t = sof tmax(W p v a s,t + b a s )</formula><p>where W p and b a s are the parameters to map the vector representation of target t to the polarity label of aspect a. To avoid overfitting, we add a dropout layer with dropout probability of 0.5 after the embedding layer. We stop the training process of our model after 10 epochs and select the model that achieves the best performance on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and Resources</head><p>We evaluate our method on two datasets: SentiHood ( <ref type="bibr" target="#b28">Saeidi et al. 2016</ref>) and a subset of Semeval 2015 ( <ref type="bibr" target="#b21">Pontiki et al. 2015</ref>). SentiHood was built by querying Yahoo! Answers with location names of London city. <ref type="table" target="#tab_2">Table 2</ref> shows statistics of SentiHood. The whole dataset is split into train, test, and development set by the authors. Overall, the entire dataset contains 5,215 sentences, with 3,862 sentences containing a single target and 1,353 sentences containing multiple targets. It also shows that there are approximately two third of targets annotated with aspect-based sentiment polarity (train set: 2476 out of 2977; test set:1241 out of 1898; development set: 619 out of 955). On average, each sentimentbearing target has been annotated with 1.37 aspects. To show the generalizability of our methods, we build a subset of the dataset used by Semeval-2015. We remove sentences containing no targets as well as NULL targets. To be comparable with SentiHood, we combine targets with the same surface form within the same sentence as mentions of the same target. In total, we have 1,197 targets left in the training set and 542 targets left in the testing set. On average, each target has 1.06 aspects.  To inject the commonsense knowledge, we use a syntaxbased concept parser 1 to extract a set of concept candidates at each time step, and use AffectiveSpace 2 as the concept embeddings. In case no concepts are extracted, a zero vector is used as the concept input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Setting</head><p>We evaluate our method on two sub-tasks of targeted ABSA: 1) aspect categorization and 2) aspect-based sentiment classification. Following <ref type="bibr" target="#b28">Saeidi et al. (Saeidi et al. 2016</ref>), we treat the outputs of aspect-based classification as hierarchical classes. For aspect categorization, we output the label (e.g., in the 3-class setting, it outputs 'Positive', 'Negative', or 'None') with the highest probability for each aspect. For aspect-based sentiment classification, we ignore the scores of 'None'. For evaluating the aspect-based sentiment classification, we simply calculate the accuracy averaged over aspects. We evaluate aspect categorization as a multi-label classification problem so that results are averaged over targets instead of aspects.</p><p>We evaluate our methods and baseline systems using both loose and strict metrics. We report scores of three widely used evaluation metrics of multi-label classifier: Macro-F1, Micro-F1, and strict Accuracy. Given the dataset D, the ground-truth aspect categories of the target t â D is denoted as Y t , while the predicted aspect categories denoted as Ì Y t . The three metrics can be computed as</p><formula xml:id="formula_13">â¢ Strict accuracy (Strict Acc.): 1 D â tâD Ï(Y t = Ì Y t )</formula><p>, where Ï(â) is an indicator function.</p><p>â¢ Macro-F1 = 2 Ma-PÃMa-R Ma-P+Ma-R , which is based on MacroPrecision (Ma-P) and Micro-Recall (Ma-R) with Ma-P</p><formula xml:id="formula_14">= 1 |D| â tâD |Ytâ© Ì Yt| Ì Yt</formula><p>, and Ma-R= 1</p><formula xml:id="formula_15">|D| â tâD |Ytâ© Ì Yt| Yt . â¢ Micro-F1 = 2 Mi-PÃMi-R Mi-P+Mi-R</formula><p>, which is based on MicroPrecision (Mi-P) and Micro-Recall (Mi-R), where Mi-P=  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Comparison</head><p>We compare our proposed method with the methods that have been proposed for targeted ABSA as well as methods proposed for ABSA or targeted sentiment analysis but applicable to targeted ABSA. Furthermore, we also compare the performances of several variants of our proposed method in order to highlight our technical contribution. We run the model for multiple times and report the results that perform best in the development set. For Semeval-2015 dataset, we report the results of the final epoch.</p><p>â¢ TDLSTM: TDLSTM ( ) adopts Bi-LSTM to encode the sequential structure of a sentence and represents a given target using a vector averaged on the hidden outputs of target instances.</p><p>â¢ LSTM + TA: Our method learns an instance attention on top of the outputs of LSTM to model the contribution of each instance.</p><p>â¢ LSTM + TA + SA: In addition to target instance attention, we add a sentence-level attention to the model. â¢ LSTM + TA + DMN SA: The sentence-level attention is replaced by a dynamic memory network with multiple hops (Tang, Qin, and Liu 2016). We run the memory network with different numbers of hops and report the result with 4 hops (best performance on development set of SentiHood). We exclude the case of zero hops as it corresponds to Bi-LSTM + TA + SA.  <ref type="table" target="#tab_4">Table 3 and Table 4</ref> show the performance on SentiHood and Semeval-15 dataset, respectively. In comparison with the non-attention baseline (Bi-LSTM+Avg.), we can find that our best attention-based model significantly improves aspect categorization (by more than 20%) and sentiment classification (approximately 10%) on SentiHood. However, it is notable that, on the Semeval-2015 dataset, the improvement is relatively smaller. We conjecture the reason is that SentiHood has masked the target as a special word "LOCA-TION", which resulted less informative than the full name of aspect targets that are used by <ref type="bibr">Semeval-2015.</ref> Hence, using only the hidden outputs regarding the target does not suffice to represent the sentiment of the whole sentence in SentiHood. Compared with target averaging model, the target-level attention achieves some improvement (even though not significant), as the target attention is capable of identifying the part of target expressions with higher sentiment salience. On the other hand, it is notable that the two-step attention achieves significant improvement on both aspect categorization and sentiment classification, indicating that the target-and aspect-dependent sentence attention could retrieve information relevant to both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of Attention Model</head><p>To our surprise, using multiple hops in the sentence-level attention fails to bring in any improvement. The performance even falls down significantly on Semeval-2015 with a much smaller number of training instances but larger aspect set than SentiHood. We conjecture the reason is that using multi-hops increases the number of parameter to learn, which makes it less applicable to small and sparse datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Attention</head><p>We visualize the attention vectors of sentence-level attention in <ref type="figure" target="#fig_4">Figure 3</ref> with respect to "Transition-location" and "Price" aspects. The two attention vectors have encoded quite different concerns in the word sequence. In the first example, the 'Transition-location' attention attends to the word "long", which is expressing a negative sentiment towards the target. In comparison, the 'Price' attention attends more to the word 'cheap', which is related to the aspect. That is to say, the two attention vectors are capable of distinguishing information related to different aspects. As visualized in <ref type="figure">Figure 4</ref>, the target-level attention is capable of selecting the part of target expression of which the aspect or sentiment is easier to be resolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of Knowledge-Embedded LSTM</head><p>It can be seen from <ref type="table" target="#tab_4">Table 3</ref> and 4 that injecting knowledge into the model improves the performance in general. Since AffectiveSpace encodes the information about affective properties that are semantically related to the aspects, it is reasonable to find out that it can improve performance on both tasks. The results show that our proposed Sentic LSTM outperforms baseline methods, even if not significantly.</p><p>An important outcome of the experiments is that Sentic LSTM significantly outperforms a baseline (LSTM + TA + SA + KB feat) feeding the knowledge features to the input layer, which confirms the efficacy of using a knowledge output gate to control the flow of background knowledge. Furthermore, the superior performance of Sentic LSTM over Recall LSTM and KBA indicates that the activated knowledge concepts can also help filtering the information that conflicts with the background knowledge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed a neural architecture for the task of targeted ABSA. We explicitly modeled the attention as a two-step model which encodes targets and full sentence. The target-level attention learns to attend to the sentiment-salient part of a target expression and generates a more accurate representation of the target, while the sentence-level attention searches for the target-and aspect-dependent evidence over the full sentence. Moreover, we proposed an extension of the LSTM cell so that it could more effectively incorporate affective commonsense knowledge when encoding the sequence into a vector. In the future, we would like to collectively analyze the sentiment of multiple targets co-occurring in the same sentence and investigate the role of commonsense knowledge in modeling the relation between targets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Ratinov and Roth 2009; Rahman and Ng 2011; Nakashole and Mitchell 2015). Most recently, neural se- quential models (Ahn et al. 2016; Yang and Mitchell 2017) leverage the lower-dimensional continuous representation of knowledge concepts as additional inputs. However, these ap- proaches have treated the computation of neural sequential models as a black-box without tight integration of knowl- edge and computational structure. The proposed model, termed Sentic LSTM, is inspired by (Xu et al. 2016), which adds a knowledge recall gate to the cell state of LSTM. How- ever, our method differs from (Xu et al. 2016) in the way of using external knowledge to generate the hidden outputs and controlling the information flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the attentive neural architecture</figDesc><graphic url="image-1.png" coords="3,53.79,592.96,238.96,88.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A sketch of SenticNet semantic network</figDesc><graphic url="image-2.png" coords="5,319.20,53.68,239.11,131.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>â¢</head><label></label><figDesc>LSTM + TA + SA + KB Feat: Concepts are fed into the input layer as additional features. â¢ LSTM + TA + SA + KBA: This is an integration of the method proposed by (Yang and Mitchell 2017), which learns an attention over the concept embeddings (the em- beddings are combined with the hidden output before be- ing fed into the classifier). â¢ Recall LSTM + TA + SA: LSTM is extended with a re- call knowledge gate as in (Xu et al. 2016). â¢ Sentic LSTM + TA + SA: The encoder is replaced with the proposed knowledge-embedded LSTM. The word embedding of the input layer is initialized by a pre-trained skip-gram model (Mikolov et al. 2013) with 150 hidden units on a combination of Yelp 3 and Amazon review dataset (He and McAuley 2016) and 50 hidden units for the bi-directional LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of sentence-level attention</figDesc><graphic url="image-4.png" coords="7,54.00,624.43,238.56,56.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>where v a is the aspect embedding of aspect a, H â v t is the operation concatenating v t to each h i ; W (1) m â R dmÃd h is the matrix mapping row vectors of H to a d m dimensional space, and W (2) m â R 1Ãdm</head><label>where</label><figDesc>maps each new row vector to a unnormalized attention weight.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Example of SenticNet assertions</head><label>1</label><figDesc></figDesc><table>SenticNet 
IsA-pet KindOf-food Arises-joy ... 

dog 
0.981 
0 
0.789 
... 
cupcake 
0 
0.922 
0.910 
... 
rotten fish 
0 
0.459 
0 
... 
police man 
0 
0 
0 
... 
win lottery 
0 
0 
0.991 
... 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : SentiHood dataset</head><label>2</label><figDesc></figDesc><table>Train Dev 
Test 

Targets 
3,806 955 1,898 
Targets w/ Sentiment 
2,476 619 1,241 
Aspects per Target(w/ Sentiment) 
1.37 
1.37 
1.35 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>â tâD |Ytâ© Ì Yt| â tâD Ì Yt , and Mi-R= â tâD |Ytâ© Ì Yt| â tâD Yt .</head><label></label><figDesc></figDesc><table>1 http://github.com/senticnet 
2 http://sentic.net/downloads </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 : System performance on SentiHood dataset</head><label>3</label><figDesc></figDesc><table>Aspect Categorization 
Sentiment 
Strict Acc. (%) Macro F1 (%) 
Micro F1 (%) 
Sentiment Acc. (%) 
dev 
test 
dev 
test 
dev 
test 
dev 
test 

TDLSTM 50.27 50.83 59.03 58.17 55.72 55.78 82.60 
81.82 
LSTM + TA 54.17 52.02 62.90 61.07 60.56 59.02 83.80 
84.29 
LSTM + TA + SA 68.83 66.42 79.36 76.69 79.14 76.64 86.00 
86.75 
LSTM + TA + DMN SA 60.66 60.14 68.89 70.19 67.28 68.37 84.80 
83.36 

LSTM + TA + SA + KB Feat 69.38 64.76 80.00 76.33 79.79 76.08 87.00 
88.70 
LSTM + TA + SA + KBA 68.08 65.12 78.68 76.40 78.73 76.46 87.40 
87.98 
Recall LSTM + TA + SA 68.64 64.66 78.44 75.61 78.53 75.91 86.80 
86.85 
Sentic LSTM + TA + SA 69.20 67.43 78.84 78.18 79.09 77.66 88.80 
89.32 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 : System performance on Semeval-2015 dataset</head><label>4</label><figDesc></figDesc><table>Aspect Categorization 
Sentiment 
Strict Acc. Macro F1 Micro F1 Sentiment Acc. 

TDLSTM 
65.49 
70.56 
69.00 
68.57 
LSTM+TA 
66.42 
71.71 
70.06 
69.24 
LSTM+TA+SA 
63.46 
70.73 
66.18 
74.28 
LSTM+TA+DMN SA 
48.33 
52.73 
51.39 
69.07 

LSTM+TA+SA+KB Feat 
65.68 
74.46 
70.71 
76.13 
LSTM+TA+SA+KBA 
67.34 
74.36 
71.78 
73.10 
Recall LSTM + TA + SA 
66.05 
72.90 
69.66 
74.11 
Sentic LSTM + TA + SA 
67.34 
76.44 
73.82 
76.47 

</table></figure>

			<note place="foot" n="3"> http://yelp.com.sg/dataset/challenge</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was conducted within the Rolls-Royce@NTU Corp Lab with support from the National Research Foundation Singapore under the Corp Lab@University Scheme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>PÃ¤rnamaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00318</idno>
		<title level="m">A neural knowledge language model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AffectiveSpace 2: Enabling affective intuition for concept-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="508" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SenticNet 4: A semantic resource for sentiment analysis based on conceptual primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2666" to="2677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A Practical Guide to Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feraco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<pubPlace>Cham, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sentiment analysis is a big suitcase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian network based extreme learning machine for subjectivity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ragusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gastaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zunino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of The Franklin Institute</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="463" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for targetdependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 52nd ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 25th WWW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nrc-Canada-</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aspect specific sentiment analysis using hierarchical deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Representation Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multilingual sentiment analysis: From formal to informal and scarce resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cornforth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="499" to="527" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Label embedding for zero-shot fine-grained named entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning-based document modeling for personality detection from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="79" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A knowledgeintensive model for prepositional phrase attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="365" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shirai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2509" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 8th SemEval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 12: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Androutsopoulos</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 9th SemEval</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semeval-2016 task 5: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">;</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Smadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>De Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kotelnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>JimÃ©nez-Zafra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>EryiË Git</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
	<note>In the 10th SemEval</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A deeper look into sarcastic tweets using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1601" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aspect extraction for opinion mining with a deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coreference resolution with world knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 49th ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="814" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 13th CoNLL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sentihood: Targeted aspect based sentiment analysis dataset for urban neighbourhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1546" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effective lstms for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the EMNLP 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dcu: Aspect-based polarity classification for semeval task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tounsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 8th SemEval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="223" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attentionbased lstm for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">;</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the EMNLP 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tdparse: Multi-target-specific sentiment recognition on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Procter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 15th EACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="483" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Word polarity disambiguation using bayesian model and opinionlevel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="369" to="380" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Incorporating loose-structured knowledge into lstm with recall gate for conversation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Leveraging knowledge bases in lstms for improving machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 55th ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1436" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
