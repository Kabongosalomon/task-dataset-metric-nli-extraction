<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Feature Learning with C-SVDDNet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Tan</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Feature Learning with C-SVDDNet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we investigate the problem of learning feature representation from unlabeled data using a single-layer K-means network. A K-means network maps the input data into a feature representation by finding the nearest centroid for each input point, which has attracted researchers' great attention recently due to its simplicity, effectiveness, and scalability. However, one drawback of this feature mapping is that it tends to be unreliable when the training data contains noise. To address this issue, we propose a SVDD based feature learning algorithm that describes the density and distribution of each cluster from Kmeans with an SVDD ball for more robust feature representation. For this purpose, we present a new SVDD algorithm called C-SVDD that centers the SVDD ball towards the mode of local density of each cluster, and we show that the objective of C-SVDD can be solved very efficiently as a linear programming problem. Additionally, traditional unsupervised feature learning methods usually take an average or sum of local representations to obtain global representation which ignore spatial relationship among them. To use spatial information we propose a global representation with a variant of SIFT descriptor. The architecture is also extended with multiple receptive field scales and multiple pooling sizes. Extensive experiments on several popular object recognition benchmarks, such as STL-10, MINST, Holiday and Copydays shows that the proposed C-SVDDNet method yields comparable or better performance than that of the previous state of the art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Feature Learning with C-SVDDNet Dong Wang and Xiaoyang Tan</head><p>Abstract-In this paper, we investigate the problem of learning feature representation from unlabeled data using a single-layer K-means network. A K-means network maps the input data into a feature representation by finding the nearest centroid for each input point, which has attracted researchers' great attention recently due to its simplicity, effectiveness, and scalability. However, one drawback of this feature mapping is that it tends to be unreliable when the training data contains noise. To address this issue, we propose a SVDD based feature learning algorithm that describes the density and distribution of each cluster from Kmeans with an SVDD ball for more robust feature representation. For this purpose, we present a new SVDD algorithm called C-SVDD that centers the SVDD ball towards the mode of local density of each cluster, and we show that the objective of C-SVDD can be solved very efficiently as a linear programming problem. Additionally, traditional unsupervised feature learning methods usually take an average or sum of local representations to obtain global representation which ignore spatial relationship among them. To use spatial information we propose a global representation with a variant of SIFT descriptor. The architecture is also extended with multiple receptive field scales and multiple pooling sizes. Extensive experiments on several popular object recognition benchmarks, such as STL-10, MINST, Holiday and Copydays shows that the proposed C-SVDDNet method yields comparable or better performance than that of the previous state of the art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Learning good feature representation from unlabeled data is the key to make progress in recognition and classification tasks, and has attracted great attention and interest from both academia and industry recently. A representative method for this is the deep learning (DL) approach <ref type="bibr" target="#b0">[1]</ref> with its goal to learn multiple layers of abstract representations from data. Among others, one typical DL method is the so called convolutional neural network (ConvNet), which consists of multiple trainable stages stacked on top of each other, followed by a supervised classifier <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b2">[3]</ref>. Many variations of ConvNet network have been proposed as well for different vision tasks <ref type="bibr" target="#b3">[4]</ref> [5] <ref type="bibr" target="#b5">[6]</ref> [7] <ref type="bibr" target="#b7">[8]</ref> with great success.</p><p>In these methods layers of representation are usually obtained by greedily training one layer at a time on the lower level <ref type="bibr" target="#b4">[5]</ref> [9] <ref type="bibr" target="#b2">[3]</ref>, using an unsupervised learning algorithm. Hence the performance of single-layer learning has a big effect on the final representation. Neural network based singlelayer methods, such as autoencoder <ref type="bibr" target="#b9">[10]</ref> and RBM (Restricted Boltzmann Machine, <ref type="bibr" target="#b10">[11]</ref>), are widely used for this but they usually have many parameters to adjust, which is very timeconsuming in practice.</p><p>That motivates more simple and more efficient methods for single-layer feature learning. Among others K-means clustering algorithm is a commonly used unsupervised learning method, which maps the input data into a feature representation simply by associating each data point to its nearest cluster center. There is only one parameter involved in the K-means based method, i.e., the number of clusters, hence the model is very easy to use in practice. Coates et al. <ref type="bibr" target="#b11">[12]</ref> shows that the K-means based feature learning network is capable to achieve superior performance compared to sparse autoencoder, sparse RBM and GMM (Guassian Mixture Model). However, the K-means based feature representation may be too terse, and does not take the non-uniform distribution of cluster size into account -Intuitively, clusters containing more data are likely to be part of the features with higher influential power, compared to the smaller ones.</p><p>In this paper, we proposed a SVDD (Support Vector Data Description, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>) based method to address these issues. The key idea of our method is to use SVDD to measure the density of each cluster resulted from K-means clustering, based on which more robust feature representation is built. Actually the K-means algorithm lacks a robust definition of the size of its clusters, since the nearest center principle is not robust against the noise or outliers commonly encountered in real world applications. We advocate that the SVDD could be a good way to address this issue. Actually SVDD is a widely used tool to find a minimal closed spherical boundary to describe the data belonging to the target class and therefore, given a cluster of data, we are expecting SVDD to generate a ball containing the normal data except outliers. Performing this procedure on all the clusters of K-means, we will finally obtain K SVDD balls on which our representation can be built. In addition, to take the cluster size into account, we use the distance from the data to each ball's surface instead of the center as the feature.</p><p>One possible problem of this method, however, may come from the instability of SVDD's center, due to the fact that its position is mainly determined by the support vectors on the boundary and the noise in the data may deviate the center far from the mode (c.f., <ref type="figure">Fig. 3 (left)</ref>). Hence the resulting SVDD ball may not be consistent with the data's distribution when used for feature representation. To address this issue, we add a new constraint to the original SVDD objective function to make the model align better with the data. In addition, we show that our modified SVDD can be solved very efficiently as a linear programming problem, instead of as a quadratic one. Usually we need to compute hundreds of clusters, and a linear programming solution can thus save us large amounts of time. The proposed method is further extended by adopting a set of receptive fields with different sizes to capture multiscale information ranging from detailed edge-like features to part-level features. A preliminary version of this work appeared in <ref type="bibr" target="#b15">[16]</ref>, and the feasibility and effectiveness of the proposed C-SVDD-based method (called C-SVDDNet) is verified extensively on several object recognition and image retrieval benchmarks with competitive performance.</p><p>The remaining parts of this paper are organized as follows: In Section II, preliminaries are provided regarding unsupervised feature learning representation, then we detail our improved feature learning method in Section III. In Section IV, we investigate the performance of our method empirically over several popular datasets. We conclude this paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. UNSUPERVISED FEATURE LEARNING</head><p>The goal of unsupervised feature learning is to automatically discover useful hidden patterns/features in large datasets without relying on a supervisory signal, and those learnt patterns can be utilized to create representations that facilitate subsequent supervised learning (e.g., object classification). Compared to supervised learning, unsupervised learning has its unique characteristics and advantages. Among others, one of the most important one is that it can be used to learn consistent patterns from unlabelled data, which are often free and easy to obtain. Such patterns distinguish from noise since by definition noise can be thought of as random variations presented in the data. This implies many potential applications of unsupervised learning, e.g., to transfer knowledge from one domain to another related domain, to regularize the behavior of a supervised algorithm, and to represent the data in a compact but effective manner. Due to these reasons, unsupervised learning are regarded as the future of deep learning <ref type="bibr" target="#b16">[17]</ref>.</p><p>There are many kinds of unsupervised learning methods in computer vision, such as Bag of Words (BoW) <ref type="bibr" target="#b17">[18]</ref>, Vector of Linearly Aggregated Descriptors (VLAD) <ref type="bibr" target="#b18">[19]</ref>, Fisher vector (FV) <ref type="bibr" target="#b19">[20]</ref>, and so on. A typical pipeline for unsupervised feature learning includes three steps. The first step is to train a set of local filters from the unlabeled training data. This is usually done by running K-means (for BoW, VLAD) or GMM (for FV) on lots of local patches sampled from the dataset and then using the centers of clusters as filter bank. The second step is to partition a given image into patches and encode them into a set of feature vectors using the learnt filter bank. These feature vectors are finally combined and normalized as the feature representation for the input image. In what follows we give a brief review on these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bag of Words and Its Variants</head><p>The simple and basic unsupervised feature learning method is the BoW model. In this model local filters are usually the centers of clusters from K-means. These filters are looked as bins, which serves to pool the local patches nearest to them. This can be regarded as a "hard voting" method:</p><formula xml:id="formula_0">f k (x) = 1 if k = argmin j c j − x 2 2 0 otherwise (1)</formula><p>where f k (x) is the value that a patch x was encoded as with the k-th filter c k . In BoW, we simply count the number of patches in each bin to get a histogram representation. Thus it is a very coarse way to encode the information of an input image. Alternatively, VLAD <ref type="bibr" target="#b18">[19]</ref> and FV <ref type="bibr" target="#b19">[20]</ref> encode each data point x with a vector instead of a simple count number as in BoW, which effectively improve the richness and robustness of the feature representation. Particularly, FV captures the first and second order difference between an input x and the centres of a GMM, denoted as c k ,</p><formula xml:id="formula_1">f uk (x) = 1 N √ π k q ik Σ − 1 2 k (x − c k ) f vk (x) = 1 N √ 2π k q ik [(x − c k )Σ − 1 2 k (x − c k ) − 1]<label>(2)</label></formula><p>Then the FV coding for an local patch x is a vector of <ref type="bibr" target="#b18">[19]</ref> is a simplified version of FV, with the difference signal between a patch x and a filter c k defined as f k (x) = x − c k . As in FV, these difference signals are concatenated into a Kdim vector for feature representation. Obviously, both FV and VLAD encode much richer information than that of BoW, hence being more discriminative in subsequent tasks such as object classification.</p><formula xml:id="formula_2">[f T u1 (x), f T v1 (x), f T u2 (x), f T v2 (x), ..., f T uK (x), f T vK (x)] T . VLAD</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coates et al.'s Method</head><p>To the best of our knowledge, the work of <ref type="bibr" target="#b11">[12]</ref> is the first "deep" unsupervised learning method that are based on the K-means method, hence having close connection with the aforementioned BoW, VLAD and FV methods. Particularly, after learning a filter bank, instead of using it as basin of attraction like in BoW or as references for calculating difference vectors, it is utilized to generate a series of feature maps, one for each filter. This has at least two potential advantages: 1) compared to VLAD and FV, the encoded information is even more rich; 2) the feature maps preserve the spatial information well and hence the whole procedure could be repeated, leading to a deep unsupervised learning architecture.</p><p>Furthermore, to deal with the problem of "hard coding" in K-means, the following "triangle" encoding is proposed <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_3">f k (x) = max{0, µ(z) − z k (x)}<label>(3)</label></formula><p>where z k (x) = x−c k 2 , and µ(z) is the mean of the elements of z. This activation function outputs 0 for the feature f k that has an above average distance to the centroid c k . This model leads to a less sparse representation (roughly half of the features could be set to be 0). Note that this "triangle" encoding strategy essentially allows us to learn a distributed representation using the simple K-means method instead of more complicated network-based methods (e.g., autoencoder and RBM), hence saving much time in training. Coastes et al. <ref type="bibr" target="#b11">[12]</ref> shows that this strategy actually leads to comparable performance to, if not better than, those based on network methods. However, this method does not take the characteristics of each cluster into consideration. Actually, the number of data point in each cluster is usually different, so is the distribution of data points in each cluster. We believe that these differences would make a difference in feature representation as well. Unfortunately the aforementioned K-means feature mapping scheme completely ignores these and only uses the position of center for feature encoding. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, although the data point x has the same distance to the centers C 1 and C 2 of two clusters, it should be assigned a different score on C 1 than on C 2 since the former cluster C 1 is much bigger than the latter. In practice such unequal clusters are not uncommon and the K-means method by itself can not reliably grasp the size of its clusters due to the existence of outliers. To this end, we propose a SVDD based method to describe the density and distribution of each cluster and use this for more robust feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED METHOD</head><p>In this section, after presenting an overview of the proposed method, we give the details of our Centered-SVDD method for feature encoding, and compare it with the K-means "triangle" encoding method. Then we describe our SIFT-based postpooling layer and discuss how to extend the method to extract multi-scale information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview of the Proposed Method</head><p>A typical single-layer network contains several components: an input image is first mapped into a set of feature maps using filter banks (or dictionary), which are then subjected to a pooling/subsampling operation to condense the information contained in the feature maps. Finally, the pooled feature maps are concatenated to a feature vector, which serves as the representation for the subsequent classification/cluster tasks. There are several design options in this procedure, where the size of filter bank and that of the pooling grids are the major tradeoff one has to make.</p><p>Generally speaking, bigger filter banks help each sample find its nearby representative points more accurately but at the cost of yielding a high-dimensional representation, hence a crude pooling/subsampling is needed to reduce the dimensionality. Overall this type of architecture emphasizes more on the global aspects of the samples than on the local ones (e.g., local texture, local shape, etc.). Actually, Coates et al. show that this kind of network is able to yield state of the art results on several challenging datasets <ref type="bibr" target="#b11">[12]</ref>. On the other hand, other works use smaller filter banks but highlight the importance of detailed local information in constructing the representation, usually based on some complicated feature encoding strategy, as done in PCANet <ref type="bibr" target="#b20">[21]</ref> or Fisher Vector <ref type="bibr" target="#b21">[22]</ref>.</p><p>In this work, we follow the second design choice, based on the consideration that the learned representation should preserve enough local spatial information for the subsequent processing. Compared to <ref type="bibr" target="#b11">[12]</ref>, we use an improved feature encoding method named C-SVDD (detailed in the next section) and adopt the architecture of relatively small dictionary. Different to <ref type="bibr" target="#b20">[21]</ref> or <ref type="bibr" target="#b21">[22]</ref>, we learn filter banks for feature encoding but add a SIFT-based post-pooling processing procedure onto the network, which essentially projects the responses of a pooling operation into a more compact and robust representation space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Using SVDD Ball to Cover Unequal Clusters</head><p>Assume that a dataset contains N data objects, {x i }, i = 1, ..., n and a ball is described by its center a and the radius R. The goal of SVDD (Support Vector Data Description, <ref type="bibr" target="#b12">[13]</ref>) is to find a closed spherical boundary around the given data points. In order to avoid the influence of outliers, SVDD actually faces the tradeoff between two conflicting goals, i.e., minimizing the radius while covering as many data points as possible. This can be formulated as the following objective,</p><formula xml:id="formula_4">min a,R,ξi R 2 + λ N i=1 ξ i s.t. x i − a 2 ≤ R 2 + ξ i ξ i ≥ 0,<label>(4)</label></formula><p>where the slack variable ξ represents the penalty related with the deviation of the i-th training data point outside the ball, and λ is a user defined parameter controlling the degree of regularization imposed on the objective. With the KKT conditions, we have a = N i=1 x i , i.e., the center a of the ball is a linear combination of the data x i . The dual function of Eq.( 4) is</p><formula xml:id="formula_5">max i α i x i , x i − i j α i α j x i , x j s.t. i α i = 1 , α i ∈ [0, λ] , i = 1, ..., N,<label>(5)</label></formula><p>where α i and α j are Lagrangian multipliers. By solving the quadratic programming problem we can get the center a and the radius R.</p><p>The SVDD method can be understood as a type of one-class SVM and its boundary is solely determined by support vectors points. SVDD allows us to summarize a group of data points in a nice and robust way. Hence it is natural to use SVDD ball to model each cluster from K-means, thereby combining the strength of both models. In particular, for a given data point we first compute its distance h k to the surface of each SVDD ball C k , and then use the following modified "triangle" encoding method for feature representation (c.f., E.q.( 3)),</p><formula xml:id="formula_6">f k (x) = max{0, g(h) − h k (x)},<label>(6)</label></formula><p>where h k (x) = ||x − R k || 2 is the distance from the point x to the surface of the k-th SVDD ball, while g(h) is the average of the values h k . Shown in <ref type="figure">Fig. 2</ref> for a data point x, C i , i = 1, 2 respectively are the centroids of two SVDD balls with R i , i = 1, 2 being the radius. Since the distances from x to C 1 and C 2 are equal, <ref type="figure">Fig. 2</ref>. Using the SVDD ball to cover the clusters of K-means, where two SVDD balls cover two clusters with different sizes, respectively. For a test point x, we encode its feature using its distance h to the surface of an SVDD ball. This can be calculated by subtracting the length R of the radius of the ball from the distance z between x to the ball center C. Hence for two SVDD balls with different size, the encoded features for the same point x would be different.</p><p>center of SVDD center of C-SVDD center of K-means centering <ref type="figure">Fig. 3</ref>. Illustration of the difference between SVDD and C-SVDD. Note that after centering the SVDD ball (left), the center of C-SVDD ball (right) aligns better with the high density region of the data points.</p><p>x will be assigned the same scores on the two ball with the Kmeans scheme (c.f., E.q. <ref type="formula" target="#formula_3">( 3)</ref>). However, if we take the density and size of the clusters into accounts, the score from C 2 should be higher in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The C-SVDD Model</head><p>Although SVDD ball provides a robust way to describe the cluster of data, one unwelcome property of the ball is that it may not align well with the distribution of data points in that cluster. As illustrated in <ref type="figure">Fig. 3 (left)</ref>, although the SVDD ball covers the cluster C 1 well, its center is biased to the region with low density. This should be avoided since it actually gives suboptimal estimates on the distribution of the cluster of data.</p><p>To address this issue, inspired by the observation that the centers of K-means are always located at the corresponding mode of their local density, we propose to shift the SVDD ball to the centroid of the data such that it may fit better with the distribution of the data in a cluster. Our new objective function is then formulated as, 1</p><formula xml:id="formula_7">min R,ξi R 2 + λ N i=1 ξ i s.t. x i − a 2 ≤ R 2 + ξ i a = 1 N N i=1 x i ξ i ≥ 0,<label>(7)</label></formula><p>and its Lagrange function is as follows,</p><formula xml:id="formula_8">L(R, ξ, α, β) = R 2 + λ N i=1 ξ i + N i=1 α i { x i − a 2 − R 2 − ξ i } − N i=1 β i ξ i ,<label>(8)</label></formula><p>where α i ≥ 0 and β i ≥ 0 are the corresponding Lagrange multipliers. According to KKT Conditions, we have,</p><formula xml:id="formula_9">∂L ∂R = 2R − 2R N i=1 α i = 0 , N i=1 α i = 1 (9) ∂L ∂ξ i = λ − α i − β i = 0<label>(10)</label></formula><p>Taking Eq. <ref type="formula">(9)</ref> and Eq.(10) into the Lagrange function <ref type="formula" target="#formula_8">(8)</ref> we get that</p><formula xml:id="formula_10">L(R, ξ, α, β) = N i=1 α i { x i − a 2 }.</formula><p>Recalling that a = 1 N N i=1 x i , one has the following dual function,</p><formula xml:id="formula_11">max i α i x i , x i − 2 N i j α i x i , x j s.t. i α i = 1 , α i ∈ [0, λ] , i = 1, ..., N.<label>(11)</label></formula><p>This can be reformulated as</p><formula xml:id="formula_12">min 2 N α T He − α T F s.t. α T e = 1 , α i ∈ [0, λ] , i = 1, ..., N,<label>(12)</label></formula><p>where</p><formula xml:id="formula_13">H = ( x i , x j )) N ×N , F = ( x i , x i ) N ×1 , e = (1, 1, ..., 1) T .</formula><p>This objective function is linear to α, and thus can be solved efficiently with a linear programming algorithm.</p><p>Since the model is centered towards the mode of the distribution of the data points in a cluster, we named our method as C-SVDD (centered-SVDD). <ref type="figure">Fig. 3</ref> shows the difference between SVDD and C-SVDD, where the left is from SVDD and the right from C-SVDD. We can see that our new model aligns better with the density of the data points, as expected. It is also worth mentioning that the normalization parameter λ plays an important role in our model -a larger λ value <ref type="bibr" target="#b0">1</ref> We choose the squared L2 norm distance as a convenient for optimization. There are also other robust distance such as non-squared L2 norm distance <ref type="bibr" target="#b22">[23]</ref>.</p><p>would allow more noise to enter the ball, while λ = 0, the C-SVDD model actually reduces to the naive single-cluster K-means. More discussions on setting this value empirically will be given in Section IV.</p><p>After the model is trained, we use the modified "triangle" encoding (E.q. 6) for feature encoding, with almost the same computational complexity with its K-means counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. K-means Encoding vs. C-SVDD Encoding</head><p>To this end, it will be useful to take a brief discussion on the difference of two kinds of feature maps, i.e., K-meansbased "triangle" encoding (E.q. 3) and our C-SVDD-based one 2 . For this a pilot experiment is conducted. Particularly, we learn a very small dictionary containing only five atoms using five face images, by clustering ZCA-whitened patches randomly sampled from the faces, and then take these for feature encoding. <ref type="figure" target="#fig_2">Fig.4</ref> illustrates the face images used for dictionary learning (top) and the five learnt atoms (leftmost). The feature maps of face images encoded by the K-means encoding method and those by the C-SVDD encoding method are respectively shown in <ref type="figure" target="#fig_2">Fig.4</ref> (a) and <ref type="figure" target="#fig_2">Fig.4 (b)</ref>, where each row is corresponding to one dictionary atom next to it and each column corresponding to one face.</p><p>By comparing the feature maps shown in <ref type="figure" target="#fig_2">Fig.4</ref> (a) and <ref type="figure" target="#fig_2">Fig.4  (b)</ref>, one can see that the C-SVDD-based ones contain more detailed information than the K-means feature maps for the first three atoms, while the responses of the last two atoms are largely suppressed by our method (c.f., last two rows of <ref type="figure" target="#fig_2">Fig.4 (b)</ref>). To further understand this phenomenon, we plot the entropy of each atom (by treating them as a small image patch) in <ref type="figure">Fig.5 (c)</ref>. The figure shows that the entropy of the last two atoms is much smaller than that of the first three ones, which indicates that the local appearance patterns captured by these last two atoms are much simpler than those by the first three. Hence these two atoms will tend to be widely used by many faces, resulting in reduced discriminative capability in distinguishing different subjects. In this sense, it will be useful to suppress their responses (c.f., the last two rows of <ref type="figure" target="#fig_2">Fig.4 (b)</ref>).</p><p>It is also useful to inspect the distribution of local facial patches attracted by these atoms. <ref type="figure">Fig.5 (a)</ref> gives the results. It can be seen that this distribution is not uniform and the number of local patches attracted by the fourth atom is significantly larger than those by other atoms. As a result, for K-means encoding method, the feature maps yielded by this atom show much more rich details than others (see the fourth row of <ref type="figure" target="#fig_2">Fig.4 (a)</ref>), potentially indicating that it could play more important roles than others in the subsequent classification task. However, as explained above, since this atom actually contains much less information than the first three atoms (low entropy and being a "common word"), it is really not good to over-emphasize its importance in feature encoding.</p><p>This drawback of K-means feature mapping is largely bypassed by our C-SVDD-based scheme. As shown <ref type="figure">Fig.5 (b)</ref>, the fourth atom actually represents a very small cluster. In fact, the radius of C-SVDD ball corresponding to the more informative atom tends to be large, and one major advantage of our C-SVDD-based strategy is that it is capable to exploit this characteristic of dictionary atoms for more effective feature encoding, as shown in the first three rows of <ref type="figure" target="#fig_2">Fig.4 (b)</ref>. This partially explains the superior performance of the proposed C-SVDD method compared to its K-means counterpart (c.f., experimental results in Section IV).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Encoding Feature Maps with SIFT Representation</head><p>Traditional unsupervised methods like Bag of Words (BoW) model <ref type="bibr" target="#b17">[18]</ref> usually generate a global feature representation by simply histogramming over local codings, while ignoring spatial relationship between local patches.</p><p>One problem of preserving spatial information in feature representation is due to the huge dimension of feature maps. Suppose that the size of a receptive field is r × r, and the size of an input image is D × D. After densely extracting patches and encoding them, we would obtain K feature maps, one for each filter, with each of size S×S (S = D−r+1). Particularly, for small images with D = 96, and a small dictionary with size K = 256 and with size of its filter r = 5, the resulting dimension of K feature maps is nearly 2M , which is too large for many applications. One can use such methods as average pooling or max pooling to reduce the size of feature maps. For p × p sized pooling blocks, the size of a feature map can be reduced to S p × S p . In the above example if we set p = 5, the dimension of each map becomes 19 × 19 = 361, which is still too big when concatenating K maps. However, if we choose a bigger pooling window, the more spatial information will be lost.</p><p>In this paper we proposed a variant of SIFT-representation to address the above issues. SIFT is a widely used descriptor in computer vision and is helpful to suppress the noise and improve the invariant properties of the final feature representation. The way we get SIFT representation is not in general way which extracts a 128-bit SIFT-descriptors densely. This will also cause to a very high dimensionality. For example, if we extract 128 dimensional SIFT-descriptors densely in 256 feature maps with the size of 16 × 16 in pixel, the dimension of the obtained representation vector will be as high as over 11.8M (250 × 19 × 19 × 128 = 11, 829, 248). To address this issue, we first divide each feature map into m × m blocks and then only extract an 8-bit gradient histogram from each block in the same way as SIFT does. This results in a feature representation with dimension of m × m × 8 for each map (Such as if m=3, then the dim is only 72bit). In this way we significantly reduce the dimensionality while preserving rich information for the subsequent task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Multi-scale Receptive Field Voting</head><p>Next we extend our method to exploit multi-scale information for better feature learning. A multi-scale method is a way to describe the objects of interest in different sizes of context. This would be useful since patches of a fixed size can seldom characterize an object well -actually they can only capture local appearance information limited in that size. For example, if the size is very small, information about edges  could be captured but the information on how to combine these into more meaningful patterns such as motifs, parts, poselets, and object, is lost, while information about these entities at different levels is valuable in that they are not only discriminative by itself but complementary to each other as well. Most popular manually designed feature descriptors, such as SIFT or HoG, address this problem to some extend by pooling image gradients into edglets-like features, but it is still unclear, for example, how to assemble edglets into motifs using these methods. Convolutional neural network provides a simple and comprehensive solution to this issue by automatically learn hierarchies of features ranging from edglets to objects. However, during this procedure, information on where those high-level patterns are found becomes more and more ambiguous. Because our C-SVDDNet is a single-layer network, it is difficult to learn multi-scale information in a hierarchical way. Instead, we take a naive way to obtain multi-scale information by using receptive fields of different sizes. In particular, we fetch patches with S i × S i , i = 1, 2, 3 squares in size from training images and use these to train dictionary atoms with corresponding size through K-means. <ref type="figure">Fig.6</ref> shows some examples of atoms we learnt on a face dataset. One can see that these feature extractors are similar to those learnt using a typical ConvNet. Specifically, with the increasing window size, the learnt features become more understandable -for example, as shown in <ref type="figure">Fig.6 (c)</ref>, using a receptive field with size of 20 × 20 on face images of 64 × 64, we successfully learned facial parts such as the eyes, the mouth, and so on, while a smaller receptive field gives us some oriented filters, as shown in <ref type="figure">Fig.6 (a)</ref>. At each scale we train several networks with different pooling window. One advantage of this method is that it is very efficient to learn and is effective in capture salient features in a multi-scale context. However, it will not tell us how the bigger patterns are explained by smaller ones -such information would be useful from a generative angle.</p><p>To use the learnt multi-scale information for classification, we train a separate classifier on the output layer of the corresponding network (view) according to different receptive sizes and different pooling sizes, then combine them under a boosting framework. Particularly, assume that the total number of categories is C, and we have M scales (with K different number of pooling sizes for each scale), then we have to learn M × K × C output nodes. These nodes are corresponding to M × K multi-class classifiers. Let us denote the parameter of the t− th classifier θ t ∈ R D×C (D is the dimension of feature representation) as θ t = [w t1 , w t2 , ..., w tC ], where w tk is the weight vector for the k-th category. We first train these parameters using a series of one-versus-rest L 2 -SVM classifiers, and then normalize the outputs of each classifier using a soft max function,</p><formula xml:id="formula_14">f tk (x i ) = exp(w T tk x i ) C c=1 exp(w T tc x i ).<label>(13)</label></formula><p>Finally, the normalized predictions f tk are combined to make the final decision,</p><formula xml:id="formula_15">g(x i ) = argmax c t a T tc f t (x i ).<label>(14)</label></formula><p>where f t = {f t1 , f t2 , ..., f tC } is the output vector of the t-th classifier, and the corresponding combination coefficients a tc are trained using the following objective,</p><formula xml:id="formula_16">min ac i max(0, 1 − t a T tc f t (x i )) 2 + λ||a c || 2 (15)</formula><p>This is the same type of one-versus-rest L 2 -SVM mentioned before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND ANALYSIS</head><p>To evaluate the performance of the proposed C-SVDDNet, we conduct extensive experiments on four datasets including two object classification datasets(STL-10 <ref type="bibr" target="#b11">[12]</ref>, MINST <ref type="bibr" target="#b1">[2]</ref>) and two image retrieval datasets(Holiday <ref type="bibr" target="#b23">[24]</ref>, INRIA Copydays <ref type="bibr" target="#b24">[25]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Settings</head><p>All the images undergo whitening preprocessing before feeding them into the network. The whitening operation linearly transforms the data such that their covariance matrix becomes unit sphere, hence justifying the Euclidean distance we use in the K-means clustering procedure.</p><p>Unless otherwise noted, the parameter settings listed in Table.I apply to all experiments. The influence of some important parameters, such as the number of filters, will be investigated in more detail in the subsequent sections. For single scale network the receptive field is set to be 5 × 5 by default across all the datasets, as recommended in <ref type="bibr" target="#b11">[12]</ref>, while in multi-scale version, we use receptive fields in three scales, as shown in <ref type="table">Table.</ref>I.</p><p>For C-SVDD ball there is a regularization parameter λ to set. This parameter allows us to control the amount of noise we are willing to tolerant to. As can be seen from E.q.1, a small λ value encourages a tight ball. We set λ = 1 by default for most datasets except for those with too noisy background are set to 0.005. Furthermore, the centers in C-SVDD are set as the same as those in k-means, so that we can safely ignore the effect of the initialization of k-means.</p><p>Throughout the experiments, we use Coates' K-means "triangle" encoding method <ref type="bibr" target="#b11">[12]</ref> (c.f., Section ??) as baseline (denoted as 'K-means'), while its direct counterpart method by simply replacing "triangle" encoding with C-SVDD encoding is denoted as 'C-SVDD'. Furthermore, we denote the proposed single layer network as 'C-SVDDNet', and its multi-scale version as 'MSRV + C-SVDDNet'. In addition, we re-evaluate the baseline method <ref type="bibr" target="#b11">[12]</ref> within the proposed network by replacing its component of C-SVDD with the K-means-based encoding, denoted as 'K-meansNet'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis of the Proposed Method</head><p>First of all we conduct extensive experiments on the STL-10 dataset to investigate the behavior of the proposed method. The STL-10 is a large image dataset popularly used to evaluate algorithms of unsupervised feature learning or self-taught learning. Besides 100,000 unlabeled images, it contains 13,000 labeled images from 10 object classes, among which 5,000 images are partitioned for training while the remaining 8,000   images for testing. All the images are color images with 96 × 96 pixels in size. There are 10 pre-defined overlapped folds of training images, with 1000 images in each fold. In each fold, a classifier is trained on a set of 1000 training images, and tested on all 8000 testing images. In consistence with <ref type="bibr" target="#b11">[12]</ref>, we report the average accuracy across 10 folds. For unsupervised feature learning we randomly select 20,000 unlabeled data. The size of spatial pooling is 4 × 4, hence the size of feature maps fed for SIFT representation is 23 × 23. For multi-scale receptive voting we use 2 scale (5 × 5 and 7 × 7), on each of which we perform spatial pooling in 5 sizes ranging from 2 × 2 to 6 × 6.</p><p>Do we really need a large number of local features? By the number of features, we mean the number of filters K used for feature extraction, which is equal to the number of dictionary atoms. One of the major conclusions of Coates et al.'s series of controlled experiments on single layer unsupervised feature learning network <ref type="bibr" target="#b11">[12]</ref> is that compared to the choice of particular learning algorithm, the parameters that define the feature extraction pipeline, especially the number of features, have much more deep impact on the performance. Using a K-means network with 4000 features, for example, they are able to achieve surprisingly good performance on several benchmark datasets -even better than those with much deeper architectures such as Deep Boltzmann Machine <ref type="bibr" target="#b25">[26]</ref> and Sparse Auto-encoder <ref type="bibr" target="#b11">[12]</ref>.</p><p>However, one drawback accompanying this large dictionary is that a very crude pooling size has to be adopted (e.g., 46 × 46 over 92 × 92 feature maps) to condense the resulting feature maps, otherwise the dimensionality of the final feature representation could be prohibitively high. For example, a 3×3 pooling over 4000 feature maps with 92 × 92 in size would lead to a total number of features over 3.8M. Hence the first question we investigate is that whether such a large number of features are really needed all the time? <ref type="figure" target="#fig_5">Fig.7</ref> gives the performance curves according to varying number of features with different methods on the STL-10 dataset. Besides the aforementioned methods, in this figure we also give the results of random dictionary (i.e, local dictionary atoms are obtained randomly without being fine tuned by k-means, denoted as "Random" ) and of the combination of random dictionary and SIFT representation (denoted as 'RandomNet').</p><p>It can be seen that with the increasing number of features, the performance of both K-means and C-SVDD methods rises, which is consistent with the results by Coates et al. <ref type="bibr" target="#b11">[12]</ref>. One possible explanation is that since both K-means encoding and C-SVDD encoding use the learnt dictionary to extract non-linear features, more dictionary atoms help to disentangle factors of variations in images. In our opinion the capability to learn a large number of atoms at relatively low computational cost is one of the major advantages of K-means based methods for unsupervised feature learning over other algorithms such as Gaussian Mixture Model (GMM), sparse coding, and RBM. For example, it is difficult for a GMM to learn a dictionary with over 800 atoms <ref type="bibr" target="#b11">[12]</ref>.</p><p>On the other hand, a too large dictionary can increase the redundancy and decrease the efficiency. Hence it is desirable to reduce the number of features while not hurting the performance too much. <ref type="figure" target="#fig_5">Fig.7</ref> shows that our C-SVDD encoding method consistently works better than the K-means encoding at different number of features, and combining C-SVDD encoding and SIFT-based representation dramatically reduces the needs for large dictionary without scarifying the performance. Actually, <ref type="table">Table.</ref>II and <ref type="figure" target="#fig_5">fig.7</ref> show that using our C-SVDD encoding and the SIFT feature representation, the dictionary size reduces by 10 times (from 4,800 <ref type="bibr" target="#b26">[27]</ref> to 500) while the performance improves by 12% (from 53.80% <ref type="bibr" target="#b26">[27]</ref> to 65.92%).</p><p>As for the random dictionary (denoted as 'Random' and 'RandomNet' in <ref type="figure" target="#fig_5">Fig.7)</ref>, it is interesting to see that when the number of atoms is small, random atoms perform much worse than those finetuned by k-means. But as the size of dictionary increases, the performance difference between the random dictionary and K-means dictionary begins to reduce. For example, at 500 features, using random atoms gives a performance of 54.77%, slightly worse than that of k-means (56.63%), and the performance of RandomNet (62.45%) is also close to that of K-meansNet (63.07%). However the performance of both random methods is all much lower than that of the C-SVDD based methods.</p><p>Effect of the pooling size To investigate the effect of different pooling sizes on the performance using the proposed method, we conduct a series of experiments on the STL-10 dataset. Particularly, for a 96 × 96 original image, we use a receptive  rf5/pl2 rf5/pl3 rf5/pl4 rf5/pl5 rf5/pl6 rf7/pl2 rf7/pl3 rf7/pl4 rf7/pl5 rf7/pl6 All <ref type="figure">Fig. 9</ref>.</p><p>Detailed performance of 10 different representations and their ensemble on the STL-10 dataset. These representations are obtained by combining different receptive field size (rfs) and pooling size (pls), where rfs indicates a receptive field of s × s, and plm denotes a pooling block of m × m in pixel. field of 5 × 5 in pixel for feature extraction and obtain a layer of feature maps with 92 × 92. The pooling blocks are set to be m × m such that the size of final feature maps after pooling is 92 m × 92 m . We vary m × m from 1 × 1 to 31 × 31 and record the yielded accuracy. <ref type="figure" target="#fig_6">Fig.8</ref> gives the results under different settings. We can see from the figure that generally for the one layer K-means-based network we need bigger block sizes for improved translation invariance, but adding a robust SIFT encoding layer after pooling effectively reduces the needs for large pooling size while obtaining better performance. One possible reason is that this tends to characterize more detailed information of the objects to be represented.</p><p>Effect of the multi-scale receptive field voting <ref type="figure">Fig.9</ref> gives the detailed accuracy of 10 representations using 2 sizes of receptive fields and 5 sizes of pooling blocks. One can see that different representation leads to different prediction accuracy  68.00 ± 0. Contribution of components To illustrate the contributions of the individual stages of the proposed method (i.e., C-SVDDbased encoding, SIFT-representation and multi-scale voting), we conduct a series of experiments on the STL-10 dataset by removing each of the three main stages in turn while leaving the remaining stages in place (the comparison is thus against our full method). <ref type="figure" target="#fig_1">Fig.10</ref> gives the results. In general each stage is beneficial and (not shown) the results are cumulative over the stages, but the SIFT stage seems to contribute most to the performance improvement. This suggests that taking spatial information into global representation is of importance. <ref type="table">Table.</ref>II gives our results on the STL-10 dataset. The major challenges of this dataset lie in that its images are captured in the wild with cluttered background, objects in various scales and poses. As before, we compared our method with several feature learning methods with state of the art performance. One can see that our one scale C-SVDD  <ref type="bibr" target="#b27">[28]</ref>, Selective Receptive Fields (SRF) <ref type="bibr" target="#b26">[27]</ref>, and Discriminative Sum-Product Networks (DSPN) <ref type="bibr" target="#b29">[30]</ref>). This also indicates that spatial information preserving using SIFT is indeed useful in unsupervised feature learning. Also note that replacing the proposed C-SVDD encoding with K-means encoding leads to nearly 3.0% performance loss, while fusing the multi-scale information gives us about 2.3% improvement in accuracy, exceeding the current best performer <ref type="bibr" target="#b31">[32]</ref> on this challenging dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Object Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STL-10 dataset</head><p>MINST dataset The MNIST is one of the most popular datasets in pattern recognition. It consists of grey valued images of handwritten digits between 0 and 9. It has a training set of 60,000 examples, and a test set of 10,000 examples, all of which have been size-normalized and centered in a fixed-size image with 28 × 28 in pixel. In training we use a dictionary with 400 atoms for feature mapping, and after pooling/subsampling we break each feature map into 9 blocks to extract SIFT features. For multi-scale receptive voting, we use 3 types of receptive fields: 5×5, 7×7 and 9×9. Combined these with two settings for pooling sizes (i.e., 1 × 1 and 2 × 2, respectively), 6 different views/representations can be obtained for each image in this dataset. <ref type="table">Table.</ref>III gives our experimental results on the MINST dataset. It is well-known that deep learning has achieved great success on this task of digit recognition. For example, only 95 among 10,000 test digits are misclassified by the Deep Boltzmann Machines <ref type="bibr" target="#b25">[26]</ref>, while Convolutional Deep Belief Networks <ref type="bibr" target="#b32">[33]</ref> and Maxout Networks <ref type="bibr" target="#b35">[36]</ref> respectively reduce this number to 82 and 45. Our simple single layer network (MSRV+C-SVDDnet) achieves an error as low as 0.35%, which is highly competitive to other complex methods using deep architecture. <ref type="figure" target="#fig_1">Fig.11</ref> shows all the 35 misclassified digits by our method, and one can see that these misclassified digits are very confusing even for human beings. Compared to the original K-means network <ref type="bibr" target="#b11">[12]</ref>, the proposed method reduces the error rate by 65%, with much smaller number of filters. This reveals that at least on this dataset with clean background, it is very beneficial to focus more on the representation of the details of the image, rather than emphasizing too much on its global aspects using a large number of filters and a large pooling size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Image Retrieval</head><p>Holiday dataset INRIA Holiday dataset consists of 1491 images from personal holiday photos. There are 500 queries, most of which have 1-2 ground truth images. mAP (mean average precision) is employed to measure the retrieval accuracy. We resize all the images to 96 × 96. In training we use a dictionary with 256 atoms for feature mapping, and after pooling/subsampling we break each feature map into 4 blocks to extract SIFT features. Thus the dimension of final representation is 8196. And we also run PCA for dimensionality reduction as <ref type="bibr" target="#b18">[19]</ref>. For multi-scale receptive voting, we use 2 types of receptive fields: 5×5 and 7×7. Combined these with four settings for pooling sizes (i.e., 3×3, 4×4, 5×5 and 6×6, respectively), 8 different views/representations can be obtained for each image in this dataset. Note that in image retrieval task, we can not train classifiers so that we just concatenate all the views' representations to combine multi-scale information. In retrieval stage we use Euclidean distance in nearest neighbor searching as in <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b38">[39]</ref>, facilitating a fair comparison between various feature representation methods on this task. <ref type="table">Table.</ref>IV gives our experimental results on this dataset. We compare our method with BOW, VLAD, FV under different dimension ( reduced through PCA). BoW takes a 20k sized filter bank but has the lowest mAP (45.2%). Replacing BoW with K-means triangle encoding improves mAP by 10% (55.2%), but still needs a large filter bank of 3.2K.</p><p>Previous state-of-art unsupervised feature learning methods, i.e., VLAD and FV <ref type="bibr" target="#b18">[19]</ref>, can achieve a high mAP of 62.1% 62.6% respectively. And both of them only take a small set of filters of size 256. In <ref type="bibr" target="#b38">[39]</ref> Arandjelovic combines VLAD with adaptive filter bank and a new normalization to achieve an accuracy of 64.6%. Our proposed C-SVDDNet can get a mAP of 66.0% with 256 filters as well. It outperforms VLAD by 3.9% and VLAD+adapt+innorm by 1.4%. Even if we reduce its dimension to smaller sizes with PCA, it consistently achieves the best performance among the compared ones. Also note that replacing K-means encoding with C-SVDD encoding results in significant improvement (from K-means 55.2% to C-SVDD 57.4%, and from K-meansNet 62.5% to C-SVDDNet 66.0%). When concatenating multi-scale representation from 8 views, we are able to achieve the highest mAP of 70.2%, without using any supervision information.</p><p>Copydays dataset INRIA Copydays dataset was designed to evaluate near-duplicate detection <ref type="bibr" target="#b18">[19]</ref>. The dataset contains 157 original images. To obtain query images relevant in a copy detection scenario, each image of the dataset has been transformed with three different types of transformation: image resizing, cropping (Here we use only the queries with the cropping parameter fixed to 50%), strong transformations (print and scan, occlusion, change in contrast, perspective effect, blur, etc). There is in total 229 transformed images, each of which has only a single matching image in the database. All images are resized to 75 × 75. We use 2 types of receptive fields 5 × 5 and 7 × 7, together with three pooling sizes (i.e., 3 × 3, 4 × 4 and 5 × 5 respectively), which result 6 different views. To challenge ourself in this experiments we also merge the database with 10k web images as <ref type="bibr" target="#b18">[19]</ref> does.</p><p>It is a large scale retrieval task. <ref type="table">Table.</ref>V gives our exper-imental results on this dataset. We can see that in the 50% cropped circumstance, our C-SVDDNet with only 256 filters can be robust enough to achieve a mAP of 100% as BoW with 20k filters. When reducing its dimension to 128 bits, it still performs the second best. In the strong transformation setting, our C-SVDDNet achieves a mAP of 62.2% which outperforms VLAD (59.2%) and FV (59.6%) by nearly 3%. Furthermore, one can see that C-SVDD encoding allows our C-SVDDNet improve upon K-meansNet by 6.8% in terms of mAP. When reduced to 128 bits, our C-SVDDNet achieves a mAP of 52% under the difficult cases of strong transformation, which outperforms other compared methods by more than 10%, while our multi-scale version improves the mAP by 3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a simple one-layer neural network termed C-SVDDNet for unsupervised feature learning. One of the major advantages of the proposed method is that it allows effective feature representation for many applications, such as object classification and image retrieval, by exploiting unlabeled data which are often cheap and readily available. We show that when properly combined with the SIFT descriptors, such representation could be made even more efficient and discriminant. Extensive experiments on several challenging object classification datasets and image retrieval datasts demonstrate that the proposed method significantly outperforms previous state of the art unsupervised feature learning methods such as Bag of Word, VLAD <ref type="bibr" target="#b18">[19]</ref>, and FV <ref type="bibr" target="#b19">[20]</ref>.</p><p>Additionally, we show that for feature representation, a very big dictionary is not necessary, as one could accumulate rich information in each feature map and preserve them with compact encoding (e.g, using the proposed method). This significantly reduces the computational cost. Last but not least, we show that one can use multi-scale information to further improve the performance without training many layers of networks -after all training several shallow networks is much easier than training a deep one.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Dong</head><label></label><figDesc>Wang and Xiaoyang Tan are with the Department of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, P.R. China. Corresponding author: Xiaoyang Tan (x.tan@nuaa.edu.cn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the unequal cluster effect, where the distances from a test point x to two cluster centers C 1 and C 2 are equal but the size of two clusters are different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of feature maps of five face images (a) using K-means (c) and C-SVDD (d) respectively, based on five local dictionary atoms (b), where maps in each row are corresponding to one atom next to it while each column corresponding to one face. For the response values in a feature map, the darker the lower.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Distribution of the number of patches attracted by each atom (a), the radius of the corresponding SVDD ball (b), and the entropy (c) over the five atoms shown in Fig.4 (leftmost) Features of different scales learnt from face images. The size of original face images is 64 × 64 in pixels. (a) size = 5 (b) size = 10 (c) size = 20</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>The effect of different number of features on the performance with different methods on the STL-10 dataset. All parameters here (such as pooling size) are set as the same as inFig.8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>The effect of different pooling sizes on the performance with the proposed method on the STL-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>The contribution of the three major components of the proposed method to the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>55 BoW(K = 4800 D = 4800) 51.50 ± 0.6 VLAD(K = 512 D = 40960) 57.60 ± 0.6 FV (K = 256 D = 40960) 59.10 ± 0.8 K-means (K = 4800 D = 19200) [27] (2011) 53.80 ± 1.6 C-SVDD (K = 4800 D = 19200) 54.60 ± 1.5 K-meansNet (K = 500 D = 36000) 63.07 ± 0.6 C-SVDDNet (K = 500 D = 36000) 65.92 ± 0.6 MSRV+K-meansNet 64.96 ± 0.4 MSRV+C-SVDDNet 68.23 ± 0.5 but combining them leads to better performance. This shows that the representations captured with different receptive fields and pooling sizes are complementary to each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>All misclassified 35 handwritten digits among 10,000 test examples by our method. The small digit in each white square is the ground truth label of the corresponding image, and the one in the green square is the prediction made by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DEFAULT</head><label>I</label><figDesc>PARAMETER SETTINGS FOR OUR METHODS. , 7 × 7, 9 × 9 size of average pooling 4 × 4 * , 1 × 1, 3 × 3 λ of C-SVDD 1 * , 0.005 *-default setting for the non-multi-scale network.</figDesc><table><row><cell>Parameter</cell><cell>value</cell></row><row><cell>#clusters</cell><cell>≤500</cell></row><row><cell>Size of receptive field</cell><cell>5 × 5</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARATIVE</head><label>II</label><figDesc>PERFORMANCE (%) ON THE STL-10 DATASET.</figDesc><table><row><cell>Algorithm</cell><cell>Accuracy(%)</cell></row><row><cell>Selective Receptive Fields (3 Layers) [27] (2011)</cell><cell>60.10 ± 1.0</cell></row><row><cell>Trans. Invariant RBM (TIRBM) [28] (2012)</cell><cell>58.70</cell></row><row><cell>Simulated visual fixation ConvNet [29] (2012)</cell><cell>61.00</cell></row><row><cell cols="2">Discriminative Sum-Prod. Net (DSPN) [30] (2012) 62.30 ± 1.0</cell></row><row><cell>Hierarchical Matching Pursuit (HMP) [31] (2013)</cell><cell>64.50 ± 1.0</cell></row><row><cell>Deep Feedforward Networks [32] (2014)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARATIVE</head><label>III</label><figDesc>PERFORMANCE (%) ON THE MINST DATASET.</figDesc><table><row><cell>Algorithm</cell><cell>Error(%)</cell></row><row><cell>Deep Boltzmann Machines [26] (2009)</cell><cell>0.95</cell></row><row><cell cols="2">Convolutional Deep Belief Networks [33] (2009) 0.82</cell></row><row><cell>Multi-column deep neural networks [34] (2012)</cell><cell>0.23</cell></row><row><cell>Network in Network [35] (2013)</cell><cell>0.47</cell></row><row><cell>Maxout Networks [36] (2013)</cell><cell>0.45</cell></row><row><cell>Regularization of neural networks [37] (2013)</cell><cell>0.21</cell></row><row><cell>PCANet [21] (2014)</cell><cell>0.62</cell></row><row><cell>Deeply-Supervised Nets [38] (2014)</cell><cell>0.39</cell></row><row><cell>K-means (1600 features)</cell><cell>1.01</cell></row><row><cell>C-SVDD (1600 features)</cell><cell>0.99</cell></row><row><cell>K-meansNet (400 features)</cell><cell>0.45</cell></row><row><cell>C-SVDDNet (400 features)</cell><cell>0.43</cell></row><row><cell>MSRV+K-meansNet</cell><cell>0.36</cell></row><row><cell>MSRV+C-SVDDNet</cell><cell>0.35</cell></row><row><cell cols="2">network obtains 65.92% accuracy, using a filtering dictionary</cell></row><row><cell cols="2">of 500 atoms, outperforms several other feature encoding</cell></row><row><cell cols="2">methods, such as Bag of Words (BoW), Vector of Linearly</cell></row><row><cell cols="2">Agregated Descriptors (VLAD), Fisher vector (FV) and other</cell></row><row><cell cols="2">unsupervised deep learning methods (e.g, Trans. Invariant</cell></row><row><cell>RBM (TIRBM)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARATIVE</head><label>IV</label><figDesc>PERFORMANCE (MAP %) ON THE HOLIDAY DATASET.</figDesc><table><row><cell>Algorithm</cell><cell>K</cell><cell>D</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Holidays (mAP %)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>best</cell><cell cols="3">D' = 2048 D' = 512</cell><cell>D' = 128</cell><cell>D' = 64</cell><cell>D' = 32</cell></row><row><cell>BoW [19] (2012)</cell><cell cols="2">20000 20000</cell><cell></cell><cell>45.2</cell><cell>41.80</cell><cell>44.9</cell><cell></cell><cell>45.2</cell><cell>44.4</cell><cell>41.8</cell></row><row><cell>FV [19] (2012)</cell><cell>256</cell><cell>16384</cell><cell></cell><cell>62.6</cell><cell>62.6</cell><cell>57.0</cell><cell></cell><cell>53.8</cell><cell>50.6</cell><cell>48.6</cell></row><row><cell>VLAD [19] (2012)</cell><cell>256</cell><cell>16384</cell><cell></cell><cell>62.1</cell><cell>62.1</cell><cell>56.7</cell><cell></cell><cell>54.2</cell><cell>51.3</cell><cell>48.1</cell></row><row><cell cols="2">VLAD+adapt+innorm [39] (2013) 256</cell><cell>16384</cell><cell></cell><cell>64.6</cell><cell>-</cell><cell>-</cell><cell></cell><cell>62.5</cell><cell>-</cell><cell>-</cell></row><row><cell>K-means</cell><cell>3200</cell><cell>12800</cell><cell></cell><cell>55.2</cell><cell>54.5</cell><cell>54.9</cell><cell></cell><cell>51.6</cell><cell>48.3</cell><cell>44.5</cell></row><row><cell>C-SVDD</cell><cell>3200</cell><cell>12800</cell><cell></cell><cell>57.4</cell><cell>56.8</cell><cell>57.0</cell><cell></cell><cell>53.1</cell><cell>50.5</cell><cell>46.8</cell></row><row><cell>K-meansNet</cell><cell>256</cell><cell>8192</cell><cell></cell><cell>62.5</cell><cell>59.8</cell><cell>62.5</cell><cell></cell><cell>61.3</cell><cell>55.5</cell><cell>49.5</cell></row><row><cell>C-SVDDNet</cell><cell>256</cell><cell>8192</cell><cell></cell><cell>66.0</cell><cell>63.7</cell><cell>65.8</cell><cell></cell><cell>64.7</cell><cell>59.3</cell><cell>52.1</cell></row><row><cell>MSRV+K-meansNet</cell><cell>256</cell><cell cols="2">8192×8</cell><cell>66.5</cell><cell>65.0</cell><cell>66.3</cell><cell></cell><cell>65.3</cell><cell>58.3</cell><cell>51.5</cell></row><row><cell>MSRV+C-SVDDNet</cell><cell>256</cell><cell cols="2">8192×8</cell><cell>70.2</cell><cell>68.6</cell><cell>69.8</cell><cell></cell><cell>68.5</cell><cell>62.5</cell><cell>53.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">COMPARATIVE PERFORMANCE (MAP %) ON THE COPYDAYS DATASET.</cell></row><row><cell>Algorithm</cell><cell></cell><cell>K</cell><cell>D</cell><cell></cell><cell cols="2">crop 50%</cell><cell cols="2">transformations</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>best</cell><cell>D' = 128</cell><cell>best</cell><cell>D' = 128</cell></row><row><cell cols="2">BoW [19] (2012)</cell><cell>20k</cell><cell>20k</cell><cell></cell><cell>100.0</cell><cell>100.0</cell><cell>54.3</cell><cell>29.6</cell></row><row><cell>FV [19] (2012)</cell><cell></cell><cell>64</cell><cell cols="2">4096</cell><cell>98.7</cell><cell>92.7</cell><cell>59.6</cell><cell>41.2</cell></row><row><cell cols="2">VLAD [19] (2012)</cell><cell>64</cell><cell cols="2">4096</cell><cell>97.7</cell><cell>94.2</cell><cell>59.2</cell><cell>42.7</cell></row><row><cell>K-means</cell><cell></cell><cell cols="3">3200 12800</cell><cell>95.2</cell><cell>91.5</cell><cell>47.6</cell><cell>32.80</cell></row><row><cell>C-SVDD</cell><cell></cell><cell cols="3">3200 12800</cell><cell>97.4</cell><cell>93.8</cell><cell>52.2</cell><cell>36.60</cell></row><row><cell>K-meansNet</cell><cell></cell><cell>256</cell><cell cols="2">8192</cell><cell>96.8</cell><cell>94.3</cell><cell>55.4</cell><cell>37.80</cell></row><row><cell>C-SVDDNet</cell><cell></cell><cell>256</cell><cell cols="2">8192</cell><cell>100.0</cell><cell>98.1</cell><cell>62.2</cell><cell>52.0</cell></row><row><cell cols="2">MSRV+K-meansNet</cell><cell>256</cell><cell cols="3">8192×6 99.7</cell><cell>97.9</cell><cell>58.2</cell><cell>41.8</cell></row><row><cell cols="3">MSRV+C-SVDDNet 256</cell><cell cols="3">8192×6 100.0</cell><cell>100.0</cell><cell>65.6</cell><cell>55.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Hereinafter we will call them respectively "K-means encoding" and "C-SVDD encoding" for short without confusion.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.5538</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition?&quot; in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1112.6209</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The segmentation of the left ventricle of the heart from ultrasound data using deep learning architectures and derivative-based search methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="968" to="982" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evolvable rough-blockbased neural network and its biomedical application to hypoglycemia detection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nuryani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cybernetics IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1338" to="1349" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlinearly activated neural network for solving time-varying complex sylvester equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cybernetics IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1397" to="1407" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hyperfeatures-multilevel local coding for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Ninth European Conf. Computer Vision</title>
		<meeting>Ninth European Conf. Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="30" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geometry of the restricted boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Cueto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sturmfels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algebraic Methods in Statistics and Probability</title>
		<editor>M. Viana and H. Wynn</editor>
		<imprint>
			<biblScope unit="volume">516</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2010" />
			<publisher>AMS</publisher>
		</imprint>
	</monogr>
	<note>Contemporary Mathematics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Arbor</title>
		<imprint>
			<biblScope unit="volume">1001</biblScope>
			<biblScope unit="page">48109</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fault detection based on svdd and cluster algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronics, Communications and Control (ICECC), 2011 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2050" to="2052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient particle filtering via sparse kernel density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Burlina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2480" to="2490" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Centering svdd for unsupervised feature representation in object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="376" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on statistical learning in computer vision, ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pcanet: A simple deep learning baseline for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.3606</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimal mean robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hamming embedding and weak geometry consistency for large scale image search-extended version</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluation of gist descriptors for web-scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sandhawalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Image and Video Retrieval</title>
		<meeting>the ACM International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning invariant representations with local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6418</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning of invariant features via simulated fixations in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3212" to="3220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discriminative learning of sum-product networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3248" to="3256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for rgb-d based object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Committees of deep feedforward networks trained with few data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miclut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="736" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<ptr target="http://arxiv.org/abs/1312.4400" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5185</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">All about vlad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
