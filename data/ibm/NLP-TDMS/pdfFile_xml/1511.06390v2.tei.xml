<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2016 UNSUPERVISED AND SEMI-SUPERVISED LEARNING WITH CATEGORICAL GENERATIVE ADVERSARIAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
							<email>springj@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<postCode>79110</postCode>
									<settlement>Freiburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2016 UNSUPERVISED AND SEMI-SUPERVISED LEARNING WITH CATEGORICAL GENERATIVE ADVERSARIAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method -which we dub categorical generative adversarial networks (or CatGAN) -on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning non-linear classifiers from unlabeled or only partially labeled data is a long standing problem in machine learning. The premise behind learning from unlabeled data is that the structure present in the training examples contains information that can be used to infer the unknown labels. That is, in unsupervised learning we assume that the input distribution p(x) contains information about p(y | x) -where y ∈ {1, . . . , K} denotes the unknown label. By utilizing both labeled and unlabeled examples from the data distribution one hopes to learn a representation that captures this shared structure. Such a representation might, subsequently, help classifiers trained using only a few labeled examples to generalize to parts of the data distribution that it would otherwise have no information about. Additionally, unsupervised categorization of data is an often sought-after tool for discovering groups in datasets with unknown class structure. This task has traditionally been formalized as a cluster assignment problem, for which a large number of well studied algorithms can be employed. These can be separated into two types: (1) generative clustering methods such as Gaussian mixture models, k-means, and density estimation algorithms, which directly try to model the data distribution p(x) (or its geometric properties); (2) discriminative clustering methods such as maximum margin clustering (MMC) <ref type="bibr" target="#b35">(Xu et al., 2005)</ref> or regularized information maximization (RIM) <ref type="bibr" target="#b19">(Krause et al., 2010)</ref>, which aim to directly group the unlabeled data into well separated categories through some classification mechanism without explicitly modeling p(x). While the latter methods more directly correspond to our goal of learning class separations (rather than class exemplars or centroids), they can easily overfit to spurious correlations in the data; especially when combined with powerful non-linear classifiers such as neural networks.</p><p>More recently, the neural networks community has explored a large variety of methods for unsupervised and semi-supervised learning tasks. These methods typically involve either training a generative model -parameterized, for example, by deep Boltzmann machines (e.g. <ref type="bibr" target="#b28">Salakhutdinov &amp; Hinton (2009)</ref>, <ref type="bibr" target="#b9">Goodfellow et al. (2013)</ref>) or by feed-forward neural networks (e.g. <ref type="bibr" target="#b2">Bengio et al. (2014)</ref>, <ref type="bibr" target="#b18">Kingma et al. (2014)</ref>) -, or training autoencoder networks (e.g. <ref type="bibr" target="#b12">Hinton &amp; Salakhutdinov (2006)</ref>, <ref type="bibr" target="#b33">Vincent et al. (2008)</ref>). Because they model the data distribution explicitly through reconstruction of input examples, all of these models are related to generative clustering methods, and are typically only used for pre-training a classification network. One problem with such reconstructionbased learning methods is that, by construction, they try to learn representations which preserve all information present in the input examples. This goal of perfect reconstruction is often directly opposed to the goal of learning a classifier which is to model p(y|x) and hence to only preserve information necessary to predict the class label (and become invariant to unimportant details)</p><p>The idea of the categorical generative adversarial networks (CatGAN) framework that we develop in this paper then is to combine both the generative and the discriminative perspective. In particular, we learn discriminative neural network classifiers D that maximize mutual information between the inputs x and the labels y (as predicted through the conditional distribution p(y|x, D)) for a number of K unknown categories. To aid these classifiers in their task of discovering categories that generalize well to unseen data, we enforce robustness of the classifier to examples produced by an adversarial generative model, which tries to trick the classifier into accepting bogus input examples.</p><p>The rest of the paper is organized as follows: Before introducing our new objective, we briefly review the generative adversarial networks framework in Section 2. We then derive the CatGAN objective as a direct extension of the GAN framework, followed by experiments on synthetic data, <ref type="bibr">MNIST (LeCun et al., 1989)</ref> and <ref type="bibr">CIFAR-10 (Krizhevsky &amp; Hinton, 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GENERATIVE ADVERSARIAL NETWORKS</head><p>Recently, <ref type="bibr" target="#b10">Goodfellow et al. (2014)</ref> introduced the generative adversarial networks (GAN) framework. They trained generative models through an objective function that implements a two-player zero sum game between a discriminator D -a function aiming to tell apart real from fake input data -and a generator G -a function that is optimized to generate input data (from noise) that "fools" the discriminator. The "game" that the generator and the discriminator play can then be intuitively described as follows. In each step the generator produces an example from random noise that has the potential to fool the discriminator. The discriminator is then presented a few real data examples, together with the examples produced by the generator, and its task is to classify them as "real" or "fake". Afterwards, the discriminator is rewarded for correct classifications and the generator for generating examples that did fool the discriminator. Both models are then updated and the next cycle of the game begins.</p><p>This process can be formalized as follows. Let X = {x 1 , . . . x N } be a dataset of provided "real" inputs with dimensionality I (i.e. x ∈ R I ). Let D denote the mentioned discriminative function and G denote the generator function. That is, G maps random vectors z ∈ R Z to generated inputs x = G(z) and we assume D to predict the probability of example x being present in the dataset X : <ref type="bibr">x)</ref> . The GAN objective is then given as</p><formula xml:id="formula_0">p(y = 1 | x, D) = 1 1+e −D(</formula><formula xml:id="formula_1">min G max D E x∼X log p(y = 1 | x, D) + E z∼P (z) log 1 − p(y = 1 | G(z), D) ,<label>(1)</label></formula><p>where P (z) is an arbitrary noise distribution which -without loss of generality -we assume to be the uniform distribution P (z i ) = U(0, 1) for the remainder of this paper. If both the generator and the discriminator are differentiable functions (such as deep neural networks) then they can be trained by alternating stochastic gradient descent (SGD) steps on the objective functions from Equation <ref type="formula" target="#formula_1">(1)</ref>, effectively implementing the two player game described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CATEGORICAL GENERATIVE ADVERSARIAL NETWORKS (CATGANS)</head><p>Building on the foundations from Section 2 we will now derive the categorical generative adversarial networks (CatGAN) objective for unsupervised and semi-supervised learning. For the derivation we first restrict ourselves to the unsupervised setting, which can be obtained by generalizing the GAN framework to multiple classes -a limitation that we remove by considering semi-supervised learning in Section 3.3. It should be noted that we could have equivalently derived the CatGAN model starting from the perspective of regularized information maximization (RIM) -as described in the appendix -with an equivalent outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PROBLEM SETTING</head><p>As before, let X = {x 1 , . . . x N } be a dataset of unlabeled examples. We consider the problem of unsupervisedly learning a discriminative classifier D from X , such that D classifies the data into an a priori chosen number of categories (or classes) K. Further, we require D(x) to give rise to a conditional probability distribution over categories; that is K k=1 p(y = k|x, D) = 1. The goal of learning then is to train a probabilistic classifier D whose class assignments satisfy some goodness of fit measures. Notably, since the true class distribution over examples is not known we have to resort to an intermediary measure for judging classifier performance, rather than just minimizing, e.g., the negative log likelihood. Specifically, we will, in the following, always prefer D for which the conditional class distribution p(y|x, D) for a given example x has high certainty and for which the marginal class distribution p(y|D) is close to some prior distribution P (y) for all k. We will henceforth always assume a uniform prior over classes, that is we expect that the amount of examples per class in X is the same for all k: ∀k, k ∈ K : p(y = k|D) = p(y = k |D) 1 A first observation about this problem is that it can naturally be considered as a "soft" or probabilistic cluster assignment task. It could thus, in principle, be solved by probabilistic clustering algorithms such as regularized information maximization (RIM) <ref type="bibr" target="#b19">(Krause et al., 2010)</ref>, or the related entropy minimization <ref type="bibr" target="#b11">(Grandvalet &amp; Bengio, 2005)</ref>, or the early work on unsupervised classification with phantom targets by <ref type="bibr">Bridle et al. (1992)</ref>. All of these methods are prone to overfitting to spurious correlations in the data 2 , a problem that we aim to mitigate by pairing the discriminator with an adversarial generative model to whose examples it must become robust. We note in passing, that our method can be understood as a robust extension of RIM -in which the adversary provides an adaptive regularization mechanism. This relationship is made explicit in Section B in the appendix.</p><p>A somewhat obvious, yet important, second observation that can be made is that the standard GAN objective cannot directly be used to solve the described problem. The reason for this is that while optimization of Equation (1) does result in a discriminative classifier D -which must capture the statistics of the provided input data -this classifier is only useful for determining whether or not a given example x belongs to X . In principle, we could hope that a classifier which can model the data distribution might also learn a feature representation (e.g. in case of neural networks the hidden representation in the last layer of D) useful for extracting classes in a second step; for example via discriminative clustering. It is, however, instructive to realize that the means by which the function D performs the binary classification task -of discriminating real from fake examples -are not restricted in the GAN framework and hence the classifier will focus mainly on input features which are not yet correctly modeled by the generator. In turn, these features need not necessarily align with our concept of classes into which we want to separate the data. They could, in the worst case, be detecting noise in the data that stems from the generator.</p><p>Despite these issues there is a principled, yet simple, way of extending the GAN framework such that the discriminator can be used for multi-class classification. To motivate this, let us consider a change in protocol to the two player game behind the GAN framework (which we will formalize in the next section): Instead of asking D to predict the probability of x belonging to X we can require D to assign all examples to one of K categories (or classes), while staying uncertain of class assignments for samples from the generative model G -which we expect will help make the classifier robust. Analogously, we can change the problem posed to the generator from "generate samples that belong to the dataset" to "generate samples that belong to precisely one out of K classes".</p><p>If we succeeded at training such a classifier-generator pair -and simultaneously ensured that the discovered K classes coincide with the classification problem we are interested in (e.g. D satisfies the goodness of fit criteria outlined above) -we would have a general purpose formulation for training a classifier from unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CATGAN OBJECTIVE</head><p>As outlined above, the optimization problem that we want to solve differs from the standard GAN formulation from Eq. (1) in one key aspect: instead of learning a binary discriminative function, we <ref type="figure">Figure 1</ref>: Visualization of the information flow through the generator (in green) and discriminator (in violet) neural networks (left). A sketch of the three parts (i) -(iii) of the objective function L D for the discriminator (right). To obtain certain predictions the discriminator minimizes the entropy of p(y|x, D), leading to a peaked conditional class distribution. To obtain uncertain predictions for generated samples the the entropy of p(y|G(z), D) is maximized which, in the limit, would result in a uniform distribution. Finally, maximizing the marginal class entropy over all data-points leads to uniform usage of all classes. aim to learn a discriminator that separates the data into K categories by assigning a label y to each example x. Formally, we define the discriminator D(x) for this setting as a differentiable function predicting logits for K classes: D(x) ∈ R K . The probability of example x belonging to one of the K mutually exclusive classes is then given through a softmax assignment based on the discriminator output:</p><formula xml:id="formula_2">p(y = k|x, D) = e D k (x) K k=1 e D k (x)</formula><p>.</p><p>(2)</p><p>As in the standard GAN formulation we define the generator G(z) to be a function mapping random noise z ∈ R Z to generated samplesx ∈ R I :</p><formula xml:id="formula_3">x = G(z), with z ∼ P (z),<label>(3)</label></formula><p>where P (z) again denotes an arbitrary noise distribution. For the purpose of this paper both D and G are always parameterized as multi-layer neural networks with either linear or sigmoid output.</p><p>As informally described in Section 3.1, the goodness of fit criteria -in combination with the idea that we want to use a generative model to regularize our classifier -directly dictate three requirements that a learned discriminator should fulfill, and two requirements that the generator should fulfill. We repeat these here before turning them into a learnable objective function (a visualization of the requirements is shown in <ref type="figure">Figure 1</ref>).</p><p>Discriminator perspective. The requirements to the discriminator are that it should (i) be certain of class assignment for samples from D, (ii) be uncertain of assignment for generated samples, and (iii) use all classes equally 3 .</p><p>Generator perspective. The requirements to the generator are that it should (i) generate samples with highly certain class assignments, and (ii) equally distribute samples across all K classes.</p><p>We will now address each of these requirements in turn -framing them as maximization or minimization problems of class probabilities -beginning with the perspective of the discriminator. Note that without additional (label) information about the K classes we cannot directly specify which class probability p(y = k|x, D) should be maximized to meet requirement (i) for any given x. We can, nonetheless, formally capture the intuition behind this requirement through information theoretic measures on the predicted class distribution. The most direct measure that can be applied to this problem is the Shannon entropy H, which is defined as the expected value of the information carried by a sample from a given distribution. Intuitively, if we want the class distribution p(y | x, D) conditioned on example x to be highly peaked -i.e. D should be certain of the class assignmentwe want the information content H[p(y | x, D)] of a sample from it to be low, since any draw from said distribution should almost always result in the same class. If we, on the other hand, want the conditional class distribution to be flat (highly uncertain) for examples that do not belong to X -but instead come from the generator -we can maximize the entropy H[p(y | G(z), D)], which, at the optimum, will result in a uniform conditional distribution over classes and fulfill requirement (ii). Concretely, we can define the empirical estimate of the conditional entropy over examples from X as</p><formula xml:id="formula_4">E x∼X H p(y | x, D) = 1 N N i=1 H p(y | x i , D) = 1 N N i=1 − K k=1 p(y = k | x i , D) log p(y = k | x i , D).<label>(4)</label></formula><p>The empirical estimate of the conditional entropy over samples from the generator can be expressed as the expectation of H[p(y | G(z), D)] over the prior distribution P (z) for the noise vectors z, which we can further approximate through Monte-Carlo sampling yielding</p><formula xml:id="formula_5">E z∼P (z) H p(y | D(z), D) ≈ 1 M M i=1 H p(y | G(z i ), D) , with z i ∼ P (z),<label>(5)</label></formula><p>and where M denotes the number of independently drawn samples (which we simply set equal to N ). To meet the third requirement that all classes should be used equally -corresponding to a uniform marginal distribution -we can maximize the entropy of the marginal class distribution as measured empirically based on X and samples from G:</p><formula xml:id="formula_6">H X p(y | D) = H 1 N N i=1 p(y | x i , D) , H G p(y | D) ≈ H 1 M M i=1 p(y | G(z i ), D) , with z i ∼ P (z).<label>(6)</label></formula><p>The second of these entropies can readily be used to define the maximization problem that needs to be satisfied for the requirement (ii) imposed on the generator. Satisfying the condition (i) from the generator perspective then finally amounts to minimizing rather than maximizing Equation <ref type="formula" target="#formula_5">(5)</ref>.</p><p>Combining the definition from Equations (4,5,6) we can define the CatGAN objective for the discriminator, which we refer to with L D , and for the generator, which we refer to with L G as</p><formula xml:id="formula_7">L D = max D H X p(y | D) − E x∼X H p(y | x, D) + E z∼P (z) H p(y | G(z), D) , L G = min G −H G p y | D + E z∼P (z) H p(y | G(z), D) ,<label>(7)</label></formula><p>where H denotes the empirical entropy as defined above and we chose to define the objective for the generator L G as a minimization problem to make the analogy to Equation <ref type="formula" target="#formula_1">(1)</ref> apparent. This formulation satisfies all requirements outlined above and has a simple information theoretic interpretation: Taken together the first two terms in L D are an estimate of the mutual information between the data distribution and the predicted class distribution -which the discriminator wants to maximize while minimizing information it encodes about G(z). Analogously, the first two terms in L G estimate the mutual information between the distribution of generated samples and the predicted class distribution.</p><p>Since we are interested in optimizing the objectives from Equation <ref type="formula" target="#formula_7">(7)</ref> on large datasets we would like both L G and L D to be amenable to to optimization via mini-batch stochastic gradient descent on batches X B of data -with size B N -drawn independently from X . The conditional entropy terms in Equation <ref type="formula" target="#formula_7">(7)</ref> both only consist of sums over per example entropies, and can thus trivially be adapted for batch-wise computation. The marginal entropies H X [p(y | D)] and H G [p y | D ], however, contain sums either over the whole dataset X or over a large set of samples from G within the entropy calculation and therefore cannot be split into "per-batch" terms. If the number of categories K that the discriminator needs to predict is much smaller than the batch size B, a simple fix to this problem is to estimate the marginal class distributions over the B examples in the random mini-batch only:</p><formula xml:id="formula_8">H X [p(y | D)] ≈ H[ 1 B x∈X B p(y | x i , D)]. For H G [p(y | D)]</formula><p>we can, similarly, calculate an estimate using B samples only -instead of using M = N samples. We note that while this approximation is reasonable for the problems we consider (for which K &lt;= 10 and B = 100) it will be problematic for scenarios in which we expect a large number of categories. In such a setting one would have to estimate the marginal class distribution over multiple batches (or periodically evaluate it on a larger number of examples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">EXTENSION TO SEMI-SUPERVISED LEARNING</head><p>We will now consider adapting the formulation from Section 3.2 to the semi-supervised setting. Let X L = {(x 1 , y 1 ), (x L , y L )} be a set of L labeled examples, with label vectors y i ∈ R K in one-hot encoding, that are provided in addition to the N unlabeled examples contained in X .</p><p>These additional examples can be incorporated into the objectives from Equation <ref type="formula" target="#formula_7">(7)</ref> by calculating a cross-entropy term between the predicted conditional distribution p(y | x, D) and the true label distribution of examples from X L (instead of the entropy term H used for unlabeled examples). The cross-entropy for a labeled data pair (x, y) is given as</p><formula xml:id="formula_9">CE y, p(y | x, D) = − K i=1 y i log p(y = y i | x, D).<label>(8)</label></formula><p>The semi-supervised CatGAN problem is then given through the two objectives L L D (for the discriminator) and L L G (for the generator) with</p><formula xml:id="formula_10">L L D = max D H X p(y | D) − E x∼X H p(y | x, D) + E z∼P (z) H p(y | G(z), D) +λE (x,y)∼X L CE y, p(y | x, D) ,<label>(9)</label></formula><p>where λ is a cost weighting term and where L L G is the same as in Equation <ref type="formula" target="#formula_7">(7)</ref>:</p><formula xml:id="formula_11">L L G = L G .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">IMPLEMENTATION DETAILS</head><p>In our experiments both the generator and the discriminator are always parameterized through neural networks. The details of architectural choices for each considered benchmark are given in the appendix, while we only cover major design choices in this section.</p><p>GANs are known to be hard to train due to several unfortunate circumstances. First, the formulation from Equation <ref type="formula" target="#formula_1">(1)</ref> can become unstable if the discriminator learns too quickly (in which case the loss for the generator saturates). Second, the generator might get stuck generating one mode of the data or it may start wildly switching between generating different modes during training.</p><p>We therefore take two measures to stabilize training. First, we use batch normalization <ref type="bibr" target="#b16">(Ioffe &amp; Szegedy, 2015)</ref> in all layers of the discriminator and all but the last layer (the layer producing generated examplesx) of the generator. This helps bound the activations in each layer and we empirically found it to prevent mode switching of the generator as well as to increase generalization capabilities of the discriminator in the few labels case. Additionally, we regularize the discriminator by applying noise to its hidden layers. While we did find dropout  to be effective for this purpose, we found Gaussian noise added to the batch normalized hidden activations to yield slightly better performance. We suspect that this is mainly due to the fact that dropout noise can severely affect mean and variance computation during batch-normalization -whereas Gaussian noise on the activations for which to compute these statistics is a natural assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMPIRICAL EVALUATION</head><p>The results of our empirical evaluation are given in Tables 1, 2 and 3. As can be seen, our method is competitive to the state of the art on almost all datasets. It is only slightly outperformed by the Ladder network utilizing denoising costs in each layer of the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CLUSTERING WITH CATGANS</head><p>Since categorization of unlabeled data is inherently linked to clustering we performed a first set of experiments on common synthetic datasets that are often used to evaluate clustering algorithms. We  <ref type="figure">Figure 2</ref>: Comparison between k-means (left), RIM (middle) and CatGAN (rightmost three) -with neural networks -on the "circles" dataset with K = 2. Blue and green denote class assignments to the two different classes. For CatGAN we visualize class assignments -both on the dataset and on a larger region of the input domain -and generated samples. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>PI-MNIST test error (%) with n labeled examples n = 100 n = 1000 All MTC <ref type="bibr" target="#b27">(Rifai et al., 2011)</ref> 12.03 100 0.81 PEA <ref type="bibr" target="#b0">(Bachman et al., 2014)</ref> 10.79 2.33 1.08 PEA+ <ref type="bibr" target="#b0">(Bachman et al., 2014)</ref> 5.21 2.67 -VAE+SVM <ref type="bibr" target="#b18">(Kingma et al., 2014)</ref> 11.82 (± 0.25) 4.24 (± 0.07) -SS-VAE <ref type="bibr" target="#b18">(Kingma et al., 2014)</ref> 3.33 (± 0.14) 2.4 (± 0.02) 0.96 Ladder Γ-model <ref type="bibr" target="#b26">(Rasmus et al., 2015)</ref>  compare the CatGAN algorithm with standard k-means clustering and RIM with neural networks as discriminative models, which amounts to removing the generator from the CatGAN model and adding 2 regularization (see Section B in the appendix for an explanation). We considered three standard synthetic datasets -with feature dimensionality two, thus x ∈ R 2 -for which we assumed the optimal number of clusters K do be known: the "two moons" dataset (which contains two clusters), the "circles" arrangement (again containing two clusters) and a simple dataset with three isotropic Gaussian blobs of data.</p><p>In <ref type="figure">Figure 2</ref> we show the results of that experiment for the "circles" dataset (plots for the other two experiments are relegated to Figures 4-6 in the appendix due to space constraints). In summary, the simple clustering assignment with three data blobs is solved by all algorithms. For the two more difficult examples both k-means and RIM fail to "correctly" identify the clusters: (1) k-means fails due to the euclidean distance measure it employs to evaluate distances between data points and cluster centers, (2) in RIM the objective function only specifies that the deep network has to separate the data into two equal classes, without any geometric constraints 4 . In the CatGAN model, on the other hand, the discriminator has to place its decision boundaries such that it can easily detect a non-optimal adversarial generator which seems to coincide with the correct cluster assignment. Additionally, the generator quickly learns to generate the datasets in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">UNSUPERVISED AND SEMI-SUPERVISED LEARNING OF IMAGE FEATURES</head><p>We next evaluate the capabilities of the CatGAN model on two image recognition datasets. We performed experiments using fully connected and convolutional networks on <ref type="bibr">MNIST (LeCun et al., 1989)</ref> and CIFAR-10 <ref type="bibr" target="#b20">(Krizhevsky &amp; Hinton, 2009)</ref>. We either used the full set of labeled examples or a reduced set of labeled examples and kept the remaining examples for semi-supervised or unsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>MNIST test error (%) with n labeled examples n = 100 All EmbedCNN <ref type="bibr" target="#b34">(Weston et al., 2012)</ref> 7.75 -SWWAE <ref type="bibr">(Zhao et al., 2015)</ref> 8.71 ±0.34 0.71 Small-CNN <ref type="bibr" target="#b26">(Rasmus et al., 2015)</ref> 6.43 (± 0.84) 0.36 Conv-Ladder Γ-model <ref type="bibr" target="#b26">(Rasmus et al., 2015)</ref>    We performed experiments using two setups: (1) using a subset of labeled examples we optimized the semi-supervised objective from Equation <ref type="formula" target="#formula_7">(7)</ref>, and (2) using no labeled examples we optimized the unsupervised objective from Equation <ref type="formula" target="#formula_10">(9)</ref> with K = 20 "pseudo" categories. In setup (2) learning was followed by a category matching step. In this second step we simply looked at 100 examples from a validation set (we always kept 10000 examples from the training set for validation) for which we assume the correct labeling to be known, and assigned each pseudo category y k to be indicative of one of the true classes c i ∈ {1 . . . 10}. Specifically we assign y k to the class i for which the count of examples that were classified as y k and belonged to c i was maximal. This setup hence bears some similarity to one-shot learning approaches from the literature (see e.g. <ref type="bibr">Fei-Fei et al. (2006)</ref> for an application to computer vision). Since no learning is involved in the actual matching step wesomewhat colloquially -refer to this setup as half-shot learning.</p><p>The results for the experiment on the permutation invariant MNIST (PI-MNIST) task are listed in <ref type="table">Table 1</ref>. The table also lists state-of-the-art results for this benchmark as well as two baselines: a version of our algorithm where the generator is removed -but all other pieces stay in place -which we call RIM + NN due to the relationship between our algorithm and RIM; and the discriminator stemming from a standard GAN paired with an SVM trained based on features from it 5 .</p><p>While both the RIM and GAN training objectives do produce features that are useful for classifying digits, their performance is far worse than the best published result for this setting. The semisupervised CatGAN, on the other hand, comes close to the best results, works remarkably well even with only 100 labeled examples, and is only outperformed by the Ladder network with a specially designed denoising objective in each layer. Perhaps more surprisingly the half-shot learning procedure described above results in a classifier that achieves 9.7% error without the need for any label information during training.</p><p>Finally, we performed experiments with convolutional discriminator networks and deconvolutional <ref type="bibr" target="#b36">(Zeiler et al., 2011)</ref> generator networks (using the same up-sampling procedure from <ref type="bibr" target="#b7">Dosovitskiy et al. (2015)</ref>) on MNIST and CIFAR-10. As before, details on the network architectures are given in the appendix. The results are given in <ref type="table" target="#tab_2">Table 2</ref> and 3 and are qualitatively similar to the PI-MNIST results; notably the unsupervised CatGAN again performs very well, achieving a classification error of 4.27. The discriminator trained with the semi-supervised CatGAN objective performed well on both tasks, matching the state of the art on CIFAR-10 with reduced labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EVALUATION OF THE GENERATIVE MODEL</head><p>Finally, we qualitatively evaluate the capabilities of the generative model. We trained an unsupervised CatGAN on MNIST, LFW and CIFAR-10 and plot samples generated by these models in <ref type="figure" target="#fig_1">Figure 3</ref>. As an additional quantitative evaluation we compared the unsupervised CatGAN model trained on MNIST with other generative models based on the log likelihood of generated samples (as measured through a Parzen-window estimator). The full results of this evaluation are given in <ref type="table" target="#tab_7">Table 6</ref> in the appendix. In brief: The CatGAN model performs comparable to the best existing algorithms, achieving a log-likelihood of 237 ± 6 on MNIST; in comparison, <ref type="bibr" target="#b10">Goodfellow et al. (2014)</ref> report 225 ± 2 for GANs. We note, however, that this does not necessarily mean that the CatGAN model is superior as comparing generative models with respect to log-likelihood measured by a Parzen-window estimate can be misleading (see <ref type="bibr" target="#b31">Theis et al. (2015)</ref> for a recent in-depth discussion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATION TO PRIOR WORK</head><p>As highlighted in the introduction our method is related to, and stands on the shoulders of, a large body of literature on unsupervised and semi-supervised category discovery with machine learning methods. While a comprehensive review of these methods is out of the scope for this paper we want to point out a few interesting connections.</p><p>First, as already discussed, the idea of minimizing entropy of a classifier on unlabeled data has been considered several times already in the literature <ref type="bibr">(Bridle et al., 1992;</ref><ref type="bibr" target="#b11">Grandvalet &amp; Bengio, 2005;</ref><ref type="bibr" target="#b19">Krause et al., 2010)</ref>, and our objective function falls back to the regularized information maximization from <ref type="bibr" target="#b19">Krause et al. (2010)</ref> when the generator is removed and the classifier is additionally 2 regularized 6 . Several researchers have recently also reported successes for unsupervised learning with pseudo-tasks, such as self-supervised labeling a set of unlabeled training examples <ref type="bibr" target="#b22">(Lee, 2013)</ref>, learning to recognize pseudo-classes obtained through data augmentation <ref type="bibr" target="#b8">(Dosovitskiy et al., 2014)</ref> and learning with pseudo-ensembles <ref type="bibr" target="#b0">(Bachman et al., 2014)</ref>, in which a set of models (with shared parameters) are trained such they agree on their predictions, as measured through e.g. cross-entropy. While on first glance these appear only weakly related, they are strongly connected to entropy minimization as, for example, concisely explained in <ref type="bibr" target="#b0">Bachman et al. (2014)</ref>.</p><p>From the generative modeling perspective, our model is a direct descendant of the generative adversarial networks framework <ref type="bibr" target="#b10">(Goodfellow et al., 2014)</ref>. Several extensions to this framework have been developed recently, including conditioning on a set of variables <ref type="bibr">(Gauthier, 2014;</ref><ref type="bibr" target="#b24">Mirza &amp; Osindero, 2014)</ref> and hierarchical generation using Laplacian pyramids <ref type="bibr" target="#b5">(Denton et al., 2015)</ref>. These are orthogonal to the methods developed in this paper and a combination of, for example, CatGANs with more advanced generator architectures is an interesting avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have presented categorical generative adversarial networks, a framework for robust unsupervised and semi-supervised learning. Our method combines neural network classifiers with an adversarial generative model that regularizes a discriminatively trained classifier. We found the proposed method to yield classification performance that is competitive with state-of-the-art results for semi-supervised learning for image classification and further confirmed that the generator, which is learned alongside the classifier, is capable of generating images of high visual fidelity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A ON THE RELATION BETWEEN CATGAN AND GAN</head><p>In this section we will make the relation between the CatGAN objective from Equation <ref type="formula" target="#formula_7">(7)</ref> in the main paper and the GAN objective given by Equation (1) more directl apparent. Starting from the CatGAN objective let us consider the case K = 1. In this case the conditional probabilities should model binary dependent variables (and are thus no longer multinomial). The "correct" choice for the discriminative model is a logistic classifier with output D(x) ∈ R with conditional probability p(y = 1 | x, D) given as <ref type="bibr">x)</ref> . Using this definition The discriminator loss L D from Equation <ref type="formula" target="#formula_7">(7)</ref> can be expanded to give</p><formula xml:id="formula_12">p(y = 1 | x, D) = e D(x) e D(x) +1 = 1 1+e −D(</formula><formula xml:id="formula_13">L 1 D = max D −E x∼X H p(y | x, D) + E z∼P (z) H p(y | G(z), D) = max D E x∼X p x log p x + (1 − p x ) log(1 − p x ) +E z∼P (z) − p G(z) log p G(z) − (1 − p G(z) ) log(1 − p G(z) ) ,<label>(10)</label></formula><p>where we introduced the short notation p x = p(y = 1 | x, D), p G(z) = p(y = 1 | G(z), D) and dropped the entropy term H X p(y | D) concerning the empirical class distribution as we only consider one class and hence the classes are equally distributed by definition. Equation <ref type="formula" target="#formula_1">(10)</ref> now is similar to the GAN objective but pushes the conditional probability for samples from X to 0 or 1 and the probability for generated samples towards 0.5. To obtain a classifier which predicts p(y = 1 | x, D) we can replace the entropy H p(y | x, D) with the cross-entropy CE 1, p(y | x, D) yielding</p><formula xml:id="formula_14">L 1 D = max D E x∼X log p(y = 1 | x, D) +E z∼P (z) − p G(z) log p G(z) − (1 − p G(z) ) log(1 − p G(z) ) ,<label>(11)</label></formula><p>which is equivalent to the discriminative part of the GAN formulation except for the fact that optimization of Equation <ref type="formula" target="#formula_1">(11)</ref> will result in examples from the generator being pushed towards the decision boundary of p(y = 1 | G(z), D) = 0.5 rather than p(y = 1 | G(z), D) = 0. An equivalent derivation can be made for the generator objective L G leading to a symmetric objective -just as in the GAN formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ON THE RELATION BETWEEN CATGAN AND RIM</head><p>In this section we re-derive CatGAN as an extension to the RIM framework from <ref type="bibr" target="#b19">Krause et al. (2010)</ref>. As in the main paper we will restrict ourselves to the unsupervised setting but an extension to the semi-supervised setting is straight-forward. The idea behind RIM is to train a discriminative classifier, which we will suggestively call D, from unlabeled data. The objective that is maximized for this purpose is the mutual information between the data distribution and the predicted class labels, which can be formalized as</p><formula xml:id="formula_15">L RIM = max D H X p(y | D) − E x∼X H p(y | x, D) − γR(D),<label>(12)</label></formula><p>where the entropy terms are defined as in the main paper and R(D) is a regularization term acting on the discriminative model. In <ref type="bibr" target="#b19">Krause et al. (2010)</ref> D was chosen as a logistic regression classifier and R(D) consisted of 2 regularization on the discriminator weights. If we instantiate D to be a neural network we obtain the baseline RIM + NN which we considered in our experiments.</p><p>To connect the RIM objective to the CatGAN formulation from Equation <ref type="formula" target="#formula_7">(7)</ref> we can set let R(D) = −E z∼P (z) H p(y | G(z), D) , that is we let R(D) measure the negative entropy of samples from the generator. With this setting we achieve equivalence between L RIM and L D . If we now also train the generator G alongside the discriminator D using the objective L G we arrive at the CatGAN formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ON DIFFERENT PRIORS FOR THE EMPIRICAL CLASS DISTRIBUTION</head><p>In the main paper we always assumed a uniform prior over classes, that is we enforced that the amount of examples per class in X is the same for all k: ∀k, k ∈ K : p(y = k |D) = p(y = k |D).</p><p>This was achieved by maximizing the entropy of the class distribution H X p(y | D) . If this prior assumption is not valid our method could be extended to different prior distributions P (y) similar to how RIM can be adapted (see Section 5.2 of <ref type="bibr" target="#b19">Krause et al. (2010)</ref>). This becomes easy to see ny noticing the relationship between the Entropy and the KL divergence:</p><formula xml:id="formula_16">H X p(y | D) = log(K) − KL(p(y | D) U )</formula><p>where U denotes the discrete uniform distribution. We can thus simply drop the constant term log(K) and use −KL(p(y | D) U) directly, allowing us to replace U with an arbitrary prior P (y) -as long as we can differentiate through the computation of the KL divergence (or estimate it via sampling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DETAILED EXPLANATION OF THE TRAINING PROCEDURE</head><p>As mentioned in the main Paper we perform training by alternating optimization steps on L D and L G . More specifically, we use batch size B = 100 in all experiments and approximate the expectations in Equation <ref type="formula" target="#formula_7">(7)</ref> and Equation <ref type="formula" target="#formula_10">(9)</ref> using 100 random examples from X , X L and the generator G(z) respectively. We then do one gradient ascent step on the objective for the discriminator followed by one gradient descent step on the objective for the generator. We also added noise to all layers as mentioned in the main paper. Since adding noise to the network can result in instabilities in the computation of the entropy terms from our objective (due to small values inside the logarithms which are multiplied with non-negative probabilities) we added noise only to the terms not appearing inside logarithms. That is we effectively replace H[p(y | x, D)] with the cross-entropy CE[p(y | x, D), p(y | x,D)], whereD is the network with added noise and additionally truncate probabilities to be bigger than 1e − 4. During our evaluation we experimented with Adam <ref type="bibr" target="#b17">(Kingma &amp; Ba, 2015)</ref> for adapting learning rates but settled for a hybrid between <ref type="bibr" target="#b29">(Schaul et al., 2013)</ref> and rmsprop <ref type="bibr" target="#b32">(Tieleman &amp; Hinton, 2012)</ref>, called SMORMS3 (Funk, 2015) which we found slightly easier to use as it only has one free parameter -a maximum learning rate -which we did always set to 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 DETAILS ON NETWORK ARCHITECTURES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.1 SYNTHETIC BENCHMARKS</head><p>For the synthetic benchmarks we used neural networks with three hidden layers, containing 100 leaky rectified linear units each (leak rate 0.1), both for the discriminator and the generator (where applicable). Batch normalization was used in all layers (with added Gaussian noise with standard deviation 0.05) and the dimensionality of the noise vectors z for the CatGAN model was chosen to be 10 for. Note that while such large networks are most certainly an "overkill" for the considered benchmarks, we did chose these settings to ensure that learning was easily possible. We also experimented with smaller networks but did not find them to result in better decision boundaries or more stable learning.  For the permutation invariant MNIST task we used fully connected generator and discriminator networks with leaky rectified linearities (and a leak rate of 0.1). For the discriminator we used the same architecture as in <ref type="bibr" target="#b26">Rasmus et al. (2015)</ref>, consisting of a network with 5 hidden layers (with sizes 1000, 500, 250, 250, 250 respectively). Batch normalization was applied to each of these layers and Gaussian noise was added to the batch normalized responses as well as the pixels of the input images (with a standard deviation of 0.3). The generator for this task consisted of a network with three hidden layers (with hidden sizes 500, 500, 1000) respectively. The output of this network was of size 784 = 28 × 28, producing pixel images, and used a sigmoid nonlinearity. The noise dimensionality for vectors z was chosen as Z = 128 and the cost weighting factor λ was simply set to λ = 1. Note that on MNIST the classifier quickly learns to classify the few labeled examples leading to a vanishing supervised cost term; in a sense the labeled examples serve more as a "class initialization" in these experiments. We note that we found many different architectures to work well for this benchmark and merely settled on the described settings to keep our results somewhat comparable to the results from <ref type="bibr" target="#b26">Rasmus et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.3 CNNS FOR MNIST AND CIFAR-10</head><p>Full details regarding the CNN architectures used both for the generator and the discriminator are given in <ref type="table" target="#tab_5">Table 4</ref> for MNIST and in <ref type="table" target="#tab_6">Table 5</ref> for CIFAR-10. They are similar to the models from <ref type="bibr" target="#b26">Rasmus et al. (2015)</ref> who, in turn, derived them from the best models found by <ref type="bibr" target="#b30">Springenberg et al. (2015)</ref>. In the Table ReLU denotes rectified linear units, lReLU denotes leaky rectified linear units (with leak rate 0.1), fc stands for a fully connected layer, conv for a convolutional layer and perforated up-sampling denotes the deconvolution approach derived in <ref type="bibr" target="#b7">Dosovitskiy et al. (2015)</ref> and <ref type="bibr" target="#b25">Osendorfer et al. (2014)</ref>. <ref type="table" target="#tab_7">Table 6</ref> shows the sample log-likelihood for samples from an unsupervised CatGAN model. The CatGAN model performs comparable to the best existing algorithms; except for GMMN + AE which does not generate images directly but generates hidden layer activations of an AE that then reconstructs an image. As noted in the main paper we however want to caution the reader comparing generative models with respect to log-likelihood as measured by a Parzen-window estimate can be misleading (see <ref type="bibr" target="#b31">Theis et al. (2015)</ref> for a recent in-depth discussion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 QUANTITATIVE EVALUATION OF THE GENERATIVE MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>Log-likelihood GMMN <ref type="bibr" target="#b23">(Li et al., 2015)</ref> 147 ± 2 GSN  214 ± 1 GAN <ref type="bibr" target="#b10">(Goodfellow et al., 2014)</ref> 225 ± 2 CatGAN 237 ± 6 GMMN + AE <ref type="bibr" target="#b23">(Li et al., 2015)</ref> 282 ± 2 In <ref type="figure">Figure 4</ref>, 4 and 6 we show the results of training k-means, RIM and CatGAN models on the three synthetic datasets from the main paper. Only the CatGAN model "correctly" clusters the data and, as an aside, also produces a generative model capable of generating data points that are almost indistinguishable from those present in the dataset. It should be mentioned that there exist clustering algorithms -such as <ref type="bibr">DBSCAN (Ester et al., 1996)</ref> or spectral clustering methods -which can correctly identify the clusters in the datasets by making additional assumptions on the data distribution.  <ref type="figure">Figure 4</ref>: Comparison between k-means, RIM and CatGAN -with neural networks -on the "blobs" dataset, with K = 3. In the decision boundary plots cyan denotes points whose class assignment is close to chance level (∀k : p(y = k, x, D) &lt; 0.55). Note that the class identity is not known a priori as all models are trained unsupervisedly (hence the different color/class assignments for different models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 ADDITIONAL VISUALIZATIONS OF SAMPLES FROM THE GENERATIVE MODEL</head><p>We depict additional samples from an unsupervised CatGAN model trained on MNIST and Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b14">(Huang et al., 2007)</ref> in <ref type="figure" target="#fig_5">Figures 7 and 8</ref>. The architecture for the MNIST model is the same as in the semi-supervised experiments and the architecture for LFW is the same as for the CIFAR-10 experiments.</p><p>Published as a conference paper at ICLR 2016 k-means RIM + NN data + class assignment CatGAN generated examples decision boundaries <ref type="figure">Figure 5</ref>: Comparison between k-means, RIM and CatGAN -with neural networks -on the "two moons" dataset, with K = 2. In the decision boundary plots cyan denotes points whose class assignment is close to chance level (∀k : p(y = k, x, D) &lt; 0.55). Note that the class identity is not known a priori as all models are trained unsupervisedly (hence the different color/class assignments for different models).    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Exemplary images produced by a generator trained using the semi-supervised CatGAN objective. We show samples for a generator trained on MNIST (left) CIFAR-10 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparison between k-means, RIM and CatGAN -with neural networks -on the "circles" dataset. This figure complements Figure 2 from the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Samples generated by the generator neural network G for a CatGAN model trained on the MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Samples generated by the generator neural network G for a CatGAN model trained on cropped images from the LFW dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Classification error, in percent, for different learning methods in combination with convolutional neural networks (CNNs) with a reduced number of labels.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">CIFAR-10 test error (%) with n labeled examples n = 4000 All</cell></row><row><cell>View-Invariant k-means Hui (2013)</cell><cell>27.4 (± 0.7)</cell><cell>18.1</cell></row><row><cell>Exemplar-CNN (Dosovitskiy et al., 2014)</cell><cell>23.4 (± 0.2)</cell><cell>15.7</cell></row><row><cell cols="2">Conv-Ladder Γ-model (Rasmus et al., 2015) 20.09 (± 0.46)</cell><cell>9.27</cell></row><row><cell>Conv-CatGAN (semi-supervised)</cell><cell>19.58 (± 0.58)</cell><cell>9.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Classification error for different methods on the CIFAR-10 dataset (without data augmen- tation) for the full dataset and a reduced set of 400 labeled examples per class.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>CoRR, abs/1506.02351, 2015. URL http://arxiv.org/abs/1506.02351.</figDesc><table><row><cell>Zhao, Junbo, Mathieu, Michael, Goroshin, Ross, and Lecun, Yann. Stacked what-where auto-</cell></row><row><cell>encoders.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The discriminator and generator CNNs used for MNIST.</figDesc><table><row><cell></cell><cell>Model</cell></row><row><cell>discriminator D</cell><cell>generator G</cell></row><row><cell cols="2">Input 28 × 28 Gray image Input z ∈ R 128</cell></row><row><cell>5 × 5 conv. 32 lReLU</cell><cell>8 × 8 × 96 fc lReLU</cell></row><row><cell>3 × 3 max-pool, stride 2</cell><cell>2 × 2 perforated up-sampling</cell></row><row><cell>3 × 3 conv. 64 lReLU</cell><cell>5 × 5 conv. 64 lReLU</cell></row><row><cell>3 × 3 conv. 64 lReLU</cell><cell></cell></row><row><cell>3 × 3 max-pool, stride 2</cell><cell>2 × 2 perforated up-sampling</cell></row><row><cell>3 × 3 conv. 128 lReLU</cell><cell>5 × 5 conv. 64 lReLU</cell></row><row><cell>1 × 1 conv. 10 lReLU</cell><cell>5 × 5 conv. 1 lReLU</cell></row><row><cell>128 fc lReLU</cell><cell></cell></row><row><cell>10-way softmax</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The discriminator and generator CNNs used for CIFAR-10.</figDesc><table><row><cell></cell><cell>Model</cell></row><row><cell>generator G</cell><cell>discriminator D</cell></row><row><cell cols="2">Input 32 × 32 RGB image Input z ∈ R 128</cell></row><row><cell>3 × 3 conv. 96 lReLU</cell><cell>8 × 8 × 192 fc lReLU</cell></row><row><cell>3 × 3 conv. 96 lReLU</cell><cell></cell></row><row><cell>3 × 3 conv. 96 lReLU</cell><cell></cell></row><row><cell>2 × 2 max-pool, stride 2</cell><cell>2 × 2 perforated up-sampling</cell></row><row><cell>3 × 3 conv. 192 lReLU</cell><cell>5 × 5 conv. 96 lReLU</cell></row><row><cell>3 × 3 conv. 192 lReLU</cell><cell>5 × 5 conv. 96 lReLU</cell></row><row><cell>3 × 3 conv. 192 lReLU</cell><cell></cell></row><row><cell>3 × 3 max-pool, stride 2</cell><cell>2 × 2 perforated up-sampling</cell></row><row><cell>3 × 3 conv. 192 lReLU</cell><cell>5 × 5 conv. 96 lReLU</cell></row><row><cell>1 × 1 conv. 192 lReLU</cell><cell></cell></row><row><cell>1 × 1 conv. 10 lReLU</cell><cell>5 × 5 conv. 1 lReLU</cell></row><row><cell>global average</cell><cell></cell></row><row><cell>10-way softmax</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison between different generative models on MNIST.E.2 ADDITIONAL PLOTS FOR EXPERIMENTS ON SYNTHETIC DATA</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We discuss the possibility of using different priors in our framework in the appendix of this paper.2  In preliminary experiments we noticed that the MNIST dataset can, for example, be nicely separated into ten classes by creating 2-3 classes for common noise patterns and collapsing together several "real" classes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Since we assume a uniform prior P (y) over classes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We tried to rectify this by adding regularization (we tried both 2 regularization and adding Gaussian noise) but that did not yield any improvement</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Specifically, we first train a generator-discriminator using the standard GAN objective and then extract the last layer features from the discriminator on the available labeled examples, and use them to train an SVM.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We note that we did not find 2 regularization to help in our experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Ester, Martin, Kriegel, Hans-Peter, Sander, Jrg, and Xu, Xiaowei. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proc. of 2nd International Conference on Knowledge Discovery and Data Mining (KDD), 1996. Fei-Fei, L., Fergus, R., and Perona. One-shot learning of object categories. IEEE Transactions on Pattern Analysis Machine Intelligence, 28:594-611, April 2006. Funk, Simon. SMORMS3 -blog entry: RMSprop loses to SMORMS3 -beware the epsilon! http://sifter.org/ simon/journal/20150420.html, 2015. Gauthier, Jon. Conditional generative adversarial networks for face generation. Class Project for Stanford CS231N, 2014.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The author would like to thank Alexey Dosovitskiy, Alec Radford, Manuel Watter, Joschka Boedecker and Martin Riedmiller for extremely helpful discussions on the contents of this manuscript. Further, huge thanks go to Alec Radford and the developers of Theano <ref type="bibr" target="#b3">(Bergstra et al., 2010;</ref><ref type="bibr" target="#b1">Bastien et al., 2012)</ref> and Lasagne <ref type="bibr" target="#b6">(Dieleman et al., 2015)</ref> for sharing research code. This work was funded by the the German Research Foundation (DFG) within the priority program "Autonomous learning" (SPP1597).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arnaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep generative stochastic networks trainable by backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Thibodeau-Laufer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frédéric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised classifiers, mutual information and phantom targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Bridle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heading</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soumith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lasagne: First release</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Schlter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eben</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.27878</idno>
		<ptr target="http://dx.doi.org/10.5281/zenodo.27878" />
		<imprint>
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-prediction deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 26</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sherjil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 17</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580v3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nitish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1207.0580v3" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Direct modeling of complex invariances for visual object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Y</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML). JMLR Workshop and Conference Proceedings</title>
		<meeting>the 30th International Conference on Machine Learning (ICML). JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML). JMLR Proceedings</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML). JMLR Proceedings</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semisupervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimenez</forename><surname>Shakir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative clustering by regularized information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">G</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 23</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets. CoRR, abs/1411.1784</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1411.1784" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image super-resolution with fast approximate convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Smagt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICONIP</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semisupervised learning with ladder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The manifold tangent classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">No More Pesky Learning Rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno>abs/1511.01844</idno>
		<ptr target="http://arxiv.org/abs/1511.01844" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML)</title>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<editor>Montavon, G., Orr, G., and Muller, K-R.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Maximum margin clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neufeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryce</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 17</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV, pp</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
