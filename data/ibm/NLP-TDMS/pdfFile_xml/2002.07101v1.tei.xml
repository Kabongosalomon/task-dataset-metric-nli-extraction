<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmented Normalizing Flows: Bridging the Gap Between Generative Flows and Latent Variable Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
						</author>
						<title level="a" type="main">Augmented Normalizing Flows: Bridging the Gap Between Generative Flows and Latent Variable Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose a new family of generative flows on an augmented data space, with an aim to improve expressivity without drastically increasing the computational cost of sampling and evaluation of a lower bound on the likelihood. Theoretically, we prove the proposed flow can approximate a Hamiltonian ODE as a universal transport map. Empirically, we demonstrate stateof-the-art performance on standard benchmarks of flow-based generative modeling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep invertible models have recently gained increasing interest among machine learning researchers as they constitute a powerful probabilistic toolkit. They allow for the tracking of changes in probability density and have been widely applied in many tasks, including (i) generative models <ref type="bibr" target="#b35">Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b8">Chen et al., 2019)</ref>, (ii) variational inference <ref type="bibr">(Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b37">Kingma et al., 2016;</ref><ref type="bibr" target="#b5">Berg et al., 2018)</ref>, (iii) density estimation <ref type="bibr">(Papamakarios et al., 2017;</ref><ref type="bibr" target="#b32">Huang et al., 2018)</ref>, (iv) reinforcement learning <ref type="bibr">(Mazoure et al., 2019;</ref><ref type="bibr">Ward et al., 2019)</ref>, etc.</p><p>The main challenges in designing an invertible model for the above use cases are to ensure (1) the mapping f is invertible, (2) the log-determinant of the Jacobian of f is cheap to compute, and (3) f is expressive. For use case (i), ideally we would also like to (4) invert f efficiently.</p><p>In general, it is hard to design a family of functions that satisfy all of the above. Most work within this line of research <ref type="bibr">Preliminary work.</ref> is dedicated to improving the expressivity of the bijective mapping while maintaining the computational tractability of the log-determinant of Jacobian <ref type="bibr" target="#b37">Kingma et al., 2016;</ref><ref type="bibr" target="#b32">Huang et al., 2018;</ref><ref type="bibr" target="#b8">Chen et al., 2019)</ref>.</p><p>Aside from the unfortunate trade-off between the computational budget of inversion/Jacobian log-determinant and the expressivity of the invertible mapping, generative flows suffer from the limitation of local dependency. Unlike latent variable models such as Variational Autoencoders (VAEs; <ref type="bibr" target="#b36">Kingma &amp; Welling 2014;</ref><ref type="bibr">Rezende et al. 2014</ref>) and Generative Adversarial Networks (GANs; <ref type="bibr" target="#b23">Goodfellow et al. 2014)</ref> which model the high dimensional data as coordinates in another space, most generative flows model the dependency among features only locally. Dependencies of features far away from each other can only be propagated through composition of mappings, which progressively enlarges the receptive field. Special design of parameterization like the attention mechanism can be made to address this issue <ref type="bibr" target="#b28">(Ho et al., 2019)</ref>.</p><p>In this paper, we propose to construct an invertible model on an augmented input space, which when combined with the block-wise coupling of <ref type="bibr" target="#b17">Dinh et al. (2017)</ref> satisfies all criteria <ref type="bibr">(1)</ref><ref type="bibr">(2)</ref><ref type="bibr">(3)</ref><ref type="bibr">(4)</ref>. The motivation is that to transform some distribution (such as the marginal distribution of x pictured in <ref type="figure">Figure 1</ref>) into another (e.g. standard normal) in the original input space, f needs to be capable of transporting the probability mass "non-uniformly" across its domain, whereas in an augmented input space it is possible to find a smoother transformation. For instance, if we couple the data x with an independent noise e, we can first transform e conditioned on x into z, so that conditioned on different values of z, x can be more easily centered and Gaussianized. Our proposed method also generalizes multiple variants of VAEs and possesses the advantage of transforming the data in a more globally coherent manner via first embedding the data in the augmented state space. Finally, operating on an augmented state space allows us to sidestep the topology preserving property of a diffeomorphism, which means input space can potentially be more freely deformed <ref type="bibr" target="#b18">(Dupont et al., 2019)</ref>.</p><p>Our contributions: we introduce Augmented Normalizing Flows (ANFs), an invertible generative model on the arXiv:2002.07101v1 <ref type="bibr">[cs.</ref>LG] 17 Feb 2020 <ref type="figure">Figure 1</ref>. Transforming data x (left) via augmented normalizing flows: Black dots and blue dots are marginal and joint data points, respectively. First step: augment the data x with an independent noise e. Second step: transform the augmented data e conditioned on x into z. Third step: transform the original data x conditioned on z into y, resulting in a Gaussianized joint distribution of (y, z)</p><p>real-valued data x coupled with an independent noise e. We propose a parameter estimation principle called Augmented Maximum Likelihood Estimation (AMLE), which we show amounts to maximizing a lower bound on the marginal likelihood of the original data x. Theoretically, we show that the family of ANFs with additive coupling can universally transform arbitrary data distribution into a standard Gaussian prior, augmented with a degenerate deterministic variable. To the best of our knowledge, this is the first attempt in understanding how expressivity can be improved via composing flow layers rather than widening the flow <ref type="bibr" target="#b32">(Huang et al., 2018)</ref>. Experimentally, we apply the proposed method to a suite of standard generative modelling tasks and demonstrate state-of-the-art performance in density estimation and image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Given a training set (x i ) n i=1 ∼ q(x) n , where x i ∈ X , and a family of density models {p π (x) : π ∈ P(X )}, where P(X ) is a collection of sets of parameters that can sufficiently describe the density function, the Maximum Likelihood Principle estimates the parameters by maximizing the chance of the data being generated by the assumed model:</p><formula xml:id="formula_0">π := arg max π∈P(X ) n i=1 log p π (x i ) = arg max π∈P(X ) E x [log p π (x)]<label>(1)</label></formula><p>where the latter expectation is over the empirical distributionq(x) (x i with uniformly distributed random index i ∈ {1, · · · , n}).π is known as the maximum likelihood estimate (MLE) for the parameter π. Below, we review two families of likelihood-based density models.</p><p>1. Invertible Generative Models Assume y ∼ N (0, I). Assume the data is generated via a bijective mapping x = f θ (y). Then the probability density function of f θ (y) evaluated at x can be written as</p><formula xml:id="formula_1">p θ (x) = N (f −1 θ (x); 0, I) det ∂f −1 θ (x) ∂x<label>(2)</label></formula><p>Equivalently, one can parameterize the inverse transformation x → g θ (x) with invertible mapping g θ , and define the generative transformation as f θ = g −1 θ . Much of the design effort has been dedicated to ensuring (1) the invertibility of the transformation g, and (2) efficiency in computing the log-determinant of the Jacobian in Equation 2. For example, <ref type="bibr" target="#b17">Dinh et al. (2017)</ref> propose the affine coupling:</p><formula xml:id="formula_2">g θ (x a , x b ) = concat(x a , s θ (x a ) x b + m θ (x a ))</formula><p>where s θ and m θ are parameterized by neural networks and x a and x b are two partitioning of the data vector, and compose multiple layers of transformations intertwined with permutation of elements of x.</p><p>Invertible models allow for exact computation of the likelihood, and can be composed to increase modelling capacity. Nevertheless, the expressivity of the transformation is limited due to the need to satisfy invertibility and to reduce the cost of computing the Jacobian determinant. For a comprehensive review of this topic, see <ref type="bibr" target="#b38">Kobyzev et al. (2019)</ref> and <ref type="bibr">Papamakarios et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Variational Autoencoders</head><p>Assume the data follows the generating process:</p><formula xml:id="formula_3">x ∼ p θ (x|z) where z ∼ p θ (z).</formula><p>For simplicity, we assume p θ (z) is the standard Gaussian distribution and drop the dependency on θ henceforward. Our goal is to find the MLE for θ, but the log marginal density log p θ (x) = log z p θ (x|z)p(z)dz is generally not tractable since it involves integration. Instead, one can maximize a surrogate objective known as the evidence lower bound (ELBO): where q φ (z|x) is an inference network that amortizes the cost of parameterizing the variational distribution per input instance x via conditioning. Learning and inference can be jointly achieved by drawing a stochastic estimate of the gradient of the ELBO via reparameterization (i.e. change of variable):</p><formula xml:id="formula_4">L(θ, φ; x) = E q φ (z|x) [log p θ (x|z)] − D KL (q φ (z|x)||p(z))<label>(3)</label></formula><formula xml:id="formula_5">L(θ, φ; x) = E e∼q(e) [log p θ (x|g φ (x, e)) + (4) log N (g φ (x, e); 0, I) − log q φ (g φ (x, e)|x)]</formula><p>if g φ (x, e) with e ∼ q(e) follows the same density as q φ (z|x). Conventionally, q φ (z|x) is a multivariate Gaussian distribution with diagonal covariance. We write it as N (z; µ φ (x), σ 2 φ (x)). One choice of reparameterization is g φ (x, e) = µ φ (x) + σ φ (x) e with q(e) = N (0, I).</p><p>VAEs allow one to embed the data in another space (usually of lower dimensionality), and to generate via an arbitrarily parameterized mapping. However, the log likelihood of the data is no longer tractable, so we can only maximize an approximate log likelihood. The performance of the model highly depends on the choice of the encoding distribution and the decoding distribution, as they are closely related to the tightness of the lower bound <ref type="bibr" target="#b13">(Cremer et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Augmented Maximum Likelihood</head><p>For augmented maximum likelihood, we couple each data point with an independent random variable e ∈ E drawn from q(e) (in all our experiments we set q(e) = N (0, I)), and consider a family of joint density models {p π (x, e) : π ∈ P(X × E)}. Instead of maximizing the marginal likelihood of x i 's, we maximize the joint likelihood:</p><formula xml:id="formula_6">π A := arg max π∈P(X ×E) E x,e [log p π (x, e)]<label>(5)</label></formula><p>where the expectation is over (x, e) ∼q(x)q(e). We refer to this extremum estimator as the Augmented Maximum Likelihood Estimator (AMLE). The benefit of maximizing the joint likelihood is that it allows us to make use of the augmented state space to induce structure on the marginal distribution of x in the original input space.</p><p>Lower bounding the log marginal likelihood Since the entropy of e is constant wrt the model parameter π,π A is equal to the maximizer of L A (π; x) := E e [log p π (x, e)] + H(e) averaged over all x i 's. For any x ∈ X , the quantity log p π (x) − L A (π; x) can be written as the KL divergence:</p><formula xml:id="formula_7">log p π (x) − L A (π; x) = log p π (x) − E e [ log p π (x) + log p π (e|x)] − H(e) = D KL (q(e)||p π (e|x))</formula><p>Since KL is non-negative, maximizing the joint likelihood according to Equation 5 is equivalent to maximizing a lower bound on the log marginal likelihood of x. We refer to this as the Augmentation Gap, as it reflects the incapability of the joint density to model the marginal of e independently of x.</p><p>Estimating the log marginal likelihood The log marginal likelihood log p π (x) of the data can be estimated in a way similar to <ref type="bibr" target="#b6">Burda et al. (2015)</ref>, by drawing K i.i.d. samples of e j ∼ q(e) per x to estimate the following stochastic lower bound:L A,K (π) := log</p><formula xml:id="formula_8">1 K K j=1 p π (x, e j ) q(e j )</formula><p>which can be shown to be a consistent estimator for log p π (x) and is monotonically tighter in expectation as we increase K. . Left: marginal distribution in the X -space. Right: joint distribution in the X × E-space. The first row is the inference path, where the joint data density q(x)q(e) is mapped by an encoding transform (transforming e into z conditioned on x) followed by a decoding transform (transforming x into y conditioned on z). The second row is the generation path, where the joint prior density p(y)p(z) is transformed by the inverse decoding (transforming y into x) followed by the inverse encoding (transforming z into e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Augmented Normalizing Flows (ANF)</head><p>We now demonstrate how to leverage the augmented input space to model the complex marginal distribution of the data. We consider maximizing the joint likelihood of x coupled with a random noise e ∼ q(e). Let (y, z) ∼ p(y, z) be drawn from some simple distribution, such as independent Gaussian. Assume the data x, e is deterministically generated via an invertible mapping x, e = F π (y, z), with inverse G π = F −1 π . Then analogous to Equation 2, x, e has a joint density p π (x, e) = N (G π (x, e); 0, I) det ∂G π (x, e) ∂(x, e)</p><p>For simplicity, we can choose q(e) to be the standard normal distribution. What we are left with is the choice of an invertible G π that can harness the augmented state space E to induce a complex marginal on X . Inspired by the affine coupling proposed by <ref type="bibr" target="#b17">Dinh et al. (2017)</ref>, we conditionally transform x and e, hoping the structure in the marginal of x can "leak" into E and make the joint more easily Gaussianized. Concretely, we define two types of affine coupling g enc π (x, e) = concat(x, s enc π (x) e + m enc π (x)), g dec π (x, e) = concat(s dec π (e) x + m dec π (e), e)</p><p>We refer to the pair of encoding transform and decoding transform as the autoencoding transform. We stack them up in alternating order, i.e. G π = g dec π N • g enc π N • ... • g dec π1 • g enc π1 for N ≥ 1 steps, where π = {π 1 , ..., π N } is the set of all parameters. See <ref type="figure" target="#fig_0">Figure 2</ref>-(a,b) for an illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">VAE as ANF</head><p>Variational Autoencoders are a special case of augmented normalizing flows with only "one step" of encoding and decoding transform <ref type="bibr" target="#b16">(Dinh et al., 2014)</ref>. To see this, assume the decoding distribution p θ (x|z) is a factorized Gaussian with mean µ θ (z) and standard deviation σ θ (z). By letting z = µ φ (x) + σ φ (x) · e and y = (x − µ θ (z))/σ θ (z) and applying the change of variable formula to both q φ (z|x) and p θ (x|z), we get from Equation <ref type="formula">(</ref> Averaging over the data distributionq(x), we obtain the expected joint likelihood (up to the constant H(e)) E x,e∼q(x)q(e) log N ((y, z); 0, I) det</p><formula xml:id="formula_9">∂(y, z) ∂(x, e)</formula><p>The variational gap between the log marginal likelihood and the evidence lower bound is equal to the augmentation gap since the KL divergence is invariant under the transformation between e ←→ z:</p><formula xml:id="formula_10">D KL (q(z|x)||p(z|x)) = D KL (q(e)||p(e|x))</formula><p>This gives us an alternative interpretation of inference suboptimality <ref type="bibr" target="#b13">(Cremer et al., 2018)</ref>: the inaccuracy of inferring the true posterior p(z|x) can be attributed to the incapability of the joint density to model the augmented data q(e). . 5-step ANF on 1D MoG. In the inference path (top row), we start with an encoding transform that maps e to z1 conditioned on x, followed by a decoding transform that maps x into y1 conditioned on z1. We reuse the same encoder and decoder to refine the joint variable repeatedly to obtain y5 and z5. In the generative path (bottom row), we reverse the process, starting with the inverse transform of the decoding, followed by the inverse transform of the encoding, etc. To illustrate this phenomenon, we model the density of a one dimensional mixture of Gaussian (1D MoG). In <ref type="figure">Figure</ref> 3 (left), we plotted the density histograms of the MoG distribution (blue) and a one-step ANF, i.e. VAE with Gaussian encoder and decoder (orange), trained on the MoG samples. Not surprisingly, the latter fails to represent two well separated modes of probability mass. In <ref type="figure" target="#fig_1">Figure 3</ref> (right), we visualize the joint density of the augmented data x, e ∼ q(x)q(e) throughout the transformation. We see that the transformed data y, z = g dec π1 (g enc π1 (x, e)) is not perfectly Gaussianized. In fact, if we project it horizontally we can see that the "aggregated posterior" (marginal of z) does not match the prior distribution p(z). As a result, the pushforward x, e = g enc,−1 π1 (g dec,−1 π1 (y, z)) of y, z ∼ p(y, z) does not follow the augmented data distribution q(x)q(e) well. When we fix different values of x, we have different slices of density functions for e, indicating that e and x are dependent and that p π (e|x) deviates from q(e).</p><p>We carry out the same experiment on 1D MoG with mul-tiple flow layers, which generalizes a VAE with Gaussian encoder and decoder. We set the number of flow layers (i.e. steps) to be 5. To furthermore demonstrate the benefit of transformation composition, we also tie the parameters of each encoder and decoder step, separately. That is, the same set of parameters are used at different steps of encoding and decoding to make sure capacity stays constant. Since the conditional independence assumption in VAE is relaxed, the augmented data is more successfully Gaussianized, as can be seen in <ref type="figure" target="#fig_3">Figure 4</ref>. The generated samples also follow the target joint density more closely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hierarchical Augmented Normalizing Flows</head><p>The information flow of the encoding-decoding transform just described is limited to the size of the random vector e, which makes it hard to optimize for more realistic settings such as natural images. We thus propose a second architecture by following the hierarchical variational autoencoder, which is defined by two pairs of joint distributions 1</p><formula xml:id="formula_11">p(x, z 1 , ..., z L ) = p(x|z 1 , ..., z L ) L l=1 p(z l |z l+1 , ..., z L ) q(z 1 , ..., z L |x) = L l=1 q(z l |z 1 , ..., z l−1 , x).</formula><p>When all the conditionals are Gaussian distributions, the corresponding ELBO can be similarly rearranged to be the loss function of an ANF (see <ref type="figure" target="#fig_0">Figure 2</ref>-(c)). The encoding transform for each e l is conditioned on the "transformed" preceding variables s e π,l (x, z &lt;l ) e l + m e π,l (x, z &lt;l ) due to the conditioning in q(z l |z &lt;l , x). The decoding transform on the other hand is conditioned on the "original" preceding variables s d π,l (e &gt;l ) e l + m d π,l (e &gt;l ), which is block-wise inverse autoregressive <ref type="bibr" target="#b37">(Kingma et al., 2016)</ref>. When the conditioning mappings are convolutional, the lower level transformation preserves information of the input locally, which is then combined with the deterministic path of the decoding that "sees" more of the input. More details on the architecture are described in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ANFs as Approximate Hamiltonian ODE and Universality</head><p>The affine-coupling autoencoding transform with augmented variable is reminiscent of the leap-frog integration of the Hamiltonian system <ref type="bibr">(Neal et al., 2011)</ref>. More recently, it has been shown by Taghvaei &amp; Mehta (2019) that solving a family of Hamiltonian ordinary differential equations (ODE) with an infinite time horizon gives us a transport map from the initial (data) distribution to an arbitrary target distribution with a log-concave density function. This suggests we can develop an approximation theorem by using ANFs to approximately, numerically solve the ODE.</p><p>Formally, we define scaling coefficients α t = log 2 t and β t = γ t = log t 2 . Let p(x) be the standard normal density, and q(x) be the data distribution. Let q 0 = q and Φ : X → R be some convex function. Define the Hamiltonian ODE:</p><formula xml:id="formula_12">x t = e αt−γt e t , x 0 ∼ q 0 e t = −e αt+βt+γt ∇ log q t (x t ) p(x t ) , e 0 = ∇Φ(x 0 )</formula><p>whereẋ t andė t are the time derivatives of x and e at time t, and q t is the marginal density of x t .</p><p>Second, we construct a sequence of encoding and decoding functions m enc n and m dec n parameterized by neural networks, and define the following (additive) invertible mappings</p><formula xml:id="formula_13">e π 1 = e π 0 + m enc 1 (x π 0 ) x π n+1 = x π n + 2 · m dec n+1 (e π n+1 ) ∀ n ≥ 0 (7) e π n+1 = e π n + 2 · m enc n+1 (x π n ) ∀ n ≥ 1 (8)</formula><p>with e π 0 = 0 and x π 0 ∼ q 0 . The step size parameter will be chosen to depend on the depth coefficient N , i.e. the number of steps of the joint transformation.</p><p>Assume our target distribution lies within a family of distributions Q satisfying Assumption 1 in the Appendix F (some smoothness condition on the time derivatives and Φ). We can then set the encoding and decoding functions to be arbitrarily close to the time derivatives by the universal approximation of neural networks <ref type="bibr" target="#b14">(Cybenko, 1989)</ref>, and by taking the depth N to be arbitrarily large, we can approximate the transport map induced by the Hamiltonian ODE arbitrarily well, which gives rise to the following universal approximation theorem (the proof is relegated to the Appendix F): Theorem 1. For any q ∈ Q, we can find a sequence (x π N , e π N ) of ANFs of the additive form <ref type="bibr">(7,</ref><ref type="bibr">8)</ref></p><formula xml:id="formula_14">, such that if x π 0 , e π 0 ∼ q(x)δ 0 (e) and x ∞ , e ∞ ∼ p(x)δ 0 (e), then (x π N , e π N ) → (x ∞ , e ∞ ) in distribution.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>In the literature of normalizing flows, much work has been done to improve expressivity while maintaining computational tractability. For example, <ref type="bibr" target="#b16">Dinh et al. (2014;</ref> introduce an affine coupling that partitions the features into two groups so that the Jacobian is a block-triangular matrix. The resulting mapping is relatively restricted since it only models partial dependency. <ref type="bibr" target="#b37">Kingma et al. (2016)</ref> further exploits the ordered dependency by constructing an  inverse autoregressive mapping but its inversion requires a computation time linear in dimensionality (Papamakarios et al., 2017), and does not even have a closed-form formula in the more general non-affine setting <ref type="bibr" target="#b32">(Huang et al., 2018)</ref>. <ref type="bibr" target="#b3">Behrmann et al. (2018)</ref> propose a residual form of f whose Jacobian log-determinant can be stochastically estimated  but inversion is achieved iteratively, not in one pass.</p><p>Normalizing flows have also been used as (1) an inference machine in the context of variational inference for continuous latent variable models <ref type="bibr" target="#b37">(Kingma et al., 2016;</ref><ref type="bibr">Tomczak &amp; Welling, 2016;</ref><ref type="bibr" target="#b5">Berg et al., 2018)</ref>, and (2) a trainable component of the latent variable model <ref type="bibr" target="#b9">(Chen et al., 2017;</ref><ref type="bibr" target="#b1">Agrawal &amp; Dukkipati, 2016;</ref><ref type="bibr" target="#b31">Huang et al., 2017)</ref>. ANFs lie at the intersection of normalizing flows and latent variable models when a specific type of block-conditioning transformation is applied, and allow us to unifyingly view flow-based priors as marginal transformation in the space of e, and amortized flows for improving posterior inference as different variants of the encoding transform. Another way of improving the inference machine's expressivity is to consider a hierarchical model; in fact, ANFs can be viewed as a generalization of the auxiliary variable method for hierarchical variational inference <ref type="bibr" target="#b0">(Agakov &amp; Barber, 2004;</ref><ref type="bibr">Ranganath et al., 2016)</ref>; see Appendix D for the connection and C for more discussion on future direction.</p><p>Finally, <ref type="bibr" target="#b18">Dupont et al. (2019)</ref> also employs augmentation to improve the expressivity and stability of a neural ODE <ref type="bibr" target="#b7">(Chen et al., 2018a)</ref>, and they believe such a method can be used to reduce the cost of training a continuous normalizing flow <ref type="bibr" target="#b24">(Grathwohl et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Large-Scale Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Quantitative results</head><p>In the more realistic settings, we augment the data with a hierarchy of noise, as described in the last part of Section 4. See Appendix B for more experimental details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic vs. deterministic features</head><p>We conduct an ablation study on the effect of composing multiple encodingdecoding transformations (N steps) versus increasing the number of stochastic layers (L layers). We monitor the bits per dim (BPD) of the test set of CIFAR 10 <ref type="bibr" target="#b39">(Krizhevsky et al., 2009)</ref> throughout training. <ref type="figure" target="#fig_4">Figure 5</ref> shows that increasing the number of flow layers can more effectively improve the likelihood of the model than increasing the number of stochastic layers.  <ref type="bibr" target="#b43">(Liu et al., 2015)</ref>, and compare with other state-of-the-art density models. In <ref type="table" target="#tab_0">Table 1</ref>, we see that ANFs set a few new records in terms of BPD on the standard benchmarks in the non-autoregressive category. We use the importance sampling described in Section 3 to estimate the log likelihood. The augmentation gap is around 0.01 BPD for all benchmarks, indicating the augmented flow is capable of achieving good likelihood estimate and high inference precision at the same time.  <ref type="table" target="#tab_1">Table 2</ref>, we see that ANF obtains better scores than all the other explicit density models, and is close to matching the FID of the orignal WGAN-GP by <ref type="bibr" target="#b25">Gulrajani et al. (2017a)</ref>. The generated samples are presented in <ref type="figure" target="#fig_5">Figure 6</ref> and Appendix E. Since the encoding-decoding transformation has a receptive field that is wide enough to cover the entire raw data, the generated samples also look more globally coherent.</p><p>Lossy reconstruction As a hierarchical model, ANF can be used to perform inference for the higher level representation, and sample the lower level details for reconstruction. We do this by sampling e 1 , ..., e L , obtaining the corresponding y, z 1 , ..., z L ← G π (x, e 1 , ..., e L ), randomizing all but the last representations y , z 1 , ..., z L−1 ∼ N (0, I), and reconstructing from the new joint representation x , e 1 , ..., e L ← G −1 π (y , z 1 , ..., z L−1 , z L ). Similar to other hierarchical models <ref type="bibr" target="#b26">(Gulrajani et al., 2017b;</ref><ref type="bibr" target="#b4">Belghazi et al., 2018)</ref>, ANF is also capable of retaining global, semantic information of the raw data stored in its higher level code; this is shown in <ref type="figure" target="#fig_6">Figure 7</ref>.</p><p>Interpolation We also perform interpolation in the latent space between real images. Previous works such as <ref type="bibr" target="#b35">Kingma &amp; Dhariwal (2018)</ref> perform linear interpolation of the form h(u, v, t) = tu + (1 − t)v for t ∈ [0, 1], which we observe has a non-smooth transition (e.g. sudden color change). We hypothesize this is due to the fact that convex combination of two vectors would result in an increase and then a decrease in the density of the standard Gaussian prior. This is undesirable since the interpolated points are atypical because Gaussian samples are known to concentrate around the shell (of radius proportional to square root dimensionality). Hence, we propose the rescaled interpolation</p><formula xml:id="formula_15">h (u, v, t) = h(||u||, ||v||, t) ||h(u, v, t)|| · h(u, v, t)<label>(9)</label></formula><p>where || · || denotes the L2 norm, to make sure the scale of the resulting point is a linear interpolation of the scales of the two input vectors. The result in <ref type="figure" target="#fig_7">Figure 8</ref> shows that the transition is extremely smooth (see Appendix A for a side-by-side comparison with linear interpolation) and the intermediate images are realistic looking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this work, we propose the Augmented Normalizing Flows and a corresponding variational lower bound on the marginal likelihood. We show that the proposed method can be used to approximate a Hamiltonian dynamical system as a universal transport map and achieves competitive or better results than state-of-the-art flow-based methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Interpolation</head><p>We compare linear interpolation with rescaled interpolation (rescaling is done separately for each stochastic layer). We see that the middle points of linear interpolation tend to be more yellowish, and rescaled interpolation results in a smoother and direct transition between two input vectors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Details</head><p>To model natural images, we employ a more intricate architecture with a higher modeling capacity described in B.1. Section B.3 describes the parameter initialization scheme and parameterization constraints that are imposed to stablize training. In B.4, we propose an objective-annealing technique that biases the autoencoding transform to focus on Gaussianizing the raw data more at the early stage of training. We found this to be helpful for optimization. All the hyperparameters used in the experiments are summarized in B.5. We parameterize all of the feedforward layers with weight normalization (Salimans &amp; <ref type="bibr" target="#b37">Kingma, 2016)</ref>. For the encoding transforms, we first map x using a convolutional layer composed with an activation function, followed by a pooling layer to obtain the first set of conditional features h. We now use these features to conditionally transform each of the stochastic units in the order e 1 , ..., e L , using an encoding block. The encoding block outputs a set of modified conditional features, which is then used to modify the next e l (except when the feature map size is halved, in which case an additional pair of convolutional layer and pooling layer is first applied).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Residual autoencoding blocks</head><p>Encode transform and decode transform Each encoding block has a nested residual structure, taking in the conditional features as input to transform the corresponding stochastic unit e, illustrated in <ref type="figure" target="#fig_9">Figure 10</ref>. The conditional features are first convolved and then fed into a nested Real NVP block. The output of the Real NVP block (applied twice, see below) is then convolved and added to the convolved conditional features. We apply non-linearity before convolving again and adding to the the original conditional features via skip connection.</p><p>The decode transform is similar, except we convolve the incoming stochastic unit to modify the conditional features, thus having a shorter computational path to reconstruct the data.</p><p>Real NVP block The stochastic unit is split into two halves (e 1 and e 2 ) using the checkerboard mask. The convolved conditional feature is concatenated with the masked stochastic unit (the part that is not masked is denoted as e 1 ) to transform the part of the stochastic unit being masked out (e 2 ). The same Real NVP block is reused (using the same set of parameters), alternating the pattern of the mask to transform the other half of the stochastic unit (with e 1 and e 2 swapped). We found sharing the parameters of these two consecutive Real NVP transformations to improve the convergence of the likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Variational dequantization</head><p>We use the variational dequantization proposed by <ref type="bibr" target="#b28">Ho et al. (2019)</ref> in all our density estimation experiments. We first map the input image x into a deterministic feature x space using a convolutional network of the following form:</p><p>conv(stride=2) -&gt; act -&gt; conv -&gt; act -&gt; bilinear_upsample <ref type="formula" target="#formula_1">(stride=2)</ref> where act denotes activation function. Note that the input to this convolution network is rescaled to [0, 1] via x/(2 n_bits −1), where n_bits is the number of bits. We then use the Real NVP block to transform a standard Gaussian noise, where x acts as the conditional feature. We apply two Real NVP blocks to obtain u_logit, and transform it into u using the logistic sigmoid activation function so that each element of u lies within (0, 1). We then perturb the data by (x + u)/2 n_bits , which is then passed through a clip operator for numerical stability. We define clip to be</p><formula xml:id="formula_16">clip(x, δ) = x · (1 − δ) + 0.5 · δ</formula><p>where δ = 0.1. Finally, we pass the clipped value into the logit function (inverse sigmoid) to obtain the dequantized data. We have taken into account the probability density of u_logit and all the changes of variable (i.e. sigmoid, rescaling, clipping, and logit) when computing the lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Initialization and parameterization constraint</head><p>Unless it is otherwise stated, we initialize all the convolutional kernels using truncated normal distribution with standard deviation 0.1, and for weight normalization, the rescaling parameter is set to be 1.0 and the shifting parameter 0.0. Only for the last layer of the Real NVP block we initialize g to be 0.0. We apply a split operator to this last layer to obtain a "shift" coefficient and "log scale" coefficient for affine transformation. The last layer has double the dimensionality of the stochastic unit to be transformed. The split operator simply splits it in two parts. For the log scale coefficient, we apply the log-sigmoid function to make sure after exponentiation, it is bounded between 0 and 1 (similar to <ref type="bibr" target="#b35">Kingma &amp; Dhariwal (2018)</ref>). Since the pre-log-sigmoid is initialized to be 0, we add in a constant that depends on the total number of transformations that will be apply to the stochastic unit such that the overall transformation (after composition) will rescale the raw input by a factor of 0.95 (without considering activation normalization). This is to ensure the entire transformation is more robust to variation of depths at initialization.</p><p>We also apply activation normalization <ref type="bibr" target="#b35">(Kingma &amp; Dhariwal, 2018)</ref> with data-dependent initialization that standardizes the transformed feature, after each encoding transform and each decoding transform. We clip the log scale coefficient at ±2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Deterministic warm up</head><p>Due to our choice of flow, our instantiation of ANF resembles a VAE. It has been previously shown that starting off with less regularization and noise injection is beneficial to training, a technique known as deterministic warm up <ref type="bibr">(Raiko et al., 2007;</ref><ref type="bibr">Sønderby et al., 2016)</ref>. Similarly, if we expand the objective of ANF with autoencoding transform (affine coupling), we get log p(x, e) = log N (y T ; 0, I) + log N (z T ; 0, I)</p><formula xml:id="formula_17">+ T t=1 d j=1 log s enc πt (y t−1 ) j + T t=1 d j=1 log s dec πt (z t ) j</formula><p>where d is the dimensionality of the augmented data e, and (y t , z t ) are defined recursively as</p><formula xml:id="formula_18">z t = s enc πt (y t−1 ) z t−1 + m enc πt (y t−1 ) y t = s dec πt (z t ) y t−1 + m dec πt (z t )</formula><p>with the initial values z 0 = e, y 0 = x. We modify the objective by lowering the weighting of s enc πt and log N (z T ; 0, I) such that the network can focus more on Gaussianizing the raw input x. We defined the modified objective as L(π; x, e, β) := log N (y T ; 0, I) + where π is all the trainable parameters. We linearly anneal the weighting coefficient β from 0 to 1 for the first α iterations of the training. Note that in practice we apply the same β to all augmented data in the hierarchical setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Hyperparameters</head><p>Notation summary for hyperparameters:</p><p>• L: number of stochastic units (e 1 , ..., e L ).</p><p>• N : number of steps (encoding-decoding pairs).</p><p>• K: number of samples for importance sampling.</p><p>• λ: decoupled weight decay coefficient for Adam.</p><p>• c: number of channels (all deterministic features).</p><p>• c : number of channels for the l'th stochastic unit (stochastic features). Power denotes repetition.</p><p>• µ: feature map size (squared).</p><p>• k: kernel size (except for the data space layer).</p><p>• b: batch size.</p><p>• s: step size.</p><p>• a: annealing schedule (number of parameter updates).</p><p>• u: number of updates.   <ref type="formula" target="#formula_0">(2013)</ref> where it was used for density estimation. Differentiable bijective models were first introduced to the deep learning community as likelihood-based generative models by Rippel &amp; Adams (2013); <ref type="bibr" target="#b16">Dinh et al. (2014)</ref>, and as an inference machine by <ref type="bibr">Rezende &amp; Mohamed (2015)</ref>. Most development within this line of research is dedicated to improving the expressivity of the bijective mapping while maintaining computational tractability of the log-determinant of the Jacobian. Each family of flows can be characterized by the "trick" used to achieve this, e.g.</p><p>• Partial ordered dependency. If the mapping has a partial and ordered dependency, its Jacobian matrix will be a triangular matrix, the determinant of which can be computed in linear time. This includes the following: • Low rank transform. If the mapping is of a particular residual form, the Jacobian determinant can be computed readily using the matrix determinant lemma <ref type="bibr">(Rezende &amp; Mohamed, 2015)</ref> or its higher rank generalization <ref type="bibr" target="#b5">(Berg et al., 2018)</ref>.</p><p>• Lipschitz residual flow. If the nonlinear block of a residual mapping is no more than 1-Lipschitz, the overall mapping is invertible, and its Jacobian can be estimated using power series expansion, the Hutchinsons trace estimator <ref type="bibr" target="#b3">(Behrmann et al., 2018)</ref> and the Russian roulette estimator .</p><p>• Special convolutional forms. Certain structure of convolutional kernels can also be designed to to ensure tractability, such as via using 1 × 1 convolution <ref type="bibr" target="#b35">(Kingma &amp; Dhariwal, 2018</ref><ref type="bibr">), masking (Oord et al., 2016</ref><ref type="bibr">van den Oord et al., 2016;</ref><ref type="bibr" target="#b30">Hoogeboom et al., 2019;</ref><ref type="bibr">Song et al., 2019;</ref><ref type="bibr" target="#b46">Ma et al., 2019)</ref> or imposing certain repeated structure <ref type="bibr" target="#b34">(Karami et al., 2019)</ref>.</p><p>In this work, we introduce the augmentation trick, which generalizes flow-based methods in an orthogonal yet complementary manner. In particular, we employ the coupling proposed by <ref type="bibr" target="#b17">Dinh et al. (2017)</ref> to transform the augmented data; one potential alternative is to replace it with any of the tricks mentioned above.</p><p>Architectures and parameter sharing. As the autoencoding transform we use generalizes VAEs and hierarchical VAEs, one potential direction is to consider parameterizations that have shared components which are shown to be conducive to training, such as the ResNet with top-downn inference <ref type="bibr" target="#b37">(Kingma et al., 2016)</ref> and the bidirectional inference machine <ref type="bibr" target="#b47">(Maaløe et al., 2019)</ref>. As a generalization of VAEs, ANFs can also be applied to latent variable models of different graphical representations, such as variational recurrent neural networks <ref type="bibr" target="#b11">(Chung et al., 2015)</ref> and models of sets <ref type="bibr" target="#b20">(Edwards &amp; Storkey, 2017)</ref>; for example, the set flow proposed by <ref type="bibr">Rasul et al. (2019)</ref> is an instance of permutation-invariant ANF applied to sets. Another avenue for improving parameter efficiency is to consider tying the weights of different steps of transformations. As our theory suggests, consecutive transformations of the discretized Hamiltonian ODE would differ only slightly if the time derivatives are smooth enough. This means it would be sufficient to consider a single network which also takes in time embedding as input for all transformations.</p><p>Approximate Hamiltonian flows. Our approximation theory builds on the result of Wang &amp; Li (2019), which follows the optimal control framework of <ref type="bibr">Wibisono et al. (2016)</ref>. The augmented variable is treated as the costate, which is deterministically dependent on the state, i.e. the input data. Therefore we set the initial augmented distribution to be a Dirac point mass for the approximation theory to hold. Our theorem can be improved if one can show some time trajectories with the augmented variable drawn independently from a non-degenerate initial distribution are convergent to the prior distribution. We leave this for future work. Meanwhile, the same proof technique can be used to study the approximation capability of different families of flows. In particular, the residual flows <ref type="bibr" target="#b3">(Behrmann et al., 2018;</ref><ref type="bibr" target="#b8">Chen et al., 2019)</ref> and their continuous counterpart <ref type="bibr" target="#b7">(Chen et al., 2018a;</ref><ref type="bibr" target="#b24">Grathwohl et al., 2019)</ref> can be used to approximate the deterministic Langevin diffusion, since (1) one can replace the Brownian motion term with the gradient of the log marginal density without modifying its Fokker-Planck equation (see <ref type="bibr" target="#b29">Hoffman &amp; Ma (2019)</ref> or the appendix of Wang &amp; Li (2019)) and (2) the first-order Langevin dynamic is known to be convergent to its stationary distribution <ref type="bibr">(Roberts et al., 1996)</ref>.</p><p>Gradient-based flows. As the theory suggests, gradient of the potential can be used to guide the evolution of the particle. This has been previously explored by <ref type="bibr" target="#b19">Duvenaud et al. (2016)</ref>. Normalizing flows for variational inference. The most well-known application of normalizing flows is to improve the variational distribution to approximate posterior distribution of (1) the latent representations (Rezende &amp; Mohamed, 2015; <ref type="bibr" target="#b37">Kingma et al., 2016;</ref><ref type="bibr">Tomczak &amp; Welling, 2016;</ref><ref type="bibr" target="#b5">Berg et al., 2018)</ref> and <ref type="formula" target="#formula_1">(2)</ref> the parameters of neural networks <ref type="bibr" target="#b44">(Louizos &amp; Welling, 2017;</ref><ref type="bibr" target="#b40">Krueger et al., 2017;</ref><ref type="bibr" target="#b33">Huang et al., 2019)</ref>. ANF can also be applied to inference problems, with slight modification of the target potential. We show in Appendix D that one can augment the target distribution with an independent distribution and infer the joint target altogether. This boils down to the hierarchical variational method <ref type="bibr" target="#b0">(Agakov &amp; Barber, 2004;</ref><ref type="bibr">Ranganath et al., 2016)</ref> as a special case when one step of autoencoding transform is applied.</p><p>Variational gap. The joint likelihood that we maximize is a variational objective lower-bounding the marginal likelihood of the data. One potential avenue for improvement is to reduce this bias (the augmentation gap) throughout training, by closing up the gap via importance sampling <ref type="bibr" target="#b6">(Burda et al., 2015)</ref> or using an unbiased estimate of the marginal likelihood <ref type="bibr" target="#b45">(Luo et al., 2020)</ref>.</p><p>Representation learning. Considering invertible transformations in an augmented data space allows us to sidestep the topology-preserving property of a homeomorphism. The issue of this property is discussed and addressed by <ref type="bibr" target="#b12">Cornish et al. (2019)</ref> by converting the flow into a latent-variable model. <ref type="bibr" target="#b18">Dupont et al. (2019)</ref> adopt the same technique by augmenting the data space and apply the augmented continuous time flow to discriminative tasks. We hypothesize this can potentially improve the representation learned by an invertible model, for example in a semi-supervised setting <ref type="bibr">(Nalisnick et al., 2019;</ref><ref type="bibr" target="#b2">Atanov et al., 2019)</ref> or as a component of a reversible model for memory-efficient backpropagation <ref type="bibr" target="#b22">(Gomez et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Augmented Normalizing Flows for Variational Inference</head><p>Augmented normalizing flows can also be used for inference tasks where our goal is to approximate an unnormalized densityp(z) with a parametric distribution q(z). This includes variational training of energy based models <ref type="bibr" target="#b15">(Dai et al., 2017;</ref><ref type="bibr">Zhai et al., 2016)</ref>, entropy regularized policy gradient in reinforcement learning <ref type="bibr">(Mazoure et al., 2019;</ref><ref type="bibr">Ward et al., 2019)</ref>, probability distillation <ref type="bibr">(Oord et al., 2018)</ref>, and variational Bayesian inference of latent variables <ref type="bibr" target="#b36">(Kingma &amp; Welling, 2014)</ref>.</p><p>We focus on the case of variational inference (but the same technique can be used for other applications), wherep(z) = p(x, z), and our goal is to maximize the ELBO</p><formula xml:id="formula_19">E z log p(x, z) q(z)</formula><p>where we can apply the standard change of variable to get q(z) = q(e) ∂g(e) ∂e −1 with z = g(e) as described in Section 2.</p><p>Alternatively, we can augment the target distributionp(z) with an independent p(v), and jointly transform a base distribution q(e)q(u) into q(z, v) to approximatep(z)p(v) via an invertible map e, u → G(e, u). Concretely, we maximize the following quantity</p><formula xml:id="formula_20">E z,v log p(x, z)p(v) q(z, v) = E e,u log p(x, G(e, u)| 1 )p(G(e, u)| 2 ) q(e, u)</formula><p>∂G(e, u) ∂(e, u)</p><p>where | 1 and | 2 denote the first and the second coordinates, respectively. This lower-bounds the ELBO since</p><formula xml:id="formula_22">E z log p(x, z) q(z) − E z,v log p(x, z)p(v) q(z, v) = E z,v log q(v|z) p(v) = E z [D KL (q(v|z)||p(v))]</formula><p>is non-negative.</p><p>Auxiliary variable for hierarchical variational inference. The above derivation for applying ANF to variational inference is reminiscent of the auxiliary variable method <ref type="bibr" target="#b0">(Agakov &amp; Barber, 2004;</ref><ref type="bibr">Ranganath et al., 2016)</ref>. To see this, assume we parameterize G(e, u) as the composition g enc • g dec , where </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Proofs</head><p>Define the scaling coefficients α t = log 2 t and β t = γ t = log t 2 . Let p(x) be the standard normal density, and q(x) be the data distribution. Let q 0 = q and Φ : X → R be some continuous function. Define the following Hamiltonian ordinary differential equation (ODE):</p><p>x t = e αt−γt e t ,</p><p>x 0 ∼ q 0 (11)</p><formula xml:id="formula_23">e t = −e αt+βt+γt ∇ log q t (x t ) p(x t ) , e 0 = ∇Φ(x 0 )<label>(12)</label></formula><p>whereẋ t andė t are the time derivatives of x and e at time t, and q t is the marginal density of x t .</p><p>Proposition 1. For some convex Φ, the trajectories of x t and e t following (11,12) converge in distribution to x ∞ and e ∞ , respectively, where x ∞ ∼ p(x) and e ∞ ∼ δ 0 (i.e. a point mass at 0).</p><p>Proof. By Theorem 1 of <ref type="bibr">Taghvaei &amp; Mehta (2019)</ref> and Appendix C.4 of Wang &amp; Li (2019) (for an extension to high dimensional cases), since α t , β t and γ t satisfy the scaling condition in Taghvaei &amp; Mehta (2019) and log p is convex, x t converges in KL divergence to x ∞ and e t converges to 0 almost surely (which implies convergence in distribution). Pinsker's inequality implies x t → x ∞ in total variation, d TV , which has a dual representation:</p><formula xml:id="formula_24">d TV (x t , x ∞ ) = sup f :X →[−1,1] E[f (x t )] − E[f (x ∞ )]</formula><p>This implies for any bounded, continuous f ,</p><formula xml:id="formula_25">|E[f (x t )] − E[f (x ∞ )]| ≤ d TV (x t , x ∞ ) · ||f || ∞ which converges to 0 as t → ∞. By Portmanteau's Lemma, x t → x ∞ in distribution.</formula><p>We first construct a sequence of encoding functions m enc n and decoding functions m dec n parameterized by neural networks, and define the following (volume preserving) invertible mappings e π 1 = e π 0 + m enc 1 (x π 0 ) x π n+1 = x π n + 2 · m dec n+1 (e π n+1 ) ∀ n ≥ 0 e π n+1 = e π n + 2 · m enc n+1 (x π n ) ∀ n ≥ 1 with e π 0 = 0 and x π 0 ∼ q 0 . The step size parameter will be chosen to depend on the depth coefficient N , i.e. the number of layers of the joint transformation.</p><p>Below we prove ANF of the above form can universally transform q(x)δ 0 (e) into p(x)δ 0 (e). We make the following assumption on the family of q: Assumption 1. We assume the gradient of the convex function in Proposition (1) ∇Φ is continuous, and that f (e, t) := e αt−γt e and g(x, t) := −e αt+βt+γt log qt(x) p(x) have a bounded second time derivative (on the trajectories x t and e t which are also functions of time), and are uniformly Lipschitz; that is,</p><formula xml:id="formula_26">max ||f ||, ||g ||, sup e =e ,t&gt;0</formula><p>||f (e, t) − f (e , t)|| ||e − e || , sup</p><p>x =x ,t&gt;0</p><p>||g(x, t) − g(x , t)|| ||x − x || ≤ K for some K ≥ 0, where we define the single-argument vector functions f (t) = f (e t , t) and g(t) = g(x t , t) as the time derivatives of the trajectories (x t , e t ).</p><p>We denote by Q the family of probability measures that satisfies this assumption.</p><p>Before we move on to approximation, we start with a lemma for bounding approximation error by solving recursion using the technique of generating functions.</p><p>Lemma 1. If for any N &gt; 0, {d n : 0 ≤ n ≤ N } is a sequence of real numbers satisfying</p><formula xml:id="formula_27">d n ≤ c N 2 + c N 2 n−1 t=1 t s=1 d s for some constant c, then max 0≤n≤N d n → 0 as N → ∞</formula><p>Proof. We would like to bound the error d n explicitly. To do so, we first note that the sequence {d n } is no larger than {D n }, which is recursively defined as</p><formula xml:id="formula_28">D 0 = 0 D n+1 = C + C n t=1 t s=1 D s<label>(13)</label></formula><p>for n ≥ 0, where for simplicity we let C = c/N 2 . Now to express D n+1 explicitly, we use the method of generating function, following the recipe of Wilf (2005) (see Chapter 1 for a brief introduction). Define function f to be a power series whose coefficients are D n 's; that is, f (x) = n≥0 D n x n . Multiply both sides of (13) by x n and summing over the indices of non-negative integers n ≥ 0 give us</p><formula xml:id="formula_29">f (x) x = C 1 − x + Cf (x) (1 − x) 2</formula><p>After rearrangement, we have</p><formula xml:id="formula_30">f (x) x 2 − (2 + C)x + 1 x(1 − x) 2 = C 1 − x ⇒ f (x) x = C(1 − x) x 2 − (2 + C) + 1</formula><p>which can be decomposed into the partial fractions</p><formula xml:id="formula_31">f (x) x = C 1+a2 a 1 − x + C 1+a1 a 2 − x<label>(14)</label></formula><p>where a 1 and a 2 are the roots of the quadratic function x 2 − (2 + C)x + 1, which satisfy a 1 + a 2 = 2 + C and a 1 a 2 = 1.</p><p>For sufficiently small x, we can break <ref type="formula" target="#formula_0">(14)</ref> into the geometric series</p><formula xml:id="formula_32">f (x) x = C a 1 (1 + a 2 ) n≥0 x a 1 n + C a 2 (1 + a 1 ) n≥0 x a 2 n</formula><p>This means for n &gt; 0, since a 1 a 2 = 1, the coefficient of f (x) can be expressed as D n = C 1 + a 2 1 a n 1 + C 1 + a 1 (a 1 a 2 ) n a n 2 = C 1 (1 + a 1 )a n−1 1 + a n 1 1 + a 1</p><p>Now let a 1 be the larger root. Solving x 2 − (2 + C)x + 1 yields</p><formula xml:id="formula_34">a 1 = 2 + C + √ C 2 + 4C 2 =: 1 + r where r := C 2 + C 2 4 + C.</formula><p>We show that the parenthesis in (15) can be controlled asymptotically (i.e. does not exceed certain constant for sufficiently large N ), and that since C diminishes, D n converges. First, since r &gt; 0, a 1 &gt; 1 and 1 (1 + a 1 )a n−1 1 &lt; 1 2</p><p>Second, since (1 + r) n ≤ e nr for n ≥ 0 and r ≥ −1,</p><formula xml:id="formula_35">a n 1 = (1 + r) n ≤ e nr ≤ exp CN 2 + C 2 N 2 4 + CN 2 = exp c 2N + c 2 4N 2 + c which converges to exp( √ c) as N → ∞.</formula><p>Finally, since C → 0 as N → ∞ and d n ≤ D n , d n → 0 for all n ≤ N as N → ∞.</p><p>We are now ready to show the result of the pointwise approximation of the Hamiltonian ODE using ANFs with affine (more specifically, additive) coupling.</p><p>Proposition 2. Let x t and e t be trajectories (mappings of x 0 ∈ X = R d ) following the Hamiltonian ODE (11,12) described in Proposition 1 dependent on some initial distribution q 0 ∈ Q. For each T &gt; 0, we can choose some number of layers N of the joint transformation and a sequence of pairs of m enc n and m dec n (dependent on T ) for 1 ≤ n ≤ N , such that ||x π N − x T || → 0 and ||e π N − e T || → 0 as T → ∞ pointwise for x 0 ∈ X = R d .</p><p>Proof. Fix q 0 ∈ Q and T &gt; 0 and some compact subset X 0 ⊂ X . We first consider all points x 0 in X 0 , and show that (x π n , e π n ) can be used to approximate (x T , e T ) uniformly well. We consider a N -step joint transformation, and set = T 2N &gt; 0. We start with approximating e by e π 1 . Since e π 0 is 0, by the universal approximation theorem (UAT) of neural networks <ref type="bibr" target="#b14">(Cybenko, 1989)</ref>, we can choose some m enc 1 such that ||e − e π 1 || = ||e − m enc 1 || ≤ 2 for all x 0 ∈ X 0 . We proceed with an approximate leap-frog integration of the dynamic, using the neural encoders and decoders to approximate the time derivatives. Let E 1 := e π 1 (X 0 ) where e π 1 := m enc 1 , which is compact, since X 0 is compact and e π 1 is continuous wrt X 0 . Again, by the UAT, we can choose some m dec 1 such that ||f (e, ) − m dec 1 (e)|| &lt; 2 for all e ∈ E 1 . Likewise, we let X 1 := x π 1 (X 0 ) where x π 1 := (2 m dec 1 • e π 1 + Id)(X 0 ) with Id being the identity map, such that X 1 is also compact since x π 1 is continuous wrt X 0 , and choose m enc 2 such that ||g(x, 2 ) − m enc 2 (x)|| &lt; 2 for all x ∈ X 1 . Repeating the same construction for m dec n and m enc n for n ≤ N , we have</p><p>x π n+1 = x π n + 2 m dec n+1 (e π n+1 ) (16) e π n+1 = e π n + 2 m enc n+1 (x π n )</p><p>with m dec n and m enc n chosen such that 1. ||f (e, 2n + ) − m dec n+1 (e)|| &lt; 2 for all e ∈ E n+1 := e π n+1 (X 0 ) where e π n+1 := 2 m enc n+1 • x π n + e π n is a continuous map of X 0 ; and 2. ||g(x, 2n ) − m enc n+1 (x)|| &lt; 2 for all x ∈ X n := x π n (X 0 ) where x π n := 2 m dec n • e π n + x π n−1 is a continuous map of X 0 .</p><p>Such choices of m enc n and m dec n are possible since by construction X n−1 and E n are compact. Equations <ref type="bibr">(16,</ref><ref type="bibr">17)</ref> are approximate midpoint methods as they use functions to approximate the time derivatives evaluated at midpoints of their counterparts. The exact midpoint method has a cubic error rate of h 3 24 f (ξ), for some ξ between the midpoint and the approximating point, where h is the interval width of each iteration; see Section 5.4 of <ref type="bibr" target="#b21">Epperson (2013)</ref>. That is,</p><formula xml:id="formula_37">x 2n +2 = x 2n + 2 f (e 2n + , 2n + ) + 3 3 f (ξ x n+1 )<label>(18)</label></formula><p>for some ξ x n+1 between the two steps. Similarly, e 2n + = e 2n − + 2 g(x 2n , 2n ) + 3 3 g (ξ e n+1 )</p><p>for some ξ e n+1 between the two steps. Subtracting (16) from <ref type="formula" target="#formula_0">(18) yields</ref> x 2n +2 − x π n+1 = x 2n − x π n + 2 f (e 2n + , 2n + ) − 2 m dec n+1 (e π n+1 ) + 3 3 f (ξ x n+1 )</p><p>By triangle inequality, we have</p><p>x 2n +2 − x π n+1 ≤ x 2n − x π n + 2 f (e 2n + , 2n + ) − 2 m dec n+1 (e π n+1 ) + 3 3 f (ξ x n+1 ) ≤ x 2n − x π n + 2 f (e 2n + , 2n + ) − m dec n+1 (e π n+1 ) propagated error</p><formula xml:id="formula_39">+ 3 3 f (ξ x n+1 ) truncated error</formula><p>The error on the RHS consists of two parts: (1) the first two terms constitute the propagated error from the previous steps and (2) the third term is a newly introduced truncation error due to the Taylor expansion.</p><p>By triangle inequality again, f (e 2n + , 2n + ) − m dec n+1 (e π n+1 ) = f (e 2n + , 2n + ) − f (e π n+1 , 2n + ) + f (e π n+1 , 2n + ) − m dec n+1 (e π n+1 ) ≤ f (e 2n + , 2n + ) − f (e π n+1 , 2n + )</p><p>midpoint deviation + f (e π n+1 , 2n + ) − m dec n+1 (e π n+1 ) approximation error</p><p>Again the RHS can be decomposed into two error parts: (1) a midpoint deviation resulting from performing midpoint numerical integration which would not vanish even if the neural network is replaced with the true time derivative, and (2) an approximation error due to the inaccuracy of approximating the time derivative.</p><p>Letting d x n = ||x 2n − x π n || and d e n = ||e 2n − − e π n ||, and applying the properties of the Assumption 1, we have d x n+1 ≤ d x n + 2 (Kd e n+1 + 2 ) + 3 K 3 = d x n + 2 Kd e n+1 + 3 K 3 + 2</p><p>owing to the uniform error bound of the neural decoder ||f (e, 2n + ) − m dec n+1 (e)|| &lt; 2 for all e ∈ E n+1 and the fact that e π n+1 (x 0 ) ∈ E n+1 since x 0 ∈ X 0 . The same can be done to obtain a bound on d e n+1 by subtracting <ref type="formula" target="#formula_0">(17)</ref>  </p><p>d e n+1 ≤ d e n + 2 K d x n + 3 K for n ≥ 1</p><formula xml:id="formula_41">1 N 2 + T 2 K 2 N 2 n−1 t=1 t s=1 d e s<label>(22)</label></formula><p>By Lemma 1, we know that the elements of both sequences of error d x n and d e n converge uniformly on 1 ≤ n ≤ N to 0 as N → ∞. In particular, for all T &gt; 0, δ &gt; 0 and compact subset X 0 of R d , there exists some large enough integer N (T, δ, X 0 ) &gt; 0 for which a joint transformation of N (T, δ, X 0 ) layers parameterized by some neural encoders and decoders satisfies d x N (T,δ,X0) ≤ δ and d e N (T,δ,X0) ≤ δ for all x 0 ∈ X 0 .</p><p>Consider some positive value B &gt; 0. We let X 0 = [−B, B] d , T = B and δ = 1 B . We can find a sequence of models with an error rate d <ref type="figure">x  N (B,1/B,[−B</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmented Normalizing Flows</head><p>The lemma below shows if one can approximate the solution of an ODE (||y n − x n || → 0, i.e. x n and y n are asymptotically indistinguishable) and if the limit of the solution is a transport map (x n d → x ∞ ), then the approximation also forms a transport map (y n d → x ∞ ).</p><p>Lemma 2. Let x ∞ , (x n : n ≥ 0) and (y n : n ≥ 0) be random variables. If x n → x ∞ in distribution and if ||x n − y n || → 0 almost surely as n → ∞, then y n → x ∞ in distribution.</p><p>Proof. Let Λ : R d → R be an arbitrary bounded and Lipschitz continuous function. Then</p><formula xml:id="formula_43">|E [Λ (x ∞ ) − Λ (y n )]| ≤ |E [Λ (x ∞ ) − Λ(x n ) + Λ(x n ) − Λ (y n )]| ≤ |E [Λ (x ∞ ) − Λ(x n )]| + E [|Λ (x n ) − Λ (y n )|]</formula><p>First, since x n → x ∞ in distribution and since Λ is bounded and continuous, by the Portmanteau Lemma the first term of the RHS converges to 0 as n → ∞. Second, since y n is almost surely asymptotically indistinguishable from x n (let Ω be the almost sure set), and since the Lipschitzness of Λ implies uniform continuity, the following are true</p><p>• For all &gt; 0, there exists a δ &gt; 0 such that ||x − y|| ≤ δ implies |Λ(x) − Λ(y)| ≤ .</p><p>• For any δ &gt; 0, there exists a integer N &gt; 0 such that for all n ≥ N , ||x n − y n || ≤ δ for all ω ∈ Ω.</p><p>These imply ||Λ(x n ) − Λ(y n )|| → 0 on Ω. Then</p><formula xml:id="formula_44">E [|Λ (x n ) − Λ (y n )|] = E Ω [|Λ (x n ) − Λ (y n )|] E1 + E Ω c [|Λ (x n ) − Λ (y n )|] E2</formula><p>converges to 0, since (1) boundedness of Λ and the Bounded Convergence Theorem imply E 1 → 0 and (2) sup x Λ(x) &lt; ∞ implies E 2 ≤ 2 sup x Λ(x)P(Ω c ) = 0. Finally, since Λ is arbitrary, by the Portmanteau Lemma again, y n converges in distribution to x ∞ as n → ∞.</p><p>We now are ready to prove Theorem 1, which we restate below. The main idea is to notice that ANFs can be made pointwise inseparable from the Hamiltonian ODE, which implies weak convergence since the Hamiltonian ODE converges in distribution.</p><p>Theorem 1. For any q ∈ Q, we can find a sequence (x π N , e π N ) of ANFs of the additive form <ref type="bibr">(7,</ref><ref type="bibr">8)</ref>, such that if x π 0 , e π 0 ∼ q(x)δ 0 (e) and x ∞ , e ∞ ∼ p(x)δ 0 (e), then (x π N , e π N ) → (x ∞ , e ∞ ) in distribution.</p><p>Proof. First, by Proposition 1, x B → x ∞ in distribution as B → ∞. Second, x B and x π N (B,1/B,[−B,B] d ) chosen from Proposition 2 are almost surely asymptotically indistinguishable. Thus, by Lemma 2, x π N (B,1/B,[−B,B] d ) converges in distribution to x ∞ . The same holds for the augmented variable e. Let (x π N ) and (e π N ) denote such sequences. By Theorem 2.7 of <ref type="bibr">Van der Vaart (2000)</ref>, (x π N , e π N ) → (x ∞ , e ∞ ) in distribution (as e ∞ = 0 is a constant).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) Augmented normalizing flow with block coupling and (b) the reverse path for generation. (c) Hierarchical augmented normalizing flow. The horizontal connections indicate deterministic features that will be concatenated with the stochastic features in the next transform block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Density modeling of 1D MoG with VAE (aka 1-step ANF)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>4) L(θ, φ; x) = E e∼q(e) log N (y; 0, I) − i log σ θ,i (z) + (6) log N (z; 0, I) + j log σ φ,j (x) + H(e)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4. 5-step ANF on 1D MoG. In the inference path (top row), we start with an encoding transform that maps e to z1 conditioned on x, followed by a decoding transform that maps x into y1 conditioned on z1. We reuse the same encoder and decoder to refine the joint variable repeatedly to obtain y5 and z5. In the generative path (bottom row), we reverse the process, starting with the inverse transform of the decoding, followed by the inverse transform of the encoding, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Comparing increasing number of layers of stochastic units (L) versus increasing number of layers of autoencoding transforms (S). (x-axis): number of updates. (y-axis): upper bound of bits per dim (BPD) on CIFAR 10 test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Unconditionally generated samples from models trained on MNIST (top row), CIFAR 10 (middle row), and 5-bit CelebA (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Lossy reconstruction. Left: original data. Right: reconstruction from the topmost representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Left: comparison of linear and rescaled interpolations. Right: rescaled interpolation of input data (first and last columns).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>(left) linear interpolation, (right) rescaled interpolation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Architecture of a single encode transform block and a single decode transform block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>πt (y t−1 ) j + β   log N (z T ; 0, I) + πt (z t ) j  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>-</head><label></label><figDesc><ref type="bibr" target="#b16">Dinh et al. (2014;</ref>;<ref type="bibr" target="#b35">Kingma &amp; Dhariwal (2018)</ref>; Ho et al. (2019) use a block-wise conditioning in the mapping, and -Kingma et al. (2016); Chen et al. (2017); Papamakarios et al. (2017); Huang et al. (2018) generalize block-wise dependency to temporal dependency wherein all the variables prior to the current variable of a given ordering are inputs of the conditioning to transform the current variable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Salimans et al. (2015) on the other hand propose a hierarchical model inspired by the Hamiltonian dynamic, and Song et al. (2017); Levy et al. (2018) generalize Hamiltonian Monte Carlo (HMC) with trainable neural components. Similarly, one can parameterize a Gradient-based augmented generative flow to model the data distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>g</head><label></label><figDesc>enc (e, u) = concat(e, s enc (e) u + m enc (e)), g dec (e, u) = concat(s dec (u) e + m dec (u), u) with s enc , s dec &gt; 0. Then Equation (10) becomes E e,u   logp(x, s dec (u) e + m dec (u))p(s enc (z) u + m enc (z)) q(e, u) + log i s dec (u) i + log j s enc (z) j   where z := s dec (u) e + m dec (u), which is equivalent to E z,u log p(x, z)N (u; −m enc (e)/s enc (e), s enc (e) −2 ) N (z; m dec (u), s dec (u) 2 )q(u) = E z,u log p(x, z)r(u|z) q(z|u)q(u)where q(z|u) = N (z; m dec (u), s dec (u) 2 ) and r(u|z) = N (u; −m enc (e)/s enc (e), s enc (e) −2 ). This shows hierarchical variational methods are a special case of ANF, and the latter can potentially be used to improve the joint expressivity of the former through additional composition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>,B] d ) ≤ 1/B and d e N (B,1/B,[−B,B] d ) ≤ 1/B converging pointwise on R d to 0 as B → ∞. This implies d x N (B,1/B,[−B,B] d ) = x B − x π N (B,1/B,[−B,B] d ) → 0 pointwise as B → ∞. The same holds for the augmented variable e.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Bits-per-dim estimates of standard benchmarks (the lower the better). Results of Flow++, MaCow, and ANF are models that employ variational dequantization instead of uniform noise injection. Details can be found in the appendix.</figDesc><table><row><cell>Model</cell><cell cols="5">MNIST CIFAR 10 ImageNet 32 ImageNet 64 CelebA-HQ</cell></row><row><cell>Models with autoregressive components</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VAE + IAF (Kingma et al., 2016)</cell><cell>-</cell><cell>3.11</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PixelCNN (Oord et al., 2016)</cell><cell>-</cell><cell>3.14</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PixelCNN (multiscale) (Reed et al., 2017)</cell><cell>-</cell><cell>-</cell><cell>3.95</cell><cell>3.70</cell><cell>-</cell></row><row><cell>PixelSNAIL (Chen et al., 2018b)</cell><cell>-</cell><cell>2.85</cell><cell>3.80</cell><cell>-</cell><cell>-</cell></row><row><cell>SPN (Menick &amp; Kalchbrenner, 2019)</cell><cell>-</cell><cell>-</cell><cell>3.79</cell><cell>3.52</cell><cell>0.61</cell></row><row><cell>Flow-based models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Real NVP (Dinh et al., 2017)</cell><cell>1.06</cell><cell>3.49</cell><cell>4.28</cell><cell>3.98</cell><cell>-</cell></row><row><cell>Glow (Kingma &amp; Dhariwal, 2018)</cell><cell>1.05</cell><cell>3.35</cell><cell>4.09</cell><cell>3.81</cell><cell>1.03</cell></row><row><cell>FFJORD (Grathwohl et al., 2019)</cell><cell>0.99</cell><cell>3.40</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Residual (Chen et al., 2019)</cell><cell>0.97</cell><cell>3.28</cell><cell>4.01</cell><cell>3.76</cell><cell>0.99</cell></row><row><cell>Flow++ (Ho et al., 2019)</cell><cell>-</cell><cell>3.09</cell><cell>3.86</cell><cell>3.69</cell><cell>-</cell></row><row><cell>MaCow (Ma et al., 2019)</cell><cell>-</cell><cell>3.16</cell><cell>-</cell><cell>3.69</cell><cell>0.67</cell></row><row><cell>ANF (ours)</cell><cell>0.93</cell><cell>3.05</cell><cell>3.92</cell><cell>3.66</cell><cell>0.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>PixelIQN 1 i-ResNet 2 Glow 2 Residual Flow 2 VAE+Glow 3 ANF DCGAN 4 WGAN-GP (TTUR) 4 Evaluation on Inception Score (IS, the higher the better) and Fréchet Inception Distance (FID, the lower the better) of models trained on CIFAR 10. Results taken from 1 Ostrovski et al. (2018), 2 Chen et al. (2019), 3 Morrow &amp; Chiu (2019), and 4 Gulrajani et al. (2017a); Heusel et al. (2017). Parenthesis indicates two time-scale update rule for WGAN-GP.</figDesc><table><row><cell>IS</cell><cell>(↑)</cell><cell>4.60</cell><cell>5.29</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6.49</cell><cell>6.16</cell><cell>7.86</cell></row><row><cell cols="2">FID (↓)</cell><cell>65.93</cell><cell>49.46</cell><cell>65.01</cell><cell>46.90</cell><cell>46.37</cell><cell>42.14</cell><cell>30.60</cell><cell>37.7</cell><cell>29.3 (24.8)</cell></row></table><note>PixelCNN 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Density estimation We perform density modelling on the MNIST handwritten digit dataset (LeCun et al., 1998), CI-FAR 10 (Krizhevsky et al., 2009), downscaled versions of ImageNet (32 × 32 and 64 × 64) (Oord et al., 2016) and the celebrity face dataset CelebA</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>7.2. Qualitative resultsSample quality For quantitative evaluation of sample quality, we report the Inception Score (IS)<ref type="bibr" target="#b37">(Salimans et al., 2016)</ref> and the Fréchet Inception Distance (FID)<ref type="bibr" target="#b27">(Heusel et al., 2017)</ref>, expanding the table of Ostrovski et al.(2018). We found the FID score of WGAN-GP<ref type="bibr" target="#b25">(Gulrajani et al., 2017a)</ref> reported inOstrovski et al. (2018)  is worse than the one reported in the literature, so we include the original values of IS and FID of GANs from the original works of<ref type="bibr" target="#b25">Gulrajani et al. (2017a)</ref> and<ref type="bibr" target="#b27">Heusel et al. (2017)</ref> for more realistic comparison. In</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Mazoure, B., Doan, T., Durand, A., Hjelm, R. D., and Pineau, J. Leveraging exploration in off-policy algorithms via normalizing flows. In Conference on Robot Learning, 2019. Menick, J. and Kalchbrenner, N. Generating high fidelity images with subscale pixel networks and multidimensional upscaling. In International Conference on Learning Representations, 2019. Journal of Machine Learning Research, 2007. Ranganath, R., Tran, D., and Blei, D. Hierarchical variational models. In International Conference on Machine Learning, pp. 324-333, 2016. Rasul, K., Schuster, I., Vollgraf, R., and Bergmann, U. Set flow: A permutation invariant normalizing flow. arXiv preprint arXiv:1909.02775, 2019. International Conference on Machine Learning, pp. 2912-2921. JMLR. org, 2017. Rezende, D. J. and Mohamed, S. Variational inference with normalizing flows. In International Conference on Machine Learning, 2015. pp. 6076-6085, Long Beach, California, USA, 09-15 Jun 2019. PMLR. Tomczak, J. M. and Welling, M. Improving variational auto-encoders using householder flow. arXiv preprint arXiv:1611.09630, 2016.</figDesc><table><row><cell>van den Oord, A., Kalchbrenner, N., Espeholt, L.,</cell><cell>In Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic</cell></row><row><cell>kavukcuoglu, k., Vinyals, O., and Graves, A. Conditional</cell><cell>backpropagation and approximate inference in deep gen-</cell></row><row><cell>image generation with pixelcnn decoders. In Neural In-</cell><cell>erative models. In International Conference on Machine</cell></row><row><cell>Morrow, R. and Chiu, W.-C. Variational autoencoders with formation Processing Systems 29, pp. 4790-4798. 2016.</cell><cell>Learning, 2014.</cell></row><row><cell>normalizing flow decoders. 2019. Van der Vaart, A. W. Asymptotic statistics, volume 3. Cam-</cell><cell>Rippel, O. and Adams, R. P. High-dimensional probability</cell></row><row><cell>Nalisnick, E., Matsukawa, A., Teh, Y. W., Gorur, D., and bridge university press, 2000.</cell><cell>estimation with deep density models. arXiv preprint</cell></row><row><cell>Lakshminarayanan, B. Hybrid models with deep and in-</cell><cell>arXiv:1302.5125, 2013.</cell></row><row><cell>Wang, Y. and Li, W. Accelerated information gradient flow, vertible features. In International Conference on Machine Learning, 2019. 2019.</cell><cell>gence of langevin distributions and their discrete approxi-Roberts, G. O., Tweedie, R. L., et al. Exponential conver-</cell></row><row><cell>Ward, P. N., Smofsky, A., and Bose, A. J. Improving explo-</cell><cell>mations. Bernoulli, 2(4):341-363, 1996.</cell></row><row><cell>ration in soft-actor-critic with normalizing flows policies.</cell><cell></cell></row><row><cell>arXiv preprint arXiv:1906.02771, 2019.</cell><cell></cell></row><row><cell>Wibisono, A., Wilson, A. C., and Jordan, M. I. A variational</cell><cell></cell></row><row><cell>perspective on accelerated methods in optimization. pro-</cell><cell></cell></row><row><cell>ceedings of the National Academy of Sciences, 113(47):</cell><cell></cell></row><row><cell>E7351-E7358, 2016.</cell><cell></cell></row><row><cell>Wilf, H. S. generatingfunctionology. AK Peters/CRC Press,</cell><cell></cell></row><row><cell>2005.</cell><cell></cell></row><row><cell>Zhai, S., Cheng, Y., Feris, R., and Zhang, Z. Generative</cell><cell></cell></row><row><cell>adversarial networks as variational training of energy</cell><cell></cell></row><row><cell>based models. arXiv preprint arXiv:1611.01799, 2016.</cell><cell></cell></row><row><cell></cell><cell>Tabak, E. G. and Turner, C. V. A family of nonparametric</cell></row><row><cell></cell><cell>density estimation algorithms. Communications on Pure</cell></row><row><cell></cell><cell>and Applied Mathematics, 66(2):145-164, 2013.</cell></row><row><cell></cell><cell>Tabak, E. G., Vanden-Eijnden, E., et al. Density estimation</cell></row><row><cell></cell><cell>by dual ascent of the log-likelihood. Communications in</cell></row><row><cell></cell><cell>Mathematical Sciences, 8(1):217-233, 2010.</cell></row></table><note>Neal, R. M. et al. Mcmc using hamiltonian dynamics. Hand- book of markov chain monte carlo, 2(11):2, 2011. Oord, A. v. d., Kalchbrenner, N., and Kavukcuoglu, K. Pixel recurrent neural networks. In International Conference on Machine Learning, 2016. Oord, A. v. d., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., Driessche, G. v. d., Lockhart, E., Cobo, L. C., Stimberg, F., et al. Parallel wavenet: Fast high-fidelity speech synthesis. In International Confer- ence on Machine Learning, 2018. Ostrovski, G., Dabney, W., and Munos, R. Autoregressive quantile networks for generative modeling. In Interna- tional Conference on Machine Learning, 2018. Papamakarios, G., Pavlakou, T., and Murray, I. Masked autoregressive flow for density estimation. In Neural Information Processing Systems, pp. 2338-2347, 2017. Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., and Lakshminarayanan, B. Normalizing flows for probabilistic modeling and inference. arXiv preprint arXiv:1912.02762, 2019. Raiko, T., Valpola, H., Harva, M., and Karhunen, J. Building blocks for variational bayesian learning of latent variable models.Reed, S., van den Oord, A., Kalchbrenner, N., Colmenarejo, S. G., Wang, Z., Chen, Y., Belov, D., and de Freitas, N. Parallel multiscale autoregressive density estimation.Salimans, T. and Kingma, D. P. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Neural Information Processing Sys- tems, pp. 901-909, 2016. Salimans, T., Kingma, D. P., and Welling, M. Markov chain monte carlo and variational inference: Bridging the gap. In International Conference on Machine Learning, pp. 1218-1226, 2015. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. In Neural Information Processing Systems, pp. 2234-2242, 2016. Sønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K., and Winther, O. Ladder variational autoencoders. In Neural Information Processing Systems, pp. 3738-3746, 2016. Song, J., Zhao, S., and Ermon, S. A-nice-mc: Adversarial training for mcmc. In Neural Information Processing Systems, pp. 5140-5150, 2017. Song, Y., Meng, C., and Ermon, S. Mintnet: Building invert- ible neural networks with masked convolutions. In Neural Information Processing Systems, pp. 11002-11012, 2019.Taghvaei, A. and Mehta, P. Accelerated flow for probability distributions. In Chaudhuri, K. and Salakhutdinov, R. (eds.), International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Hyperparameter details of density estimation tasks. C. Extended related work and future direction Normalizing flows. The term Normalizing Flow was originally coined by Tabak et al. (2010); Tabak &amp; Turner</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Mila 2 Google 3 CIFAR fellow. Correspondence to: Chin-Wei Huang &lt;chin-wei.huang@umontreal.ca&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This particular factorization of the variational distribution is known as the bottom-up inference. We leave the top-down inference<ref type="bibr" target="#b37">(Kingma et al., 2016)</ref> and the bidirectional inference<ref type="bibr" target="#b47">(Maaløe et al., 2019)</ref>, which benefit more from parameter sharing, for future work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>CW would like to thank Matt Hoffman for a discussion on deterministic Langevin transitions and Amirhossein Taghvaei for referencing the work of Wang &amp; Li. Special thanks to people who have provided their feedback and advice during discussion or while reviewing the manuscript, including Valentin Thomas, Joey Bose, and Eeshan Dhekane; to Taoli Cheng and Bogdan Mazoure for volunteering for the internal review at Mila; and to Ahmed Touati, Christos Tsirigotis and Jose Gallego for proofreading parts of the proof.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where K = max{K, K 3 + 2}. Summing d x 1 , ..., d x n and subtracting d x 1 + ... + d x n−1 from both sides yield</p><p>Note that d x 0 = 0. Similarly, summing d e 2 , ..., d e n and subtracting d e 2 + ... + d e n−1 from both sides yield</p><p>To recursively express d x n in terms of itself (except for d e 1 ), we sum over the sequence d e 1 , ..., d e n again n t=1</p><p>Since n ≤ N , n t=1 t ≤ n 2 , d e 1 ≤ 2 and = T 2N , the above can be rearranged and further bounded by</p><p>The same can be done for (24) to analyze d e n . </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An auxiliary variational method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">V</forename><surname>Agakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="561" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep variational inference without pixel-wise reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dukkipati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05209</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semi-conditional normalizing flows for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Volokhova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sosnovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00505</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01071</idno>
		<title level="m">Hierarchical adversarially learned inference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sylvester normalizing flows for variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Importance weighted autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Residual flows for invertible generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational lossy autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pixelsnail: An improved autoregressive generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cornish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Caterini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deligiannidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doucet</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13833</idno>
		<title level="m">A. Localised generative flows</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inference suboptimality in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of control, signals and systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Calibrating energy-based generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nice</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<title level="m">Non-linear independent components estimation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Augmented neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Early stopping as nonparametric variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1070" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards a neural statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An introduction to numerical methods and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Epperson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2214" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betterncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A latent variable model for natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pixelvae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flow++</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Langevin dynamics as nonparametric variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Emerging convolutions for generative normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learnable explicit density for continuous latent space and variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Touati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02248</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural autoregressive flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Touati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04282</idno>
		<title level="m">Stochastic neural network with kronecker flow</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Invertible convolutional flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duckworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5636" to="5646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09257</idno>
		<title level="m">Normalizing flows: Introduction and ideas</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courville</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04759</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">A. Bayesian hypernetworks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalizing hamiltonian monte carlo with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multiplicative normalizing flows for variational bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2218" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unbiased estimation of log marginal probability for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Masked convolutional generative flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5891" to="5900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Biva: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liévin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
