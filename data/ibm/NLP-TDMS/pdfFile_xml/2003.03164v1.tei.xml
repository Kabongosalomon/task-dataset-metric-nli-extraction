<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A successful point cloud registration often lies on robust establishment of sparse matches through discriminative 3D local features. Despite the fast evolution of learning-based 3D feature descriptors, little attention has been drawn to the learning of 3D feature detectors, even less for a joint learning of the two tasks. In this paper, we leverage a 3D fully convolutional network for 3D point clouds, and propose a novel and practical learning mechanism that densely predicts both a detection score and a description feature for each 3D point. In particular, we propose a keypoint selection strategy that overcomes the inherent density variations of 3D point clouds, and further propose a self-supervised detector loss guided by the on-the-fly feature matching results during training. Finally, our method achieves state-ofthe-art results in both indoor and outdoor scenarios, evaluated on 3DMatch and KITTI datasets, and shows its strong generalization ability on the ETH dataset. Towards practical use, we show that by adopting a reliable feature detector, sampling a smaller number of features is sufficient to achieve accurate and fast point cloud alignment. [code release]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Point cloud registration aims to find an optimal transformation between two partially overlapped point cloud fragments, which is a fundamental task in applications such as simultaneous localization and mapping (SLAM) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref> and 3D Lidar-based mapping <ref type="bibr" target="#b30">[31]</ref>. In above contexts, the local keypoint detection and description serve as two keys for obtaining robust point cloud alignment results.</p><p>The recent research on 3D local feature descriptors has shifted to learning-based approaches. However, due to the difficulty of acquiring ground-truth data, most existing works often overlook the keypoint detection learning in point cloud matching, and instead randomly sample a set of points for feature description. Apparently, this strategy might suffer from several drawbacks. First, the randomly sampled points are often poorly localized, result-ing in inaccurate transformation estimates during geometric verification such as RANSAC <ref type="bibr" target="#b8">[9]</ref>. Second, those random points might appear in non-salient regions like smooth surfaces, which may lead to indiscriminative descriptors that adversely introduce noise in later matching steps. Third, to obtain a full scene coverage, an oversampling of random points is required that considerably decreases the efficiency of the whole matching process. In essence, we argue that a small number of keypoints suffice to align point clouds successfully, and well-localized keypoints can further improve the registration accuracy. The imbalance of detector and descriptor learning motivates us to learn these two tightly coupled components jointly.</p><p>However, the learning-based 3D keypoint detector has not received much attention in previous studies. One attempt made by 3DFeat-Net <ref type="bibr" target="#b13">[14]</ref> predicts a patch-wise detection score, whereas only limited spatial context is considered and a dense inference for the entire point cloud is not applicable in practice. Another recent work USIP <ref type="bibr" target="#b15">[16]</ref> adopts an unsupervised learning scheme that encourages keypoints to be covariant under arbitrary transformations. However, without a joint learning of detection and description, the resulting detector might not match the capability of the descriptor, thus preventing the release of the potential for a fully learned 3D feature. Instead, in this paper, we seek for a joint learning framework that is able to not only predict keypoints densely, but also tightly couple the detector with a descriptor with shared weights for fast inference.</p><p>To this end, we draw inspiration from D2-Net <ref type="bibr" target="#b6">[7]</ref> in 2D domain for a joint learning of a feature detector and descriptor. However, the extension of D2-Net for 3D point clouds is non-trivial. First, a network that allows for dense feature prediction in 3D is needed instead of previous patchbased architectures. In this work, we resort to KPConv <ref type="bibr" target="#b32">[33]</ref>, a newly proposed convolutional operation on 3D point clouds, to build a fully convolutional network to consume an unstructured 3D point cloud directly. Second, we adapt D2-Net to handle the inherent density variations of 3D point clouds, which is the key to achieve highly repeatable keypoints in 3D domain. Third, observing that the original loss in D2-Net does not guarantee convergence in our context, we propose a novel self-supervised detector loss guided by the on-the-fly feature matching results during training, so as to encourage the detection scores to be consistent with the reliability of predicted keypoints. To summarize, our contributions are threefold:</p><p>1. We leverage a fully convolutional network based on KPConv, and adopt a joint learning framework for 3D local feature detection and description, without constructing dual structures, for fast inference. 2. We propose a novel density-invariant keypoint selection strategy, which is the key to obtaining repeatable keypoints for 3D point clouds. <ref type="bibr" target="#b2">3</ref>. We propose a self-supervised detector loss that receives meaningful guidance from the on-the-fly feature matching results during training, which guarantees the convergence of tightly coupled descriptor and detector.</p><p>We demonstrate the superiority of our method over the state-of-the-art methods by conducting extensive experiments on both 3DMatch of indoor settings, and KITTI, ETH of outdoor settings. To our best knowledge, we are the first to handle the Dense Detection and Description of 3D local Features for 3D point clouds in a joint learning framework. We refer to our approach as D3Feat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">3D Local Descriptors</head><p>Early approaches to extract 3D local descriptors are mainly hand-crafted <ref type="bibr" target="#b11">[12]</ref>, which generally lack robustness against noise and occlusion. To address this, recent studies on 3D descriptors have shifted to learning-based approaches, which is the main focus of our paper. Patch-based networks. Most existing learned local descriptors require point cloud patches as input. Several 3D data representations have been proposed for learning local geometric features in 3D data. Early attempts like <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b37">38]</ref> use multi-view image representation for descriptor learning. Zeng et al. <ref type="bibr" target="#b35">[36]</ref> and Gojcic et al. <ref type="bibr" target="#b10">[11]</ref> convert 3D patches into a voxel grid of truncated distance function (TDF) values and smoothed density value (SDV) representation respectively. Deng et al. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref> build their network upon Point-Net to directly consume unordered point sets. Such patchbased methods suffer from efficiency problem as the intermediate network activations are not reused across adjacent patches, thus severely limits their usage in applications that requires high resolutional output. Fully-convolutional networks. Although fully convolutional networks introduced by Long et al. <ref type="bibr" target="#b16">[17]</ref> have been widely used in the 2D image domain, it has not been extensively explored in the context of 3D local descriptor. Fully convolutional geometric feature (FCGF) <ref type="bibr" target="#b1">[2]</ref> is the first to adopt a fully convolutional setting for dense feature description on point clouds. It uses sparse convolution proposed in <ref type="bibr" target="#b0">[1]</ref> to extract feature descriptors and achieves rotation invariance by simple data augmentation. However, their method does not handle keypoint detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">3D Keypoint Detector</head><p>Unlike the exploration of learning-based 3D local descriptors, most existing methods for 3D keypoint detection are hand-crafted. A comprehensive review of such methods can be found in <ref type="bibr" target="#b33">[34]</ref>. The common trait among hand-crafted approaches is their reliance on local geometric properties of point clouds. Therefore, severe performance degradation occurs when such detectors are applied to real-world 3D scan data where noise and occlusion commonly exist. To make the detector learnable, Unsupervised Stable Interest Point (USIP) <ref type="bibr" target="#b15">[16]</ref> proposes an unsupervised method to learn keypoint detection. However, USIP is unable to densely predict the detection scores and its network has the risk to be degenerate if the desired keypoint number is small. In contrast, our network is able to predict dense detection scores without having the risk of degeneracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Joint Learned Descriptor and Detector</head><p>In 2D image matching, several works have tackled the keypoint detection and description problems jointly <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19]</ref>. However, adapting these methods to the 3D domain is challenging and less explored. As we know, 3DFeat-Net <ref type="bibr" target="#b13">[14]</ref> is the only work that attempts to jointly learn the keypoint detector and descriptor for 3D point clouds. However, their method focuses more on learning the feature descriptor with an attention layer estimating the salience of each point patches as its by-product, and thus the performance of their keypoint detector is not guaranteed. Besides, their method takes point patches as input, which is inefficient as addressed before. In contrast, we aim to detect keypoint locations and extract per-point features in a single forward pass for efficient usage. Specifically, our proposed network serves a dual role by fusing the detector and descriptor network into a single one, thus saving memory and computation consumption by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Joint Detection and Description Pipeline</head><p>Inspired by D2-Net, a recent approach proposed by Dusmanu et al. for 2D image matching <ref type="bibr" target="#b6">[7]</ref>, instead of training separate networks for keypoint detection and description, we design a single neural network that plays a dual role: a dense feature descriptor and a feature detector. However, adapting the idea of D2-Net to the 3D domain is non-trivial because of the irregular nature and varying sparsity of point clouds. In the following, we will first describe the fundamental steps to perform feature description and detection on irregular 3D point clouds, and then explain our strategy of dealing with the sparsity variance in the 3D domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dense Feature Description</head><p>To address the issue of convolution on irregular point clouds and better capture the local geometry information, Thomas et al. proposed the Kernel Point Convolution (KP-Conv) <ref type="bibr" target="#b32">[33]</ref>, which uses kernel points carrying convolution weights to emulate the kernel pixels in 2D convolution, and then defines the convolution operation on the raw point clouds. We adpot KPConv as our backbone network to perform dense feature extraction. Below we first briefly review the formulas of KPConv.</p><p>Given a set of points P ∈ R N ×3 and a set of features F in ∈ R N ×Din represented in a matrix form, let x i and f i denote the i-th point in P and its corresponding feature in F in , respectively. The general convolution by kernel g at point x is defined as</p><formula xml:id="formula_0">(F in * g) = xi∈Nx g(x i − x)f i ,<label>(1)</label></formula><p>where N x is the radius neighborhood of point x, and x i is a supporting point in this neighborhood. The kernel function is defined as</p><formula xml:id="formula_1">g(x i − x) = K k=1 h(x i − x,x k )W k ,<label>(2)</label></formula><p>where h is the correlation function between the kernel point x k and the supporting point x i , W k is the weight matrix of the kernel pointx k , and K is the number of kernel points. We refer readers to the original paper <ref type="bibr" target="#b32">[33]</ref> for more details.</p><p>The original formulation of KPConv is not invariant to point density. Thus we add a density normalization term, which sums up the number of supporting points in the neighborhood of x, to Equation 1 to ensure that convolution is sparsity invariant:</p><formula xml:id="formula_2">(F in * g) = 1 |N x | xi∈Nx g(x i − x)f i .<label>(3)</label></formula><p>Based on the normalized kernel point convolution, we adopt a UNet-like structure with skip connections and resid-ual blocks to build a fully convolutional network <ref type="bibr" target="#b16">[17]</ref>, as illustrated in <ref type="figure" target="#fig_0">Fig. 1 (Left)</ref>.</p><p>Unlike patch-based methods which only support sparse feature description, our network is able to perform dense feature description under a fully convolutional setting. The output of our network is a dense feature map in the form of a two-dimensional matrix F ∈ R N ×c , where c is the dimension of the feature vector. The descriptor associated with point x i is denoted as d i ,</p><formula xml:id="formula_3">d i = F i: , d i ∈ R c ,<label>(4)</label></formula><p>where F i: denotes the i-th row of two-dimensional matrix F . The descriptors are L2-normalized to unit length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dense Keypoint Detection</head><p>Dusmanu et al. <ref type="bibr" target="#b6">[7]</ref> detect keypoints on 2D images based on the local maximum across the spatial and channel dimensions of the feature maps, and use a softmax operator to evaluate the local-max score of a pixel. Due to the regular structure of images, their approach simply selects the neighboring pixels as the neighborhood. To extend their approach to 3D, this strategy might be replaced by radius neighborhood to handle the non-uniform sampling setting of point clouds. However, the number of neighboring points in the radius neighborhood can vary greatly. In this case, if we simply use a softmax function to evaluate the local maximum in the spatial dimension, the local regions with few points (e.g., regions close to the boundaries of indoor scenes or far away from Lidar center of outdoor scenes) would inherently have higher scores. To handle this problem, we propose a density-invariant saliency score to evaluate the saliency of a certain point compared with its local neighborhood.</p><p>Given the dense feature map F ∈ R N ×c , we regard F as a collection of 3D response D k (k = 1, ..., c):</p><formula xml:id="formula_4">D k = F :k , D k ∈ R N ,<label>(5)</label></formula><p>where F :k denotes the k-th column of the two-dimensional matrix F . The criterion of point x i to be a keypoint is</p><formula xml:id="formula_5">x i is a keypoint ⇐⇒ k = arg max t D t i and i = arg max j∈Nx i D k j ,<label>(6)</label></formula><p>where N xi is the radius neighborhood of x i . This means that the most preeminent channel is first selected, and then verified by whether or not it is a maximum of its spatial local neighborhood on that particular response map D k . During training, we soften the above process to make it trainable by applying two scores, as illustrated in <ref type="figure" target="#fig_0">Fig. 1 (Right)</ref>. The details are given below. Density-invariant saliency score. This score aims to evaluate how salient a point is compared with other points in its local neighborhood. In D2-Net <ref type="bibr" target="#b6">[7]</ref>, the score evaluating the local-max is defined as</p><formula xml:id="formula_6">α k i = exp(D k i ) xj ∈Nx i exp(D k j ) .<label>(7)</label></formula><p>This formulation, however, is not invariant to sparsity. Sparse regions inherently have higher scores than dense areas because the scores are normalized by sum. We therefore design a density-invariant saliency score as follows:</p><formula xml:id="formula_7">α k i = ln(1 + exp(D k i − 1 |N xi | xj ∈Nx i D k j )).<label>(8)</label></formula><p>In this formulation, the saliency score of a point is calculated as the difference between its feature and the mean feature of its local neighborhood. Thus it measures the relative saliency of a center point with respect to that of the supporting points in the local region. Besides, using the average response in place of sum (c.f., Equation 7) prevents the score from being affected by the number of the points in the neighborhood. In our experiments (Section 6.4), we will show that our saliency score significantly improves the network's ability to handle point cloud keypoint detection with varying density. Channel max score. This score is designed to pick up the most preeminent channel for each point:</p><formula xml:id="formula_8">β k i = D k i max t (D t i ) .<label>(9)</label></formula><p>Finally, both of the scores are taken into account for the final keypoint detection score:</p><formula xml:id="formula_9">s i = max k (α k i β k i ).<label>(10)</label></formula><p>Thus after we obtain the keypoint score map of an input point cloud, we select points with top scores as keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Joint Optimizating Detection &amp; Description</head><p>Designing a proper supervision signal is the key to joint learning of a descriptor and a detector. In this section, We will first describe the metric learning loss for the descriptor, and then design a detector loss from a self-supervised perspective. Descriptor loss. To optimize the descriptor network, many works attempt to use metric learning strategies, like contrastive loss and triplet loss. We will utilize the contrastive loss, since from our experiments we have found it to give better convergence performance. As for the sampling strategy, we adopt the hardest in batch strategy proposed in <ref type="bibr" target="#b21">[22]</ref> to make the network focus on hard pairs. Given a pair of partially overlapped point cloud fragments P and Q, and a set of n pairs of corresponding 3D points. Suppose (A i , B i ) is a correspondence pair and the two points have their corresponding descriptors d Ai and d Bi and scores s Ai and s Bi . The distance between a positive pair is defined as the Euclidean distance between their descriptors as follows:</p><formula xml:id="formula_10">d pos (i) = ||d Ai − d Bi || 2 .<label>(11)</label></formula><p>The distance between a negative pair is,</p><formula xml:id="formula_11">d neg (i) = min{||d Ai − d Bj || 2 } s.t.||B j − B i || 2 &gt; R,<label>(12)</label></formula><p>where R is the safe radius, and B j is the hardest negative sample that lies outside the safe radius of the true correspondences. The contrastive margin loss is defined as</p><formula xml:id="formula_12">L desc = 1 n i max(0, d pos (i) − M pos ) + max(0, M neg − d neg (i)) ,<label>(13)</label></formula><p>where M pos is the margin for positive pairs and M neg is the margin for negative pairs. Detector loss. To optimize the detector network, we seek for a loss formulation that encourages the easily matchable correspondences to have higher keypoint detection scores than the correspondences which are hard to match. In <ref type="bibr" target="#b6">[7]</ref>, Dusmanu et al. proposed an extension to the triplet margin loss to jointly optimize the descriptor and the detector:</p><formula xml:id="formula_13">L det = i s Ai s Bi i s Ai s Bi max(0, M + d pos (i) 2 − d neg (i) 2 ),<label>(14)</label></formula><p>where M is the triplet margin. They claimed that in order to minimize the loss, the detector network should predict high scores for most discriminative correspondences and vice versa. However, their loss does not provide explicit guidance for the score term, and experimentally we found that their origin loss formulation does not guarantee convergence in our context. Thus we design a loss term to explicitly guide the gradient of the scores. From a self-supervised perspective, we use the on-the-fly feature matching results to evaluate the discriminativeness of each correspondence, which will guide the gradient flow of the score of each keypoint. If a correspondence is already matchable under the current descriptor network, we want the score to be higher and vice versa. Specifically, we define the detector loss as</p><formula xml:id="formula_14">L det = 1 n i (d pos (i) − d neg (i))(s Ai + s Bi ) . (15)</formula><p>Intuitively, if d pos (i) − d neg (i) &lt; 0, it indicates that this correspondence can be correctly matched using the nearest neighbor search, and the loss term will encourage the scores s Ai and s Bi of the two points in the correspondence pair to be higher. Conversely, if d pos (i) − d neg (i) &gt; 0, the correspondence is not discriminative enough for the current descriptor network to establish a correspondence, and thus the loss will encourage the scores s Ai and s Bi to be lower. In order to minimize the loss, the detector should predict higher scores for matchable correspondences and lower scores for non-matchable correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>Training. Our network is implemented in TensorFlow.</p><p>During training, we use all the point cloud fragment pairs with more than 30% overlap. For each pair of fragments P and Q, we establish the correspondence set by first randomly sampling n anchor points from P , then applying the ground-truth transformation to these points, and performing nearest neighbor search in fragment Q. The correspondence is accepted only if the Euclidean distance between two points is less than a threshold. We use grid subsampling to control the number of points and ensure the spatial consistency of point clouds.</p><p>For the first layer, we use 0.03m as our grid size. The neighborhood radius is 2.5 times the grid size of the current layer. The local neighborhood for keypoint detection is the same as the first radius neighborhood. For each point cloud fragment, we apply data augmentation including adding Gaussian noise with standard deviation 0.005, random scaling ∈ [0.9, 1.1], and random rotation angle ∈ [0 • , 360 • ) around an arbitrary axis. We use a batch size of 1 and correspondence number n = 64. We use positive margin M pos = 0.1, negative margin M neg = 1.4 for the contrastive loss. The loss terms for the detector and the descriptor are equally weighted. We optimize the network using the Momentum optimizer with an initial learning rate of 0.1, which is exponentially decayed every epoch, and the momentum is set to 0.98. The network converges in about 100 epochs. Testing. During testing, our keypoint detection module adopts the hard selection strategy formulated by <ref type="bibr">Equation 6</ref> instead of soft selection to get better quality keypoints. This strategy emulates non-maximum suppression and avoids having the selected keypoints lie too close to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>The following sections are organized as follows. First, we demonstrate our method (D3Feat) regarding point cloud registration on 3DMatch dataset <ref type="bibr" target="#b35">[36]</ref> (indoor settings) and KITTI <ref type="bibr" target="#b9">[10]</ref> dataset (outdoor settings), with the provided data split to train D3Feat. Next, we study the generalization ability of D3Feat on ETH dataset <ref type="bibr" target="#b24">[25]</ref> (outdoor settings), using the model trained on 3DMatch dataset. Finally, we more specifically evaluate the keypoint repeatability on 3DMatch and KITTI datasets, to clearly demonstrate the performance of our detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Indoor Settings: 3DMatch dataset</head><p>We follow the same protocols in 3DMatch dataset <ref type="bibr" target="#b35">[36]</ref> to prepare the training and testing data. In particular, the test set contains 8 scenes with partially overlapped point cloud fragments and their corresponding transformation matrices. For each fragment, 5000 randomly sampled 3D points are given for methods that do not include a detector.</p><p>Evaluation metrics. Following <ref type="bibr" target="#b1">[2]</ref>, two evaluation metrics are used including 1) Feature Matching Recall, a.k.a. the percentage of successful alignment whose inlier ratio is above some threshold (i.e., τ 2 = 5%), which measures the matching quality during pairwise registration. 2) Registration Recall, a.k.a. the percentage of successful alignment whose transformation error is below some threshold (i.e., RMSE &lt; 0.2m), which better reflects the final performance in practice use. Besides, the intermediate result, Inlier Ratio, is also reported. In above metrics 1) and 3), a match is considered as an inlier if the distance between its corresponding points is smaller than τ 1 = 10cm under groundtruth transformation. For metric 2), we use a RANSAC with 50,000 max iterations (following <ref type="bibr" target="#b35">[36]</ref>) to estimate the transformation matrics.</p><p>Comparisons with the state-of-the-arts. We compare D3Feat with the state-of-the-arts on 3DMatch dataset. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the Feature Matching Recall is reported regarding two levels of data difficulty: the original point cloud fragments (left columns), and the randomly rotated fragments (right columns). In terms of D3Feat, we report the results both on 5000 randomly sampled points (D3Feat(rand)) and on 5000 keypoints predicted by our detector network (D3Feat(pred)). In both settings, D3Feat achieves the best performance when a learned detector is equipped.</p><p>Moreover, we demonstrate the robustness of D3Feat by varying the error threshold (originally defined as τ 1 = 10cm and τ 2 = 5%) in Feature Matching Recall. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we observe that the D3Feat performs consistently well under all conditions. In particular, D3Feat notably outperforms other methods (left figure) under a more strict inlier ratio threshold, e.g., τ 2 = 20%, which indicates the advantage of adopting a detector to derive more distinctive features. In terms of varying τ 1 , D3Feat performs slightly worse than PerfectMatch <ref type="bibr" target="#b10">[11]</ref> when smaller inlier distance error is tolerated, which can be probably ascribed to the small voxel size (1.875cm) used in PerfectMatch, while D3Feat is performed using 3cm voxel downsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Origin</head><p>Rotated FMR (%) STD FMR (%) STD FPFH <ref type="bibr" target="#b27">[28]</ref> 35   Performance under different number of keypoints. To better demonstrate the superiority of a joint learning with a detector, we further report the results when reducing the sampled point number from 5000 to 2500, 1000, 500 or even 250. As shown in <ref type="table" target="#tab_3">Table 2</ref>, when no detector is equipped, the performance of PerfectMatch, FCGF or D3Feat(rand) drops at a similar magnitude as the number of sampled points get smaller. However, once enabling the proposed detector (D3Feat(pred)), our method is able to maintain a high matching quality regarding all evaluation metrics, and outperform all comparative methods by a large margin.</p><p>It is also noteworthy that, regarding Inlier Ratio, D3Feat(pred) is the only method that achieves improved results when smaller number of points is considered. This  strongly indicates that the detected keypoints have been properly ranked, and the top points receive higher probability to be matched, which is a desired property that a reliable kepoint is expected to acquire.</p><p>Rotation invariance. Previous works such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref> use sophisticated input representations or per-point local reference frames to acquire rotation invariance. However, as also observed in FCGF <ref type="bibr" target="#b1">[2]</ref>, we find that a fully convolutional network (e.g., KPConv <ref type="bibr" target="#b32">[33]</ref> as used in this paper) is able to empirically achieve strong rotation invariance through lowcost data augmentation, as shown in the right columns of <ref type="table" target="#tab_1">Table 1</ref>. We will provide more details about this experiment in supplementary materials.</p><p>Ablation study. To study the effect of each component of D3Feat, we conduct thorough ablative experiments on 3DMatch dataset. Specifically, to address the importance of the proposed detector loss, we compare 1) the model trained with the original D2-Net loss (D2 Triplet) and 2) the model trained with the improved D2-Net loss (D2 Contrastive). To demonstrate the benefit from a joint learning of two tasks, we use only the contrastive loss to train a model without the detector learning (w/o detector). Other training or testing settings are kept the same for a fair comparison. As shown in <ref type="table" target="#tab_3">Table 2</ref>, the proposed detector loss (D3Feat(rand)) not only guarantees better convergence than D2 Triplet, but also delivers a notable performance boost over D2 Contrastive. Besides, by comparing w/o detector and D3Feat(rand), it can be seen that a joint learning of a detector is also advantageous to strengthen the descriptor itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Outdoor Settings: KITTI dataset</head><p>KITTI odometry dataset comprises 11 image sequences of outdoor driving scenarios for point cloud registration. Following the data splitting method in FCGF <ref type="bibr" target="#b1">[2]</ref>, we use sequences 0 to 5 for training, 6 to 7 for validation, and 8 to 10 for testing. The ground truth transformations are provided by GPS. To reduce the noise in ground truth, we use the standard iterative closest point (ICP) method to refine the alignment. Besides, only pairs which are at least 10m away from each other are selected. Finally, we train D3Feat with 30cm voxel downsampling for preprocessing. Evaluation metrics. We evaluate the estimated transformation matrices by two error metrics: Relative Translation Error (RTE) and Relative Rotation Error (RRE) proposed in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>. The registration is considered accurate if the RTE is below 2m and RRE is below 5 • following <ref type="bibr" target="#b13">[14]</ref>.</p><p>Comparisions to the state-of-the-arts. We compare D3Feat with FCGF <ref type="bibr" target="#b1">[2]</ref> and 3DFeat-Net <ref type="bibr" target="#b13">[14]</ref>. For 3DFeat-Net, we report the results as presented in <ref type="bibr" target="#b1">[2]</ref>. For FCGF, we use the authors' implementation trained on KITTI. To compute the transformation, we use RANSAC with 50,000 max iterations. As shown in <ref type="table" target="#tab_5">Table 3</ref>, D3Feat outperforms the state-of-the-arts regarding all metrics. Besides, we also show the registration results under different numbers of points in <ref type="table" target="#tab_6">Table 4</ref>. In most cases, D3Feat fails to align only one pair of fragments. Even using only 250 points, D3Feat still achieves 99.63% success rate, while FCGF drops to 68.62% due to the lack of a reliable detector.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RTE(cm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Outdoor settings: ETH Dataset</head><p>In order to evaluate the generalization ability of D3Feat, we use the model trained on 3DMatch dataset and test it on four outdoor laser scan datasets (Gazebo-Summer, Gazebo-Winter, Wood-Summer and Wood-Autumn) from the ETH dataset <ref type="bibr" target="#b24">[25]</ref>, following the protocols defined in <ref type="bibr" target="#b10">[11]</ref>. The evaluation metric is again Feature Matching Recall under the same settings as in previous evaluations. For comparisons, we use the PerfectMatch model with 6.25cm voxel size (16 voxels per 1m), while for FCGF, we use the model trained on 5cm voxel size which we find perform significantly better than the model with 2.5cm on this dataset. For D3Feat, we report the results on both 6.25cm and 5cm to compare with PerfectMatch and FCGF, respectively.  For a fair comparison, the pre-sampled points provided in <ref type="bibr" target="#b10">[11]</ref> are used for PerfectMatch, FCGF and D3Feat(rand) to extract the features. As shown in <ref type="table" target="#tab_8">Table 5</ref>, under 5cm voxel size setting, D3Feat demonstrates better generalization ability than FCGF. Once the detector is enabled, the results are improved remarkably. However, on this dataset, PerfectMatch is still the best performing method, whose generalization ability can be mainly ascribed to the use of smoothed density value (SDV) representation, as explained in the original paper <ref type="bibr" target="#b10">[11]</ref>. Nevertheless, by comparing the results of D3Feat(rand) and D3Feat(pred), we can still find the significant improvement brought by our detector. The visualization results on ETH can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Keypoint Repeatability</head><p>Besides the reliability, a keypoint is also desired to be repeatable. Thus we compare our detector in repeatability with a learning-based method USIP <ref type="bibr" target="#b15">[16]</ref> and hand-crafted methods: ISS <ref type="bibr" target="#b36">[37]</ref>, Harris-3D <ref type="bibr" target="#b12">[13]</ref>, and SIFT-3D <ref type="bibr" target="#b17">[18]</ref> on the 3DMatch test set and KITTI test set. Evaluation metric. We use the relative repeatability proposed in <ref type="bibr" target="#b15">[16]</ref> as the evaluation metric. Specifically, given two partially overlapped point clouds and the ground truth pose, a keypoint in the first point cloud is considered repeatable if its distance to the nearest keypoint in the second point cloud is less than some threshold, under the ground truth transformation. Next, the relative repeatability is calculated as the number of repeatable keypoints over the number of detected keypoints. This threshold is set to 0.1m and 0.5m for 3DMatch and KITTI datasets, respectively. Implementation. We compare the full model D3Feat, and a baseline model that directly extends the D2-Net design to 3D domain (denoted as D3Feat(base)). We use PCL <ref type="bibr" target="#b26">[27]</ref> to implement the classical detectors. For USIP, we take the origin implementation as well as the pretrained model to test on the 3DMatch test set. For the KITTI dataset, since USIP and D3Feat use different processing and splitting strategies and USIP requires surface normal and curvature as input, the results are not directly comparable. Nevertheless, for the sake of completeness, we run and test the original implementation of USIP on our processed KITTI data. For each detector, we generate 4, 8, 16, 32, 64, 128, 256, 512 keypoints and calculate the relative repeatability respectively.</p><p>Ablation study. To recap, direct extension from D2-Net to 3D domain brings the problem that points located in more sparse region would have higher probability of being selected as keypoints, which prevents the network from predicting highly repeatable keypoints, as explained in Sec. 3. The proposed density-invariant selection strategy enables the network to handle the inherent density variations of point clouds. In the following, we will demonstrate the effect of proposed keypoint selection strategy through both qualitative and quantitative results.</p><p>Qualitative results. The relative repeatability in relation to the number of keypoints is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. Thanks to the proposed self-supervised detector loss, our detector is encouraged to give higher scores to points with distinctive local geometry while giving lower scores to points with homogeneous neighborhood. Therefore although we do not explicitly supervise the repeatability of keypoints, D3Feat can still achieve comparable repeatability to USIP. D3Feat generally outperforms all the other detectors on 3DMatch and KITTI dataset over all different number of keypoints except worse than USIP on KITTI. In addition, the proposed saliency score significantly improves the repeatability of D3Feat over D3Feat(base), indicating the effectiveness of the proposed keypoint selection strategy.  Qualitative results. As shown in <ref type="figure" target="#fig_4">Fig. 4 and Fig. 5</ref>, the proposed density-invariant selection strategy overcomes the density variations. More qualitative results are included in the supplementary materials.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper we have designed a dual-role fully convolutional network for dense feature detection and description on point clouds. A novel density-invariant saliency score has been proposed to help select keypoints under varying densities. Also a self-supervised detector loss has been proposed to guide the network to predict highly repeatable keypoints and enable joint improvement for both detector and descriptor. Extensive experiments on indoor 3DMatch and outdoor KITTI both show the effectiveness of our detector network and the distinctiveness of descriptor network. Our model outperforms the state-of-the-art methods especially when using a small number of keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Supplementary Material</head><p>In this supplementary material, we first explain the details about our modification on KPConv <ref type="bibr" target="#b32">[33]</ref>, then we analyze the contribution of rotation augmentation by an ablation study on 3DRotatedMatch dataset. We further conduct the ablative experiments on the detector loss as well as incorporating our detector with FCGF. Finally, we provide additional details about the experiments on 3DMatch, KITTI and ETH datasets and some further visualization results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Implementation Details</head><p>Normalization term As explained in the main paper, the original formulation of KPConv is not invariant to point density. Thus we add a density normalization term, which sums up the number of the supporting points in the neighborhood, to ensure that the convolution is sparsity invariant. To demonstrate the effectiveness of the normalization term, we train networks with and without the normalization term in the same settings with what is described in the main paper, and report the registration results on 3DMatch dataset. During testing, instead of using voxel downsample, we use uniform downsample i.e. uniform down sample in Open3D <ref type="bibr" target="#b38">[39]</ref> implementation, by which the density variations of input point clouds is enlarged. We evaluate the model on sample rate = 15, which leads to a similar average number of points for the point clouds in the test set. The result is shown in <ref type="table" target="#tab_10">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Voxel</head><p>Uniform  As shown in <ref type="table" target="#tab_10">Table 6</ref>, the normalization term is effective to handle neighborhoods with different sparsity levels. When using a uniform downsample strategy, D3Feat with the normalization term is able to maintain a high matching quality, and outperform the model without the normalization term by a large margin.</p><p>Network architecture We adopt the architecture for segmentation tasks proposed in KPConv <ref type="bibr" target="#b32">[33]</ref>. The number of channels in the encoder part are <ref type="bibr">(64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024)</ref>. Skip connections are used between the corresponding layers of the encoder part and the decoder part. The output features are processed by a 1×1 convolution to get the final 32 dimensional features. Other settings are the same with original paper <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Ablation on Rotation Invariance</head><p>In experiments, we find that a fully convolutional network is able to empirically achieve strong rotation invariance through low cost data augmentation. To demonstrate the effectiveness of simple data augmentation on the robustness to rotation, we further train the model without rotation augmentation and evaluate it on 3DRotatedMatch dataset. The result is shown in <ref type="table">Table 7</ref>.  <ref type="table">Table 7</ref>: Feature match recall on 3DRotatedMatch with and without rotation augmentation.</p><p>Without rotation augmentation, D3Feat fails on 3DRo-tatedMatch because the network cannot learn the rotation invariance from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Ablation on Detector Loss</head><p>To better analyze the contribution of the proposed detector loss, we re-create a table below, which derives original results from <ref type="table" target="#tab_3">Table 2</ref>, and the results from our model without the detector loss and performing on the predicted keypoints(w/o detector (pred)). The effect of detector loss can be seen from the comparison between w/o detector and D3Feat on both random (rand) or predicted (pred) points. It is shown that the detector loss not only strengthens the descriptor itself (given random keypoints), but also boosts the performance of the detector (given predicted keypoints). It is also noteworthy that only D3Feat(pred) improves the Inlier Ratio when reducing the number of points, which clearly indicates that the detector loss helps to better rank the keypoints regarding distinctiveness.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Ablation on Detector with FCGF</head><p>Since the detection scores are computed on top of the extracted dense descriptors, it is easy to incorporate our de-tector with other dense feature description models, such as FCGF <ref type="bibr" target="#b1">[2]</ref>. To demonstrate the usability of our method, we train the FCGF with the proposed joint learning method and evaluate it on the 3DMatch dataset, as shown in <ref type="table">Table 9</ref>. The model FCGF + detector is trained using the proposed detector loss under the same setting with <ref type="bibr" target="#b1">[2]</ref>   <ref type="table">Table 9</ref>: Evaluation results of FCGF trained with the proposed detector loss.</p><p>The result shows that FCGF can indeed benefit from the proposed joint learning and maintain a high performance given a smaller number of points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.">Runtime</head><p>To demonstrate the efficiency of our method, we compare the runtime of D3Feat with FCGF [2] on 3DMatch in <ref type="table" target="#tab_1">Table 10</ref>. For a fair comparison, we use the same voxel size (2.5cm, roughly 20k points) with FCGF to measure the runtime of D3Feat including both detection and description. The performance difference mainly lies in the sparse convolution used by FCGF, which is time-consuming in hashing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CPU</head><p>GPU Time(s) FCGF[2] Intel 10-core 3.0GHz(i7-6950)</p><p>Titan-X 0.36</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D3Feat</head><p>Intel 4-core 4.0GHz(i7-4790K) GTX1080 0.13 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6.">Evaluation Metric for 3DMatch</head><p>Feature matching recall Feature matching recall is first proposed in <ref type="bibr" target="#b4">[5]</ref>, which measures the quality of features without using a RANSAC pipeline. Given two partially overlapped point cloud P and Q, and the descriptor network denoted as a non-linear function f mapping from input points to feature descriptors, the correspondence set for the fragments pairs is obtained by mutually nearest neighbor search in feature space,</p><formula xml:id="formula_15">Ω = {p i ∈ P, q j ∈ Q|f (p i ) = nn(f (q j ), f (P )), f (q j ) = nn(f (p i ), f (Q))},<label>(16)</label></formula><p>where nn() denotes the nearest neighbor search based on the Euclidean distance. Finally the feature matching recall is defined as, <ref type="bibr" target="#b16">(17)</ref> where M is the set of point cloud fragment pairs which have more than 30% overlap, and T m is the ground truth transformation between the fragment pair m ∈ M . τ 1 is the inlier distance threshold between a correspondence pair, and τ 2 is the inlier ratio threshold of the fragment pair. Following the setting of <ref type="bibr" target="#b35">[36]</ref>, the correspondence which have less than τ 1 = 10cm euclidean distance between their descriptors are seen as inliers, and the fragment pairs which have more than τ 2 = 5% inlier correspondences will be counted as one match. The evaluation metric is based on the theoretical analysis that RANSAC need k = 55258 iterations to achieve 99.9% confidence of finding at least 3 correspondence with inlier ratio 5%.</p><formula xml:id="formula_16">R = 1 |M | |M | m=1 1 1 |Ω| (i,j)∈Ω 1(||p i −T m q j || &lt; τ 1 ) &gt; τ 2</formula><p>Registration recall Registration recall <ref type="bibr" target="#b35">[36]</ref> measures the quality of features within a reconstruction system, which firstly uses a robust local registration algorithm like RANSAC to estimate the rigid transformation between two point clouds, then calculate the RMSE of the ground truth correspondence under the estimated transformation. The ground truth correspondence set for fragments pair P and Q is given, Ω * = {p * ∈ P, q * ∈ Q}</p><p>then the registration recall is defined as,</p><formula xml:id="formula_18">R = 1 |M | |M | m=1 1 1 |Ω * | (p * ,q * )∈Ω * ||p * −T m q * || 2 &lt; 0.2 ,<label>(19)</label></formula><p>whereT is the transformation matrix estimated by RANSAC. In our experiment, we run a maximum of 50,000 iterations on the initial correspondence set to estimate the transformation between fragments following <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7.">Dataset Preprocessing</head><p>This section provides the steps to process the datasets including 3DMatch, 3DRotatedMatch, KITTI and ETH.</p><p>3DMatch For training set, we follow the steps in <ref type="bibr" target="#b35">[36]</ref> to get fused point cloud fragments and corresponding poses. We find all the fragments pairs that have more than 30% overlap to build the training set. During training, we alternate between selecting the nearby fragment as the corresponding pair, or randomly selecting from all the overlapped fragments for fast convergence. For test set, we directly use the point cloud fragments and ground truth poses provided by the authors without performing any preprocess to extract the dense feature and score map.</p><p>3DRotatedMatch Our model is inherently translation invariant because we are using the relative coordinates. So in order to test the robustness of our model to rotation, we create the 3DRotatedMatch test set following <ref type="bibr" target="#b3">[4]</ref>. We rotate all the fragments in 3DMatch test set along all three axes with random sampled angle from a uniform distribution over [0, 2π).</p><p>KITTI The training set of KITTI odometry dataset contains 11 sequences, we use sequence 0 to 5 for training, sequence 6 to 7 for validation and the last three for testing. Since GPS ground truth is noisy, we first use ICP to refine the alignment and then verify by whether enough correspondence pairs can be found. We select Lidar scan pairs with at least 10m intervals to obtain 1358 pairs for training, 180 pairs for validation and 555 for testing.</p><p>ETH For a fair comparison, we directly use the raw point clouds, the ground-truth transformations along with the overlap ratio provided by the authors of <ref type="bibr" target="#b10">[11]</ref> to extract the features and evaluate the registration results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.8.">Qualitative Visualization</head><p>We show some challenging registration results in <ref type="figure">Fig</ref>     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(Left) The network architecture of D3Feat. Each block indicates a ResNet block using KPConv to replace image convolution. All layers except the last one are followed by batch normalization and ReLU. (Right) Keypoint detection. After dense feature extraction, we calculate the keypoint detection scores by applying saliency score and channel max score. This figure is best viewed with color and zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Feature matching recall in relation to inlier ratio threshold τ 2 (Left) and inlier distance threshold τ 1 (Right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Relative repeatability when different numbers of keypoints are detected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualization on 3DMatch. The first row are detected using D3Feat while the second row are detected using naïve local max score. Points close to the boundaries are marked with black boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visualization on KITTI. The first row are detected using D3Feat while the second row are detected using naïve local max score. Best view with color and zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>- ure 6</head><label>6</label><figDesc>and more visualizations of detected keypoints on 3DMatch, ETH, KITTI inFigure 7,<ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref> respectively.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results on the 3DMatch dataset. The first two columns are input point cloud fragments, and the third column presents the registration results. Best view with color and zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of keypoints on the 3DMatch dataset. Best view with color and zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of keypoints on ETH dataset. Best view with color and zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of keypoints on the KITTI dataset. Best view with color and zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Feature matching recall at τ 1 = 10cm, τ 2 = 5%. FMR and STD indicate the feature matching recall and its standard deviation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Evaluation results on the 3DMatch dataset under different numbers of keypoints.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparisons on the KITTI dataset. The results of 3DFeat-Net are taken from [2]. For RTE and RRE, the lower of the values, the better. 96.57 96.39 96.03 93.69 90.09 68.62 D3Feat 99.81 99.81 99.81 99.81 99.81 99.63</figDesc><table><row><cell>All</cell><cell>5000 2500 1000</cell><cell>500</cell><cell>250</cell></row><row><cell>FCGF [2]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Success rate on the KITTI dataset under different numbers of keypoints.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Feature matching recall at τ 1 = 10cm, τ 2 = 5% on the ETH dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Feature matching recall at τ 1 = 10cm, τ 2 = 5%. Voxel indicates voxel downsample with voxel size = 0.03m, while Uniform indicates uniform downsample with sample rate = 15.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on the proposed detector loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>for 100 epochs.</figDesc><table><row><cell># Keypoints</cell><cell>5000 2500 1000 500 250</cell></row><row><cell></cell><cell>Registration Recall (%)</cell></row><row><cell>FCGF[2]</cell><cell>87.3 85.8 85.8 81.0 73.0</cell></row><row><cell cols="2">FCGF + detector 86.7 87.8 88.3 85.4 81.5</cell></row><row><cell></cell><cell>Inlier Ratio (%)</cell></row><row><cell>FCGF[2]</cell><cell>56.9 54.5 49.1 43.3 34.7</cell></row><row><cell cols="2">FCGF + detector 53.5 53.2 53.6 53.2 51.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Average runtime per fragment on 3DMatch test set.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsuperpoint: End-to-end unsupervised interest point detector and descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kragh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Brodskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karstoft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04011</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ppfnet: Global context aware local features for robust 3d point matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">D2-net: A trainable cnn for joint description and detection of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d point cloud registration for localization using a deep neural network autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of The ACM</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The kitti dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The perfect match: 3d point cloud matching with smoothed densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comprehensive performance evaluation of 3d local feature descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Alvey vision conference</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3dfeat-net: Weakly supervised local 3d features for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning compact geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Usip: Unsupervised stable interest point detection from 3d point clouds. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Distinctive image features from scale-invariant keypoints. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Contextdesc: Local descriptor augmentation with cross-modality context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Geodesc: Learning local descriptors by integrating geometry constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast and accurate registration of structured point clouds with small overlaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Working hard to know your neighbor&apos;s margins: Local descriptor learning loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fastslam: A factored solution to the simultaneous localization and mapping problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Montemerlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wegbreit</surname></persName>
		</author>
		<idno>2002. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lf-net: learning local features from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Challenging data sets for point cloud registration algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Humenberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06195</idno>
		<title level="m">R2d2: Repeatable and reliable detector and descriptor</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">3d is here: Point cloud library (pcl). ICRA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Slam++: Simultaneous localisation and mapping at the level of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shot: Unique signatures of histograms for surface and texture description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Performance evaluation of 3d keypoint detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning local geometric descriptors from rgb-d reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Intrinsic shape signatures: A shape descriptor for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning and matching multi-view descriptors for registration of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Open3d: A modern library for 3d data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
