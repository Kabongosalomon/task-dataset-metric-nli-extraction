<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do Less and Achieve More: Training CNNs for Action Recognition Utilizing Action Images from the Web</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
							<email>shugaoma@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
							<email>sbargal@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
							<email>jmzhang@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
							<email>lsigal@disneyresearch.com</email>
							<affiliation key="aff1">
								<orgName type="department">Disney Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
							<email>sclaroff@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Do Less and Achieve More: Training CNNs for Action Recognition Utilizing Action Images from the Web</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Figure 1: Sample action images from our dataset. Action images on the Web often capture well-framed descriminative poses of the actions they represent. Left to right: Hammer Throw, Body Weight Squats, Jumping Jack, Basketball, Tai Chi, Cricket Shot, Lunges, Still Rings. Utilizing web action images in training CNNs, for all these action classes, results in more than 10% absolute increase in recognition accuracy in videos compared to CNNs trained only on video frames (see Fig. 3).</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, attempts have been made to collect millions of videos to train CNN models for action recognition in videos. However, curating such large-scale video datasets requires immense human labor, and training CNNs on millions of videos demands huge computational resources. In contrast, collecting action images from the Web is much easier and training on images requires much less computation. In addition, labeled web images tend to contain discriminative action poses, which highlight discriminative portions of a video's temporal progression. We explore the question of whether we can utilize web action images to train better CNN models for action recognition in videos. We collect 23.8K manually filtered images from the Web that depict the 101 actions in the UCF101 action video dataset. We show that by utilizing web action images along with videos in training, significant performance boosts of CNN models can be achieved. We then investigate the scalability of the process by leveraging crawled web images (unfiltered) for UCF101 and ActivityNet. We replace 16.2M video frames by 393K unfiltered images and get comparable performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref> show that deep convolutional neural networks (CNNs) are promising for action recognition in videos. However, CNN models typically have millions of parameters <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>, and usually large amounts of training data are needed to avoid overfitting. For this purpose, work is underway to construct datasets consisting of millions of videos <ref type="bibr" target="#b12">[13]</ref>. However, the collection, pre-processing, and annotation of such datasets can require a lot of human effort. Moreover, storing and training on such large amounts of data can consume substantial computational resources.</p><p>In contrast, collecting and processing images from the Web is much easier. For example, one may need to look through all, or most, video frames to annotate the action, but often a single glance is enough to decide on the action in an image. Videos and web images also have complementary characteristics. A video of 100 frames may convey a complete temporal progression of an action. In contrast, 100 web action images may not capture the temporal progression, but do tend to provide more variations in terms of camera viewpoint, background, body part visibility, clothing, etc. Moreover, videos often contain many redundant and uninformative frames, e.g. , standing postures, whereas action images tend to focus on discriminative portions of the action <ref type="figure">(Fig. 1</ref>). This property can further focus the learning, making action images inherently more valuable.</p><p>In this work, we ask the question: Can web action images be leveraged to train better CNN models and to reduce the burden of curating large amounts of training videos?</p><p>This is not a question with an easy yes or no answer. First, web action images are usually photos, such as profes-sional photos, commercial photos, or artistic photos, which can differ significantly from video frames. This can introduce domain shift artifacts between videos and images. Second, adding web action images in training may have different effects for different actions and for different CNN models. Furthermore, the performance improvement as a function of the Web image set size should be studied.</p><p>We start by collecting a large web action image dataset that contains 23.8K images of 101 action classes. Our dataset is more than double the size of the largest previous action image dataset <ref type="bibr" target="#b31">[32]</ref>, both in the number of images and the number of actions. And, to the best of our knowledge, this is the first action image dataset that has one-toone correspondence in action classes with the large-scale action recognition video benchmark dataset, UCF101 <ref type="bibr" target="#b13">[14]</ref>. Images of the dataset are carefully labeled and curated by human annotators; we refer to them as filtered images. Our dataset will be made publicly available for research.</p><p>For a thorough investigation, we train CNN models of different depths and analyze the effect of adding web action images to the training set of video frames for different action classes. We also train and evaluate models with varying numbers of action images to explore marginal gain as a function of the web image set size. We find that by combining web action images with video frames in training, a spatial CNN can achieve an accuracy of 83.5% on UCF101, which is more than 10% absolute improvement over a spatial CNN trained only on videos <ref type="bibr" target="#b20">[21]</ref>. When combining with motion features, we can achieve 91.1% accuracy, which is the highest result reported to-date on UCF101. We also replace videos by images to demonstrate that our performance gains are due to images providing complementary information to that available in videos, and not solely due to additional training data.</p><p>We then further investigate how our approach can be made scalable. We crawl a dataset of web images for UCF101 from the web. These crawled images are not manually labeled; we refer to them as unfiltered images. We compare the performance of filtered and unfiltered images on UCF101. Using more unfiltered images we obtain similar performance to that obtained using fewer filtered images. We also crawl a dataset of web images for ActivityNet <ref type="bibr" target="#b0">[1]</ref>; a larger scale action recognition video dataset. We obtain comparable performance when replacing half the training videos in ActivityNet (which correspond to 16.2M frames) by 393K unfiltered web images. Both crawled datasets will be made publicly available for research.</p><p>In summary, our contributions are:</p><p>• We study the utility of filtered web action images for video-based action recognition using CNNs. By including filtered web action images in training we improve the accuracy of spatial CNN models for action recognition by 10.5%.</p><p>• We study the utility of unfiltered crawled web action images, a more scalable approach, for video-based action recognition using CNNs. We obtain comparable performance when replacing half ActivityNet videos (16.2M frames) with 393K unfiltered web images.</p><p>• We collect the largest web action image dataset todate. This dataset is in one-to-one correspondence with the 101 actions in the UCF101 benchmark. We also collect two crawled action image datasets corresponding to the classes of UCF101 and ActivityNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action recognition is an important research topic for which a large number of methods have been proposed <ref type="bibr" target="#b29">[30]</ref>. Among these, due to promising performance on realistic videos including web videos and movies, bag-of-words approaches that employ expertly-designed local space-time features have been widely used. Some representative works include space-time interest points <ref type="bibr" target="#b15">[16]</ref> and dense trajectories <ref type="bibr" target="#b24">[25]</ref>. Advanced feature encoding methods, e.g. Fisher vector encoding <ref type="bibr" target="#b18">[19]</ref>, can be used to further improve the performance of such methods <ref type="bibr" target="#b25">[26]</ref>. Besides bag-of-words approaches, other works make an effort to explicitly model the space-time structures of human actions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> by using, for example, HCRFs and MRFs.</p><p>CNN models learn discriminative visual features at different granularities, directly from data, which may be advantageous in large-scale problems. CNN models may implicitly capture higher-level structural patterns in the features learned at the last layers of the CNN model. In addition, CNN features may also be used within structured models like HCRFs and MRFs to further improve performance.</p><p>Some recent works propose the use of CNN models for action recognition in videos <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>. Ji et al. <ref type="bibr" target="#b9">[10]</ref> use 3D convolution filters within a CNN model to learn space-time features. Karpathy et al. <ref type="bibr" target="#b12">[13]</ref> construct a video dataset of millions of videos for training CNNs and also evaluate different temporal fusion approaches. Simonyan and Zisserman <ref type="bibr" target="#b21">[22]</ref> use two separate CNN streams: one CNN is trained to model spatial patterns in individual video frames and the other CNN is trained to model the temporal patterns of actions, based on stacks of optical flow. Ng et al. <ref type="bibr" target="#b17">[18]</ref> use a recurrent neural network that has long shortterm memory (LSTM) cells. In all of these works, the CNN models are trained only on videos. Our findings regarding the use of web action images in training may help in further improving the performance of these works.</p><p>Web action images have been used for training non-CNN models for action recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> and event recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref> in videos. Ikizler-Cinbis et al. <ref type="bibr" target="#b8">[9]</ref> use web action images to train linear regression classifiers for small-scale action classification tasks (5 or 8 action classes). Chen et </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Web Action Image Dataset</head><p>To study the usefulness of web action images for learning better CNN models for action recognition, we collect action images that correspond with the 101 action classes in the UCF101 video dataset.</p><p>For each action class, we automatically download images from the Web (Google, Flickr, etc.) using corresponding key phrases, e.g. pushup training for the class pushup, and then manually remove irrelevant images or drawings and cartoons. We also include 2769 images of relevant actions from the Standford40 dataset <ref type="bibr" target="#b31">[32]</ref>. The resulting dataset comprises 23.8K images. Because the images are automatically collected, and then filtered for irrelevant ones, the number of images per category varies. Each class has at least 100 images and most classes have 150-300 images. We will make our dataset publicly available for research. <ref type="table" target="#tab_0">Table 1</ref> compares existing action image datasets with our new dataset. Both in the number of images and the number of actions, our dataset exceeds double the scale of existing datasets. More importantly, to the best of our knowledge, this is the first action image dataset that has one-to-one action class correspondence with a large-scale action recognition benchmark video dataset. We believe that our dataset will enable further study of the relationship between action recognition in videos and in still images.</p><p>UCF101 action classes are divided into five types: Human-Object Interaction, Body-Motion Only, Human-Human Interaction, Playing Musical Instruments, and Sports <ref type="bibr" target="#b22">[23]</ref>. <ref type="figure" target="#fig_0">Fig. 2</ref> shows sample images in our dataset for five action classes, one in each of the five action types. These action images collected from the Web are originally produced in a variety of settings, such as amateur vs. professional photos, artistic vs. educational vs. commercial photos, etc. For images collected in each action category, wide variation can exist in viewpoint, lighting, human pose, body part visibility, and background clutter. For example, commercial photos may have clear backgrounds while backgrounds of amateur photos may contain much more clutter. Such variance also differs for different types of actions. For example, for Sports, there is significant variance in body pose among images that capture different phases of the actions, whereas body pose variance is minimal in images of Playing Musical Instruments.</p><p>Many of the collected action images significantly differ from video frames in camera viewpoint, lighting, human pose, and background. One interesting thing to notice is that action images often capture defining poses of an action that are highly discriminative, e.g. standing with both hands over head and legs spread in jumping jack <ref type="figure" target="#fig_0">(Fig. 2, row 2)</ref>. In contrast, videos may have many frames containing poses that are common to many actions, e.g. in jumping jack the upright standing pose with hands down. Also, n images will have more unique content than n video frames, for example more clothing variation. Clearly there exists a compromise between temporal information available in videos and discriminative poses and variety of unique content in images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training CNNs with Web Action Images</head><p>Spatial CNNs trained on single video frames for action recognition are explored in <ref type="bibr" target="#b20">[21]</ref>. Karpathy et al. <ref type="bibr" target="#b12">[13]</ref> observe that spatio-temporal networks show similar performance compared to spatial models. A spatial CNN effectively classifies actions in individual video frames, and action classification for a video is accomplished via fusion of the spatial CNN's outputs over multiple frames, e.g. via voting or SVM. Because the spatial CNN is trained on single video frames, its parameters can be learned by fine-tuning of a CNN that was trained for a different task, e.g. , using a CNN that is pre-trained on ImageNet <ref type="bibr" target="#b3">[4]</ref>. The fine-tuning approach is especially beneficial in training a CNN model for action classification in videos, since we often only have limited training samples; given the large number of parameters in a CNN, initializing the parameters to random values leads to overfitting and inferior performance as shown in <ref type="bibr" target="#b20">[21]</ref>. In this work, we study improving the spatial CNN for action recognition using web action images as training data in fine-tuning. This is then combined with motion features via state-of-the-art techniques.</p><p>In our experiments and analysis, we explore the following key questions:</p><p>• Is it beneficial to train CNNs with web action images in addition to video frames and, if so, which action classes benefit most?</p><p>• How do different CNN architectures, in particular ones with different depths, perform when web action images are used as additional training data?</p><p>• How do the performance gains change when more web action images are used in training the CNN?</p><p>• Are performance gains solely due to additional training data or also due to a single image being more informative than a randomly sampled video frame?</p><p>• Can we make the procedure of leveraging web images scalable by using crawled (unfiltered) web images rather than manually filtered ones?</p><p>We experiment on three CNN architectures: M2048 <ref type="bibr" target="#b1">[2]</ref>, VGG16, and VGG19 <ref type="bibr" target="#b21">[22]</ref>. To avoid cluttering the discussion, implementation details are provided later in Sec. 5.</p><p>Is adding web images beneficial? Significant performance gains are achieved when we train spatial CNNs using our web action image dataset as auxiliary training data (see <ref type="table" target="#tab_1">Table 2</ref>). For example, with the VGG19 CNN architecture, 5.7% absolute improvement in mean accuracy is achieved. Most encouragingly, such improvements are easy to implement, without the need to introduce additional complexity to the CNN architecture and/or requiring significantly longer training time.</p><p>We further analyze which classes improve the most. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the 25 action classes for which the largest improvement in accuracy is achieved with the three different CNN architectures on UCF101 split1. The 25 action classes of top average accuracy improvement over all three tested architectures are also shown (rightmost column), all of which have no less than 10% absolute increase in accuracy and 10 classes have more than 20% absolute improvement. Some action classes are consistently improved irrespective of the CNN architecture used, such as push ups, YoYo, handstand walking, brushing teeth, jumping jack, etc. This suggests that utilizing web action images in CNN training is widely applicable.</p><p>While classification accuracy improvements in actions that are relatively stationary such as Playing Daf and Brushing Teeth are somewhat expected, it is interesting to see that improvements for actions of fast body motion such as Jumping Jack and Body Weight Squats are also significant.</p><p>Are images benefitial irrespective of CNN depth? While there are numerous ways that CNN architectures may differ from each other, here we focus on one of the most important factors. We evaluate the performance changes for CNNs of different depths when web action images are used in addition to video frames in training. We train spatial CNNs of three depths: 7 layers (M2048), 16 layers (VGG16) and 19 layers (VGG19). These are the prototypical choices of CNN depths in recent works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows the mean accuracy of the three CNN models trained with and without web action images on UCF101 split1. Using web action images in training leads to a consistent 5% ∼ 9% absolute improvement for all three architectures of different depths. This shows the usefulness of web action images and suggests a wide applicability of this approach. Furthermore, our results in action recognition confirm <ref type="bibr" target="#b21">[22]</ref>'s observation that deeper CNNs of 16-19 layers significantly outperform the shallower 7-layer architecture. However, the margin of performance gain diminishes when we increase the depth from 16 to 19. Does adding more web images improve accuracy? We further explore how, for the same CNN architecture, the number of web action images used as additional training data can influence the classification accuracy of the resulting CNN model. We sample 1/10, 1/5, 1/3 and 2/3 of the images of each action in our dataset, and for each sampled set we train the spatial CNN by fine-tuning VGG16 using both the training videos and sampled action images. For each sample size, we repeat the experiment three times, each with a different randomly sampled set of web action images. The evaluation is performed on UCF101 split1. <ref type="figure" target="#fig_2">Fig. 4</ref> summarizes the results of this experiment. The increase in classification accuracy is most significant at the beginning of the curve, i.e. when a few thousand web action images are used in training. This increase continues as more web action images are used, even though the increase becomes slower. Firstly, this indicates that using web action images in training can make a significant difference in performance by providing additional supervision to that provided by video frames. Secondly, it indicates that it is good practice to collect a moderate number of web action images for each action as a cost-effective way to boost model performance (e.g. , 100 ∼ 300 images per action for a dataset of the same scale as UCF101).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do web images complement video frames?</head><p>Although augmenting with images is more efficient than augmenting with videos, we further investigate whether the achieved performance gains are solely due to additional training data or whether a web image provides more information to the learning algorithm than a video frame. This is done by replacing video frames by web images, keeping the total number of training samples constant. For each sample size, we repeat the experiment three times, each with a different randomly sampled set of web action images. The evaluation is performed on UCF101 split1 and a VGG16 model. <ref type="figure">Fig. 5</ref> summarizes the results of this experiment. A consistent improvement in performance is achieved when half the video frames are replaced by web images. The number of training samples (images and video frames) required to obtain the maximum accuracy presented in <ref type="figure" target="#fig_2">Fig. 4</ref> is much <ref type="figure">Figure 5</ref>: Performance of the spatial CNNs (VGG16) trained on UCF101 split1 using video frames only and replacing 50% of the video frames by web images. less (50K vs. 230K). This suggests that images are augmenting the information learnt by the classifier. We posit that discriminative poses in action images may provide implicit supervision, in training, to help learn better discriminative models for classification.</p><p>Can this be made scalable? While we have demonstrated the ability to collect a filtered dataset for our desired classes, this is not scalable. Given a different dataset having the same order of magnitude as UCF101 we would have to manually label a dataset for its classes. Given an even larger dataset with more classes and more samples per class, this becomes very cumbersome although still better than collecting videos. We now investigate the possibility of using crawled (unfiltered) web images for the same purpose. We assume that more images will be required if they are unfiltered, and so we crawl 207K unfiltered images from the Web corresponding to the classes of UCF101. <ref type="table" target="#tab_2">Table 3</ref> summarizes the results of this experiment. The performance of using unfiltered images approaches that of manually filtered images, but the number of web images utilized is much larger. We further investigate whether all the crawled unfiltered images are required to obtain such performance. We do this by randomly selecting one quarter (65.5K) of the 207K unfiltered web images. We select 3 random samples and report the average result in <ref type="table" target="#tab_2">Table 3</ref>. Three quarters of the images only contribute with an additional accuracy of 1%; this is consistent with <ref type="figure" target="#fig_2">Fig. 4</ref> observations.</p><p>Having demonstrated the feasibility of using crawled web images, we now apply this to a larger-scale dataset: Ac-tivityNet <ref type="bibr" target="#b0">[1]</ref>. ActivityNet contains more classes (203) and more samples per class than UCF101. ActivityNet classes are more diverse; they belong to the categories: Personal Care, Eating and Drinking, Household, Caring and Helping, Working, Socializing and Leisure, and Sports and Exercises. "ActivityNet provides samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Using insights from the experiments performed on UCF101 split1 in Section 4, we now perform experiments following the standard evaluation protocol <ref type="bibr" target="#b11">[12]</ref> and report the average accuracy over the three provided splits. We also perform experiments on ActivityNet. Following <ref type="bibr" target="#b0">[1]</ref>, we evaluate classification performance on both trimmed and untrimmed videos. Trimmed videos contain exactly one activity. Untrimmed videos contain one or more activities. We use the mAP (mean average precision) in evaluating performance. Results reported on ActivityNet are produced using the validation data, as the authors are reserving the test data for a potential future challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Experimental Setup for UCF101</head><p>Fine-tuning: We use the Caffe <ref type="bibr" target="#b10">[11]</ref> software for fine-tuning CNNs. We use models VGG16, VGG19 <ref type="bibr" target="#b21">[22]</ref>, and M2048 <ref type="bibr" target="#b1">[2]</ref> that are pre-trained on ImageNet by the corresponding authors. We only test M2048 on the first split for analysis, as it is shown to be significantly inferior to the other two architectures ( <ref type="table" target="#tab_1">Table 2</ref>). Due to hardware limitations, we use a small batch size: 20 for M2048 and 8 for VGG16 and VGG19. Accordingly, we use a smaller learning rate than those used in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. For M2048, the initial learning rate 10 −3 is changed to 10 −4 after 40K iterations; training stops at 80K iterations. For both VGG16 and VGG19, the initial learning rate 10 −4 is changed to 10 −5 after 40K iterations, and is further lowered to 2 × 10 −6 after 80K iterations. Training stops at 100K iterations. Momentum and weight decay coefficients are always set to 0.9 and 5×10 −4 . In each model, all layers are fine-tuned except the last fully connected layer which has to be changed to produce output of 101 dimensions with initial parameter values sampled from a zero-mean Gaussian distribution with σ = 0.01.</p><p>We resize video frames to 256×256, and random crops to 224×224 with random horizontal flipping for training. For web action images, since their aspect ratios vary significantly, we first resize the short dimension to 256 while keeping the aspect ratio, and subsequently crop six 256 × 256 patches along the longer dimension in equal spacing. Random cropping of 224×224 with random horizontal flipping is further applied to these image patches in training. Equal numbers of web images and video frames are sampled in each training batch.</p><p>Video Classification: A video is classified by fusing over the CNN outputs for the individual video frames. For a test video, we select 20 frames of equal temporal spacing. From each of the frames, 10 samples are generated following <ref type="bibr" target="#b14">[15]</ref>: four corners and the center (each is 224×224) are first cropped from the 256×256 frame, making 5 samples; horizontal flipping of these samples makes another 5. Their classification scores are averaged to produce the frame's scores. We classify each frame to the class of the highest score, and the class of the video is then determined by voting of the frames' classes.</p><p>We also test SVM fusion, concatenating the CNN outputs for the 20 frames (averaged over the 10 cropped and flipped samples) from the second fully-connected layer (fc7), i.e. the 15th layer in VGG16 and 18th layer in VGG19. This produces a vector of 81,920 (4096 × 20) dimensions, which is then L2 normalized. One-vs-rest linear SVMs are then trained on these features for video classification. The SVM parameter C = 1 in all experiments.</p><p>Combining with Motion Features: The output of spatial CNNs can be combined with motion features to achieve significantly better performance, as shown in <ref type="bibr" target="#b20">[21]</ref>. We present an alternative by combining the output of the spatial CNNs with the conventional expert-designed features, namely the improved dense trajectories with Fisher encoding (IDT-FV) <ref type="bibr" target="#b25">[26]</ref>. We follow the same settings in <ref type="bibr" target="#b25">[26]</ref> to compute the IDT-FV for each video except that we do not use a spacetime pyramid. The IDT-FV of each video is then combined with the concatenated fc7 outputs of 20 frames to form the final feature vector for a video. One-vs-rest linear SVMs are then trained on these features for video classification. The SVM parameter C = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Experimental Setup for ActivityNet</head><p>We use the Caffe <ref type="bibr" target="#b10">[11]</ref> software for fine-tuning CNNs. We use a VGG19 model <ref type="bibr" target="#b21">[22]</ref> that is pre-trained on ImageNet by the authors. Due to hardware limitations, we use a small batch size of 8. Accordingly, we use a smaller learning rate than <ref type="bibr" target="#b21">[22]</ref>. The initial learning rate 10 −4 is changed to 10 −5 after 80K iterations. Training stops at 160K iterations. Momentum and weight decay coefficients are set to 0.9 and 5 × 10 −4 . All layers are fine-tuned except the last fully con- <ref type="table">Table 4</ref>: Mean accuracy of spatial CNNs (averaged over three splits) on UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy (%) slow fusion CNN <ref type="bibr" target="#b12">[13]</ref> 65.4 spatial CNN <ref type="bibr" target="#b20">[21]</ref> 73  Model Accuracy (%) IDT-FV <ref type="bibr" target="#b25">[26]</ref> 85.9 Two-stream CNN <ref type="bibr" target="#b20">[21]</ref> 88.0 RCNN using LSTM <ref type="bibr" target="#b17">[18]</ref> 88.6 VGG16 + Images + IDT-FV 91.1 VGG19 + Images + IDT-FV 90.8</p><p>nected layer which has to be changed to produce output of 203 dimensions with initial parameter values sampled from a zero-mean Gaussian distribution with σ = 0.01.</p><p>Resizing and cropping of images and frames are performed in the same way as previously described for UCF101. Samples in each training batch are randomly selected from web action images and video frames with equal probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Experimental Results for UCF101</head><p>Here we report the performance of our spatial CNNs averaged over three splits of UCF101 <ref type="table">(Table 4)</ref>, as well as the performance of our models when motion features are also used ( <ref type="table" target="#tab_4">Table 5</ref>).</p><p>As seen in <ref type="table">Table 4</ref>, all our spatial CNNs trained using both videos and images improved ∼10% (absolute) in accuracy over the spatial CNN of <ref type="bibr" target="#b20">[21]</ref>, which is a 7-layer model. We believe this improvement is due to two main factors: using a deeper model and using web action images in training. Comparing the performance of the spatial CNN of <ref type="bibr" target="#b20">[21]</ref> to the deeper models trained only on videos (rows 3 and 6 in <ref type="table">Table 4</ref>), we find that the improvements solely due to differences of CNN architectures are 4.9% and 4.8% for VGG16 and VGG19 respectively. When web action images are used in addition to videos in training (rows 4 and 7 in <ref type="table">Table 4</ref>), these improvements are doubled: 9.5% and 10.3% respectively.  <ref type="table">Table 4</ref> show that, in the models we tested, the simple approach of using web action images in training contributes at least equally with introducing significant complexities to the CNN model, i.e. , adding at least 9 more layers. It is also interesting to note that, without using optical flow data, our spatial CNNs already approach performance attained using state-of-the-art expert designed features that use optical flow, i.e. IDT-FV <ref type="bibr" target="#b25">[26]</ref> in <ref type="table" target="#tab_4">Table 5</ref>. Performance gains obtained by our approach are especially encouraging compared to deepening the model or incorporating motion features, as leveraging web images during training will not add any additional computational or memory burden during test time.</p><p>The slow fusion CNN <ref type="bibr" target="#b12">[13]</ref> is not a spatial CNN as it is trained on multiple video frames instead of single video frames. We list it here as it presents a different approach; collecting millions of web videos for training. However, despite the fact that 1M web videos are used as pre-training data, its performance is far lower than our models.</p><p>We further test the features learned by our spatial CNNs when combined with motion features, i.e. Fisher encoding on improved dense trajectories. <ref type="table" target="#tab_4">Table 5</ref> compares our results with state-of-the-art methods that also use motion features. Our method (VGG16 + Images + IDT-FV) outperforms all, improving by 2.5% over <ref type="bibr" target="#b17">[18]</ref> that trains recurrent CNNs with long short-term memory cells; by 3.1% over <ref type="bibr" target="#b20">[21]</ref>, which combines two separate CNNs trained on video frames and optical flow respectively; and by 5.2% over <ref type="bibr" target="#b25">[26]</ref> that uses Fisher encoding on improved dense trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Experimental Results for ActivityNet</head><p>Here we report the performance of our spatial CNNs on Ac-tivityNet for the task of action classification in trimmed and untrimmed videos with and without auxiliary web images ( <ref type="table" target="#tab_5">Table 6</ref>). We then further investigate the use of web images as a substitute for many training videos ( <ref type="table" target="#tab_6">Table 7)</ref>.</p><p>In <ref type="table" target="#tab_5">Table 6</ref> we observe that utilizing web images still helps ∼1.5% even with a very large scale dataset like Ac-tivityNet. Using a random sample of approximately one quarter of the crawled web images gives nearly the same results, suggesting that performance gains diminish as the  <ref type="figure" target="#fig_2">(Figure 4</ref>).</p><p>In <ref type="table" target="#tab_6">Table 7</ref> we observe that comparable performance is achieved when half the training videos, are replaced by web images (rows 1 and 4 in <ref type="table" target="#tab_6">Table 7</ref>). A similar pattern is observed when repeating the experiment at a smaller scale. This suggests that using a relatively small number of web images can help us reduce the effort of curating and storing millions of video frames for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We show that utilizing web action images in training CNN models for action recognition is an effective and lowcost approach to improve performance. We also show that while videos contain a lot of useful temporal information to describe an action, and while it is more beneficial to use videos only than to use web images only, web images can provide complementary information to a finite set of videos allowing for a significant reduction in the video data required for training.</p><p>We observe that this approach is applicable even when different CNN architectures are used. It is also applicable using filtered image datasets or using unfiltered web crawled images. We expect that our findings should also be useful in improving the performance of the models of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Sample images from our action image dataset. Each row shows images of one action. Top to bottom: Hula Hoop, Jumping Jack, Salsa Spin, Drumming, Frisbee Catch. Variations in background, camera viewpoint and body part visibility are common in web images of the same action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The 25 action classes with the largest accuracy improvement in the three CNN architectures as well as on average over the three architectures. The blue bars show the accuracy of CNN models trained only on videos. The green bars show the absolute increase in accuracy of CNN models trained using both web action images and training videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Performance of the spatial CNNs (VGG16) trained on UCF101 split1 using different numbers of web action images as additional training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our action image dataset with existing action image datasets. Visibility varies? refers to variance in the partial visibility of the human bodies.</figDesc><table><row><cell>Dataset</cell><cell>No. of actions</cell><cell>No. of images</cell><cell>Clutter?</cell><cell>Poses vary?</cell><cell>Visibility varies?</cell></row><row><cell>Gupta [7]</cell><cell>6</cell><cell>300</cell><cell>Small</cell><cell>Small</cell><cell>No</cell></row><row><cell>Ikizler [8]</cell><cell>5</cell><cell>1727</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>VOC2012 [6]</cell><cell>11</cell><cell>4588</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>PPMI [31]</cell><cell>24</cell><cell>4800</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Standford40 [32]</cell><cell>40</cell><cell>9532</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Ours</cell><cell>101</cell><cell>23800</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell cols="6">al. [3] use static action images to generate synthetic sam-</cell></row><row><cell cols="6">ples for training SVM action classifiers and evaluate on</cell></row><row><cell cols="6">a small test set of 78 videos comprising 5 action classes.</cell></row><row><cell cols="6">In [5], Duan et al. use SVMs trained on SIFT features of</cell></row><row><cell cols="6">web action images in their video event recognition system</cell></row><row><cell cols="6">and evaluate on datasets with 5∼6 different events. Wang</cell></row><row><cell cols="6">et al. [27] exploit semantic groupings of Web images for</cell></row><row><cell cols="6">video event recognition and evaluate on the same datasets</cell></row><row><cell>as [5]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>. Sun et al. [24] localize actions temporally using a domain transfer from web images. In contrast, our work gives the first thorough study on combining web action im- ages with videos for training CNN models for large-scale action recognition.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy on UCF101 split1 using three different CNN architectures.</figDesc><table><row><cell>Model</cell><cell># layers</cell><cell># param. (in Millions)</cell><cell>Accuracy video only</cell><cell>Accuracy video + images</cell></row><row><cell>M2048</cell><cell>7</cell><cell>91</cell><cell>66.1%</cell><cell>75.2%</cell></row><row><cell>VGG16</cell><cell>16</cell><cell>138</cell><cell>77.8%</cell><cell>83.5%</cell></row><row><cell>VGG19</cell><cell>19</cell><cell>144</cell><cell>78.8%</cell><cell>83.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy on UCF101 split1 using spatial CNN (VGG16) of manually filtered and unfiltered web images. * means average of three random sample sets.</figDesc><table><row><cell>Image Type</cell><cell cols="2"># Images Accuracy (%)</cell></row><row><cell>Manually filtered</cell><cell>23.8K</cell><cell>83.5</cell></row><row><cell>Unfiltered (all)</cell><cell>207K</cell><cell>83.1</cell></row><row><cell>Unfiltered (rand select)</cell><cell>65.5K</cell><cell>82.1*</cell></row><row><cell cols="3">hours." [1] Mostly, videos have a duration between 5 and</cell></row><row><cell cols="3">10 minutes and have a 30 FPS frame rate. About 50% of</cell></row><row><cell cols="3">the videos are in HD resolution. We crawl 393K unfiltered</cell></row><row><cell cols="3">images from the Web corresponding to the classes of Activ-</cell></row><row><cell cols="3">ityNet. Results on ActivityNet are reported in Section 5.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Mean accuracy (averaged over three splits) when combining spatial CNNs with motion features for UCF101.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Although ActivityNet is large-scale, using unfiltered web images still helps in both trimmed and untrimmed classification. * means average of three random sample sets.</figDesc><table><row><cell>Model</cell><cell># Images</cell><cell>Untrimmed Classification mAP (%)</cell><cell>Trimmed Classification mAP (%)</cell></row><row><cell>fc8 [1]</cell><cell>none</cell><cell>25.3</cell><cell>38.1</cell></row><row><cell>DF [1]</cell><cell>none</cell><cell>28.9</cell><cell>43.7</cell></row><row><cell>Ours (video frames only)</cell><cell>none</cell><cell>52.3</cell><cell>47.7</cell></row><row><cell>Ours (unfiltered: all)</cell><cell>393K</cell><cell>53.8</cell><cell>49.5</cell></row><row><cell>Ours (unfiltered: rand select)</cell><cell>103K</cell><cell>53.3*</cell><cell>49.3*</cell></row><row><cell>Results reported in</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparable performance is achieved when half the training videos of ActivityNet are replaced by 393K images (row 4 vs. row 1). * means average of three random sample sets.</figDesc><table><row><cell>Experiment</cell><cell cols="3"># Frames # Images mAP (%)</cell></row><row><cell>All vids</cell><cell>32.3M</cell><cell>none</cell><cell>47.7</cell></row><row><cell>1/2 vids</cell><cell>16.2M</cell><cell>none</cell><cell>40.9*</cell></row><row><cell>1/4 vids</cell><cell>8.1M</cell><cell>none</cell><cell>33.4*</cell></row><row><cell>1/2 vids + imgs</cell><cell>16.2M</cell><cell>393K</cell><cell>46.3*</cell></row><row><cell>1/4 vids + imgs</cell><cell>8.1M</cell><cell>393K</cell><cell>41.7*</cell></row></table><note>number of web action images greatly increase. This result is consistent with results on UCF101</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Watching unlabeled video helps learn new human actions from very few labeled snapshots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Observing humanobject interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1775" to="1789" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning actions from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Web-based classifiers for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1031" to="1045" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/ICCV13-Action-Workshop/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R Z</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08909</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<idno>ECCV. 2010. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Poselet key-framing: A model for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00983</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lear-inria submission for the thumos workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video annotation via image groups from the web. Multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video action detection with relational dynamic-poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hidden part models for human action recognition: Probabilistic versus max margin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1310" to="1323" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of visionbased methods for action representation, segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="241" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grouplet: A structured image representation for recognizing human and object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
