<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohong</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Seo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Suk</forename><surname>Choi</surname></persName>
						</author>
						<title level="a" type="main">Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing (NLP). Most previous models for relation classification rely on the high-level lexical and syntatic features obtained by NLP tools such as WordNet, dependency parser, part-ofspeech (POS) tagger, and named entity recognizers (NER). In addition, state-of-the-art neural models based on attention mechanisms do not fully utilize information of entity that may be the most crucial features for relation classification. To address these issues, we propose a novel end-to-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing (LET) method. Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET. Experimental results on the SemEval-2010 Task 8, one of the most popular relation classification task, demonstrate that our model outperforms existing state-ofthe-art models without any high-level features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classifying semantic relations between entity pairs in sentences plays a vital role in various NLP tasks, such as information extraction, question answering and knowledge base population <ref type="bibr" target="#b13">[14]</ref>. A task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence. For example, given a sentence with tagged entity pair, crash and attack, this sentence is classified into the re-lation Cause-Effect(e1,e2) 1 between the entity pair like <ref type="figure" target="#fig_0">Figure 1</ref>. A first entity is surrounded by e1 and /e1 , and a second entity is surrounded by e2 and /e2 .</p><p>Most previous relation classification models rely heavily on high-level lexical and syntactic features obtained from NLP tools such as WordNet, dependency parser, part-of-speech (POS) tagger, and named entity recognizer (NER). The classification models relying on such features suffer from propagation of implicit error of the tools and they are computationally expensive.</p><p>Recently, many studies therefore propose end-toend neural models without the high-level features. Among them, attention-based models, which focus to the most important semantic information in a sentence, show state-of-the-art results in a lot of NLP tasks. Since these models are mainly proposed for solving translation and language modeling tasks, they could not fully utilize the information of tagged entities in relation classification task. However, tagged entity pairs could be powerful hints for solving relation classification task. For example, even if we do not consider other words except the crash and attack, we intuitively know that the entity pair has a relation Cause-Effect(e1,e2) 1 better than Component-Whole(e1,e2) 1 in <ref type="figure" target="#fig_0">Figure 1</ref> To address these issues, We propose a novel endto-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing (LET). To capture the context of sentences, We obtain word representations by self attention mechanisms and build the recurrent neural architecture with Bidirectional Long Short-Term Memory (LSTM) networks. Entity-aware attention focuses on the most important semantic information considering entity pairs with word positions relative to these pairs and latent types obtained by LET.</p><p>The contributions of our work are summarized as follows: <ref type="bibr" target="#b0">(1)</ref> We propose an novel end-to-end recurrent neural model and an entity-aware attention mechanism with a LET which focuses to semantic information of entities and their latent types; (2) Our model obtains 85.2% F1-score in SemEval-2010 Task 8 and it outper-</p><formula xml:id="formula_0">Figure 2:</formula><p>The architecture of our model (best viewed in color). Entity 1 and 2 corresponds to the 3 and (n − 1)-th words, respectively, which are fed into the LET.</p><p>forms existing state-of-the-art models without any highlevel features; <ref type="bibr" target="#b2">(3)</ref> We show that our model is more interpretable since it's decision making process could be visualized with self attention, entity-aware attention, and LET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are several studies for solving relation classification task. Early methods used handcrafted features through a series of NLP tools or manually designing kernels <ref type="bibr" target="#b15">[16]</ref>. These approaches use high-level lexical and syntactic features obtained from NLP tools and manually designing kernels, but the classification models relying on such features suffer from propagation of implicit error of the tools.</p><p>On the other hands, deep neural networks have shown outperform previous models using handcraft features. Especially, many researches tried to solve the problem based on end-to-end models using only raw sentences and pre-trained word representations learned by Skip-gram and Continuous Bag-of-Words <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>. Zeng et al. employed a deep convolutional neural network (CNN) for extracting lexical and sentence level features <ref type="bibr" target="#b29">[30]</ref>. Dos Santos et al. proposed model for learning vector of each relation class using ranking loss to reduce the impact of artificial classes <ref type="bibr" target="#b1">[2]</ref>. Zhang and Wang used bidirectional recurrent neural network (RNN) to learn long-term dependency between entity pairs <ref type="bibr" target="#b30">[31]</ref>. Fur-thermore, Zhang et al. proposed bidirectional LSTM network (BLSTM) utilizing position of words, POS tags, named entity information, dependency parse <ref type="bibr" target="#b31">[32]</ref>. This model resolved vanishing gradient problem appeared in RNNs by using BLSTM.</p><p>Recently, some researcher have proposed attentionbased models which can focus to the most important semantic information in a sentence. Zhou et al. combined attention mechanisms with BLSTM <ref type="bibr" target="#b33">[34]</ref>. Xiao and Liu split the sentence into two entities and used two attention-based BLSTM hierarchically <ref type="bibr" target="#b20">[21]</ref>. Shen and Huang proposed attention-based CNN using word level attention mechanism that is able to better determine which parts of the sentence are more influential <ref type="bibr" target="#b7">[8]</ref>.</p><p>In contrast with end-to-end model, several works proposed models utilizing the shortest dependency path (SDP) between entity pairs of dependency parse trees. SDP-LSTM model proposed by Yan et al. and deep recurrent neural networks (DRNNs) model proposed by Xu et al eliminate irrelevant words out of SDP and use neural network based on the meaningful words composing SDP <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we introduce a novel recurrent neural model that incorporate an entity-aware attention mechanism with a LET method in detail. As shown in <ref type="figure">Fig-</ref>ure 2, our model consists of four main components: <ref type="bibr" target="#b0">(1)</ref> Word Representation that maps each word in a sentence into vector representations; (2) Self Attention that captures the meaning of the correlation between words based on multi-head attention <ref type="bibr" target="#b19">[20]</ref>; (3) BLSTM which sequentially encodes the representations of self attention layer; (4) Entity-aware Attention that calculates attention weights with respect to the entity pairs, word positions relative to these pairs, and their latent types obtained by LET. After that, the features are averaged along the time steps to produce the sentencelevel features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Representation</head><p>Let a input sentence is denoted by S = {w 1 , w 2 , ..., w n }, where n is the number of words. We transform each word into vector representations by looking up word embedding matrix W word ∈ R dw×|V | , where d w is the dimension of the vector and |V | is the size of vocabulary. Then the word representations X = {x 1 , x 2 , ..., x n } are obtained by mapping w i , the i-th word, to a column vector x i ∈ R dw are fed into the next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self Attention</head><p>The word representations are fixed for each word, even though meanings of words vary depending on the context. Many neural models encoding sequence of words may expect to learn implicitly of the contextual meaning, but they may not learn well because of the long-term dependency problems <ref type="bibr" target="#b0">[1]</ref>. In order for the representation vectors to capture the meaning of words considering the context, we employ the self attention, a special case of attention mechanism, that only requires a single sequence. Self attention has been successfully applied to various NLP tasks such as machine translation, language understanding, and semantic role labeling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>We adopt the multi-head attention formulation <ref type="bibr" target="#b19">[20]</ref>, one of the methods for implementing self attentions. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the multi-head attention mechanism that consists of several linear transformations and scaled dot-product attention corresponding to the center block of the figure. Given a matrix of n vectors, query Q, key K, and value V , the scaled dot-product attention is calculated by the following equation:</p><formula xml:id="formula_1">(3.1) Attention(Q, K, V ) = softmax( QK √ d w )V</formula><p>In multi-head attention, the scaled dot-product attention with linear transformations is performed on r parallel heads to pay attention to different parts. Then formulation of multi-head attention is defined by the follows:</p><formula xml:id="formula_2">(3.2) MultiHead(Q, K, V ) = W M [head 1 ; ...; head r ] (3.3) head i = Attention(W Q i Q, W K i K, W V i V )</formula><p>where [;] indicates row concatenation and r is the number of heads. The weights</p><formula xml:id="formula_3">W M ∈ R dw×dw , W Q i ∈ R dw/r×dw , W K i ∈ R dw/r×dw , and W V i ∈ R dw/r×dw are learnable parameter for linear transformation. W M is</formula><p>for concatenation outputs of scaled dot-product attention and the others are for query, key, value of i-th head respectively.</p><p>Because our work requires self attention, the input matrices of multi-head attention, Q, K, and V are all equivalent to X, the word representation vectors. As a result, outputs of multi-head attention are denoted by M = {m 1 , m 2 , ..., m n } = MultiHead(X, X, X), where m i is the output vector corresponding to i-th word. The output of self attention layer is the sequence of representations whose include informative factors in the input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bidirectional LSTM Network</head><p>For sequentially encoding the output of self attention layer, we use a BLSTM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref> that consists of two sub LSTM networks: a forward LSTM network which encodes the context of a input sentence and a backward LSTM network which encodes that one of the reverse sentence. More formally, BLSTM works as follows:</p><formula xml:id="formula_4">(3.4) − → h t = −−−−→ LST M (m t ) (3.5) ← − h t = ←−−−− LST M (m t ) (3.6) h t = [ − → h t ; ← − h t ]</formula><p>The representation vectors M obtained from self attention layer are forwarded into to the network step by step. At the time step t, the hidden state</p><formula xml:id="formula_5">h t ∈ R 2d h of a BLSTM is obtained by concatenating − → h t ∈ R d h , the hid- den state of forward LSTM network, and ← − h t ∈ R d h , the backward one, where d h is dimension of each LSTM's state. (3.7) − → h t ∈ R d h ← − h t ∈ R d h</formula><p>3.4 Entity-aware Attention Mechanism Although many models with attention mechanism achieved state-of-the-art performance in many NLP tasks. However, for the relation classification task, these models lack of prior knowledge for given entity pairs, which could be powerful hints for solving the task. Relation classification differs from sentence classification in that information about entities is given along with sentences.</p><p>We propose a novel entity-aware attention mechanism for fully utilizing informative factors in given entity pairs. Entity-aware attention utilizes the two additional features except H = {h 1 , h 2 , ..., h n }, (1) relative position features, (2) entity features with LET, and the final sentence representation z, result of the attention, is computed as follows:</p><formula xml:id="formula_6">(3.8) u i = tanh(W H [h i ; p e1 i ; p e2 i ] + W E [h e1 ; t 1 ; h e2 ; t 2 ]) (3.9) α i = exp(v u i ) n j=1 exp(v u j ) (3.10) z = n i=1 α i h i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Relative Position Features</head><p>In relation classification, the position of each word relative to entities has been widely used for word representations <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Recently, position-aware attention is published as a way to use the relative position features more effectively <ref type="bibr" target="#b32">[33]</ref>.</p><p>It is a variant of attention mechanisms, which use not only outputs of BLSTM but also the relative position features when calculating attention weights. We adopt this method with slightly modification as shown in Equation 3.8. In the equation, p e1 i ∈ R dp and p e2 i ∈ R dp corresponds to the position of the i-th word relative to the first entity (e 1 -th word) and second entity (e 2 -th word) in a sentence respectively, where e j∈{1,2} is a index of j-th entity. Similar to word embeddings, the relative positions are converted to vector representations by looking up learnable embedding matrix W pos ∈ R dp×(2L−1) , where d p is the dimension of the relative position vectors and L is the maximum sentence length.</p><p>Finally, the representations of BLSTM layer take into account the context and the positional relationship with entities by concatenating h i , p e1 i , and p e2 i . The representation is linearly transformed by W H ∈ R da×(2d h +2dp) as in the Equation 3.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Entity Features with Latent Type</head><p>Since entity pairs are powerful hints for solving relation classification task, we involve the entity pairs and their types in the attention mechanism to effectively train relations between entity pairs and other words in a sentence. We employ the two entity-aware features. The first is the hidden states of BLSTM corresponding to positions of entity pairs, which are high-level features representing entities. These are denoted by h ei ∈ R 2d h , where e i is index of i-th entity.</p><p>In addition, latent types of the entities obtained by LET, our proposed novel method, are the second one. Using types as features can be a great way to improve performance, since the types of entities alone can be inferred the approximate relations. Because the annotated types are not given, we use the latent type representations by applying the LET inspired by latent topic clustering, a method for predicting latent topic of texts in question answering task <ref type="bibr" target="#b25">[26]</ref>. The LET constructs the type representations by weighting K latent type vectors based on attention mechanisms. The mathematical formulation is the follows:</p><formula xml:id="formula_7">(3.11) a j i = exp((h ej ) c i ) K k=1 exp((h ej ) c k ) (3.12) t j∈{1,2} = K i=1 a j i c i</formula><p>where c i is the i-th latent type vector and K is the number of latent entity types. As a result, entity features are constructed by concatenating the hidden states corresponding entity positions and types of entity pairs. After linear transformation of the entity features, they add up with the representations of BLSTM layer as in Equation 3.8, and the representation of sentence z ∈ R 2d h is computed by Equations from 3.8 to 3.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Classification and Training</head><p>The sentence representation obtained from the entity-aware attention z is fed into a fully connected softmax layer for classification. It produces the conditional probability p(y|S, θ) over all relation types:</p><formula xml:id="formula_8">(3.13) p(y|S, θ) = softmax(W O z + b O )</formula><p>where y is a target relation class and S is the input sentence. The θ is whole learnable parameters in the whole network including</p><formula xml:id="formula_9">W O ∈ R |R|×2d h , b O ∈ R |R| ,</formula><p>where |R| is the number of relation classes. A loss function L is the cross entropy between the predictions and the ground truths, which is defined as:</p><formula xml:id="formula_10">(3.14) L = − |D| i=1 log p(y (i) |S (i) , θ) + λ||θ|| 2 2</formula><p>where |D| is the size of training dataset and (S (i) , y (i) ) is the i-th sample in the dataset. We minimize the loss L using AdaDelta optimizer <ref type="bibr" target="#b28">[29]</ref> to compute the parameters θ of our model. To alleviate overfitting, we constrain the L2 regularization with the coefficient λ <ref type="bibr" target="#b12">[13]</ref>. In addition, the dropout method is applied after word embedding, LSTM network, and entity-aware attention to prevent co-adaptation of hidden units by randomly omitting feature detectors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>We evaluate our model on the SemEval-2010 Task 8 dataset, which is an commonly used benchmark for relation classification <ref type="bibr" target="#b5">[6]</ref> and compare the results with the state-of-the-art models in this area. The dataset contains 10 distinguished relations, Cause-Effect, Instrument-Agency, Product-Producer, Content-Container, Entity-Origin, Entity-Destination, Component-Whole, Member-Collection, Message-Topic, and Other. The former 9 relations have two directions, whereas Other is not directional, so the total number of relations is 19. There are 10,717 annotated sentences which consist of 8,000 samples for training and 2,717 samples for testing. We adopt the official evaluation metric of SemEval-2010 Task 8, which is based on the macro-averaged F1-score (excluding Other ), and takes into consideration the directionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We tune the hyperparameters for our model on the development set randomly sampled 800 sentences for validation. The best hyperparameters in our proposed model are shown in following  We use pre-trained weights of the publicly available GloVe model <ref type="bibr" target="#b14">[15]</ref> to initialize word embeddings in our model, and other weights are randomly initialized from zero-mean Gaussian distribution <ref type="bibr" target="#b2">[3]</ref>. <ref type="table" target="#tab_3">Table 2</ref> compares our Entity-aware Attention LSTM model with state-of-theart models on this relation classification dataset. We divide the models into three groups, Non-Neural Model, SDP-based Model, and End-to-End Model. First, the SVM <ref type="bibr" target="#b15">[16]</ref>, Non-Neural Model, was top of the SemEval-2010 task, during the official competition period. They used many handcraft feature and SVM classifier. As a result, they achieved an F1-score of 82.2%. The second is SDP-based Model such as MVRNN <ref type="bibr" target="#b17">[18]</ref>, FCM <ref type="bibr" target="#b26">[27]</ref>, DepNN <ref type="bibr" target="#b8">[9]</ref>, depLCNN+NS <ref type="bibr" target="#b21">[22]</ref>, SDP-LSTM <ref type="bibr" target="#b23">[24]</ref>, and DRNNs <ref type="bibr" target="#b22">[23]</ref>. The SDP is reasonable features for detecting semantic structure of sentences. Actually, the SDP-based models show high performance, but SDP may not always be accurate and the parsing time is exponentially increased by long sentences. The last model is End-to-End Model automatically learned internal representations can occur between the original inputs and the final outputs in deep learning. There are CNN-based models such as CNN <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">14]</ref>, CR-CNN <ref type="bibr" target="#b1">[2]</ref>, and Attention-CNN <ref type="bibr" target="#b7">[8]</ref> and RNN-based models such as BLSTM <ref type="bibr" target="#b31">[32]</ref>, Attention-BLSTM <ref type="bibr" target="#b33">[34]</ref>, and Hierarchical-BLSTM (Hier-BLSTM) <ref type="bibr" target="#b24">[25]</ref> for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model F1</head><p>Non  Our proposed model achieves an F1-score of 85.2% which outperforms all competing state-of-theart approaches except depLCNN+NS, DRNNs, and Attention-CNN. However, they rely on high-level lexical features such as WordNet, dependency parse trees, POS tags, and NER tags from NLP tools.</p><p>The experimental results show that the LET is effective for relation classification. The LET improve a performance of 0.5% than the model not applied it. The model showed the best performance with three types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Visualization</head><p>There are three different visualization to demonstrate that our model is more interpretable. First, the visualization of self attention shows where each word focus on parts of a sentence. By showing the words that the entity pair attends, we can find the words that well represent the relation between them. Next, the entity-aware attention visualization shows where the model pays attend to a sentence. This visualization result highlights important words in a sentence, which are usually important keywords for classification. Finally, we visualize representation of type in LET by using t-SNE <ref type="bibr" target="#b9">[10]</ref>, a method for dimensionality reduction, and group the whole entities in the dataset by the its latent types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Self Attention</head><p>We can obtain the richer word representations by using self attentions. These word representations are considered the context based on correlation between words in a sentence. The <ref type="figure" target="#fig_2">Figure 4</ref> illustrates the results of the self attention in the sentence, "the 〈e1〉pollution〈/e1〉was caused by the 〈e2〉shipwrek〈/e2〉", which is labeled Cause-Effect(e1,e2). There are visualizations of the two heads in the multi-head attention applied for self attention. The color density indicates the attention values, results of Equation 3.1, which means how much an entity focuses on each word in a sentence. In <ref type="figure" target="#fig_2">Figure 4</ref>, the left represents the words that pollution, the first entity, focuses on and the right represents the words that shipwreck, the second entity, focuses on. We can recognize that the entity pair is commonly concentrated on was, caused, and each other. Actually, these words play the most important role in semantically predicting the Cause-Effect(e1,e2), which is the relation class of this entity pair. <ref type="figure" target="#fig_3">Figure 5</ref> shows where the model focuses on the sentence to compute relations between entity pairs, which is the result of visualizing the alpha vectors in Equation 3.9. The important words in sentence are highlighted in yellow, which means that the more clearly the color is, the more important it is. For example, in the first sentence, the inside is strongly highlighted, which is actually the best word representing the relation Component-whole(e1,e2) between the given entity pair. As another example, in the third sentence, the highlighted assess and using represent the relation, Instrument-Agency(e2,e1) between entity pair, analysts and frequency, well. We can see that the using is more highlighted than the assess, because the former represents the relation better.  <ref type="figure" target="#fig_4">Figure 6</ref> visualizes latent type representation t j∈{1,2} in Equation 3.12 Since the dimensionality of representation vectors are too large to visualize, we applied the t-SNE, one of the most popular dimensionality reduction methods. In <ref type="figure" target="#fig_4">Figure 6</ref>, the red points represent latent type vectors c i∈K and the rests are latent type representations t j , where the colors of points are determined by the closest of the latent type vectors in the vector space of the original dimensionality. The points are generally well divided and are almost uniformly distributed without being biased to one side. <ref type="figure">Figure 7</ref> summarizes the results of extracting 50 entities in close order with each latent type vector. This allows us to roughly understand what latent types of entities are. We use a total of three types and find that similar characteristics appear in words grouped by together. In the type 1, the words are related to human's jobs and foods. The type2 has a lot of entities related to machines and engineering like engine, woofer, and motor. Finally, in type3, there are many words with bad meanings related associated with disasters and <ref type="figure">Figure 7</ref>: Sets of Entities grouped by Latent Types drugs. As a result, each type has a set of words with similar characteristics, which can prove that LET works effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Entity-aware Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Latent Entity Type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed entity-aware attention mechanism with latent entity typing and a novel end-to-end recurrent neural model which incorporates this mechanism for relation classification. Our model achieves 85.2% F1-score in SemEval-2010 Task 8 using only raw sentence and word embeddings without any high-level features from NLP tools and it outperforms existing state-of-the-art methods. In addition, our three visualizations of attention mechanisms applied to the model demonstrate that our model is more interpretable than previous models. We expect our model to be extended not only the relation classification task but also other tasks that entity plays an important role. Especially, latent entity typing can be effectively applied to sequence modeling task using entity information without NER. In the future, we will propose a new method in question answering or knowledge base population based on relations between entities extracted from our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A Sample of Relation Classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Multi-Head Self Attention. For self attention, the Q(query), K(key), and V (value), inputs of multihead attention, should be the same vectors. In our work, they are equivalent to X, the word representation vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of Self Attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of Entity-aware Attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of latent type representations using t-SNE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Hyper-parameter</cell><cell>Description</cell><cell>Value</cell></row><row><cell>d w</cell><cell>Size of Word Embeddings</cell><cell>300</cell></row><row><cell>r</cell><cell>Number of Heads</cell><cell>4</cell></row><row><cell>d h</cell><cell>Size of Hidden Layer</cell><cell>300</cell></row><row><cell>d p</cell><cell>Size of Position Embeddings</cell><cell>50</cell></row><row><cell>d a</cell><cell>Size of Attention Layer</cell><cell>50</cell></row><row><cell>K</cell><cell>Number of Latent Entity Types</cell><cell>3</cell></row><row><cell cols="2">batch_size Size of Mini-Batch</cell><cell>20</cell></row><row><cell>η</cell><cell>Initial Learning Rate</cell><cell>1.0</cell></row><row><cell>dropout rate</cell><cell>Word Embedding layer BLSTM layer Entity-aware Attention layer</cell><cell>0.3 0.3 0.5</cell></row><row><cell>λ</cell><cell>L2 Regularization Coefficient</cell><cell>10 −5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Hyperparameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Comparison with Previous Results on</cell></row><row><cell>SemEval-2010 Task 8 dataset, where the WN, WAN,</cell></row><row><cell>PF, and DEP are WordNet (hypernyms), words around</cell></row><row><cell>nominals, position features, and dependency features,</cell></row><row><cell>respectively.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Speech recognition with deep recurrent neural networks, in Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<title level="m">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based convolutional neural network for semantic relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2526" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04646</idno>
		<title level="m">A dependency-based neural network for relation classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Distributed representations of words and phrases and their compositionality</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature selection, l 1 vs. l 2 regularization, and rotational invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">78</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relation extraction: Perspective from convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the NAACL Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Utd: Classifying semantic relations by combining lexical and semantic resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation, Association for Computational Linguistics</title>
		<meeting>the 5th International Workshop on Semantic Evaluation, Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
		<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01586</idno>
		<title level="m">Deep semantic role labeling with self-attention</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic relation classification via hierarchical recurrent neural network with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1254" to="1263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07650</idno>
		<title level="m">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.03651</idno>
		<title level="m">Improved relation classification by deep recurrent neural networks with data augmentation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to rank question-answer pairs using hierarchical recurrent encoder with latent topic clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1575" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Factor-based compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Learning Semantics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="95" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01006</idno>
		<title level="m">Relation classification via recurrent neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>the 29th Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention-based bidirectional long shortterm memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
