<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARN AN EFFECTIVE LIP READING MODEL WITHOUT PAINS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEARN AN EFFECTIVE LIP READING MODEL WITHOUT PAINS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Lip Reading</term>
					<term>Deep Learning</term>
					<term>Visual Speech Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lip reading, also known as visual speech recognition, aims to recognize the speech content from videos by analyzing the lip dynamics. There have been several appealing progress in recent years, benefiting much from the rapidly developed deep learning techniques and the recent large-scale lip-reading datasets. Most existing methods obtained high performance by constructing a complex neural network, together with several customized training strategies which were always given in a very brief description or even shown only in the source code. We find that making proper use of these strategies could always bring exciting improvements without changing much of the model. Considering the non-negligible effects of these strategies and the existing tough status to train an effective lip reading model, we perform a comprehensive quantitative study and comparative analysis, for the first time, to show the effects of several different choices for lip reading. By only introducing some easy-to-get refinements to the baseline pipeline, we obtain an obvious improvement of the performance from 83.7% to 88.4% and from 38.2% to 55.7% on two largest public available lip reading datasets, LRW and LRW-1000, respectively. They are comparable and even surpass the existing state-of-the-art results. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatic lip reading aims to recognize the speech content by watching videos. It has lots of potential applications in both noisy and silent environments. However, several factors, including the lighting conditions, speaker's age, make-up, viewpoints, and so on, make lip reading a challenging task.</p><p>Fortunately, the recent progress in the following two points makes automatic lipreading possible. Firstly, the rapidly developed deep learning techniques have been proved to be able to tackle several problems which are closely related to lip reading, including action recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, sequential modeling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, an so on. Secondly, several large-scale lip reading datasets have been released in recent years <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, which provide a huge amount of data with large variations and contribute much to the progress of lip reading. By taking full advantages of these two aspects, several appealing results have been presented recently lip reading <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Most modern deep lip reading models consist two modules: the frontend module and the backend module. The frontend module often pays more attention to the local motion patterns, including the frame-level and clip-level features. While the backend module focuses more on the whole sequencelevel patterns and often is designed to learn the temporal dynamics of the sequence based on the output features of the frontend module. Although the architectures of most models can be divided into these two parts, there has never been consensus on which strategies or pipeline could bring effective learning of the lip reading model. Different work always have their own different strategies to obtain effective lip reading. For example, Stafylakis et al. <ref type="bibr" target="#b10">[11]</ref> introduced a multi-stage procedure by training the frontend and the backend separately at first and then tuning them together to obtain effective lip reading. Martinez et al. <ref type="bibr" target="#b11">[12]</ref> proposed an end-to-end procedure together with a cosine learning rate scheduling to perform training. Ma et al. <ref type="bibr" target="#b12">[13]</ref> applied the AdamW optimizer, not the traditional Adam optimizer to perform optimization.</p><p>In this paper, we perform a comprehensive quantitative study and comparative analysis to the effects of several factors for lip reading, including the learning rate scheduling, the data pre-processing, the choices for different modules in the pipeline, and so on. Besides referring to the existing lipr eading methods, we also borrow several successful tricks from the general computer vision domain and build a new basic training pipeline for effective lip reading. Finally, without changing much of the model, we obtain comparable results and even better results than the current state-of-the-art on the largest public available word-level datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Lip Reading</head><p>Research on lip-reading has a long history. Most earlier methods are based on hand-crafted features with shallow models, such as Hidden Markov Models (HMM) <ref type="bibr" target="#b13">[14]</ref>, Discrete Cosine Transform (DCT) <ref type="bibr" target="#b14">[15]</ref>, Active Appearance Model (AAM) <ref type="bibr" target="#b15">[16]</ref>, and so on. With the rapid developments of the deep learning techniques, researchers in this area begin to introduce deep neural networks in recent years. For example, Chung et al. <ref type="bibr" target="#b8">[9]</ref> proposed a multi-tower CNN architecture based on VGG-M and reported the results of deep learning methods on large scale lip reading datasets for the first time. Stafylakis et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> introduced the deep Residual Network <ref type="bibr" target="#b17">[18]</ref> as the frontend for the first time and obtained appealing results. Recently, Martinez and Ma et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> proposed to introduce a temporal convolutional neural network for the lip reading problem. With these impressive methods, state-of-the-art performance has been raised from 61.1% to 87.0% on the largest English word-level lipreading dataset LRW in only four years. However, almost all of these methods have their own customized setting in either the training procedure or data processing. So a comprehensive study and comparative analysis about which factors could really bring improvements to the lip reading task are necessary at this time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Bag Of Tricks in Deep Learning</head><p>Several training heuristics and architecture refinements have been proved to be able to improve numerous kinds of tasks in the domain of computer vision. He et al. <ref type="bibr" target="#b18">[19]</ref> have gathered bag of tricks for training, leading to improvement of the accuracy of image classification on ImageNet. Luo et al. <ref type="bibr" target="#b19">[20]</ref> have explored a strong and simple baseline for Person Re-identification by introducing several training tweaks. In the object detection area, there are also plenty of work focusing on the setting of training pipelines and architectures <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. All of these work present a common and easier way to perform the given task and provide great impetus to the follow-up developments on the target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE BASIC PIPELINE</head><p>We adopt a popular pipeline in the lip reading domain as our baseline, as shown in <ref type="figure" target="#fig_0">Fig 1(a)</ref>. ResNet-18 is used as the frontend module. The first 2D convolutional layer is modified to a 3D convolutional layer with kernel size of 5×7×7. Then a global average pooling is performed on the output of residual blocks and the output features are feeded to the backend network. The final fully connection layer's output dimension is equal to the total number of word classes. We adopt the following setting for training by default:</p><p>(1) Initialization: The parameters of the involved convolutional modules and the recurrent modules are randomly initialized with the same manner as <ref type="bibr" target="#b18">[19]</ref>. It can be divided into three folds: (1) In convolutional layers, we set the parameters to random values uniformly drawn from [−a, a], where a = 2/(d in + d out ), d in and d out are the input and the output size. (2) In batch normalization layers, we set all γ vectors set to 1 and all β vectors to 0 in the affinition opration: y = γx + β, where x is the normalized feature in BatchNorm and y is the output of the BN layer. (3) In the Gated Recurrent Unit, all parameters are drawn from (−1, 1) and parameters of the final fully connneted layer are sampled from uniform distribution [−1, 1].</p><p>(2) Data Processing: We shuffle the order of the input videos at each epoch, resize them to 96×96, and then random crop them to 88×88 as the final input to the model. We select a batch of videos in each training iteration. Each video is flipped horizontally with probability 0.5, converted to grayscale, and normalized to [0, 1]. A special setting on LRW-1000 is that we chose 40 frames for each word and put the target word at the center to make it similar to the data in LRW. We found that it can provide more context , so as to improve the performance of lip reading. This setting is adopted by default in our experiments on LRW-1000.</p><p>(3) Loss: In the backend, we average the GRU's outputs in the temporal dimension and send the results to the final fully connected layer for prediction. The cross-entropy loss is used for the optimization.   (4) Optimizer: By default, the Adam Optimizer [24] is adopted. The initial learning rate is 3e-4 and the weight decay is 1e-4. When the model is trained on a single GPU, the batch size is set to 32. When training on different devices, we linearly adjust the learning rate according to the batch size. We validate the model at the end of each epoch and the learning rate will be reduced by a factor of 2 whenever the validation error plateaus in 3 continuous epochs. The minimal learning rate is set to 1e-6.</p><p>All the experiments are performed on the LRW and LRW-1000 dataset, which are the only two public largest wordlevel lip reading datasets. The LRW dataset is an English lip reading dataset, with samples of 500-word classes from the BBC programmes. The LRW-1000 dataset is a Mandarin lipreading dataset, consisting of 1000 word/phrase classes spoken by more than 2000 speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BAG OF TRICKS FOR DEEP LIP READING</head><p>This section will introduce several tricks from different aspects, including the model refinements, training tricks, and data processing settings. We perform a case-by-case study for each refinement in the following part. <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Refinements</head><p>We perform comparison and analysis on the frontend and backend modules separately, to show their effects for lip reading respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Frontend Network</head><p>We compare different types of frontend module with the baseline's GRU backend. Although <ref type="bibr" target="#b1">2</ref>     <ref type="bibr" target="#b24">[25]</ref> in other computer vision tasks, we also introduce it to the baseline model to evaluate its effect for lip reading. As shown in the Tab. 1, the introduction of this module could lead to an stable improvement of the performance on both LRW and LRW-1000.</p><p>The Backend Network We compare three popular types of backend modules in the lip-reading area: the GRU based RNN network, the Temporal Convolutional Network <ref type="bibr" target="#b11">[12]</ref>, and the Transformer. The basic pipeline in <ref type="figure" target="#fig_0">Fig.1(a)</ref> is used with different backend modules in this part. As shown in Tab. 2, the basic pipeline with GRU based backend always performs better on both the two large datasets, with exactly the same conditions (including the same data, the same learning rate schedule, using no pre-training and so on).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Processing</head><p>In most existing methods, the lip region is cropped by a fixedsize rectangle, and the word boundary information is often discarded. Here we perform an analysis to the effects of these two factors, as shown in Tab. 3.</p><p>Face Alignment Face alignment is always helpful for face recognition. Recently, Zhang et al. <ref type="bibr" target="#b30">[31]</ref> found that face alignment can also improve the accuracy of visual speech recognition. Inspired by their work, we perform face alignment at first before the lip region extraction. We use dlib toolkit <ref type="bibr" target="#b31">[32]</ref> Year  <ref type="table">Table 5</ref>: Comaprison with existing methods. '+LS' and '+WB' means that label smoothing and word boundaries are included.</p><p>to get facial landmarks and apply Procrustes analysis to gain affine matrix due to the canonical position. Then we perform a similarity transformation to each image and center crop it using a fixed square to get the lip region. By applying such operations on LRW, we find that face alignment is helpful for lip reading, as shown in Tab.3. The aligned face video retains yaw and pitch, while removes roll rotation. We infer such operation reducing temporal jittering in face video and encourage the deep model focus on the lip movement rather than pose variations.</p><p>Word Boundary In 2018, Stafylakis et al. <ref type="bibr" target="#b16">[17]</ref> introduced the word boundary by converting it into a binary indicator at each time step and then concatenating it with the original features of the frontend network. We evaluate this manner by the baseline model for lip reading. It shows a significant improvement when introducing this information, shown in Tab.3. As described in <ref type="bibr" target="#b16">[17]</ref>, RNN has a powerful gating mechanism, and passing the word boundaries variable permits the RNN to make use of them. The out-of-boundaries frames can provide contextual and environmental information (such as the speaker, pose, light, and so on) that might be useful to classify the target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Tweaks</head><p>This section introduces several training tweaks for lip reading, which are performed on the baseline model separately to analyze their effects.</p><p>MixUp To reduce over-fitting, we introduce mixup <ref type="bibr" target="#b32">[33]</ref> as an additional data augmentation method. In this process, two samples A: (x A , y A ) and B: (x B , y B ) are selected to generate a new sample (x,ŷ) by a weighted linear interpolation as:x</p><formula xml:id="formula_0">= λx A + (1 − λ)x B ,ŷ = λy A + (1 − λ)y B</formula><p>where x i , y i denotes the training sample and the word label of data i ∈ {A, B} respectively, λ is a number randomly sampled from distribution Beta(α, α). In our implementation, we shuffle each batch S to obtain a second 'batch' S and obtain sample A and B from S and S respectively. We set α = 0.2 in our experiments. This operation encourages the deep model to behave linearly in-between training examples, and linearity is a good inductive bias from the perspective of Occam's razor. As shown in Tab.4, this augmentation could bring obvious improvements on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label Smoothing</head><p>Given an input sample belonging to word class i, we denote p i as the prediction logits and y as the annotated word label. Let N be the number of classes. Then the traditional cross-entropy loss is computed as:</p><formula xml:id="formula_1">L = − N i=1 q i log(p i ) q i = 0, y = i q i = 1, y = i<label>(1)</label></formula><p>When using label smoothing, the construction of q i is changed as:</p><formula xml:id="formula_2">q i = /N , y = i 1 − N −1 N , y = i (2)</formula><p>where is a small constant and = 0.1 in our implementation. This operation encourages the model to output a finite probability and generalize better on unseen videos. As shown in Tab.4, it is advantageous to introduce label smoothing for the word-level lip reading task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Learning Rate Scheduling</head><p>The learning rate always has a direct impact on the performance of the lip-reading model. In our basic pipeline, we reduce the learning rate by a factor of 2 when the validation error meets plateau in 3 continuous epochs. This setting may cause an abrupt reduction of the learning rate. So we also compare with cosine learning rate scheduling and exponential learning rate scheduling. (1) In exponential scheduling setting, the learning rate is multiplied by 0.95 at the end of each epoch. <ref type="formula">(2)</ref> In the cosine setting, the learning rate η t at epoch t is calculated as:</p><formula xml:id="formula_3">η t = 1 2 (1 + cos( tπ T ))η<label>(3)</label></formula><p>where η is the initial learning rate. T is the total number of epochs which is 80 in our experiments. The results in Tab. 4 show that the cosine scheduling could lead to a small improvement of the performance. In the cosine learning rate schedule, the learning rate decreases slowly at the beginning, almost linear in the middle, and slow again in the end. Compared to other schedulers, the cosine learning rate schedule starts to reduce the learning rate since the start of training while remaining relatively large, which potentially improves the training progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">The Final Pipeline</head><p>Based on the above results, we combine SE and MixUp with the basic pipeline to form our new refined pipeline as in <ref type="figure" target="#fig_0">Fig.1 (b)</ref>. Then, we introduce further the cosine learning rate scheduling, label smoothing, and word boundary for the learning process. As shown in Tab. 5, we achieve accuracy of 88.4% and 55.7% on LRW and LRW-1000 respectively without changing much of the model, by using only the above refinements. As shown in the table, our results are comparable or even surpass the existing state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we have performed a comprehensive study on the effects of several factors for lip reading. Without changing much of the main model, we achieved comparable or even better results than the current state-of-the-art. We hope that the comparison and analysis in this study could provide some valuable reference to the related researchers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Our basic pipeline (a) and our refined pipeline (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Frontend</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different frontend modules, where * denotes that the results are from existing work.</figDesc><table><row><cell></cell><cell>Backend</cell><cell cols="2">LRW LRW-1000</cell></row><row><cell>VGGM  *</cell><cell>-</cell><cell>61.1%</cell><cell>25.7%</cell></row><row><cell>ResNet-18  *</cell><cell></cell><cell>83.0%</cell><cell>38.2%</cell></row><row><cell>ResNet-34  *  ResNet-18</cell><cell>3 Layers GRU</cell><cell>83.5% 83.7%</cell><cell>-46.5%</cell></row><row><cell>SE-ResNet-18</cell><cell></cell><cell>84.1%</cell><cell>46.8%</cell></row><row><cell>Frontend</cell><cell>Backend</cell><cell cols="2">LRW LRW-1000</cell></row><row><cell></cell><cell>3 Layers GRU</cell><cell>83.7%</cell><cell>46.5%</cell></row><row><cell cols="3">GRU w/o droppout 83.1%</cell><cell>45.5%</cell></row><row><cell>ResNet-18</cell><cell>MS-TCN</cell><cell>83.4%</cell><cell>43.0%</cell></row><row><cell></cell><cell>Transfomer  *</cell><cell>76.2%</cell><cell>44.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different backend modules, where</figDesc><table /><note>* denotes that the results are from existing work.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Unless specifically noted, all the results in Tab.1-Tab.4 are performed using the the basic pipeline in Sec.3.</figDesc><table><row><cell>Data Processing</cell><cell cols="2">LRW LRW-1000</cell></row><row><cell>Baseline</cell><cell>83.7%</cell><cell>46.5%</cell></row><row><cell>Aligned Lip</cell><cell>84.2%</cell><cell>-</cell></row><row><cell cols="2">Word Boundary Input 86.5%</cell><cell>53.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different data processing strategies, where the original data provided in LRW-1000 dataset has been already aligned.</figDesc><table><row><cell cols="3">Training Tweaks LRW LRW-1000</cell></row><row><cell>Baseline</cell><cell>83.7%</cell><cell>46.5%</cell></row><row><cell>MixUp</cell><cell>84.0%</cell><cell>47.3%</cell></row><row><cell>Label Smooth</cell><cell>84.2%</cell><cell>47.0%</cell></row><row><cell cols="2">Cosine Scheduler 84.2%</cell><cell>46.6%</cell></row><row><cell>Exp Scheduler</cell><cell>83.2%</cell><cell>45.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effects of different training tweaks for lip reading.</figDesc><table /><note>deeper convolutional neural networks always perform better than shallow networks in general tasks, the ResNet-34 per- forms slightly better than ResNet-18 on LRW, as shown by the 2nd-3rd rows in Tab.1 .But when introducing our par- ticular setting on LRW-1000 and the learning rate schedule as described in Sec.3, ResNet-18 has shown an obvious im- provement on both datasets. Inspired by the success of the Squeeze-and-Extract module</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Fengdalu/ learn-an-effective-lip-reading-model-without-pains</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lrw-1000: A naturally-distributed largescale benchmark for lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 14th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Lip reading in profile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Lipnet: End-to-end sentence-level lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Combining residual networks with lstms for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08-24" />
			<pubPlace>stockholm, sweden</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lipreading using temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6319" to="6323" />
		</imprint>
	</monogr>
	<note>Stavros Petridis, and Maja Pantic</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards practical lipreading with distilled and efficient models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Stavros Petridis, and Maja Pantic</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lip reading of hearing impaired persons using hmm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Puviarasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4477" to="4481" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A pca based visual dct feature extraction method for lip-reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 International Conference on Intelligent Information Hiding and Multimedia</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lipreading using shape, shading and scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Andrew</forename><surname>Bangham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSP&apos;98 International Conference on Auditory-Visual Speech Processing</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of audiovisual word recognition using residual networks and lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Muhammad Haris Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page" from="22" to="32" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bag of freebies for training object detection neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04103</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd International Conference for Learning Representations</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-grained spatio-temporal modeling for lip-reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 30th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with two-stream deep 3d cnns for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 30th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pseudo-convolutional policy gradient for sequence-tosequence lip-reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingshuang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 15thIEEE International Conference on Automatic Face and Ges-ture Recognition</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>FG 2020</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mutual information maximization for effective lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 15thIEEE International Conference on Automatic Face and Ges-ture Recognition</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>FG 2020</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deformation flow based two-stream network for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 15thIEEE International Conference on Automatic Face and Ges-ture Recognition</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>FG 2020</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Can we read speech beyond the lips? rethinking roi selection for deep visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 15th IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>FG 2020</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">The 18th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>mixup: Beyond empirical risk minimization</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
