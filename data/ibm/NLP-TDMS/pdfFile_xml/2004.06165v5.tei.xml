<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥♠</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Furu</roleName><forename type="first">Dong</forename><forename type="middle">♥</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">♥</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation ♠ University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object Semantics</term>
					<term>Vision-and-Language</term>
					<term>Pre-training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use selfattention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar 1 , which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks. 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning cross-modal representations is fundamental to a wide range of visionlanguage (V+L) tasks, such as visual question answering, image-text retrieval, image captioning. Recent studies <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b6">5,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b47">46]</ref> on vision-language pretraining (VLP) have shown that it can effectively learn generic representations from massive image-text pairs, and that fine-tuning VLP models on task-specific data achieves state-of-the-art (SoTA) results on well-established V+L tasks.</p><p>These VLP models are based on multi-layer Transformers <ref type="bibr" target="#b40">[39]</ref>. To pre-train such models, existing methods simply concatenate image region features and text features as input and resort to the self-attention mechanism to learn semantic alignments between image regions and text in a brute force manner. However, the lack of explicit alignment information between the image regions and text poses alignment modeling a weakly-supervised learning task. In addition, visual regions are often over-sampled <ref type="bibr">[2]</ref>, noisy and ambiguous, which makes the task even more challenging. In this study, we show that the learning of cross-modal representations can be significantly improved by introducing object tags detected in images as anchor points to ease the learning of semantic alignments between images and texts. We propose a new VLP method Oscar, where we define the training samples as triples, each consisting of a word sequence, a set of object tags, and a set of image region features. Our method is motivated by the observation that the salient objects in an image can be accurately detected by modern object detectors <ref type="bibr" target="#b29">[28]</ref>, and that these objects are often mentioned in the paired text. For example, on the MS COCO dataset <ref type="bibr" target="#b22">[21]</ref>, the percentages that an image and its paired text share at least 1, 2, 3 objects are 49.7%, 22.2%, 12.9%, respectively. Our Oscar model is pre-trained on a large-scale V+L dataset composed of 6.5 million pairs, and is fine-tuned and evaluated on seven V+L understanding and generation tasks. The overall setting is illustrated in <ref type="figure" target="#fig_0">Fig 1.</ref> Although the use of anchor points for alignment modeling has been explored in natural language processing e.g., <ref type="bibr" target="#b4">[3]</ref>, to the best of our knowledge, this work is the first that explores the idea for VLP. There have been previous works that use object or image tags in V+L tasks for the sake of enhancing the feature representation of image regions, rather than for learning image-text alignments. For example, Zhou et al. <ref type="bibr" target="#b47">[46]</ref> uses the object prediction probability as a soft label and concatenate it with its corresponding region features. Wu et al. <ref type="bibr" target="#b43">[42]</ref> and You et al. <ref type="bibr" target="#b44">[43]</ref> introduce image-level labels or attributes to improve image-level visual representations.</p><p>The main contributions of this work can be summarized as follows: (i) We introduce Oscar, a powerful VLP method to learn generic image-text representations for V+L understanding and generation tasks. (ii) We have developed an Oscar model that achieves new SoTA on multiple V+L benchmarks, outperforming existing approaches by a significant margin; (iii) We present extensive experiments and analysis to provide insights on the effectiveness of using object tags as anchor points for cross-modal representation learning and downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The training data for many V+L tasks consists of image-text pairs, as shown in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. We denote a dataset of size N by D = {(I i , w i )} N i=1 , with image I and The object tags are used as anchor points to align image regions with word embeddings of pre-trained language models. (c) The word semantic space is more representative than image region features. In this example, dog and couch are similar in the visual feature space due to the overlap regions, but distinctive in the word embedding space.</p><p>text sequence w. The goal of pre-training is to learn cross-modal representations of image-text pairs in a self-supervised manner, which can be adapted to serve various down-stream tasks via fine-tuning. VLP typically employs multi-layer self-attention Transformers <ref type="bibr" target="#b40">[39]</ref> to learn cross-modal contextualized representations, based on the singular embedding of each modality. Hence, the success of VLP fundamentally relies on the quality of the input singular embeddings. Existing VLP methods take visual region features v = {v 1 , · · · , v K } of an image and word embeddings w = {w 1 , · · · , w T } of its paired text as input, and relies on the self-attention mechanism to learn image-text alignments and produce cross-modal contextual representations.</p><p>Though intuitive and effective, existing VLP methods suffer from two issues: (i) Ambiguity. The visual region features are usually extracted from over-sampled regions <ref type="bibr">[2]</ref> via Faster R-CNN object detectors <ref type="bibr" target="#b29">[28]</ref>, which inevitably results in overlaps among image regions at different positions. This renders ambiguities for the extracted visual embeddings. For example, in <ref type="figure" target="#fig_1">Fig. 2</ref>(a) the region features for dog and couch are not easily distinguishable, as their regions heavily overlap.</p><p>(ii) Lack of grounding. VLP is naturally a weakly-supervised learning problem because there is no explicitly labeled alignments between regions or objects in an image and words or phrases in text. However, we can see that salient objects such as dog and couch are presented in both image and its paired text as in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>, and can be used as anchor points for learning semantic alignments between image regions and textual units as in <ref type="figure" target="#fig_1">Fig. 2</ref>(b). In this paper we propose a new VLP method that utilizes these anchor points to address the aforementioned issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Oscar Pre-training</head><p>Humans perceive the world through many channels. Even though any individual channel might be incomplete or noisy, important factors are still perceivable since they tend to be shared among multiple channels (e.g., dog can be described visually and verbally, as in <ref type="figure" target="#fig_1">Fig. 2</ref>  <ref type="figure">Fig. 3</ref>: Illustration of Oscar. We represent the image-text pair as a triple [ word tokens , object tags , region features ], where the object tags (e.g., "dog" or "couch") are proposed to align the cross-domain semantics; when removed, Oscar reduces to previous VLP methods. The input triple can be understood from two perspectives: a modality view and a dictionary view.</p><p>VLP method Oscar to learn representations that capture channel-invariant (or modality-invariant) factors at the semantic level. Oscar differs from existing VLP in the way that the input image-text pairs are represented and the pre-training objective, as outlined in <ref type="figure">Fig. 3</ref>.</p><p>Input Oscar represents each input image-text pair as a Word-Tag-Image triple (w, q, v), where w is the sequence of word embeddings of the text, q is the word embedding sequence of the object tags (in text) detected from the image, and v is the set of region vectors of the image.</p><p>Existing VLP methods represent each input pair as (w, v). Oscar introduces q as anchor points to ease the learning of image-text alignment. This is motivated by the observation that in training data, important objects in an image are often also presented in the image-paired text, using either the same words as object tags or different but semantically similar or related words. Since the alignments between q and w, both in text, are relatively easy to identified by using pretrained BERT models <ref type="bibr" target="#b7">[6]</ref>, which are used as initialization for VLP in Oscar, the image regions from which the object tags are detected are likely to have higher attention weights than other regions, when queried by the semantically related words in the text. This alignment learning process is conceptually illustrated in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>. The process can also be interpreted as learning to ground the image objects, which might be ambiguously represented in the vision space such as dog and couch in <ref type="figure" target="#fig_1">Fig. 2</ref>(a), in distinctive entities represented in the language space, as illustrated in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>.</p><p>Specifically, v and q are generated as follows. Given an image with K regions of objects (normally over-sampled and noisy), Faster R-CNN <ref type="bibr" target="#b29">[28]</ref> is used to extract the visual semantics of each region as (v , z), where region feature v ∈ R P is a P -dimensional vector (i.e., P = 2048), and region position z a R-dimensional vector (i.e., R = 4 or 6) 3 . We concatenate v and z to form a position-sensitive region feature vector, which is further transformed into v using a linear projection to ensure that it has the same vector dimension as that of word embeddings. Meanwhile, the same Faster R-CNN is used to detect a set of high precision object tags. q is the sequence of word embeddings of the object tags.</p><p>Pre-Training Objective The Oscar input can be viewed from two different perspectives as</p><formula xml:id="formula_0">x [ w language , q, v image ] = [ w, q language , v image ] x<label>(1)</label></formula><p>where x is the modality view to distinguish the representations between a text and an image; while x is the dictionary view 4 to distinguish the two different semantic spaces, in which the input is represented. The two-view perspective allows us to design a novel pre-training objective.</p><p>A Dictionary View: Masked Token Loss. The use of different dictionaries determines the semantic spaces utilized to represent different sub-sequences. Specifically, the object tags and word tokens share the same linguistic semantic space, while the image region features lie in the visual semantic space. We define the discrete token sequence as h [w, q], and apply the Masked Token Loss (MTL) for pre-training. At each iteration, we randomly mask each input token in h with probability 15%, and replace the masked one h i with a special token <ref type="bibr">[MASK]</ref>. The goal of training is to predict these masked tokens based on their surrounding tokens h \i and all image features v by minimizing the negative log-likelihood:</p><formula xml:id="formula_1">L MTL = −E (v,h)∼D log p(h i |h \i , v)<label>(2)</label></formula><p>This is similar to masked language model used by BERT. The masked word or tag needs to be recovered from its surroundings, with additional image information attended to help ground the learned word embeddings in the vision context.</p><p>A Modality View: Contrastive Loss. For each input triple, we group h [q, v] to represent the image modality, and consider w as the language modality. We then sample a set of "polluted" image representations by replacing q with probability 50% with a different tag sequence randomly sampled from the dataset D.</p><p>Since the encoder output on the special token [CLS] is the fused vision-language representation of (h , w), we apply a fully-connected (FC) layer on the top of it as a binary classifier f (.) to predict whether the pair contains the original image representation (y = 1) or any polluted ones (y = 0). The contrastive loss is defined as</p><formula xml:id="formula_2">L C = −E (h ,w)∼D log p(y|f (h , w)).<label>(3)</label></formula><p>During the cross-modal pre-training, we utilize object tags as the proxy of images to adjust the word embedding space of BERT, where a text is similar to its paired image (or more specifically, the object tags detected from the image), and dissimilar to the polluted ones. The full pre-training objective of Oscar is:</p><formula xml:id="formula_3">L Pre-training = L MTL + L C .<label>(4)</label></formula><p>Discussion. Although other loss function designs can be considered as pretraining objectives, we perform experiments with these two losses for two reasons: (i) Each loss provides a representative learning signal from its own perspective. We deliberately keep a clear and simple form for the joint loss to study the effectiveness of the proposed dictionary and modality views, respectively. (ii) Though the overall loss is much simpler than those of existing VLP methods, it yields superior performance in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Corpus</head><p>We have built the pre-training corpus based on the existing V+L datasets, including COCO <ref type="bibr" target="#b22">[21]</ref>, Conceptual Captions (CC) <ref type="bibr" target="#b32">[31]</ref>, SBU captions <ref type="bibr" target="#b27">[26]</ref>, flicker30k <ref type="bibr" target="#b45">[44]</ref>, GQA <ref type="bibr" target="#b14">[13]</ref> etc.. In total, the unique image set is 4.1 million, and the corpus consists of 6.5 million text-tag-image triples. The detail is in Appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adapting to V+L Tasks</head><p>We adapt the pre-trained models to seven downstream V+L tasks, including five understanding tasks and two generation tasks. Each task poses different challenges for adaptation. We introduce the tasks and our fine-tuning strategy in this section, and leave the detailed description of datasets and evaluation metrics to Appendix.</p><p>Image-Text Retrieval heavily relies on the joint representations. There are two sub-tasks: image retrieval and text retrieval, depending on which modality is used as the retrieved target. During training, we formulate it as a binary classification problem. Given an aligned image-text pair, we randomly select a different image or a different caption to form an unaligned pair. The final representation of [CLS] is used as the input to the classifier to predict whether the given pair is aligned or not. We did not use ranking losses <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b19">18]</ref>, as we found that the binary classification loss works better, similarly as reported in <ref type="bibr" target="#b28">[27]</ref>. In the testing stage, the probability score is used to rank the given image-text pairs of a query. Following <ref type="bibr" target="#b20">[19]</ref>, we report the top-K retrieval results on both the 1K and 5K COCO test sets.</p><p>Image Captioning requires the model to generate a natural language description of the content of an image. To enable sentence generation, we fine-tune Oscar using the seq2seq objective. The input samples are processed to triples consisting of image region features, captions, and object tags, in the same way as that during the pre-training. We randomly mask out 15% of the caption tokens and use the corresponding output representations to perform classification to predict the token ids. Similar to VLP <ref type="bibr" target="#b47">[46]</ref>, the self-attention mask is constrained such that a caption token can only attend to the tokens before its position to simulate a uni-directional generation process. Note that all caption tokens will have full attentions to image regions and object tags but not the other way around.</p><p>During inference, we first encode the image regions, object tags, and a special token [CLS] as input. Then the model starts the generation by feeding in a [MASK] token and sampling a token from the vocabulary based on the likelihood output. Next, the [MASK] token in the previous input sequence is replaced with the sampled token and a new [MASK] is appended for the next word prediction. The generation process terminates when the model outputs the [STOP] token. We use beam search (i.e., beam size = 5) <ref type="bibr">[2]</ref> in our experiments and report our results on the COCO image captioning dataset.</p><p>Novel Object Captioning (NoCaps) [1] extends the image captioning task, and provides a benchmark with images from the Open Images dataset <ref type="bibr" target="#b18">[17]</ref> to test models' capability of describing novel objects which are not seen in the training corpus. Following the restriction guideline of NoCaps, we use the predicted Visual Genome and Open Images labels to form tag sequences, and train Oscar on COCO without the initialization of pre-training.</p><p>VQA <ref type="bibr" target="#b10">[9]</ref> requires the model to answer natural language questions based on an image. Given an image and a question, the task is to select the correct answer from a multi-choice list. Here we conduct experiments on the widely-used VQA v2.0 dataset <ref type="bibr" target="#b10">[9]</ref>, which is built based on the MSCOCO <ref type="bibr" target="#b22">[21]</ref> image corpus. The dataset is split into training (83k images and 444k questions), validation (41k images and 214k questions), and test (81k images and 448k questions) sets. Following <ref type="bibr">[2]</ref>, for each question, the model picks the corresponding answer from a shared set consisting of 3,129 answers.</p><p>When fine-tuning on the VQA task, we construct one input sequence, which contains the concatenation of a given question, object tags and region features, and then the [CLS] output from Oscar is fed to a task-specific linear classifier for answer prediction. We treat VQA as a multi-label classification problem <ref type="bibr">[2]</ref> assigning a soft target score to each answer based on its relevancy to the human answer responses, and then we fine-tune the model by minimizing the crossentropy loss computed using the predicted scores and the soft target scores. At inference, we simply use a Softmax function for prediction.</p><p>GQA <ref type="bibr" target="#b14">[13]</ref> is similar to VQA, except that GQA tests the reasoning capability of the model to answer a question. We conduct experiments on the public GQA dataset <ref type="bibr" target="#b14">[13]</ref>. For each question, the model chooses an answer from a shared set of 1, 852 candidate answers. We develop two fine-tuned models using Oscar B . One is similar to that of VQA. The other, denoted as Oscar * B in <ref type="table" target="#tab_3">Table 2</ref>(d), is first fine-tuned on unbalanced "all-split" for 5 epochs, and then fine-tuned on the "balanced-split" for 2 epochs, as suggested in <ref type="bibr" target="#b5">[4]</ref>.</p><p>Natural Language Visual Reasoning for Real (NLVR2) <ref type="bibr" target="#b37">[36]</ref> takes a pair of images and a natural language, and the goal is to determine whether the natural language statement is true about the image pair. When fine-tuning on the NLVR2 task, we first construct two input sequences, each containing the concatenation of the given sentence (the natural language description) and one image, and then two [CLS] outputs from Oscar are concatenated as the joint input for a binary classifier, implemented by an MLP 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results &amp; Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance Comparison with SoTA</head><p>To account for parameter efficiency, we compare Oscar against three types of SoTA's: (i) SoTA S indicates the best performance achieved by small models prior to the Transformer-based VLP models. (ii) SoTA B indicates the best performance achieved by VLP models of similar size to BERT base. (iii) SoTA L indicates the best performance yielded by models that have a similar size to BERT large. To the best of our knowledge, UNITER <ref type="bibr" target="#b6">[5]</ref> is the only model of BERT large size. <ref type="table">Table 1</ref> summarizes the overall results on all tasks 6 . For all the tables in this paper, Blue indicates the best result for a task, and gray background indicates results produced by Oscar. As shown in the table, our base model outperforms previous large models on most tasks, often by a significantly large margin. It demonstrates that the proposed Oscar is highly parameter-efficient, partially because the use of object tags as anchor points significantly eases the learning of semantic alignments between images and texts. Note that Oscar is pre-trained <ref type="table">Table 1</ref>: Overall results on six tasks. ∆ indicates the improvement over SoTA. SoTA with subscript S, B, L indicates performance achieved by small models, VLP of similar size to BERT base and large model, respectively. Most results are from <ref type="bibr" target="#b6">[5]</ref>, except that image captioning results are from <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b47">46]</ref>, NoCaps results are from <ref type="bibr">[1]</ref>, VQA results are from <ref type="bibr" target="#b39">[38]</ref>. We report the detailed comparison on each task in <ref type="table" target="#tab_3">Table 2</ref>. (i) VLP methods dominate empirical performance across many V+L tasks, compared with small models. Oscar outperforms all existing VLP methods on all seven tasks, and achieves new SoTA on six of them. On GQA, neural state machine (NSM) <ref type="bibr" target="#b13">[12]</ref> relies on a strong structural prior, which can also be incorporated into Oscar for improvement in the future. (ii) 12-in-1 is a recently proposed multi-task learning model <ref type="bibr" target="#b24">[23]</ref> for V+L, implemented on BERT base. We see that Oscar B outperforms 12-in-1 on almost all the tasks, except on Test-P of NLVR2. Given that our method is based on single task fine-tuning, the result demonstrates the effectiveness of our proposed pre-training scheme. (iii) overall, Oscar is the best performer on both understanding and generation tasks. On the captioning task, we further fine-tune Oscar with self-critical sequence training (SCST) <ref type="bibr" target="#b31">[30]</ref> to improve sequence-level learning. The only comparable VLP method for captioning is <ref type="bibr" target="#b47">[46]</ref>. The results in <ref type="table" target="#tab_3">Table 2</ref> (e) show that Oscar yields a much better performance, e.g., improving BLEU@4 and CIDEr by more than 2 and 10 points, respectively. (iv) The NoCaps guideline requires to only use the COCO captioning training set. Hence, we initialize with BERT, and train Oscar on the COCO training set. Constrained beam search (CBS) is used. The results in <ref type="table" target="#tab_3">Table 2</ref> (f) show that the variants of Oscar consistently outperform the previous SoTA method UpDown <ref type="bibr">[1]</ref>. The gap is much larger on the near-domain or out-of-domain cases, demonstrating the strong generalization ability of Oscar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative Studies</head><p>We visualize the learned semantic feature space of image-text pairs of the COCO test set on a 2D map using t-SNE <ref type="bibr" target="#b25">[24]</ref>. For each image region and word token, we pass it through the model, and use its last-layer output as features. Pretrained models with and without object tags are compared. The results in <ref type="figure" target="#fig_4">Fig 4</ref> reveal some interesting findings. (i) Intra-class. With the aid of object tags, the distance of the same object between two modalities is substantially reduced. For   LXMERT improves the SoTA overall accuracy ('Accu' in <ref type="table" target="#tab_3">Table 2</ref>) by 2.1%. and has 2.4</p><p>VQA and NLVR2 are the two major V+L understanding tasks widely used to evaluate the exsiting VLP models. <ref type="table" target="#tab_10">Table 9</ref> summarizes the evaluation results on VQA task. We can see that Oscar B is the best among the models with equivalent size, even slightly better than UNITER <ref type="bibr" target="#b6">[5]</ref> large. And the Oscar L creates new single-model SoTAs on VQA and NLVR2. As mentioned in Section 4, the NLVR2 fine-tuning architecture is still not the best choice, we believe there is still space to improve, we leave this for future exploration.</p><p>this is not necessarily the best fine-tuning choice for NLVR2, please refer to the Pair-biattn finetuning in UNITER <ref type="bibr" target="#b6">[5]</ref> for better choice, which introduces a multi-head attention layer to look back the concatenated text-image sequences.</p><p>One of major understanding tasks for the exsiting VLP models is VQA. From <ref type="table" target="#tab_10">Table 9</ref>, we can see that Oscar B is the best among the models with equivalent size, even slightly better than UNITER <ref type="bibr" target="#b6">[5]</ref> large.</p><p>And the Oscar L creates new single-model SoTAs on VQA and NLVR2.   VQA One major vision-and-language understanding tasks for the existing VLP models is VQA. The SoTA result for VQA is from UNITER <ref type="bibr" target="#b7">[6]</ref> large model. <ref type="table" target="#tab_4">Table 6</ref> summarized the evaluation results with the recent VLP work on VQA task, we can see that Oscar B is the best among the models with equivalent size, even slightly better (0.04%) than UNITER <ref type="bibr" target="#b7">[6]</ref> large. And the Oscar L improves the SoTA overall accuracy with 0.42% on the test-std split.</p><p>NLVR2 Another major task for the existing VLP models is NLVR2. Similarly, the SoTA model on NLVR2 is UNITER <ref type="bibr" target="#b7">[6]</ref> large. As reported in <ref type="table" target="#tab_5">Table 7</ref>, with the equivalent model sizes, Oscar outperforms UNITER by 0.31% and 0.46% absolutely on the Test public split. As mentioned in Section 4.3, the Oscar fine-tuning architecture for NLVR2 is still not the best choice, we believe there is still space to improve, we leave this for future exploration.</p><p>GQA As shown in <ref type="table" target="#tab_10">Table 9</ref>, compared with existing VLP works (LXMERT <ref type="bibr" target="#b38">[37]</ref> and 12-in-1 <ref type="bibr" target="#b25">[24]</ref>), our Oscar B results on GQA gain 0.6% accuracy, which still demonstrates the superiority of Oscar pretraining. The GQA SoTA result is from NSM <ref type="bibr" target="#b14">[13]</ref>, which equips the model with more complicated reasoning. We leave this for future work.    on VQA task. We can see that Oscar B is the best among the models with equivalent size, even slightly better than UNITER <ref type="bibr" target="#b6">[5]</ref> large. And the Oscar L creates new single-model SoTAs on VQA and NLVR2. As mentioned in Section 4, the NLVR2 fine-tuning architecture is still not the best choice, we believe there is still space to improve, we leave this for future exploration. this is not necessarily the best fine-tuning choice for NLVR2, please refer to the Pair-biattn finetuning in UNITER <ref type="bibr" target="#b6">[5]</ref> for better choice, which introduces a multi-head attention layer to look back the concatenated text-image sequences.</p><p>One of major understanding tasks for the exsiting VLP models is VQA. From <ref type="table" target="#tab_10">Table 9</ref>, we can see that Oscar B is the best among the models with equivalent size, even slightly better than UNITER <ref type="bibr" target="#b6">[5]</ref> large.</p><p>And the Oscar L creates new single-model SoTAs on VQA and NLVR2.</p><p>Since Oscar only handles one image and one text input at pre-training, the 'modality embedding' is extended to help distinguish the additional image presented in the NLVR2 task. For the Triplet setup, we concatenate the image regions and then feed into the UNITER model. An MLP transform is applied (c) NLVR2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>X. Li, X. Yin, C. Li et al.    LXMERT improves the SoTA overall accuracy ('Accu' in <ref type="table" target="#tab_3">Table 2</ref>) by 2.1%. and has 2.4</p><p>VQA and NLVR2 are the two major V+L understanding tasks widely used to evaluate the exsiting VLP models. <ref type="table" target="#tab_10">Table 9</ref> summarizes the evaluation results on VQA task. We can see that Oscar B is the best among the models with equivalent size, even slightly better than UNITER <ref type="bibr" target="#b6">[5]</ref> large. And the Oscar L creates new single-model SoTAs on VQA and NLVR2. As mentioned in Section 4, the NLVR2 fine-tuning architecture is still not the best choice, we believe there is still space to improve, we leave this for future exploration.</p><p>this is not necessarily the best fine-tuning choice for NLVR2, please refer to the Pair-biattn finetuning in UNITER <ref type="bibr" target="#b6">[5]</ref> for better choice, which introduces a multi-head attention layer to look back the concatenated text-image sequences.</p><p>One of major understanding tasks for the exsiting VLP models is VQA. From <ref type="table" target="#tab_10">Table 9</ref>, we can see that Oscar B is the best among the models with equivalent size, even slightly better than UNITER <ref type="bibr" target="#b6">[5]</ref> large.</p><p>And the Oscar L creates new single-model SoTAs on VQA and NLVR2.      train, truck, motorcycle, car). This verifies the importance of object tags in alignment learning: it plays the role of anchor points in linking and regularizing the cross-modal feature learning. We compare generated captions of different models in <ref type="figure" target="#fig_5">Fig. 5</ref>. The baseline method is VLP without object tags. We see that Oscar generates more detailed descriptions of images than the baseline, due to the use of the accurate and diverse object tags detected by Faster R-CNN. They are the anchor points in the word embedding space, guiding the text generation process. tance to four representative downstream tasks. All the ablation experiments are conducted on the base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Effect of Object Tags</head><p>To study the effect of object tags, we experiment three different settings: (i) Baseline (No Tags): this reduces the models to their previous VLP counterparts, where no tag information is exploited. (ii) Predicted Tags: we use an off-the-shelf object detector (trained on COCO dataset) to predict object tags. (iii) Ground-truth Tags: The ground-truth tags from COCO dataset are utilized to serve as a performance "upper bound" for our method. The experiments are conducted with the same BERT base model on three representative tasks, including VQA, image retrieval, and image captioning. As shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, the learning curves for fine-tuning with object tags converges significantly faster and better than the VLP method without tags on all tasks. On the VQA and retrieval tasks, training using tags only takes half of the training time to achieve the final performance of the baseline, showing that Oscar is a more practical and efficient scheme for VLP. With more accurate object detectors developed in the future, Oscar can achieve even better performance, closing the gap demonstrated by using the ground-truth tags.  tags yield minor improvement when used as features; a more promising way is to use them as anchor points, as done in Oscar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ECCV #7133</head><p>ECCV #7133</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ECCV-20 submission ID 7133 15</head><p>Attention Multimodal Embeddings It has been shown that V+L tasks can benefit from a shared embedding space to align the inter-modal correspondences between images and text. Early attempts from Socher et al. <ref type="bibr" target="#b34">[33]</ref> project words and image regions into a common space using kernelized canonical correlation analysis, and achieved excellent results for annotation and segmentation. Similar ideas were employed for image captioning <ref type="bibr" target="#b16">[15]</ref> and text-based image retrieval <ref type="bibr" target="#b30">[29]</ref>. In particular, the seminal work DeViSE <ref type="bibr" target="#b10">[9]</ref> was proposed to identify visual objects using semantic information gleaned from unannotated text. This semantic information was exploited to make predictions about image labels not observed during training, and improved such zero-shot predictions dramatically across thousands of novel labels never seen by the visual model. The idea has been extended <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b26">25]</ref>, showing that leveraging pre-trained linguistic information is highly effective to align semantics and improve sample e ciency in cross-modal transfer. Inspired by this line of research, we revisit the idea and propose to leverage the rich semantics from word embeddings in the era of modern language model pre-training. Indeed, our results on novel object captioning demonstrate strong generalization.</p><formula xml:id="formula_4">Image R. Text R. w-v w-q v-q R@1 R@5 R@1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we have introduced a new pre-training schema Oscar. It identifies object tags as anchor points to align the language and image modalities in a shared semantic space. We verify the idea by pre-training Oscar on a public corpus with 6.5 millions text-image pairs. The pre-trained model can successfully tackle a broad set of vision-and-language understanding and generation tasks, archiving new state of the arts on six established tasks. Object Tags in Pre-training To study the impact of di↵erent object tag sets in pre-trained models, we pre-train two variants: Oscar VG and Oscar OI utilizes object tags produced by the object detector trained on the visual genome (VG) dataset <ref type="bibr" target="#b17">[16]</ref> and the open images (OI) dataset <ref type="bibr" target="#b18">[17]</ref>, respectively. In this ablation, all the models are pretrained for 589k steps. The results are shown in <ref type="table" target="#tab_20">Table 5</ref>, where Baseline (No Tags) is also listed for comparison. It is clear that the Oscar scheme of using object tags as anchor points improves the baseline, regardless of which set of object tags is used. VG tags performs slightly better than OI. We hypothesize that the object detector trained on VG has a more diverse set of objects, although the object detector trained on OI has a higher precision.  <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b6">5,</ref><ref type="bibr">47,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b11">10]</ref> employ BERT-like objectives <ref type="bibr" target="#b7">[6]</ref> to learn Attention Interaction To further understand the interaction among the text, object tags and object regions, we conduct finetuning experiments by varying the attention masks for image-text retrieval. The default setting uses full attentions across all modalities. We then enable certain part of the attention masks. All models are initialized from BERT base without pre-training. <ref type="table" target="#tab_19">Table 3</ref> reports the performance on the COCO 1K test set. By comparing the results of using full attention and partial attention w-v, we see that it is beneficial to add object tags. Moreover, region features are more informative than object tags (w-v, vs. v-q) in representing an image. This suggests that tags yield minor improvement when used as features; a more promising way is to use them as anchor points, as done in Oscar. Object Tags in Pre-training To study the impact of different object tag sets in pre-trained models, we pre-train two variants: Oscar VG and Oscar OI utilizes object tags produced by the object detector trained on the visual genome (VG) dataset <ref type="bibr" target="#b17">[16]</ref> and the open images (OI) dataset <ref type="bibr" target="#b18">[17]</ref>, respectively. In this ablation, all the models are pre-trained for 589k steps. The results are shown in <ref type="table" target="#tab_22">Table 4</ref>, where Baseline (No Tags) is also listed for comparison. It is clear that the Oscar scheme of using object tags as anchor points improves the baseline, regardless of which set of object tags is used. VG tags performs slightly better than OI. We hypothesize that the object detector trained on VG has a more diverse set of objects, although the object detector trained on OI has a higher precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Vision-Language Pre-training There is a growing interest in pre-training generic models to solve a variety of V+L problems, such as visual questionanswering (VQA), image-text retrieval and image captioning etc. The existing methods <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b6">5,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b11">10]</ref> employ BERT-like objectives <ref type="bibr" target="#b7">[6]</ref> to learn crossmodal representations from a concatenated-sequence of visual region features and language token embeddings. They heavily rely on the self-attention mechanism of Transformers to learn joint representations that are appropriately contextualized in both modalities. For example, early efforts such as <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b39">38]</ref> propose a two-stream and three-stream Transformer-based framework with co-attention to fuse the two modalities, respectively. Chen et al. <ref type="bibr" target="#b6">[5]</ref> conduct comprehensive studies on the effects of different pre-training objectives for the learned generic representations. Zhou et al. <ref type="bibr" target="#b47">[46]</ref> propose the first unified model to deal with both understanding and generation tasks, using only VQA and image captioning as the downstream tasks. In this paper, the Oscar models have been applied to a wider range of downstream tasks, including both understanding and generation tasks, and have achieved new SoTA in most of them. Compared to existing VLP methods, the most salient difference of the proposed Oscar is the use of object tags for aligning elements in two modalities. It alleviates the challenge of VLP models having to figure out the cross-modal semantic alignment from scratch, and thus improves the learning efficiency. In fact, our base model already outperforms the existing large VLP models on most V+L tasks.</p><p>Object Tags Anderson et al. <ref type="bibr">[2]</ref> introduce the bottom-up mechanism to represent an image as a set of visual regions via Faster R-CNN <ref type="bibr" target="#b29">[28]</ref>, each with an associated feature vector. It enables attention to be computed at the object level, and has quickly become the de facto standard for fine-grained image understanding tasks. In this paper, we propose to use object tags to align the object-region features in <ref type="bibr">[2]</ref> in the pre-trained linguistic semantic space. The idea of utilizing object tags has been explored for image understanding <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b47">46]</ref>. Based on grid-wise region features of CNNs, Wu et al. <ref type="bibr" target="#b43">[42]</ref> employ the predicted object tags only as the input to LSTM for image captioning, while You et al. <ref type="bibr" target="#b44">[43]</ref> consider both tags and region features. Based on salient regions proposed by object detectors, Zhou et al. <ref type="bibr" target="#b47">[46]</ref> concatenate the object prediction probability vector with region features as the visual input for VLP. Unfortunately, the tags in these works are not simultaneously associated with both object regions and word embeddings of text, resulting in a lack of grounding. Our construction of object tags with their corresponding region features &amp; word embeddings yields more complete and informative representations for objects, particularly when the linguistic entity embeddings are pre-trained, as described next.</p><p>Multimodal Embeddings It has been shown that V+L tasks can benefit from a shared embedding space to align the inter-modal correspondences between images and text. Early attempts from Socher et al. <ref type="bibr" target="#b34">[33]</ref> project words and image regions into a common space using kernelized canonical correlation analysis, and achieve good results for annotation and segmentation. Similar ideas are employed for image captioning <ref type="bibr" target="#b15">[14]</ref> and text-based image retrieval <ref type="bibr" target="#b30">[29]</ref>. In particular, the seminal work DeViSE <ref type="bibr" target="#b9">[8]</ref> proposes to identify visual objects using semantic information gleaned from un-annotated text. This semantic information is exploited to make predictions of image labels that are not observed during training, and improves zero-shot predictions dramatically across thousands of novel labels that have never been seen by the vision model. The idea has been extended in <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b26">25]</ref>, showing that leveraging pre-trained linguistic knowledge is highly effective for aligning semantics and improving sample efficiency in crossmodal transfer learning. Inspired by this line of research, we revisit the idea and propose to leverage the rich semantics from the learned word embeddings in the era of neural language model pre-training. Indeed, our results on novel object captioning demonstrate that Oscar helps improve the generalizability of the pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have presented a new pre-training method Oscar, which uses object tags as anchor points to align the image and language modalities in a shared semantic space. We validate the schema by pre-training Oscar models on a public corpus with 6.5 million text-image pairs. The pre-trained models archive new state-of-the-arts on six established V+L understanding and generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Fine-tuning Settings</head><p>Image-Text Retrieval We adopt the widely used Karpathy split <ref type="bibr" target="#b15">[14]</ref> on the COCO caption dataset <ref type="bibr" target="#b22">[21]</ref> to conduct our experiments. Specifically, the dataset consists of 113, 287 images for training, 5, 000 images for validation, and 5, 000 images for testing. Each image is associated with 5 human-generated captions.</p><p>For the Oscar B model, we fine-tune with a batch size of 256 for 40 epochs. The initial learning rate is set to 2e −5 and linearly decreases. For the Oscar L model, we fine-tune with a batch size of 128 for 40 epochs. The initial learning rate is set to 1e −5 and linearly decreases. We use the validation set for parameter tuning. We compare with several existing methods, including DVSA <ref type="bibr" target="#b15">[14]</ref>, VSE++ <ref type="bibr" target="#b8">[7]</ref>, DPC <ref type="bibr" target="#b46">[45]</ref>, CAMP <ref type="bibr" target="#b42">[41]</ref>, SCAN <ref type="bibr" target="#b19">[18]</ref>, SCG <ref type="bibr" target="#b33">[32]</ref>, PFAN <ref type="bibr" target="#b41">[40]</ref>, Unicoder-VL <ref type="bibr" target="#b20">[19]</ref>, 12-in-1 <ref type="bibr" target="#b24">[23]</ref>, UNITER <ref type="bibr" target="#b6">[5]</ref>.</p><p>Image Captioning Though the training objective (i.e., seq2seq) for image captioning is different from that used in pre-training (i.e., bidirectional attentionbased mask token loss), we directly fine-tune Oscar for image captioning on COCO without additional pre-training on Conceptual Captions <ref type="bibr" target="#b32">[31]</ref>. This is to validate the generalization ability of the Oscar models for generation tasks. We use the same Karpathy split <ref type="bibr" target="#b15">[14]</ref>. During training, we randomly select 15% of caption tokens with a maximum of 3 tokens per caption to be masked out. For the Oscar B model, we fine-tune with cross-entropy loss for 40 epochs with a batch size of 256 and an initial learning rate of 3e −5 and then with CIDEr optimization <ref type="bibr" target="#b31">[30]</ref> for 5 epochs with a batch size of 64 and initial learning rate of 1e −6 .</p><p>For the Oscar L model, we fine-tune for 30 epochs with a batch size of 128 and an initial learning rate of 1e −5 and then with CIDEr optimization for another 3 epochs with a batch size of 48 and learning rate of {1e −6 , 5e −7 }. We compare with several existing methods, including BUTD <ref type="bibr">[2]</ref>, VLP <ref type="bibr" target="#b47">[46]</ref>, AoANet <ref type="bibr" target="#b12">[11]</ref>.</p><p>NoCaps Since NoCaps images are collected from Open Images. We train an object detector using the Open Images training set and applied it to generate the tags. We conduct experiments from BERT model directly without pre-training as required by the task guidelines. For the Oscar B model, we train 40 epoch with a batch size of 256 and learning rate 3e −5 ; further we perform CIDEr optimization with learning rate 1e −6 and batch size 64 for 5 epochs. During inference, we use constrained beam search for decoding. We compare Oscar with UpDown [1] on this task.</p><p>VQA For VQA training, we random sample a set of 2k images from the MS COCO validation set as our validation set, the rest of images in the training and validation are used in the VQA finetuning. For the Oscar B model, we finetune for 25 epochs with a learning rate of 5e −5 and a batch size of 128. For the Oscar L model, we fine-tune for 25 epochs with with a learning rate of 3e −5 and a batch size of 96.</p><p>GQA The fine-tuning procedure of GQA is similar to that of VQA. For the Oscar B model, we fine-tune for 5 epochs with a learning rate of 5e −5 and a batch size of 128. We compare with four existing methods, including LXMERT <ref type="bibr" target="#b39">[38]</ref>, MMN <ref type="bibr" target="#b5">[4]</ref>, 12-in-1 <ref type="bibr" target="#b24">[23]</ref>, NSM <ref type="bibr" target="#b13">[12]</ref>.</p><p>NLVR2 For the Oscar B model, we fine-tune for 20 epochs with learning rate {2e −5 , 3e −5 , 5e −5 } and a batch size of 72. For the Oscar L model, we fine-tune for 20 epochs with learning rate of {2e −5 , 3e −5 } and a batch size of 48.</p><p>B Pre-training Corpus <ref type="table" target="#tab_20">Table 5</ref> shows the statistics of image and text of the corpus. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Results</head><p>The enlarged t-SNE visualization results of Oscar and baseline (no tags) are shown in <ref type="figure" target="#fig_7">Fig. 7</ref> and <ref type="figure" target="#fig_8">Fig. 8</ref>, respectively.  For several object classes, their text and image features are largely separated (e.g., person, umbrella, zebra). The distance of image features between some objects is too small (e.g., bench, chair, couch).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Oscar pipeline. The model takes a triple as input, is pre-trained with two losses (a masked token loss over words &amp; tags, and a contrastive loss between tags and others), and fine-tuned for 5 understanding and 2 generation tasks (detailed in Sec. 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration on the process that Oscar represents an image-text pair into semantic space via dictionary look up. (a) An example of input image-text pair (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>i s t r i b u t e ECCV-20 submission ID 7133 21</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>2D visualization using t-SNE. The points from the same object class share the same color. Please refer Appendix for full visualization.Tags: sign, tree, sidewalk, train, woman, person, trees, street, bus, stairs, store, man, balcony, building, people GT: a small train on a city street with people near by .A black and red small train in shopping area. A group of people near a small railroad train in a mall .Oscar: a small train on a city street with people near by . Baseline: a train that is sitting on the side of the road .Tags: leaf, bouquet, flowers, stem, table, rose, flower, leaves, vase, plant GT: A red rose in a glass vase on a table beautiful red rose and white flowers are in a vase . The bouquet has one red rose in it.Oscar: a red rose and white flowers in a vase . Baseline: a vase filled with red and white flowers .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Examples of image captioning. Objects are colored, based on their appearance against the groud-truth (GT): all , Oscar &amp; tags , tags only .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>We perform ablation experiments over a number of design choices of Oscar in both pre-training and fine-tuning to better understand their relative impor-The learning curves of fine-tuning downstream tasks with different object tags. Each curve is with 3 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Feature visualization of Oscar. We observe small distances between text and image features of the same object; some of them are perfectly aligned, as demonstrated by the overlapping regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Feature visualization of baseline (no tags).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). With this motivation, we propose a new</figDesc><table><row><cell cols="2">Contrastive Loss</cell><cell></cell><cell cols="2">Masked Token Loss</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell></cell><cell></cell><cell cols="2">Multi-Layer Transformers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[CLS] A</cell><cell>dog</cell><cell>is</cell><cell>[MASK] on</cell><cell>a couch [SEP]</cell><cell>dog</cell><cell>couch</cell><cell>[SEP]</cell></row><row><cell>Data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Word Tokens</cell><cell></cell><cell cols="2">Object Tags</cell><cell>Region Features</cell></row><row><cell>Modality</cell><cell></cell><cell></cell><cell></cell><cell>Language</cell><cell></cell><cell>Image</cell><cell></cell></row><row><cell>Dictionary</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Language</cell><cell>Image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We pre-train two model variants, denoted as Oscar B and Oscar L , initialized with parameters θ The trainable parameters are θ = {θ BERT , W}. The AdamW Optimizer is used. Oscar B is trained for at least 1.0M steps, with learning rate 5e −5 and batch size 768. Oscar L is trained for at least 900k steps, with learning rate 1e −5 and batch size 512. The sequence length of discrete tokens h and region features v are 35 and 50, respectively.</figDesc><table><row><cell>BERT of BERT base (H = 768)</cell></row><row><cell>and large (H = 1024), respectively, where H is the hidden size. To ensure that</cell></row><row><cell>the image region features have the same input embedding size as BERT, we</cell></row><row><cell>transform the position-sensitive region features using a linear projection via ma-</cell></row><row><cell>trix W.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>OscarB 54.0 80.8 88.5 70.0 91.1 95.5 40.5 29.7 137.6 22.8 78.8 11.7 73.44 78.36 OscarL 57.5 82.8 89.8 73.5 92.2 96.0 41.7 30.6 140.0 24.5 80.9 11.3 73.82 80.37 ∆ 5.8 ↑ 4.4 ↑ 2.9 ↑ 6.9 ↑ 2.8 ↑ 1.7 ↑ 2.2 ↑ 1.3 ↑ 10.7 ↑ 1.3 ↑ 7.8 ↑ 0.5 ↑ 0.42 ↑ 0.87 ↑ on 6.5 million pairs, which is less than 9.6 million pairs used for UNITER pretraining and 9.18 million pairs for LXMERT.</figDesc><table><row><cell>Task</cell><cell cols="5">Image Retrieval R@1 R@5 R@10 R@1 R@5 R@10 B@4 M Text Retrieval Image Captioning C S</cell><cell cols="4">NoCaps C S test-std test-P VQA NLVR2</cell></row><row><cell cols="8">SoTA S 39.2 68.0 81.3 56.6 84.5 92.0 38.9 29.2 129.8 22.4 61.5 9.2</cell><cell>70.90</cell><cell>53.50</cell></row><row><cell cols="9">SoTA B 48.4 76.7 85.9 63.3 87.0 93.1 39.5 29.3 129.3 23.2 73.1 11.2 72.54</cell><cell>78.87</cell></row><row><cell cols="2">SoTA L 51.7 78.4 86.9 66.6 89.4 94.3</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>73.40</cell><cell>79.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Detailed results on V+L tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Evaluation results on COCO dataset. (Note: B for Base, L for Large)</figDesc><table><row><cell>Method</cell><cell>Size</cell><cell cols="11">Text Retrieval R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval Text Retrieval Image Retrieval</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1K Test Set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">5K Test Set</cell></row><row><cell>DVSA [14]</cell><cell>-</cell><cell cols="3">38.4 69.9 80.5</cell><cell cols="3">27.4 60.2 74.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VSE++ [7]</cell><cell>-</cell><cell>64.7</cell><cell>-</cell><cell>95.9</cell><cell>52.0</cell><cell>-</cell><cell cols="2">92.0 41.3</cell><cell>-</cell><cell cols="2">81.2 30.3</cell><cell>-</cell><cell>7 2 .4</cell></row><row><cell>DPC [46]</cell><cell>-</cell><cell cols="3">65.6 89.8 95.5</cell><cell cols="8">47.1 79.9 90.0 41.2 70.5 81.1 25.3 53.4</cell><cell>66.4</cell></row><row><cell>CAMP [42]</cell><cell>-</cell><cell cols="3">72.3 94.8 98.3</cell><cell cols="8">58.5 87.9 95.0 50.1 82.1 89.7 39.0 68.9</cell><cell>80.2</cell></row><row><cell>SCAN [18]</cell><cell>-</cell><cell cols="3">72.7 94.8 98.4</cell><cell cols="8">58.8 88.4 94.8 50.4 82.2 90.0 38.6 69.3</cell><cell>80.4</cell></row><row><cell>SCG [33]</cell><cell>-</cell><cell cols="3">76.6 96.3 99.2</cell><cell cols="8">61.4 88.9 95.1 56.6 84.5 92.0 39.2 68.0</cell><cell>81.3</cell></row><row><cell>PFAN [41]</cell><cell>-</cell><cell cols="3">76.5 96.3 99.0</cell><cell cols="3">61.6 89.6 95.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Unicoder-VL [19] B</cell><cell cols="3">84.3 97.3 99.3</cell><cell cols="8">69.7 93.5 97.2 62.3 87.1 92.8 46.7 76.0</cell><cell>85.3</cell></row><row><cell>12-in-1 [24]</cell><cell>B</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">65.2 91.0 96.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UNITER [5]</cell><cell>B</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">63.3 87.0 93.1 48.4 76.7</cell><cell>85.9</cell></row><row><cell>UNITER [5]</cell><cell>L</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">66.6 89.4 94.3 51.7 78.4</cell><cell>86.9</cell></row><row><cell>Oscar</cell><cell>B L</cell><cell cols="11">88.4 99.1 99.8 75.7 95.2 98.3 70.0 91.1 95.5 54.0 80.8 89.8 98.8 99.7 78.2 95.8 98.3 73.5 92.2 96.0 57.5 82.8 89.8 88.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>OscarB 36.5 30.3 123.7 23.1 40.5 29.7 137.6 22.8 OscarL 37.4 30.7 127.8 23.5 41.7 30.6 140.0 24.5</figDesc><table><row><cell>Method</cell><cell>cross-entropy optimization B@4 M C S</cell><cell>CIDEr optimization B@4 M C</cell><cell>S</cell></row><row><cell>BUTD [2]</cell><cell>36.2 27.0 113.5 20.3</cell><cell cols="2">36.3 27.7 120.1 21.4</cell></row><row><cell>VLP [47]</cell><cell>36.5 28.4 117.7 21.3</cell><cell cols="2">39.5 29.3 129.3 23.2</cell></row><row><cell>AoANet [11]</cell><cell>37.2 28.4 119.8 21.3</cell><cell cols="2">38.9 29.2 129.8 22.4</cell></row></table><note>Image captioning evaluation results (single model) on COCO "Karpa- thy" test split. (Note: B@4: BLUE@4, M: METEOR, C: CIDEr, S: SPICE.)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Evaluation results on VQA.</figDesc><table><row><cell>Test-dev</cell><cell>70.63</cell><cell>70.50</cell><cell>70.80</cell><cell>72.42</cell><cell>73.15</cell><cell>72.27</cell><cell>73.24</cell><cell>73.16 73.61</cell></row><row><cell>Test-std</cell><cell>70.92</cell><cell>70.83</cell><cell>71.00</cell><cell>72.54</cell><cell></cell><cell>72.46</cell><cell>73.40</cell><cell>73.44 73.82</cell></row></table><note>Method ViLBERT VL-BERT VisualBERT LXMERT 12-in-1 UNITERB UNITERL OscarB OscarL</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Evaluation results on NLVR2.</figDesc><table><row><cell>Dev</cell><cell>50.8</cell><cell>6 7 .40</cell><cell>74.90</cell><cell></cell><cell>77.14</cell><cell>78.40</cell><cell>78.12 79.19</cell></row><row><cell cols="2">Test-P 51.4</cell><cell>6 7 .00</cell><cell>74.50</cell><cell>78.87</cell><cell>77.87</cell><cell>79.50</cell><cell>78.18 79.96</cell></row></table><note>Method MAC VisualBERT LXMERT 12-in-1 UNITERB UNITERL OscarB OscarL</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Evaluation results on GQA.</figDesc><table><row><cell>Test-dev</cell><cell>60.00</cell><cell></cell><cell></cell><cell></cell><cell>61.19</cell><cell>61.58</cell></row><row><cell>Test-std</cell><cell>60.33</cell><cell>60.83</cell><cell>60.65</cell><cell>63.17</cell><cell>61.23</cell><cell>61.62</cell></row></table><note>Method LXMERT MMN [5] 12-in-1 NSM [13] OscarB OscarB ⇤</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>NoCaps val set. Models are trained on COCO only without pre-training.</figDesc><table><row><cell>Method</cell><cell cols="8">in-domain CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE near-domain out-of-domain overall</cell></row><row><cell></cell><cell></cell><cell cols="2">Validation Set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UpDown [1]</cell><cell>78.1</cell><cell>11.6</cell><cell>57.7</cell><cell>10.3</cell><cell>31.3</cell><cell>8 .3</cell><cell>55.3</cell><cell>10.1</cell></row><row><cell>UpDown + CBS [1]</cell><cell>80.0</cell><cell>12.0</cell><cell>73.6</cell><cell>11.3</cell><cell>66.4</cell><cell>9 .7</cell><cell>73.1</cell><cell>11.1</cell></row><row><cell>UpDown + ELMo + CBS [1]</cell><cell>79.3</cell><cell>12.4</cell><cell>73.8</cell><cell>11.4</cell><cell>71.7</cell><cell>9 .9</cell><cell>74.3</cell><cell>11.2</cell></row><row><cell>OscarB</cell><cell>79.6</cell><cell>12.3</cell><cell>66.1</cell><cell>11.5</cell><cell>45.3</cell><cell>9.7</cell><cell>63.8</cell><cell>11.2</cell></row><row><cell>OscarB + CBS</cell><cell>80.0</cell><cell>12.1</cell><cell>80.4</cell><cell>12.2</cell><cell>75.3</cell><cell>10.6</cell><cell>79.3</cell><cell>11.9</cell></row><row><cell>OscarB + SCST + CBS</cell><cell>83.4</cell><cell>12.0</cell><cell>81.6</cell><cell>12.0</cell><cell cols="2">77.6 10.6</cell><cell>81.1</cell><cell>11.7</cell></row><row><cell>OscarL</cell><cell>79.9</cell><cell>12.4</cell><cell>68.2</cell><cell>11.8</cell><cell>45.1</cell><cell>9.4</cell><cell>65.2</cell><cell>11.4</cell></row><row><cell>OscarL + CBS</cell><cell>78.8</cell><cell>12.2</cell><cell>78.9</cell><cell>12.1</cell><cell>77.4</cell><cell>10.5</cell><cell>78.6</cell><cell>11.8</cell></row><row><cell>OscarL + SCST + CBS</cell><cell>85.4</cell><cell>11.9</cell><cell>84.0</cell><cell>11.7</cell><cell>80.3</cell><cell>10.0</cell><cell>83.4</cell><cell>11.4</cell></row><row><cell></cell><cell></cell><cell cols="2">Test Set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OscarB + SCST + CBS</cell><cell>81.3</cell><cell>11.9</cell><cell>79.6</cell><cell>11.9</cell><cell>73.6</cell><cell>10.6</cell><cell>78.8</cell><cell>11.7</cell></row><row><cell>OscarL + SCST + CBS</cell><cell cols="2">84.8 12.1</cell><cell>82.1</cell><cell>11.5</cell><cell>73.8</cell><cell>9.7</cell><cell>80.9</cell><cell>11.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Evaluation results on VQA.</figDesc><table><row><cell>Test-dev</cell><cell>70.63</cell><cell>70.50</cell><cell>70.80</cell><cell>72.42</cell><cell>73.15</cell><cell>72.27</cell><cell>73.24</cell><cell>73.16 73.61</cell></row><row><cell>Test-std</cell><cell>70.92</cell><cell>70.83</cell><cell>71.00</cell><cell>72.54</cell><cell></cell><cell>72.46</cell><cell>73.40</cell><cell>73.44 73.82</cell></row></table><note>Method ViLBERT VL-BERT VisualBERT LXMERT 12-in-1 UNITERB UNITERL OscarB OscarL</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Evaluation results on NLVR2.</figDesc><table><row><cell>Dev</cell><cell>50.8</cell><cell>6 7 .40</cell><cell>74.90</cell><cell></cell><cell>77.14</cell><cell>78.40</cell><cell>78.07 79.12</cell></row><row><cell cols="2">Test-P 51.4</cell><cell>6 7 .00</cell><cell>74.50</cell><cell>78.87</cell><cell>77.87</cell><cell>79.50</cell><cell>78.36 80.37</cell></row></table><note>Method MAC VisualBERT LXMERT 12-in-1 UNITERB UNITERL OscarB OscarL</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Evaluation results on GQA.</figDesc><table><row><cell cols="7">Method LXMERT MMN [4] 12-in-1 NSM [12] OscarB OscarB ⇤</cell></row><row><cell>Test-dev</cell><cell>60.00</cell><cell></cell><cell></cell><cell></cell><cell>61.19</cell><cell>61.58</cell></row><row><cell>Test-std</cell><cell>60.33</cell><cell>60.83</cell><cell>60.65</cell><cell>63.17</cell><cell>61.23</cell><cell>61.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Evaluation results on GQA.</figDesc><table><row><cell>Method</cell><cell cols="2">Test-dev Test-std</cell></row><row><cell cols="2">LXMERT [39] 60.00</cell><cell>60.33</cell></row><row><cell>MMN [4]</cell><cell></cell><cell>60.83</cell></row><row><cell>12-in-1 [24]</cell><cell></cell><cell>60.65</cell></row><row><cell>NSM [12]</cell><cell></cell><cell>63.17</cell></row><row><cell>OscarB OscarB ⇤</cell><cell>61.19 61.58</cell><cell>61.23 61.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>Evaluation results on COCO dataset. (Note: B for Base, L for Large)</figDesc><table><row><cell>Method</cell><cell>Size</cell><cell cols="11">Text Retrieval R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval Text Retrieval Image Retrieval</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1K Test Set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">5K Test Set</cell></row><row><cell>DVSA [14]</cell><cell>-</cell><cell cols="3">38.4 69.9 80.5</cell><cell cols="3">27.4 60.2 74.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VSE++ [7]</cell><cell>-</cell><cell>64.7</cell><cell>-</cell><cell>9 5 .9</cell><cell>52.0</cell><cell>-</cell><cell cols="2">9 2 .0 41.3</cell><cell>-</cell><cell cols="2">8 1 .2 30.3</cell><cell>-</cell><cell>7 2 .4</cell></row><row><cell>DPC [46]</cell><cell>-</cell><cell cols="3">65.6 89.8 95.5</cell><cell cols="8">47.1 79.9 90.0 41.2 70.5 81.1 25.3 53.4</cell><cell>66.4</cell></row><row><cell>CAMP [42]</cell><cell>-</cell><cell cols="3">72.3 94.8 98.3</cell><cell cols="8">58.5 87.9 95.0 50.1 82.1 89.7 39.0 68.9</cell><cell>80.2</cell></row><row><cell>SCAN [18]</cell><cell>-</cell><cell cols="3">72.7 94.8 98.4</cell><cell cols="8">58.8 88.4 94.8 50.4 82.2 90.0 38.6 69.3</cell><cell>80.4</cell></row><row><cell>SCG [33]</cell><cell>-</cell><cell cols="3">76.6 96.3 99.2</cell><cell cols="8">61.4 88.9 95.1 56.6 84.5 92.0 39.2 68.0</cell><cell>81.3</cell></row><row><cell>PFAN [41]</cell><cell>-</cell><cell cols="3">76.5 96.3 99.0</cell><cell cols="3">61.6 89.6 95.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Unicoder-VL [19] B</cell><cell cols="3">84.3 97.3 99.3</cell><cell cols="8">69.7 93.5 97.2 62.3 87.1 92.8 46.7 76.0</cell><cell>85.3</cell></row><row><cell>12-in-1 [24]</cell><cell>B</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">65.2 91.0 96.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UNITER [5]</cell><cell>B</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">63.3 87.0 93.1 48.4 76.7</cell><cell>85.9</cell></row><row><cell>UNITER [5]</cell><cell>L</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">66.6 89.4 94.3 51.7 78.4</cell><cell>86.9</cell></row><row><cell>Oscar</cell><cell>B L</cell><cell cols="11">88.4 99.1 99.8 75.7 95.2 98.3 70.0 91.1 95.5 54.0 80.8 89.8 98.8 99.7 78.2 95.8 98.3 73.5 92.2 96.0 57.5 82.8 89.8 88.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell>Method</cell><cell>cross-entropy optimization B@4 M C S</cell><cell>CIDEr optimization B@4 M C</cell><cell>S</cell></row><row><cell>BUTD [2]</cell><cell>36.2 27.0 113.5 20.3</cell><cell cols="2">36.3 27.7 120.1 21.4</cell></row><row><cell>VLP [47]</cell><cell>36.5 28.4 117.7 21.3</cell><cell cols="2">39.5 29.3 129.3 23.2</cell></row><row><cell>AoANet [11]</cell><cell>37.2 28.4 119.8 21.3</cell><cell cols="2">38.9 29.2 129.8 22.4</cell></row><row><cell>OscarB</cell><cell cols="3">36.5 30.3 123.7 23.1 40.5 29.7 137.6 22.8</cell></row><row><cell>OscarL</cell><cell cols="3">37.4 30.7 127.8 23.5 41.7 30.6 140.0 24.5</cell></row></table><note>Image captioning evaluation results (single model) on COCO "Karpa- thy" test split. (Note: B@4: BLUE@4, M: METEOR, C: CIDEr, S: SPICE.)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8 :</head><label>8</label><figDesc>NoCaps val set. Models are trained on COCO only without pre-training.</figDesc><table><row><cell>Method</cell><cell cols="8">in-domain CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE near-domain out-of-domain overall</cell></row><row><cell>UpDown [1]</cell><cell>78.1</cell><cell>11.6</cell><cell>57.7</cell><cell>10.3</cell><cell>31.3</cell><cell>8.3</cell><cell>55.3</cell><cell>10.1</cell></row><row><cell>UpDown + CBS [1]</cell><cell>80.0</cell><cell>12.0</cell><cell>73.6</cell><cell>11.3</cell><cell>66.4</cell><cell>9.7</cell><cell>73.1</cell><cell>11.1</cell></row><row><cell>UpDown + ELMo + CBS [1]</cell><cell>79.3</cell><cell>12.4</cell><cell>73.8</cell><cell>11.4</cell><cell>71.7</cell><cell>9.9</cell><cell>74.3</cell><cell>11.2</cell></row><row><cell>OscarB</cell><cell>79.6</cell><cell>12.3</cell><cell>66.1</cell><cell>11.5</cell><cell>45.3</cell><cell>9.7</cell><cell>63.8</cell><cell>11.2</cell></row><row><cell>OscarB + CBS</cell><cell>80.0</cell><cell>12.1</cell><cell>80.4</cell><cell>12.2</cell><cell>75.3</cell><cell>10.6</cell><cell>79.3</cell><cell>11.9</cell></row><row><cell>OscarB + SCST + CBS</cell><cell>83.4</cell><cell>12.0</cell><cell>81.6</cell><cell>12.0</cell><cell cols="2">77.6 10.6</cell><cell>81.1</cell><cell>11.7</cell></row><row><cell>OscarL</cell><cell>79.9</cell><cell>12.4</cell><cell>68.2</cell><cell>11.8</cell><cell>45.1</cell><cell>9.4</cell><cell>65.2</cell><cell>11.4</cell></row><row><cell>OscarL + CBS</cell><cell>78.8</cell><cell>12.2</cell><cell>78.9</cell><cell>12.1</cell><cell>77.4</cell><cell>10.5</cell><cell>78.6</cell><cell>11.8</cell></row><row><cell>OscarL + SCST + CBS</cell><cell>85.4</cell><cell>11.9</cell><cell>84.0</cell><cell>11.7</cell><cell>80.3</cell><cell>10.0</cell><cell>83.4</cell><cell>11.4</cell></row><row><cell></cell><cell></cell><cell cols="2">Test Set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OscarB + SCST + CBS</cell><cell>81.3</cell><cell>11.9</cell><cell>79.6</cell><cell>11.9</cell><cell>73.6</cell><cell>10.6</cell><cell>78.8</cell><cell>11.7</cell></row><row><cell>OscarL + SCST + CBS</cell><cell cols="2">84.8 12.1</cell><cell>82.1</cell><cell>11.5</cell><cell>73.8</cell><cell>9.7</cell><cell>80.9</cell><cell>11.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 :</head><label>9</label><figDesc>Evaluation results on VQA.</figDesc><table><row><cell cols="9">Method ViLBERT VL-BERT VisualBERT LXMERT 12-in-1 UNITERB UNITERL OscarB OscarL</cell></row><row><cell>Test-dev</cell><cell>70.63</cell><cell>70.50</cell><cell>70.80</cell><cell>72.42</cell><cell>73.15</cell><cell>72.27</cell><cell>73.24</cell><cell>73.16 73.61</cell></row><row><cell>Test-std</cell><cell>70.92</cell><cell>70.83</cell><cell>71.00</cell><cell>72.54</cell><cell></cell><cell>72.46</cell><cell>73.40</cell><cell>73.44 73.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Evaluation results on NLVR2. Evaluation on NoCaps Val. Models are trained on COCO only without pre-training.example, the visual and textual representations for person (or zebra) in Oscar is much closer than that in the baseline method. (ii) Inter-class. Object classes of related semantics are getting closer (but still distinguishable) after adding tags, while there are some mixtures in the baseline, such as animal (person, zebra, sheep, bird), furniture (chair, couch, bench), and transportation (bus,</figDesc><table><row><cell>(f)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dev</cell><cell>50.8</cell><cell>6 7 .40</cell><cell>74.90</cell><cell></cell><cell>77.14</cell><cell>78.40</cell><cell>78.07 79.12</cell></row><row><cell cols="2">Test-P 51.4</cell><cell>6 7 .00</cell><cell>74.50</cell><cell>78.87</cell><cell>77.87</cell><cell>79.50</cell><cell>78.36 80.05</cell></row></table><note>Method MAC VisualBERT LXMERT 12-in-1 UNITERB UNITERL OscarB OscarL</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 3 :</head><label>3</label><figDesc>Retrieval results on the COCO 1K test set, with different types of attention interactions.</figDesc><table><row><cell>Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</cell><cell>13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 5 :</head><label>5</label><figDesc>Results with various pre-training schemes.</figDesc><table><row><cell>Pre-train</cell><cell>VQA dev R@1 R@5 R@10 R@1 R@5 R@10 B@4 M Text Retrieval Image Retrieval Image Captioning C S</cell></row><row><cell cols="2">Baseline (No Tags) 70.93 84.4 98.1 99.5 73.1 94.5 97.9 34.5 29.1 115.6 21.9</cell></row><row><cell>Oscar VG</cell><cell>71.70 88.4 99.1 99.8 75.7 95.2 98.3 36.4 30.3 123.4 23.0</cell></row><row><cell>Oscar OI</cell><cell>71.15 85.9 97.9 99.5 72.9 94.3 97.6 35.3 29.6 119.5 22.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 10 .</head><label>10</label><figDesc>Ablations on di↵erent types of attention maps. Models are initialized from BERT base model without pre-training. Results are reported on the COCO 1K test set.</figDesc><table><row><cell></cell><cell>R@5</cell></row><row><cell>X X</cell><cell>X X 77.3 95.6 65.2 91.5 75.4 94.8 64.2 91.4 X 32.3 57.6 25.7 60.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 4 :</head><label>4</label><figDesc>Retrieval results on the COCO 1K test set, with di↵erent types of attention interactions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 6 :</head><label>6</label><figDesc>Retrieval results on the COCO 1K test set, with di↵erent types of attention interactions.</figDesc><table><row><cell cols="2">Attention</cell><cell>Text R.</cell><cell>Image R.</cell></row><row><cell cols="4">w-v w-q v-q R@1 R@5 R@1 R@5</cell></row><row><cell>X X</cell><cell cols="3">X X 77.3 95.6 65.2 91.5 75.4 94.8 64.2 91.4 X 32.3 57.6 25.7 60.1</cell></row><row><cell>6 Related Work</cell><cell></cell><cell></cell></row><row><cell cols="4">Vision-Language Pre-training There is an growing interest in pre-training</cell></row><row><cell cols="4">generic models to solve a variety of V+L problems, such as visual question-</cell></row><row><cell cols="4">answering (VQA), image-text retrieval and image captioning etc. The exist-</cell></row><row><cell>ing methods</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 4 :</head><label>4</label><figDesc>Results with various pre-training schemes.</figDesc><table><row><cell>Pre-train</cell><cell>VQA dev R@1 R@5 R@10 R@1 R@5 R@10 B@4 M Text Retrieval Image Retrieval Image Captioning C S</cell></row><row><cell cols="2">Baseline (No Tags) 70.93 84.4 98.1 99.5 73.1 94.5 97.9 34.5 29.1 115.6 21.9</cell></row><row><cell>Oscar VG</cell><cell>71.70 88.4 99.1 99.8 75.7 95.2 98.3 36.4 30.3 123.4 23.0</cell></row><row><cell>Oscar OI</cell><cell>71.15 85.9 97.9 99.5 72.9 94.3 97.6 35.3 29.6 119.5 22.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 5 :</head><label>5</label><figDesc>Statistics of the pre-training corpus.</figDesc><table><row><cell>Source</cell><cell>COCO (train)</cell><cell>CC (all)</cell><cell>SBU (all)</cell><cell>Flicker30k VQA (train) (train) (bal-train) (train) GQA VG-QA</cell><cell>Total</cell></row><row><cell cols="6">Image/Text 112k/560k 3.0M/3.0M 840k/840k 29k/145k 83k/444k 79k/1026k 48k/484k 4.1M/6.5M</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">It includes coordinates of top-left &amp; bottom-right corners, and/or height &amp; width.<ref type="bibr" target="#b5">4</ref> A semantic space can be viewed a vector space defined by a dictionary, which maps an input to a vector representation in the semantic space. For example, BERT can be viewed as a dictionary that defines a linguistic semantic space. BERT maps an input word or word sequence into a feature vector in the semantic space.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">This is not necessarily the best fine-tuning choice for NLVR2, please refer to the Pair-biattn finetuning in UNITER<ref type="bibr" target="#b6">[5]</ref> for a better choice, which introduces a multihead attention layer to look back the concatenated text-image sequences.<ref type="bibr" target="#b7">6</ref> All the (single-model) SoTAs are from the published results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Yonatan Bisk, Hannaneh Hajishirzi, Xiaodong Liu, Sachin Mehta, Hamid Palangi and Arun Sacheti, Rowan Zellers for valuable discussions and comments, and the Microsoft Research Technical Support team for managing the GPU clusters.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<title level="m">nocaps: novel object captioning at scale. In: ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00576</idno>
		<title level="m">Guided open vocabulary image captioning with constrained beam search</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<title level="m">nocaps: novel object captioning at scale. In: ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aligning sentences in parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 29th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03230</idno>
		<title level="m">Meta module network for compositional visual reasoning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<title level="m">Vse++: Improved visual-semantic embeddings</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DeViSE: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Towards learning a generic agent for vision-and-language navigation via pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning by abstraction: The neural state machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09506</idno>
		<title level="m">GQA: A new dataset for real-world visual reasoning and compositional question answering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<title level="m">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">VilBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02315</idno>
		<title level="m">12-in-1: Multi-Task vision and language representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sacheti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07966</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Joint image-text representation by gaussian visual-semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge aware semantic concept expansion for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Zero-shot learning through crossmodal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">VL-BERT: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00491</idno>
		<title level="m">A corpus for reasoning about natural language grounded in photographs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">VideoBERT: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LXMERT: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Position focused attention network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09748</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">CAMP: Cross-Modal adaptive message passing for text-image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">What value do explicit high level concepts have in vision to language problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05535</idno>
		<title level="m">Dual-path convolutional image-text embedding with instance loss</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unified visionlanguage pre-training for image captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
