<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Voxel R-CNN: Towards High Performance Voxel-based 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
							<email>ssshi@ee.cuhk.edu.hkzhwg</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Multimedia Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiwei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Voxel R-CNN: Towards High Performance Voxel-based 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances on 3D object detection heavily rely on how the 3D data are represented, i.e., voxel-based or point-based representation. Many existing high performance 3D detectors are point-based because this structure can better retain precise point positions. Nevertheless, point-level features lead to high computation overheads due to unordered storage. In contrast, the voxel-based structure is better suited for feature extraction but often yields lower accuracy because the input data are divided into grids. In this paper, we take a slightly different viewpoint -we find that precise positioning of raw points is not essential for high performance 3D object detection and that the coarse voxel granularity can also offer sufficient detection accuracy. Bearing this view in mind, we devise a simple but effective voxel-based framework, named Voxel R-CNN. By taking full advantage of voxel features in a two stage approach, our method achieves comparable detection accuracy with state-of-the-art point-based models, but at a fraction of the computation cost. Voxel R-CNN consists of a 3D backbone network, a 2D bird-eye-view (BEV) Region Proposal Network and a detect head. A voxel RoI pooling is devised to extract RoI features directly from voxel features for further refinement. Extensive experiments are conducted on the widely used KITTI Dataset and the more recent Waymo Open Dataset. Our results show that compared to existing voxel-based methods, Voxel R-CNN delivers a higher detection accuracy while maintaining a realtime frame processing rate, i.e., at a speed of 25 FPS on an NVIDIA RTX 2080 Ti GPU. The code is available at https://github.com/djiajunustc/Voxel-R-CNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D object detection using point clouds has received substantial attention in autonomous vehicles, robotics and augmented/virtual reality. Although the recent development of deep learning has led to the surge of object detection with 2D images <ref type="bibr" target="#b21">(Ren et al. 2015;</ref><ref type="bibr" target="#b14">Liu et al. 2016;</ref><ref type="bibr" target="#b20">Redmon and Farhadi 2017;</ref><ref type="bibr" target="#b27">Szegedy et al. 2017)</ref>, it is still non-trivial to apply these 2D methods to 3D point clouds, especially when dealing with the sparsity and unstructured property of point clouds. Besides, the applications usually demand high efficiency from detection systems, making it even harder to design a 3D detector due to the much larger 3D space. Existing 3D detection methods can be broadly grouped into two categories, i.e., voxel-based and point-based. The voxel-based methods <ref type="bibr" target="#b38">(Zhou and Tuzel 2018;</ref><ref type="bibr" target="#b30">Yan, Mao, and Li 2018;</ref><ref type="bibr" target="#b9">Lang et al. 2019</ref>) divide point clouds into regular grids, which are more applicable for convolutional neural networks (CNNs) and more efficient for feature extraction due to superior memory locality. Nevertheless, the downside is that voxelization often causes loss of precise position information. Current state-of-the-art 3D detectors are mainly point-based, which take raw point clouds as input, and abstract a set of point representations with iterative sampling and grouping <ref type="bibr">(Qi et al. 2017a,b)</ref>. The advanced point-based methods <ref type="bibr" target="#b23">Shi, Wang, and Li 2019;</ref><ref type="bibr" target="#b32">Yang et al. 2020;</ref><ref type="bibr" target="#b22">Shi et al. 2020a</ref>) are top ranked on various benchmarks <ref type="bibr" target="#b3">(Geiger, Lenz, and Urtasun 2012;</ref><ref type="bibr" target="#b0">Caesar et al. 2020;</ref><ref type="bibr" target="#b26">Sun et al. 2020</ref>). This has thus led to the popular viewpoint that the precise position information in raw point clouds is crucial for accurate object localization. Despite the superior detection accuracy, point-based methods are in general less efficient because it is more costly to search nearest neighbor with the point representation for point set abstraction.</p><p>As the detection algorithms become mature, we are ready to deploy these algorithms on realistic systems. Here, a new challenge arises: can we devise a method that is as accurate as advanced point-based methods and as fast as voxelbased methods? In this work, towards this objective, we take the voxel-based framework and try to boost its accuracy. We first argue that precise positioning of raw point clouds is nice but unnecessary. We observe that voxel-based methods generally perform object detection on the bird-eye-view (BEV) representation, even if the input data are in 3D voxels <ref type="bibr" target="#b30">(Yan, Mao, and Li 2018)</ref>. In contrast, point-based methods commonly rely on abstracted point representations to restore 3D structure context and make further refinement based on the point-wise features, as in <ref type="figure" target="#fig_0">Figure 1</ref> (a) (b). By taking a close look at the underlying mechanisms, we find that the key disadvantage of existing voxel-based methods stems from the fact that they convert 3D feature volumes into BEV representations without ever restoring the 3D structure context.</p><p>Bearing this in mind, we propose to aggregate 3D structure context from 3D feature volumes. Specifically, we introduce a novel voxel-based detector, i.e., Voxel R-CNN, to take full advantage of voxel features in a two stage pipeline (see <ref type="figure" target="#fig_0">Figure 1</ref> (c)). Voxel R-CNN consists of three core modules: (1) a 3D backbone network, (2) a 2D backbone network followed by the Region Proposal Network (RPN), and (3) a detect head with a new voxel RoI pooling operation. The 3D backbone network gradually abstracts voxels into 3D feature volumes. Dense region proposals are generated by the 2D backbone and the RPN. Then, the RoI features are directly extracted from 3D feature volumes with voxel RoI pooling. In designing voxel RoI pooling, we take the neighbor-aware property (which facilitates better memory locality) to extract neighboring voxel features and devise a local feature aggregation module for further acceleration. Finally, the 3D RoI features are taken for further box refinement.</p><p>The main contribution of this work stems from the design of Voxel R-CNN, which strikes a careful balance between accuracy and efficiency. The encouraging experiment results of Voxel R-CNN also confirm our viewpoint: the precise positioning of raw points is not essential for high performance 3D object detection and coarser voxel granularity can also offer sufficient spatial context cues for this task. Please note that our Voxel R-CNN framework serves as a simple but effective baseline facilitating further investigation and downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Reflection on 3D Object Detection</head><p>In this section, we first revisit two representative baseline methods, i.e., SECOND <ref type="bibr" target="#b30">(Yan, Mao, and Li 2018)</ref> and PV-RCNN <ref type="bibr" target="#b22">(Shi et al. 2020a)</ref>, and then investigate the key factors of developing a high performance 3D object detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Revisiting</head><p>SECOND. SECOND <ref type="bibr" target="#b30">(Yan, Mao, and Li 2018</ref>) is a voxelbased one stage object detector. It feeds the voxelized data  to a 3D backbone network for feature extraction. The 3D feature volumes are then converted to BEV representations. Finally, a 2D backbone followed by a Region Proposal Network (RPN) is applied to perform detection.</p><p>PV-RCNN. PV-RCNN <ref type="bibr" target="#b22">(Shi et al. 2020a</ref>) extends SECOND by adding a keypoints branch to preserve 3D structural information. Voxel Set Abstraction (VSA) is introduced to integrate multiscale 3D voxels features into keypoints. The features of each 3D region proposals are further extracted from the keypoints through RoI-grid pooling for box refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Analysis</head><p>There exists a large gap between SECOND and PV-RCNN in terms of the detection performance (i.e., accuracy and efficiency). These two methods differ in the following ways. Firstly, SECOND is a one stage method while PV-RCNN takes detect head for box refinement. Secondly, keypoints in PV-RCNN preserve the 3D structure information, while SECOND performs detection directly on BEV representations. To verify the influence of box refinement and 3D structure information on detection performance, we add a detect head on the top of the 2D backbone network in SEC-OND. Since the BEV boxes are not aligned with the axis, we exploit Rotated RoI Align for RoI feature extraction. As illustrated in <ref type="table">Table 1</ref>, directly adding a BEV detect head on top of BEV features leads to 0.6% AP improvement for KITTI car moderate data, but has still trailed the accuracy of PV-RCNN thus far. This verifies the effectiveness of box refinement, and also demonstrates that the capacity of BEV representation is rather limited. Typically, PV-RCNN integrates voxel features into sampled keypoints with Voxel Set Abstraction. The keypoints works as an intermediate feature representation to effectively preserve 3D structure information. However, as illustrated in <ref type="table" target="#tab_1">Table 2</ref>, the point-voxel interaction takes almost half of the overall running time, which makes PV-RCNN much slower than SECOND.</p><p>Summary. In summary, by analyzing the limitations of bird-eye-view (BEV) feature representations in SECOND and the computation cost of each component in PV-RCNN,  <ref type="figure">Figure 2</ref>: An overview of Voxel R-CNN for 3D object detection. The point clouds are first divided into regular voxels and fed into the 3D backbone network for feature extraction. Then, the 3D feture volumes are converted into BEV representation, on which we apply the 2D backbone and RPN for region proposal generation. Subsequently, voxel RoI pooling directly extracts RoI features from the 3D feature volumes. Finally the RoI features are exploited in the detect head for further box refinement.</p><p>we observe the following: (a) the 3D structure is of significant importance for 3D object detectors, since the BEV representation alone is insufficient to precisely predict bounding boxes in a 3D space; and (b) the point-voxel feature interaction is time-consuming and affects the detector's efficiency. These observations motivate us to directly leverage the 3D voxel tensors and develop a voxel-only 3D object detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Voxel R-CNN Design</head><p>In this section, we present the design of Voxel R-CNN, a voxel-based two stage framework for 3D object detection. As shown in <ref type="figure">Figure 2</ref>, Voxel R-CNN includes: (a) a 3D backbone network, (b) a 2D backbone network followed by the Region Proposal Network (RPN), and (c) a voxel RoI pooling and a detect subnet for box refinement. In Voxel R-CNN, we first divide the raw point cloud into regular voxels and utilize the 3D backbone network for feature extraction. We then convert the sparse 3D voxels into BEV representations, on which we apply the 2D backbone network and the RPN to generate 3D region proposals. Subsequently, we use Voxel RoI pooling to extract RoI features, which are fed into the detect subnet for box refinement. Below we discuss these modules in detail. Since our innovation mainly lies in voxel RoI pooling, we discuss it first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Voxel RoI pooling</head><p>To directly aggregate spatial context from 3D voxel feature volumes, we propose voxel RoI pooling.</p><p>Voxel Volumes as Points. We represent the sparse 3D volumes as a set of non-empty voxel center points</p><formula xml:id="formula_0">{v i = (x i , y i , z i )} N i=1</formula><p>and their corresponding feature vectors</p><formula xml:id="formula_1">{φ i } N i=1</formula><p>. Specifically, the 3D coordinates of voxel centers Voxel Query. We propose a new operation, named voxel query, to find neighbor voxels from the 3D feature volumes. Compared to the unordered point clouds, the voxels are regularly arranged in the quantified space, lending itself to easy neighbor access. For example, the 26-neighbor voxels of a query voxel can be easily computed by adding a triplet of offsets</p><formula xml:id="formula_2">(∆ i , ∆ j , ∆ k ), ∆ i , ∆ j , ∆ k ∈ {−1, 0, 1} on the voxel indices (i, j, k)</formula><p>. By taking advantage of this property, we devise voxel query to efficiently group voxels. The voxel query is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. The query point is first quantified into a voxel, and then the neighbor voxels can be efficiently obtained by indices translation. We exploit Manhattan distance in voxel query and sample up to K voxels within a distance threshold. Specifically, the Manhattan distance D(α, β) between the voxel α = (i α , j α , k α ) and β = (i β , j β , k β ) is computed as:</p><formula xml:id="formula_3">D m (α, β) = |i α − i β | + |j α − j β | + |k α − k β |.<label>(1)</label></formula><p>Suppose there are N non-empty voxels in the 3D feature volumes and we utilize ball query to find neighboring voxels ... to a given query point, the time complexity is O(N ). Nevertheless, the time complexity of conducting voxel query is only O(K), where K is number of neighbors. The neighboraware property makes it more efficient to group neighbor voxel features with our voxel query than to group neighbor point features with ball query <ref type="bibr" target="#b19">(Qi et al. 2017b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grid Points</head><p>Voxel RoI Pooling Layer. We design the voxel RoI pooling layer as follows. It starts by dividing a region proposal into G×G×G regular sub-voxels. The center point is taken as the grid point of the corresponding sub-voxel. Since 3D feature volumes are extremely sparse (non-empty voxels account for &lt; 3% spaces), we cannot directly utilize max pooling over features of each sub-voxel as in <ref type="bibr" target="#b4">(Girshick 2015)</ref>. Instead, we integrate features from neighboring voxels into the grid points for feature extraction. Specifically, given a grid point g i , we first exploit voxel query to group a set of neighboring voxels</p><formula xml:id="formula_4">Γ i = {v 1 i , v 2 i , · · · , v K i }.</formula><p>Then, we aggregate the neighboring voxel features with a PointNet module <ref type="bibr" target="#b18">(Qi et al. 2017a</ref>) as:</p><formula xml:id="formula_5">η i = max k=1,2,··· ,K {Ψ([v k i − g i ; φ k i ])},<label>(2)</label></formula><p>where v i − g i represents the relative coordinates, φ k i is the voxel feature of v k i , and Ψ(·) indicates an MLP. The max pooling operation max(·) is performed along the channels to obtain the aggregated feature vector η i . Particularly, we exploit Voxel RoI pooling to extract voxel features from the 3D feature volumes out of the last two stages in the 3D backbone network. And for each stage, two Manhattan distance thresholds are set to group voxels with multiple scales. Then, we concatenate the aggregated features pooled from different stages and scales to obtain the RoI features.</p><p>Accelerated Local Aggregation. Even with our proposed voxel query, the local aggregation operation (i.e., PointNet module) in Voxel RoI pooling still involves large computation complexity. As shown in <ref type="figure" target="#fig_2">Figure 4 (a)</ref>, there are totally M grid points (M = r × G 3 , where r is the number of RoI, and G is the grid size), and K voxels are grouped for each grid point. The dimension of grouped feature vectors is C + 3, including the C-dim voxel features and 3-dim relative coordinates. The grouped voxels occupy a lot of memories and lead to large computation FLOPs (O(M × K × (C + 3) × C )) when applying the FC layer.</p><p>As motivated by <ref type="bibr" target="#b15">(Liu et al. 2020a;</ref><ref type="bibr" target="#b6">Hu et al. 2020)</ref>, we additionally introduce an accelerated PointNet Module to further reduce the computation complexity of Voxel Query. Typically, as shown in <ref type="figure" target="#fig_2">Figure 4 (b)</ref>, the voxel features and relative coordinates are decomposed into two streams. Given the FC layer with weight W ∈ R C ,C+3 , we divide it into W F ∈ R C ,C and W C ∈ R C ,3 . Since the voxel features are independent of the grid points, we apply a FC layer with W F on the voxel features before performing voxel query. Then, after voxel query, we only multiply the grouped relative coordinates by W C to obtain the relative position features and add them to the grouped voxel features. The FLOPs of our accelerated PointNet module are O(N × C × C + M × K × 3 × C ). Since the number of grouped voxels (M × K) is an order of magnitude higher than N , the accelerated PointNet module is more efficient than the original one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Backbone and Region Proposal Networks</head><p>We follow the similar design of <ref type="bibr" target="#b30">(Yan, Mao, and Li 2018;</ref><ref type="bibr" target="#b5">He et al. 2020;</ref><ref type="bibr" target="#b22">Shi et al. 2020a</ref>) to build our backbone networks. The 3D backbone network gradually converts the voxelized inputs into feature volumes. Then, the output tensors are stacked along the Z axis to produce BEV feature maps. The 2D backbone network consists of two components: a top-down feature extraction sub-network with two blocks of standard 3×3 convolution layers and a mutli-scale feature fusion sub-network that upsamples and concatenates the top-down features. Finally, the output of the 2D backbone network is convolved with two sibling 1 × 1 convolutional layers to generate 3D region proposals. We will detail the architecture of our backbone networks in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Detect Head</head><p>The detect head takes RoI features as input for box refinement. Specifically, a shared 2-layer MLP first transform RoI features into feature vectors. Then, the flattened features are injected into two sibling branches: one for bounding box regression and the other for confidence prediction. As motivated by <ref type="bibr" target="#b7">(Jiang et al. 2018;</ref><ref type="bibr" target="#b11">Li et al. 2019;</ref><ref type="bibr">Shi et al. 2020a,b)</ref>, the box regression branch predicts the residue from 3D region proposals to the ground truth boxes, and the confidence branch predicts the IoU-related confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Objectives</head><p>Losses of RPN. We follow <ref type="bibr" target="#b30">(Yan, Mao, and Li 2018;</ref><ref type="bibr" target="#b9">Lang et al. 2019)</ref> to devise the losses of the RPN as a combination of classification loss and box regression loss, as:</p><formula xml:id="formula_6">L RPN = 1 N fg [ i L cls (p a i , c * i ) + 1(c * i ≥ 1) i Lreg(δ a i , t * i )],<label>(3)</label></formula><p>where N fg represents the number of foreground anchors, p a i and δ a i are the outputs of classification and box regression branches, c * i and t * i are the classification label and regression targets respectively. 1(c * i ≥ 1) indicates regression loss in only calculated with foreground anchors. Here we utilize Focal Loss <ref type="bibr" target="#b13">(Lin et al. 2017</ref>) for classification and Huber Loss for box regression.</p><p>Losses of detect head. The target assigned to the confidence branch is an IoU related value, as:</p><formula xml:id="formula_7">l * i (IoU i ) =    0 IoU i &lt; θ L , IoUi−θ L θ H −θ L θ L ≤ IoU i &lt; θ H , 1 IoU i &gt; θ H ,<label>(4)</label></formula><p>where IoU i is the IoU between the i-th proposal and the corresponding ground truth box, θ H and θ L are foreground and background IoU thresholds. Binary Cross Entropy Loss is exploited here for confidence prediction. The box regression branch also uses Huber Loss as in the RPN. The losses of our detect head are computed as:</p><formula xml:id="formula_8">Lhead = 1 Ns [ i Lcls(pi, l * i (IoUi)) + 1(IoUi ≥ θreg) i Lreg(δi, t * i )],<label>(5)</label></formula><p>where N s is the number of sampled region proposals at the training stage, and 1(IoU i ≥ θ reg ) indicates that only region proposals with IoU &gt; θ reg contribute to the regression loss.  Network Architecture. The architecture of 3D backbone and 2D backbone follows the design in <ref type="bibr" target="#b30">(Yan, Mao, and Li 2018;</ref><ref type="bibr" target="#b22">Shi et al. 2020a</ref>). There are four stages in the 3D backbone with filter numbers of 16, 32, 48, 64 respectively. There are two blocks in the 2D backbone network, the first block keeps the same resolution along X and Y axes as the output of 3D backbone network, while the second block is half the resolution of the first one. The numbers of convolutional layers in these two blocks, i.e., N 1 and N 2, are both set as 5. And the feature dimensions of these blocks are <ref type="formula">(</ref>   <ref type="table">Table 5</ref>: Performance comparison on the KITTI val set with AP calculated by 11 recall positions for car class set as 0.25, and the box regression IoU threshold θ reg is set as 0.55. We randomly sample 128 RoIs as the training samples of detect head. Within the sampled RoIs, half of them are positive samples that have IoU &gt; θ reg with the corresponding ground truth boxes. We conduct data augmentation at the training stage following strategies in <ref type="bibr" target="#b9">(Lang et al. 2019;</ref><ref type="bibr" target="#b22">Shi et al. 2020a;</ref><ref type="bibr" target="#b32">Yang et al. 2020;</ref><ref type="bibr" target="#b34">Ye, Xu, and Cao 2020)</ref>. Please refer to OpenPCDet (Team 2020) for more detailed configurations since we conduct all experiments with this toolbox.</p><p>Inference. At the inference stage, we first perform nonmaximum suppression (NMS) in the RPN with IoU threshold 0.7 and keep the top-100 region proposals as the input of detect head. Then, after refinement, NMS is applied again with IoU threshold 0.1 to remove the redundant predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on KITTI Dataset</head><p>We evaluate our Voxel R-CNN on KITTI Dataset following the common protocol to report the average precision (AP) of class Car with the 0.7 (IoU) threshold. We report our performance on both val set and test set for comparison and analysis. The performance on val set is calculated with the AP setting of recall 11 positions. And the results evaluated by the test server utilize AP setting of recall 40 positions 1 .</p><p>Comparison with State-of-the-art Methods. We compare our Voxel R-CNN with several state-of-the-art methods on the KITTI test set by submitting our results to the online test server. As shown in the   <ref type="bibr" target="#b22">(Shi et al. 2020a)</ref>, with only about 1/3 running time. This verifies the spatial context in the voxel representation is almost sufficient for 3D object detection, and the voxel representation is more efficient for feature extraction. Besides, Voxel R-CNN outperforms all of the existing voxel-based models by a large margin, i.e., 2.15%, 1.83%, 2.90% absolute improvements for easy, moderate and hard level over the SA-SSD <ref type="bibr" target="#b5">(He et al. 2020</ref>).</p><p>The AP for 3D object detection and BEV object detection of our Voxel R-CNN on KITTI Dataset is presented in <ref type="table" target="#tab_6">Table 4</ref>. The results in this table are calculated by recall 40 positions with the (IoU) threshold 0.7. In addition, we report the performance on the KITTI val set with AP calculated by recall 11 positions for further comparison. As shown in Table 5, our Voxel R-CNN achieves the best performance on moderate and hard level on the val set.</p><p>Overall, the results on both val set and test set consistently demonstrate that our proposed Voxel R-CNN achieves the state-of-the-art average precision on 3D object detection and keeps the high efficiency of voxel-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Waymo Open Dataset</head><p>We also conduct experiments on the larger Waymo Open Dataset to further validate the effectiveness of our proposed Voxel R-CNN. The objects on the Waymo Open Dataset are split into two levels based on the number of points of a single object, where the LEVEL 1 objects have more than 5 points while the LEVEL 2 objects have 1∼5 points.</p><p>Comparison with State-of-the-art Methods. We evaluate our Voxel R-CNN on both LEVEL 1 and LEVEL 2 objects and compare with several top-performing methods on the Waymo Open Dataset. <ref type="table" target="#tab_8">Table 6</ref> shows that our method surpasses all previous methods with remarkable margins on all ranges of both LEVEL 1 and LEVEL 2. Specifically, with the commonly used LEVEL 1 3D mAP evaluation metric, our Voxel R-CNN achieves new state-of-the-art performance with 75.59% mAP, and outperforms previous state-of-the-art   Method (a) is the one stage baseline that performs detection on BEV features. It runs at 40.8 FPS, but with unsatisfactory AP, which indicates BEV representation alone is insufficient to precisely detect objects in a 3D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>Method (b) extends (a) with a detect head for box refinement, which leads to a boost of 4.22% moderate AP. This verifies the spatial context from 3D voxels provides sufficient cues for precise object detection. However, (b) applies time-consuming ball query and original PointNet modules to extract RoI features, leading to a decrease of 23.4 FPS.</p><p>Method (c) replaces ball query with our voxel query, which makes 1.7 FPS improvement by taking advantage of the neighbor-aware property of voxel-based representations.</p><p>Method (d) uses our accelerated PointNet module to aggregate voxel features, boosting the FPS from 17.4 to 21.4.</p><p>Method (e) is the proposed Voxel R-CNN. By extending the one stage baseline with a detect head, taking voxel query for voxel grouping, and utilizing the accelerated Point-Net module for local feature aggregation, Voxel R-CNN achieves the state-of-the-art accuracy for 3D object detection and maintain the high efficiency of voxel-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>We broadly categorize the methods for 3D object detection on point clouds into point-based <ref type="bibr" target="#b23">(Shi, Wang, and Li 2019;</ref><ref type="bibr" target="#b33">Yang et al. 2019;</ref><ref type="bibr" target="#b35">Yilun Chen and Jia 2019;</ref><ref type="bibr" target="#b32">Yang et al. 2020;</ref><ref type="bibr" target="#b22">Shi et al. 2020a</ref>) methods and voxel-based <ref type="bibr" target="#b38">(Zhou and Tuzel 2018;</ref><ref type="bibr" target="#b30">Yan, Mao, and Li 2018;</ref><ref type="bibr" target="#b31">Yang, Luo, and Urtasun 2018;</ref><ref type="bibr" target="#b25">Simon et al. 2018;</ref><ref type="bibr" target="#b9">Lang et al. 2019;</ref><ref type="bibr" target="#b16">Liu et al. 2020b</ref>) methods. Here we give a brief review on these two directions:</p><p>Point-based Methods. Point-based methods take the raw point clouds as inputs, and abstract a set of point representations with iterative sampling and grouping <ref type="bibr">(Qi et al. 2017a,b)</ref>. PointRCNN <ref type="bibr" target="#b23">(Shi, Wang, and Li 2019)</ref> introduces a 3D region proposal network based on PointNet++ <ref type="bibr" target="#b19">(Qi et al. 2017b</ref>) and utilizes point cloud RoI pooling to extract 3D region features of each proposal. STD ) exploits the PointsPool operation to extract points features of each proposal and transfer the sparse points into dense voxel representation. Then, 3D CNNs are applied on the voxelized region features for further refinement. 3DSSD ) introduces F-FPS as a supplement of D-FPS and builds a one stage anchor-free 3D object detector based on feasible representative points. Different from grouping neighboring points for set abstraction, PV-RCNN <ref type="bibr" target="#b22">(Shi et al. 2020a</ref>) devises voxel set abstraction to integrates multi-scale voxel features into sampled keypoints. By leveraging both the accurate position information from raw points and spatial context from voxel representations, PV-RCNN achieves remarkable improvements on 3D object detection.</p><p>Voxel-based Methods. Voxel-based methods typically discrete point clouds into equally spaced grids, and then capitalize on 2D/3D CNN to perform object detection. The early work VoxelNet <ref type="bibr" target="#b38">(Zhou and Tuzel 2018)</ref> first divides points into 3D voxels and uses a tiny PointNet to transform points of each voxel into a compact feature representation. Then, 3D CNNs are leveraged to aggregate spatial context and generate 3D detections. However, due to the sparsity of nonempty voxels in the large space, it is inefficient to exploit conventional convolution networks for feature extraction.</p><p>To reduce the computation cost, SECOND <ref type="bibr" target="#b30">(Yan, Mao, and Li 2018)</ref> introduces sparse 3D convolution for efficient voxel processing. SA-SSD <ref type="bibr" target="#b5">(He et al. 2020)</ref> proposes an auxiliary network and losses on the basis of SECOND <ref type="bibr" target="#b30">(Yan, Mao, and Li 2018)</ref> to preserve structure information. Different from the methods operating on voxels in 3D space, PointPillars <ref type="bibr" target="#b9">(Lang et al. 2019</ref>) groups points as "pillars" and capitalizes on a simplified PointNet <ref type="bibr">(Qi et al. 2017a,b)</ref> for pillar feature extraction before forming pseudo BEV images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present Voxel R-CNN, a novel 3D object detector with voxel-based representations. Taking the voxels as input, Voxel R-CNN first generates dense region proposals from bird-eye-view feature representations, and subsequently, utilizes voxel RoI pooling to extract region features from 3D voxel features for further refinement. By taking full advantage of voxel representations, our Voxel R-CNN strikes a careful balance between accuracy and efficiency. The encouraging results on both KITTI Dataset and Waymo Open Dataset demonstrate our Voxel-RCNN can serve as a simple but effective baseline to facilitate the investigation of 3D object detection and other downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison with state-of-the-art 3D object detection methods in two stage paradigm. Instead of aggregating RoI features from a set of point representations, our method directly extract RoI features from 3D voxel feature volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of ball query and our voxel query (performed in 3D space but shown in 2D space here) are calculated with the indices, voxel sizes and the point cloud boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of different schemes to aggregate voxel features: (a) original PointNet module; (b) accelerated Point-Net module. We only pick one grid point as an example, and the same operations are applied on all the other grid points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Shi et al. 2020a)  89.35 83.69 78.70  SECOND (Yan, Mao, and<ref type="bibr" target="#b30">Li 2018)</ref> 88.61 78.62 77.22 SECOND +BEV detect head 89.50 79.26 78.45 Table 1: Performance comparison for adding BEV detect head on the top of SECOND. These results are evaluated on the KITTI val set with average precision calculated by 11 recall positions for car class.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Easy</cell><cell>AP3D (%) Mod.</cell><cell>Hard</cell></row><row><cell cols="5">PV-RCNN (Components 3D backbone 2D backbone VSA Detect head</cell></row><row><cell>Avg. time (ms)</cell><cell>21.1</cell><cell>9.4</cell><cell>49.4</cell><cell>19.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Running time comparison for each component in</cell></row><row><cell>PV-RCNN. This result is calculated by the average over the</cell></row><row><cell>3,769 samples in the KITTI val set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>train set with 3712 samples and a val set with 3769 samples. The performance on both the val set and online test leaderboard are reported for comparison. When performing experimental studies on the val set, we use the train set for training. For test server submission, we randomly select 80% samples from the training point clouds for training, and use the remaining 20% samples for validation as in<ref type="bibr" target="#b22">(Shi et al. 2020a</ref>).</figDesc><table><row><cell>Method</cell><cell>FPS (Hz)</cell><cell>Easy</cell><cell>AP3D (%) Mod.</cell><cell>Hard</cell></row><row><cell cols="2">RGB+LiDAR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MV3D (Chen et al. 2017)</cell><cell>-</cell><cell>74.97</cell><cell>63.63</cell><cell>54.00</cell></row><row><cell>AVOD-FPN (Ku et al. 2018)</cell><cell>10.0</cell><cell>83.07</cell><cell>71.76</cell><cell>65.73</cell></row><row><cell>F-PointNet (Qi et al. 2018)</cell><cell>5.9</cell><cell>82.19</cell><cell>69.79</cell><cell>60.59</cell></row><row><cell>PointSIFT+SENet (Zhao et al. 2019)</cell><cell>-</cell><cell>85.99</cell><cell>72.72</cell><cell>64.58</cell></row><row><cell>UberATG-MMF (Liang et al. 2019)</cell><cell>-</cell><cell>88.40</cell><cell>77.43</cell><cell>70.22</cell></row><row><cell cols="2">LiDAR-only</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Point-based:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointRCNN (Shi, Wang, and Li 2019)</cell><cell>10.0</cell><cell>86.96</cell><cell>75.64</cell><cell>70.70</cell></row><row><cell>STD (Yang et al. 2019)</cell><cell>12.5</cell><cell>87.95</cell><cell>79.71</cell><cell>75.09</cell></row><row><cell>Patches (Lehner et al. 2019)</cell><cell>-</cell><cell>88.67</cell><cell>77.20</cell><cell>71.82</cell></row><row><cell>3DSSD (Yang et al. 2020)</cell><cell>26.3</cell><cell>88.36</cell><cell>79.57</cell><cell>74.55</cell></row><row><cell>PV-RCNN (Shi et al. 2020a)</cell><cell>8.9</cell><cell>90.25</cell><cell>81.43</cell><cell>76.82</cell></row><row><cell>Voxel-based:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VoxelNet (Zhou and Tuzel 2018)</cell><cell>-</cell><cell>77.47</cell><cell>65.11</cell><cell>57.73</cell></row><row><cell>SECOND (Yan, Mao, and Li 2018)</cell><cell>30.4</cell><cell>83.34</cell><cell>72.55</cell><cell>65.82</cell></row><row><cell>PointPillars (Lang et al. 2019)</cell><cell>42.4</cell><cell>82.58</cell><cell>74.31</cell><cell>68.99</cell></row><row><cell>Part-A 2 (Shi et al. 2020b)</cell><cell>-</cell><cell>87.81</cell><cell>78.49</cell><cell>73.51</cell></row><row><cell>TANet (Liu et al. 2020b)</cell><cell>28.7</cell><cell>85.94</cell><cell>75.76</cell><cell>68.32</cell></row><row><cell>HVNet (Ye, Xu, and Cao 2020)</cell><cell>31.0</cell><cell>87.21</cell><cell>77.58</cell><cell>71.79</cell></row><row><cell>SA-SSD (He et al. 2020)</cell><cell>25.0</cell><cell>88.75</cell><cell>79.79</cell><cell>74.16</cell></row><row><cell>Voxel R-CNN (ours)</cell><cell>25.2</cell><cell>90.90</cell><cell>81.62</cell><cell>77.06</cell></row><row><cell>4 Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1 Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>KITTI. The KITTI Dataset (Geiger, Lenz, and Urtasun</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2012) contains 7481 training samples and 7518 testing sam-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ples in autonomous driving scenes. As a common practice,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the training data are divided into a Waymo Open Dataset. The Waymo Open Dataset (Sun</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>et al. 2020) is the largest public dataset for autonomous driv-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ing to date. There are totally 1, 000 sequences in this dataset,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>including 798 sequences (∼158k point clouds samples) in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>training set and 202 sequences (∼40k point clouds samples)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>in validation set. Different from KITTI that only provides annotations in camera FOV, the Waymo Open Dataset pro- vides annotations for objects in the full 360 • .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on the KITTI test set with AP calculated by recall 40 positions for car class 4.2 Implementation Details Voxelization. The raw point clouds are divided into regular voxels before taken as the input of our Voxel R-CNN. Since the KITTI Dataset only provides the annotations of object in FOV, we clip the range of point clouds into [0, 70.4]m for the X axis, [−40, 40]m for the Y axis and [−3, 1]m for Z axis. The input voxel size is set as (0.05m, 0.05m, 0.1m). For the Waymo Open Dataset, the range of point clouds is clipped into [−75.2, 75.2]m for X and Y axes and [−2, 4]m for the Z axis. The input voxel size is set as (0.1m, 0.1m, 0.15m).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>64, 128)   for KITTIDataset and (128, 256)  for Waymo Open Dataset. Each region proposal is divided into a 6×6×6 grid for Voxel RoI pooling. The Manhattan distance thresholds are set as 2 and 4 for multi-scale voxel query. The output channels of the local feature aggregation module are 32 for KITTI Dataset and 64 for Waymo Open Dataset.Training. The whole architecture of our Voxel R-CNN is end-to-end optimized with the Adam optimizer. For KITTI Dataset, the network is trained for 80 epochs with the batch size 16. For Waymo Open Dataset, the network is trained for 30 epochs with the batch size 32. The learning rate is initialized as 0.01 for both datasets and updated by cosine annealing strategy. In the detect head, the foreground IoU threshold θ H is set as 0.75, background IoU threshold θ L is</figDesc><table><row><cell>IoU</cell><cell></cell><cell>AP3D (%)</cell><cell></cell><cell>APBEV (%)</cell><cell></cell></row><row><cell>Thresh.</cell><cell cols="5">Easy Moderate Hard Easy Moderate Hard</cell></row><row><cell>0.7</cell><cell>92.38</cell><cell>85.29</cell><cell>82.86 95.52</cell><cell>91.25</cell><cell>88.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance of Voxel R-CNN on the KITTI val set with AP calculated by 40 recall positions for car class</figDesc><table><row><cell>Method</cell><cell>Easy</cell><cell>AP3D (%) Moderate</cell><cell>Hard</cell></row><row><cell>Point-based:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointRCNN (Shi, Wang, and Li 2019)</cell><cell>88.88</cell><cell>78.63</cell><cell>77.38</cell></row><row><cell>STD (Yang et al. 2019)</cell><cell>89.70</cell><cell>79.80</cell><cell>79.30</cell></row><row><cell>3DSSD (Yang et al. 2020)</cell><cell>89.71</cell><cell>79.45</cell><cell>78.67</cell></row><row><cell>PV-RCNN (Shi et al. 2020a)</cell><cell>89.35</cell><cell>83.69</cell><cell>78.70</cell></row><row><cell>Voxel-based:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VoxelNet (Zhou and Tuzel 2018)</cell><cell>81.97</cell><cell>65.46</cell><cell>62.85</cell></row><row><cell>SECOND (Yan, Mao, and Li 2018)</cell><cell>88.61</cell><cell>78.62</cell><cell>77.22</cell></row><row><cell>PointPillars (Lang et al. 2019)</cell><cell>86.62</cell><cell>76.06</cell><cell>68.91</cell></row><row><cell>Part-A 2 (Shi et al. 2020b)</cell><cell>89.47</cell><cell>79.47</cell><cell>78.54</cell></row><row><cell>TANet (Liu et al. 2020b)</cell><cell>87.52</cell><cell>76.64</cell><cell>73.86</cell></row><row><cell>SA-SSD (He et al. 2020)</cell><cell>90.15</cell><cell>79.91</cell><cell>78.78</cell></row><row><cell>Voxel R-CNN (ours)</cell><cell>89.41</cell><cell>84.52</cell><cell>78.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>, our Voxel R-CNN makes the</cell></row><row><cell>best balance between the accuracy and efficiency among all</cell></row><row><cell>the methods. By taking full advantage of voxel-based rep-</cell></row><row><cell>resentation, Voxel R-CNN achieves 81.62% average preci-</cell></row><row><cell>sion (AP) on the moderate level of class Car, with the real</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Performance comparison on the Waymo Open</cell></row><row><cell>Dataset with 202 validation sequences (∼40k samples) for</cell></row><row><cell>the vehicle detection</cell></row><row><cell>time processing frame rate (25.2 FPS). Particularly, Voxel R-</cell></row><row><cell>CNN achieves comparable accuracy with the strongest com-</cell></row><row><cell>petitor, i.e., PV-RCNN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Performance of proposed method with different configurations on KITTI val set. "D.H.", "V.Q." and "A.P." stand for detect head, voxel query and accelerated PointNet module respectively. The results are evaluated with the average precision calculated by 11 recall positions for car class.</figDesc><table><row><cell>method PV-RCNN by 5.29% mAP. Especially, our method</cell></row><row><cell>outperforms PV-RCNN with +10.98% mAP in the range of</cell></row><row><cell>50m-Inf, the significant gains on the far away area demon-</cell></row><row><cell>strate the effectiveness of our proposed Voxel R-CNN for</cell></row><row><cell>detecting the objects with very sparse points.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7</head><label>7</label><figDesc>details how each proposed module influences the accuracy and efficiency of our Voxel R-CNN. The results are evaluated with AP of moderate level for car class.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The setting of AP calculation is modified from recall 11 positions to recall 40 positions on 08.10.2019. We exploits the recall 11 setting on val set for fair comparison with previous methods.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This work was supported by the National Major Program for Technological Innovation 2030-New Generation Artificial Intelligence (2018AAA0100500). The work of Wengang Zhou was supported in part by NSFC under Contract 61822208, 61632019 and 62021001, and in part by the Youth Innovation Promotion Association CAS under Grant 2018497. The work of Houqiang Li was supported by NSFC under Contract 61836011.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11621" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiview 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12671</idno>
		<title level="m">Afdet: Anchor free one stage 3d object detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structure aware single-stage 3d object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11873" to="11882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RandLA-Net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PointPillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitterecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04093</idno>
		<title level="m">Patch refinement-localized 3D object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A Closer Look at Local Aggregation Operators in Point Cloud Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01294</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TANet: Robust 3D object detection from point clouds with triple attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11677" to="11684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster rcnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PV-RCNN: Point-voxel feature set abstraction for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Complex-YOLO: An euler-region-proposal for real-time 3D object detection on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="197" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2446" to="2454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">OpenPCDet: An Open-source Toolbox for 3D Object Detection from Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/OpenPCDet" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pillar-based object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10323</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3DSSD: Pointbased 3D single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11040" to="11048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">STD: Sparse-to-dense 3D object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">HVNet: Hybrid voxel network for LiDAR based 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast point rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9774" to="9783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3D Object Detection Using Scale Invariant and Feature Reweighting Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-toend multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
