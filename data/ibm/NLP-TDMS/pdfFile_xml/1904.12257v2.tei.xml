<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatio-Temporal Filter Adaptive Network for Video Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">Haozhe</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spatio-Temporal Filter Adaptive Network for Video Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video deblurring is a challenging task due to the spatially variant blur caused by camera shake, object motions, and depth variations, etc. Existing methods usually estimate optical flow in the blurry video to align consecutive frames or approximate blur kernels. However, they tend to generate artifacts or cannot effectively remove blur when the estimated optical flow is not accurate. To overcome the limitation of separate optical flow estimation, we propose a Spatio-Temporal Filter Adaptive Network (STFAN) for the alignment and deblurring in a unified framework. The proposed STFAN takes both blurry and restored images of the previous frame as well as blurry image of the current frame as input, and dynamically generates the spatially adaptive filters for the alignment and deblurring. We then propose the new Filter Adaptive Convolutional (FAC) layer to align the deblurred features of the previous frame with the current frame and remove the spatially variant blur from the features of the current frame. Finally, we develop a reconstruction network which takes the fusion of two transformed features to restore the clear frames. Both quantitative and qualitative evaluation results on the benchmark datasets and real-world videos demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in terms of accuracy, speed as well as model size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, the hand-held and onboard video capturing devices have enjoyed widespread popularity, e.g., smartphone, action camera, unmanned aerial vehicle. The camera shake and high-speed movement in dynamic scenes often generate undesirable blur and result in blurry videos. The lowquality video not only leads to visually poor quality but also hampers some high-level vision tasks such as tracking <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref>, video stabilization <ref type="bibr" target="#b19">[20]</ref> and SLAM <ref type="bibr" target="#b17">[18]</ref>. Thus, it is of great interest to develop an effective algorithm to deblur videos for above mentioned human perception and high-level vision tasks. * Equal contribution † Corresponding author: sdluran@gmail.com.</p><p>(a) Blurry frame (b) SRN <ref type="bibr" target="#b37">[38]</ref> (c) GVD <ref type="bibr" target="#b8">[9]</ref> (d) OVD <ref type="bibr" target="#b9">[10]</ref> (e) DVD <ref type="bibr" target="#b35">[36]</ref> (f) w/o FAC (g) Ours (h) Ground truth <ref type="figure">Figure 1</ref>: One challenging example of video deblurring. Due to the large motion and spatially variant blur, the existing image (b) <ref type="bibr" target="#b37">[38]</ref> and video deblurring (c, d, e) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref> methods are less effective. By using the proposed filter adaptive convolutional (FAC) layer for frame alignment and deblurring, our method generates a much clearer image. When the FAC layers are removed (f), our method cannot perform well anymore.</p><p>Unlike single-image deblurring, video deblurring methods can exploit additional information that exists across neighboring frames. Significant progress has been made due to the use of sharper regions from neighboring frames <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b2">3]</ref> or the optical flow from consecutive frames <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>. However, directly utilizing sharp regions of surrounding frames usually generates significant artifacts because the neighboring frames are not fully aligned. Although using the motion field from two adjacent frames, such as optical flow, is able to overcome the alignment problem or approximate the non-uniform blur kernels, the estimation of motion field from blurry adjacent frames is quite challenging.</p><p>Motivated by the success of the deep neural networks in low-level vision, several algorithms have been proposed to solve video deblurring <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref>. Kim et al. <ref type="bibr" target="#b9">[10]</ref> concatenate the multi-frame features to restore the current image by a deep recurrent network. However, this method fails to make full use of the information of neighboring frames without explicitly considering alignment, and cannot perform well when the videos contain large motion. Su et al. <ref type="bibr" target="#b35">[36]</ref> align the consecutive frames to the reference frame. It shows that this method performs well when the input frames are not too blurry but are less effective for the frames containing severe blur. We also empirically find that both alignment and deblurring are crucial for deep networks to restore sharper frames from blurry videos.</p><p>Another group of methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> use single or multiple images to estimate optical flow which is treated as the approximation of non-uniform blur kernels. With the estimated optical flow, these methods usually use the existing non-blind deblurring algorithms (e.g., <ref type="bibr" target="#b45">[46]</ref>) to reconstruct the sharp images. However, these methods highly depend on the accuracy of the optical flow field. In addition, these methods can only predict line-shaped blur kernel which is inaccurate under some scenarios. To handle non-uniform blur in dynamic scenes, Zhang et al. <ref type="bibr" target="#b43">[44]</ref> develop the spatially variant recurrent neural network (RNN) <ref type="bibr" target="#b18">[19]</ref> for image deblurring, whose pixel-wise weights are learned from a convolutional neural network (CNN). This algorithm does not need additional non-blind deblurring algorithms. However, it is limited to single image deblurring and cannot be directly extended to video deblurring.</p><p>To overcome the above limitations, we propose a Spatio-Temporal Filter Adaptive Network (STFAN) for video deblurring. Motivated by dynamic filter networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22]</ref> which apply the generated filters to the input images, we propose the element-wise filter adaptive convolutional (FAC) layer. Compared with <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22]</ref>, FAC layer applies the generated spatially variant filters on down-sampled features, which allows it to obtain a larger receptive field using a smaller filter size. It also has stronger capability and flexibility due to different filters are dynamically estimated for different channels of the features. The proposed method formulates the alignment and deblurring as two element-wise filter adaptive convolution processes in a unified network. Specifically, given both blurry and restored images of the previous frame and blurry image of the current frame, STFAN dynamically generates corresponding alignment and deblurring filters for feature transformation. In contrast with estimating non-uniform blur kernels from a single blurry image <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8]</ref> or two adjacent blurry images <ref type="bibr" target="#b8">[9]</ref>, our method estimates the deblurring filters from a richer inputs: three images and the motion information of two adjacent frames obtained from alignment filters. By using FAC layer, STFAN adaptively aligns the features obtained at different time steps, without explicitly estimating optical flow and warping images, thereby leading to a tolerance of alignment accuracy. In addition, the FAC layers allow our network handle spatially variant blur better, with deblurring in the feature domain. An example in <ref type="figure">Figure 1</ref> shows that our method generates a much sharper image <ref type="figure">(Figure 1</ref>(g)) than our baseline without FAC layers <ref type="figure">(Figure 1</ref>(f)) as well as the competing methods.</p><p>The main contributions are summarized as follows:</p><p>• We propose a filter adaptive convolutional (FAC) layer that applies the generated element-wise filters to feature transformation, which is utilized for two spatially variant tasks, i.e. alignment and deblurring in the feature domain.</p><p>• We propose a novel spatio-temporal filter adaptive network (STFAN) for video deblurring. It integrates the frame alignment and deblurring into a unified framework without explicit motion estimation and formulates them as two spatially variant convolution process based on the FAC layers.</p><p>• We quantitatively and qualitatively evaluate our network on benchmark dataset and show that it performs favorably against state-of-the-art algorithms in terms of accuracy, speed as well as model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work formulates the neighboring frame alignment and non-uniform blur removal in video deblurring task as two element-wise filter adaptive convolution processes. The following is a review of relevant works on single-image deblurring, multi-image deblurring, and kernel prediction network, respectively. Single-Image Deblurring. Numerous methods have been proposed for single-image deblurring. Early researchers assume a uniform blur kernel and design some natural image priors, such as L 0 -regularized prior <ref type="bibr" target="#b42">[43]</ref>, dark channel prior <ref type="bibr" target="#b27">[28]</ref>, to compensate for the ill-posed blur removal process. However, it is hard for these methods to model spatially-varying blur under dynamic scenes. To model the non-uniform blur, the method <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b26">[27]</ref> estimate different blur kernels for different segmented the image patches. Other works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8</ref>] estimate a dense motion field and a pixel-wise blur kernel.</p><p>With the development of deep learning, many CNNbased methods have been proposed to solve dynamic scene deblurring. Method <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b3">[4]</ref> utilize CNNs to estimate the non-uniform blur kernels. However, the predicted kernels are line-shaped which are inaccurate in some scenarios, and time-consuming conventional non-blind deblurring <ref type="bibr" target="#b45">[46]</ref> is generally required to restore the sharp image. More recently, many end-to-end CNN models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref> have also been proposed for image deblurring. To obtain a large receptive field for handling the large blur, the multi-scale strategy is used in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23]</ref>. In order to deal with dynamic scene blur, Zhang et al. <ref type="bibr" target="#b43">[44]</ref> use spatially variant RNNs <ref type="bibr" target="#b18">[19]</ref> to remove blur in feature space with a generated RNN weights by a neural network. However, compared with the video-based method, the accuracy of RNN weights is highly limited to having only a single blurry image as input. To reduce the difficulty of restoration and ensures color consistency, Noroozi et al. <ref type="bibr" target="#b25">[26]</ref> build skip connections between the input and output. The adversarial loss is used in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17]</ref> to generate sharper images with more details. <ref type="figure">Figure 2</ref>: Proposed network structure. It contains three sub-networks: spatio-temporal filter adaptive network (STFAN), feature extraction network, and reconstruction network. Given the triplet images (blurry B t−1 and restored R t−1 image of the previous frame, and current input image B t ), the sub-network STFAN generates the alignment filters F align and deblurring filters F deblur in order. Then, using the proposed FAC layer , STFAN aligns deblurred features H t−1 of the previous time step with the current time step and removes blur from the features E t extracted from the current blurry image by the feature extraction network. At last, the reconstruction network is utilized to restore the sharp image from the fused features C t . k denotes the filter size of FAC layer.</p><p>Multi-Image Deblurring. Many methods utilize multiple images to solve dynamic scene deblurring from video, burst or stereo images. The algorithms by <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b31">[32]</ref> use the predicted optical flow to segment layers with different blur and estimate the blur layer-by-layer. In addition, Kim et al. <ref type="bibr" target="#b8">[9]</ref> treat optical flow as a line-shaped approximation of blur kernels, which optimize optical flow and blur kernels iteratively. The stereo-based methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b28">29</ref>] estimate depth from stereo images, which is used to predict the pixelwise blur kernels. Zhou et al. <ref type="bibr" target="#b44">[45]</ref> propose a stereo deblurring network with depth awareness and view aggregation. To improve the generalization ability, Chen et al. <ref type="bibr" target="#b1">[2]</ref> propose an optical flow based reblurring step to reconstruct the blurry input, which is employed to fine-tune deblurring network via self-supervised learning. Recently, several endto-end CNN methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref> have been proposed for video deblurring. After image alignment using optical flow, <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b14">[15]</ref> aggregate information across the neighboring frames to restore the sharp images. Kim et al. <ref type="bibr" target="#b9">[10]</ref> apply a temporal recurrent network to propagate the features from the previous time step into those of the current one. Despite the fact that motion can be the useful guidance for blur estimation, Aittala et al. <ref type="bibr" target="#b0">[1]</ref> propose a burst deblurring network in an order-independent manner by repeatedly exchanging the information between the features of the burst images.</p><p>Kernel Prediction Network. Kernel (filter) prediction network (KPN) has recently witnessed rapid progress in lowlevel vision tasks. Jia et al. <ref type="bibr" target="#b10">[11]</ref> first propose the dynamic filter network, which consists of a filter prediction network that predicts kernels conditioned on an input image, and a dynamic filtering layer that applies the generated kernels to another input. Their method shows the effectiveness on video and stereo prediction tasks. Niklaus et al. <ref type="bibr" target="#b23">[24]</ref> apply kernel prediction network to video frame interpolation, which merges optical flow estimation and frame synthesis into a unified framework. To alleviate the demand for memories, they subsequently propose separable convolution <ref type="bibr" target="#b24">[25]</ref> which estimates two separable 1D kernels to approximate 2D kernels. In <ref type="bibr" target="#b21">[22]</ref>, they utilize KPN for both burst frame alignment and denoising, using the same predicted kernels. <ref type="bibr" target="#b12">[13]</ref> reconstructs high-resolution image from low-resolution input using generated dynamic upsampling filters. However, all the above methods directly apply the predicted kernels (filters) in the image domain. In addition, Wang et al. <ref type="bibr" target="#b38">[39]</ref> propose a spatial feature transform (SFT) layer for image super-resolution. It generates transformation parameters for pixel-wise feature modulation, which can be considered as the KPN with a kernel size of 1 × 1 in the feature domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head><p>In this section, we first give an overview of our algorithm in Sec. 3.1. Then we introduce the proposed filter adaptive convolutional (FAC) layer in Sec. 3.2. Upon this layer, we show the structure of the proposed networks in Sec. 3.3. Finally, we present the loss functions that are used to constrain the network training in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Different from the standard CNN-based video deblurring methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref> that take five or three consecutive blurry frames as input to restore the sharp mid-frame, we propose a frame-recurrent method, which requires information of the previous frame and the current input. Due to the recurrent property, the proposed method is able to explore and utilize the information from a large number of previous frames without increasing the computational demands. As shown in <ref type="figure">Figure 2</ref>, the proposed STFAN generates the filters for alignment and deblurring from the triplet images (blurry and restored image of the previous time step t − 1, and current input blurry image). Then, using FAC layers, STFAN aligns the deblurred features from the previous time step with the current one and removes blur from the features extracted from the current blurry image. Finally, a reconstruction network is applied to restore the sharp image by fusing the above two transformed features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Filter Adaptive Convolutional Layer</head><p>Motivated by the Kernel Prediction Network (KPN) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22]</ref>, which applies the generated spatially variant filters to the input image, we propose the filter adaptive convolutional (FAC) layer which applies generated element-wise convolutional filters to the features, as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. The filters predicted in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22]</ref> are the same for RGB channels of each position. To be more capable and flexible for spatially variant tasks, the generated filters for FAC layer are different for each channel. Limited by large memory demand, we only consider the convolution within channels. In theory, the element-wise adaptive filters is five-dimensional (h × w × c × k × k). In practice, the dimension of the generated filter F is h × w × ck 2 and we reshape it into the five-dimensional filter. For each position (x, y, c i ) of input feature Q ∈ R h×w×c , a specific local filter F x,y,ci ∈ R k×k (reshape from 1 × 1 × k 2 ) is applied to the region centered around Q x,y,ci as follows:</p><formula xml:id="formula_0">Q(x, y, c i ) = F x,y,ci * Q x,y,ci = r n=−r r m=−r F(x, y, k 2 c i + kn + m) × Q(x − n, y − m, c i ),<label>(1)</label></formula><p>where r = k−1 2 , * donates convolution operation, F is the generated filter, Q(x, y, c i ) andQ(x, y, c i ) denote the input A large receptive field is essential to handle large motions and blurs. The standard KPN methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22]</ref> have to predict the filter much larger in size than motion blur for each pixel of the input image, which requires large computational cost and memory. In contrast, the proposed network does not require a large filter size due to the use of FAC layer on down-sampled features. The experimental results in <ref type="table" target="#tab_4">Table 4</ref> show a small filter size (e.g. 5) on intermediate feature layer is sufficient for deblurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>As shown in <ref type="figure">Figure 2</ref>, our network is composed of a spatio-temporal filter adaptive network (STFAN), a feature extraction network, and a reconstruction network. Feature Extraction Network. This network extracts features E t from the blurry image B t , which consists of three convolutional blocks and each of them has one convolutional layer with stride 2 and two residual blocks <ref type="bibr" target="#b5">[6]</ref> with LeakyReLU (negative slope λ = 0.1) as the activation functions. The extracted features are feed into STFAN for deblurring using FAC layer. Spatio-Temporal Filter Adaptive Network. The proposed STFAN consists of three modules: encoder e tri of triplet images, alignment filter generator g align , and deblurring filter generator g deblur .</p><p>Given the triplet input: the blurry image B t−1 and restored image R t−1 of the previous frame and the current blurry image B t , STFAN extracts features T t by the encoder e tri . The encoder consists of three convolutional blocks (kernel size 3) and each of them is composed of one convolutional layer with stride 2 and two residual blocks. The alignment filter generator g align takes the extracted features T t of triplet images as input to predict the adaptive filters for alignment, denoted as F align ∈ R h×w×ck 2 :</p><formula xml:id="formula_1">F align = g align (e tri (B t−1 , R t−1 , B t )),<label>(2)</label></formula><p>where generated F align contains rich motion information, which is helpful to model the non-uniform blur in the dynamic scene. To make full use of it, the deblurring filter generator g deblur takes alignment filters F align as well as the features T of triplet images to generate the spatially variant filters for deblurring, denoted as F deblur ∈ R h×w×ck 2 :</p><formula xml:id="formula_2">F deblur = g deblur (e tri (B t−1 , R t−1 , B t ), F align ),<label>(3)</label></formula><p>Both filter generators consist of one convolution layer and two residual blocks with kernel size 3 × 3, followed by a 1 × 1 convolution layer to expand the channels of output to ck 2 . With the two generated filters, two FAC layers are utilized to align the deblurred features H t−1 from the previous time step with the current frame and remove the blur from the extracted features E t of current blurry frame in the feature domain. After that, we concatenate these two transformed features as C t and restore the sharp image by the reconstruction network. To propagate the deblurred information H t to the next time step, we pass the features C t to the next iteration through a convolutional layer.</p><p>It is worth noting that both the blurry B t−1 , B t and restored R t−1 are required to learn the filters for alignment and deblurring, and thus are taken as the triplet input to STFAN. On the one hand, B t−1 and B t are crucial to capture the motion information across frames and thus benefit alignment. On the other hand, the inclusion of B t−1 and R t−1 makes it possible to implicitly exploit the blur kernel at frame t−1 for improving the deblurring at frame t. Moreover, deblurring is assumed to be more difficult but can be benefited by alignment. Thus we stack g deblur upon g align in our implementation. We will analyze the effect of taking triplet images B t−1 , R t−1 , B t as input in Sec. 5.3. Reconstruction Network. The reconstruction network is used to restore the sharp images by taking the fusion features from STFAN as input. It consists of scale convolutional blocks, each of which has one deconvolutional layer and two residual blocks as shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>To effectively train the proposed network, we consider two kinds of loss functions. The first loss is the mean squared error (MSE) loss that measures the differences between the restored frame R and its corresponding sharp ground truth S:</p><formula xml:id="formula_3">L mse = 1 CHW ||R − S|| 2 ,<label>(4)</label></formula><p>where C, H, W are dimensions of image, respectively; R and S respectively denote the restored image and the corresponding ground truth.</p><p>To generate more realistic images, we further use the perceptual loss proposed in <ref type="bibr" target="#b13">[14]</ref>, which is defined as the Euclidean distance between the VGG-19 <ref type="bibr" target="#b34">[35]</ref> features of restored frame R and ground truth S:</p><formula xml:id="formula_4">L perceptual = 1 C j H j W j ||Φ j (R) − Φ j (S)|| 2 ,<label>(5)</label></formula><p>where Φ j (·) denotes the features from the j-th convolutional layer of the pretrained VGG-19 network and C j , H j , W j are dimensions of features. In this paper, we use the features of conv3-3 (j = 15). The final loss function for the proposed network is defined as:</p><formula xml:id="formula_5">L deblur = L mse + λL perceptual ,<label>(6)</label></formula><p>where the weight λ is set as 0.01 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>In our experiments, we train the proposed network using the video deblurring dataset from <ref type="bibr" target="#b35">[36]</ref>. It contains 71 videos (6,708 blurry-sharp pairs), splitting into 61 training videos (5,708 pairs) and 10 testing videos (1,000 pairs). Data Augmentation. We perform several data augmentations for training. We first divide each video into several sequences with length 20. To add motion diversity into the training data, we reverse the order of sequence randomly. For each sequence, we perform the same image transformations. It consists of chromatic transformations such as brightness, contrast as well as saturation, which are uniformly sampled from [0.8, 1.2] and geometric transformations including randomly flipping horizontally and vertically and randomly cropping to 256×256 patches. To make our network robust in real-world scenarios, a Gaussian random noise from N (0, 0.01) is added to the input images. Experimental Settings. We initialize our neural network using the initialization method in <ref type="bibr" target="#b4">[5]</ref>, and train it using Adam <ref type="bibr" target="#b15">[16]</ref> optimizer with β 1 = 0.9 and β 2 = 0.999. We set the initial learning rate to 10 −4 and decayed by 0.1 every 400k iterations. The proposed network converges after 900k iterations. We quantitatively and qualitatively evaluate the proposed method on the video deblurring dataset <ref type="bibr" target="#b35">[36]</ref>. For a fair comparison with the most related deep learning-based algorithms <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b37">38]</ref>, we finetune all these methods by the corresponding publicly released implementations on the video deblurring dataset <ref type="bibr" target="#b35">[36]</ref>. In our experiments, we use both PSNR and SSIM as quantitative evaluation metrics for synthetic testing set. The training code, test model, and experimental results will be available to the public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>Quantitative Evaluations. We compare the proposed algorithm with the state-of-the-art video deblurring methods including conventional optical flow-based pixel-wise kernel estimation <ref type="bibr" target="#b8">[9]</ref> and CNN based methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b9">10]</ref>. We also compare it with the state-of-the-art image deblurring methods including conventional non-uniform deblurring <ref type="bibr" target="#b39">[40]</ref>, <ref type="table">Table 1</ref>: Quantitative evaluation on the video deblurring dataset <ref type="bibr" target="#b35">[36]</ref>, in terms of PSNR, SSIM, running time (sec) and parameter numbers (×10 6 ) of different networks. All existing methods are evaluated using their publicly available code. '-' indicates that it is not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Whyte <ref type="bibr">[</ref>  <ref type="figure">Figure 4</ref>: Qualitative evaluations on Video Deblurring Dataset <ref type="bibr" target="#b35">[36]</ref>. The proposed method generates much sharper images with higher PSNR and SSIM.</p><p>CNN based spatially variant blur kernel estimation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4]</ref>, and end-to-end CNN methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b37">38]</ref>. <ref type="table">Table 1</ref> shows that the proposed method performs favorably against the state-of-the-art algorithms on the testing set of dynamic scene video deblurring dataset <ref type="bibr" target="#b35">[36]</ref>. <ref type="figure">Figure 4</ref> shows some examples in the testing set from <ref type="bibr" target="#b35">[36]</ref>. It shows that the existing methods cannot keep sharp details and remove the non-uniform blur well. With temporal alignment and spatially variant deblurring, our network performs the best and restores much clearer images with more details.</p><p>Qualitative Evaluations. To further validate the generalization ability of the proposed method, we also qualitatively compare the proposed network with other algorithms on real blurry images from <ref type="bibr" target="#b35">[36]</ref>. As illustrated in <ref type="figure">Figure 5</ref>, the proposed method can restore shaper images with more image details than the state-of-the-art image and video deblurring methods. The comparison results show that our STFAN can robustly handle unknown real blur in dynamic scenes, which further demonstrates the superiority of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Running Time and Model Size</head><p>We implement the proposed network using PyTorch platform <ref type="bibr" target="#b29">[30]</ref>. To speed up, we implement the proposed FAC layer with CUDA. We evaluate the proposed method and state-of-the-art image or video deblurring methods on the same server with an Intel Xeon E5 CPU and an NVIDIA Titan Xp GPU. The traditional algorithms <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b8">9]</ref> are timeconsuming due to a complex optimization process. Therefore, <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b3">[4]</ref> utilize the CNN to estimate non-uniform blur kernels based on motion flow. However, they are still time-consuming since the traditional non-blind deblurring algorithm <ref type="bibr" target="#b45">[46]</ref> is used to restore the sharp images. DVD <ref type="bibr" target="#b35">[36]</ref> uses CNN to restore sharp images from neighboring multiple blurry frames, but they use a traditional optical flow method <ref type="bibr" target="#b30">[31]</ref> to align these input frames and is computationally expensive. With GPU implementation, the end-to-end CNN-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b9">10]</ref> are relatively efficient. To enlarge the receptive field, the networks in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b37">38]</ref> are very deep, which lead to a large model size as well as a long processing time. Even though spatially variant RNNs are used in <ref type="bibr" target="#b43">[44]</ref> to enlarge (a) Blurry image (b) Gong et al. <ref type="bibr" target="#b3">[4]</ref> (c) Nah et al. <ref type="bibr" target="#b22">[23]</ref> (d) Kupyn et al. <ref type="bibr" target="#b16">[17]</ref> (e) Zhang et al. <ref type="bibr" target="#b43">[44]</ref> (f) Tao et al. <ref type="bibr" target="#b37">[38]</ref> (g) Kim and Lee <ref type="bibr" target="#b8">[9]</ref> (h) Kim et al. <ref type="bibr" target="#b9">[10]</ref> (i) Su et al. <ref type="bibr" target="#b35">[36]</ref> (j) Ours <ref type="figure">Figure 5</ref>: Qualitative evaluations on the real blurry videos <ref type="bibr" target="#b35">[36]</ref>. The proposed method generates much clearer images.</p><p>the receptive field, they need a deep network to estimate the RNN weights and RNNs are also time-consuming. Our network uses the aligned deblurred features of the previous frame, which reduces the difficulty for the network to restore the sharp image of the current frame. In addition, the FAC layer is effective for spatially variant alignment and deblurring. Benefited from the above two merits, our networks are designed to be small and efficient. As shown in <ref type="table">Table 1</ref>, the proposed network has less running time and smaller model size than the existing end-to-end CNN methods. Even though <ref type="bibr" target="#b9">[10]</ref> runs slightly faster and has smaller model size, the proposed method performs better with the frame alignment and deblurring in the feature domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Temporal consistency</head><p>To enforce temporal consistency, we adopt the recurrent network to transfer previous feature maps over time, and propose the FAC layer for propagating information between consecutive frames via explicit alignment. <ref type="figure" target="#fig_1">Fig. 6</ref> shows that our method not only restores sharper frames but also keeps better temporal consistency. In addition, the video results are given on our [project webpage].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis and Discussions</head><p>We have shown that the proposed algorithm performs favorably against state-of-the-art methods. In this section, we conduct a number of comparative experiments for ablation study and analysis further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effectiveness of the FAC layers</head><p>The generated alignment filters and deblurring filters are visualized in <ref type="figure" target="#fig_3">Figure 7</ref>(c) and (h), respectively. According to the optical flow estimated by EpicFlow <ref type="bibr" target="#b32">[33]</ref> in <ref type="figure" target="#fig_3">Figure 7(b)</ref>, there is a vehicle moving in the video which is coherent with the alignment filters estimated by our network. Since removing different blur requires different operations and blur  is somehow related to the optical flow, our network estimates different deblurring filters for foreground vehicle and backgrounds.</p><p>To validate the effectiveness of the FAC layer for alignment and deblurring, some intermediate features are shown in <ref type="figure" target="#fig_3">Figure 7</ref>. According to <ref type="figure" target="#fig_3">Figure 7(d)</ref> and (i), the FAC layer for alignment can correctly warp the head of the vehicle from green line to purple line even without an image alignment constraint during training. As for the transformed features in <ref type="figure" target="#fig_3">Figure 7</ref>(j) for deblurring, they are sharper than those before the FAC layer in <ref type="figure" target="#fig_3">Figure 7</ref>(e), which means the deblurring branch can effectively remove blur in the feature domain.</p><p>We also conduct three experiments which replace one or both the FAC layers by concatenating the corresponding features directly, without features transformation by FAC layers. In <ref type="table" target="#tab_2">Table 2</ref>  <ref type="figure">Figure 2</ref> for clarification). It shows that the network performs worse without the help of the feature transformation by FAC layers. In addition, <ref type="figure">Figure 1</ref> also shows that our method cannot restore such a sharp image without using FAC layers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effectiveness of the A and D Branches</head><p>To validate the effectiveness of both alignment (A) and deblurring (D) branches, we compare our network with two variant networks: removing the features of the alignment branch (-, w D) and removing the features of the deblurring branch (w A, -). According to <ref type="table" target="#tab_2">Table 2</ref>, these two baseline networks do not generate satisfying deblurring results compared to our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effectiveness of the Triplet Input of STFAN</head><p>To generate adaptive alignment and deblurring filters, STFAN takes the triplet input (previous blurry image B t−1 , previous restored image R t−1 , and current blurry image B t ). <ref type="table">Table 3</ref> shows the results of two variants which take (B t−1 , B t ) and (R t−1 , B t ) as input, respectively. The triplet input leads to the best performance. As Sec. 3.3 discussed, the network can implicitly capture the motion and model dynamic scene blur better from the triplet input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Effectiveness of the Size of Adaptive Filters</head><p>To further investigate the proposed network, we test different sizes of adaptive filters, shown in <ref type="table" target="#tab_4">Table 4</ref>. The larger size of the adaptive filters leads to better performance. However, increasing the size of adaptive filters after k = 5 only <ref type="table">Table 3</ref>: Effectiveness of using triplet input of the STFAN. We replace the input of the STFAN by (B t−1 , B t ) and (R t−1 , B t ) as two variants of our network (R t−1 , B t−1 , B t ), respectively.  </p><formula xml:id="formula_6">Input (B t−1 , B t ) (R t−1 , B t ) (R t−1 , B t−1 , B t )<label>PSNR</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a novel spatio-temporal network for video deblurring based on filter adaptive convolutional (FAC) layers. The network dynamically generate elementwise alignment and deblurring filters in order. Using the generated filters and FAC layers, our network can perform temporal alignment and deblurring in the feature domain. We have shown that the formulation of two spatially variant problems in video deblurring (i.e., alignment and deblurring) as two filter adaptive convolution processes allows the proposed method to utilize features obtained at different time steps without explicit motion estimation (e.g., optical flow) and enables our method to handle spatially variant blur in dynamic scenes. The experimental results demonstrate the effectiveness of the proposed method in terms of accuracy, speed as well as model size.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Filter Adaptive Convolutional Layer features and transformed features, respectively. The proposed FAC layer is trainable and efficient, which is implemented and accelerated by CUDA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>Temporal consistency evaluation on consecutive frames from a blurry video. (zoom in for best view).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, (w/o A, w/ D), (w/ A, w/o D) and (w/o A, w/o D) represent removing FAC layers for feature domain alignment only, feature domain deblurring only and both of them, respectively (refer to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Effectiveness of the adaptive filter generator and FAC layer. (b) is the optical flow from the adjacent input blurry frames (a) and (f) according to EpicFlow [33]. (c) and (h) are the visualization of the generated alignment and deblurring filters of FAC layers, respectively. (d) and (i) are selected feature maps before and after alignment using FAC layer. (e) and (j) are selected feature maps before and after deblurring using FAC layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of different variants of structures.</figDesc><table><row><cell>The (w/o</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of different sizes of adaptive filters.</figDesc><table><row><cell>Filter Size</cell><cell cols="4">k = 3 k = 5 k = 7 k = 9</cell></row><row><cell>PSNR</cell><cell cols="4">30.95 31.24 31.27 31.30</cell></row><row><cell>SSIM</cell><cell cols="4">0.931 0.934 0.934 0.935</cell></row><row><cell>Receptive Field</cell><cell>79</cell><cell>87</cell><cell>95</cell><cell>103</cell></row><row><cell>Params (M)</cell><cell>4.58</cell><cell>5.37</cell><cell>6.56</cell><cell>8.14</cell></row><row><cell cols="5">has minor performance improvement. We empirically set</cell></row><row><cell cols="5">k = 5 as a trade-off among the computational complexity,</cell></row><row><cell cols="2">model size and performance.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Burst image deblurring using permutation invariant convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reblur2deblur: Deblurring videos via selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICCP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video deblurring for hand-held cameras using patch-based synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">64</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From motion blur to motion flow: A deep learning solution for removing heterogeneous motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmentation-free dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2766" to="2773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generalized video deblurring for dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online video deblurring via dynamic temporal blending network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual tracking in the presence of motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatio-temporal transformer network for video restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simultaneous localization, mapping and deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning recursive filters for low-level vision via a hybrid neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Full-frame video stabilization with motion inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1150" to="1163" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling and generating complex motion blur for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Burst denoising with kernel prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Motion deblurring in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chandramouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Softsegmentation guided object motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Blind image deblurring using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simultaneous stereo video deblurring and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tv-l1 optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meinhardt-Llopis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Facciolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video deblurring via semantic segmentation and pixel-wise non-linear kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stereo video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sellent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-uniform deblurring for shaken images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="186" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling blurred video with layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Depth-aware motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unnatural l0 sparse representation for natural image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring using spatially variant recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Davanet: Stereo deblurring with view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
