<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MultiFiT: Efficient Multi-lingual Language Model Fine-tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Ireland</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Aylien Ltd</orgName>
								<address>
									<settlement>Dublin 4 n-waves, Wroc≈Çaw</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Czapla</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of San Francisco</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MultiFiT: Efficient Multi-lingual Language Model Fine-tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pretrained language models are promising particularly for low-resource languages as they only require unlabelled data. However, training existing models requires huge amounts of compute, while pretrained cross-lingual models often underperform on low-resource languages. We propose Multi-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all models and code 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models (LMs) have shown striking improvements on a range of natural language processing (NLP) tasks <ref type="bibr" target="#b20">(Peters et al., 2018a;</ref><ref type="bibr" target="#b11">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b6">Devlin et al., 2018)</ref>. These models only require unlabelled data for training and are thus particularly useful in scenarios where labelled data is scarce. As much of NLP research has focused on the English language, the larger promise of these models is to bridge the digital language divide 2 and enable the application of NLP methods to many of the world's other 6,000 languages where labelled data is less plentiful.</p><p>Recently, cross-lingual extensions of these LMs have been proposed that train on multiple languages jointly <ref type="bibr" target="#b2">(Artetxe and Schwenk, 2018;</ref><ref type="bibr" target="#b15">Lample and Conneau, 2019)</ref>. These models are able to perform zero-shot learning, only requiring labelled data in the source language. However, source data in another language may often not be available, whereas obtaining a small number of labels is typically straightforward.</p><p>Furthermore such models have several downsides: a) some variants rely on large amounts of parallel data, which may not be available for truly low-resource languages; b) they require a huge amount of compute for training 3 ; and c) cross-lingual models underperform on lowresource languages-precisely the setting where they would be most useful. We are aware of two possible reasons for this: 1) Languages that are less frequently seen during training are underrepresented in the embedding space. 4 2) Infrequent scripts are over-segmented in the shared word piece vocabulary <ref type="bibr" target="#b30">(Wang et al., 2019)</ref>.</p><p>In this work, we show that small monolingual LMs are able to outperform expensive crosslingual models both in the zero-shot and the supervised setting. We propose Multi-lingual language model Fine-tuning (MultiFit) to enable practitioners to train and fine-tune language models efficiently. 5 Our model combines universal language model fine-tuning (ULMFiT; <ref type="bibr" target="#b11">Howard and Ruder, 2018)</ref> with the quasi-recurrent neural network (QRNN; <ref type="bibr" target="#b3">Bradbury et al., 2017)</ref> and subword tokenization <ref type="bibr" target="#b14">(Kudo, 2018)</ref> and can be pretrained on a single Tesla V100 GPU in a few hours. In addition, we propose to use a pretrained cross-lingual model's predictions as pseudo labels to adapt the monolingual language model to the zero-shot setting. We evaluate our models on two widely used cross-lingual classification datasets, MLDoc <ref type="bibr" target="#b26">(Schwenk and Li, 2018)</ref> and CLS <ref type="bibr" target="#b23">(Prettenhofer and Stein, 2010)</ref> where we outperform the stateof-the-art zero-shot model LASER <ref type="bibr" target="#b2">(Artetxe and Schwenk, 2018)</ref> and multi-lingual BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> in the supervised setting-even without any pretraining. In the zero-shot setting, we outperform both models using pseudo labels-and report significantly higher performance with as little as 100 examples. We finally show that information from monolingual and cross-lingual language models is complementary and that pretraining makes models robust to noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Pretrained language models Pretrained language models based on an LSTM <ref type="bibr" target="#b20">(Peters et al., 2018a;</ref><ref type="bibr" target="#b11">Howard and Ruder, 2018</ref>) and a Transformer <ref type="bibr" target="#b24">(Radford et al., 2018;</ref><ref type="bibr" target="#b6">Devlin et al., 2018)</ref> have been proposed. Recent work <ref type="bibr" target="#b21">(Peters et al., 2018b)</ref> suggests that-all else being equal-an LSTM outperforms the Transformer in terms of downstream performance. For this reason, we use a variant of the LSTM as our language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-lingual pretrained language models</head><p>The multi-lingual BERT model is pretrained on the Wikipedias of 104 languages using a shared word piece vocabulary. LASER <ref type="bibr" target="#b2">(Artetxe and Schwenk, 2018)</ref> is trained on parallel data of 93 languages with a shared BPE vocabulary. XLM <ref type="bibr" target="#b15">(Lample and Conneau, 2019)</ref> additionally pretrains BERT with parallel data. These models enable zero-shot transfer, but achieve lower results than monolingual models. In contrast, we focus on making the training of monolingual language models more efficient in a multi-lingual context. Concurrent work <ref type="bibr">(Mulcaire et al., 2019)</ref> pretrains on English and another language, but shows that cross-lingual pretraining only helps sometimes.</p><p>Multi-lingual language modeling Training language models in non-English languages has only recently received some attention. <ref type="bibr" target="#b12">Kawakami et al. (2017)</ref> evaluate on seven languages. <ref type="bibr" target="#b4">Cotterell et al. (2018)</ref> study 21 languages. <ref type="bibr" target="#b7">Gerz et al. (2018)</ref> create datasets for 50 languages. All of these studies, however, only create small datasets, which are inadequate for pretraining language models. In contrast, we are among the first to report the  performance of monolingual language models on downstream tasks in multiple languages.</p><p>3 Our method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-lingual Fine-Tuning</head><p>We propose Multi-lingual Fine-tuning (MultiFit). Our method uses the ULMFiT model <ref type="bibr" target="#b11">(Howard and Ruder, 2018)</ref> with discriminative fine-tuning as foundation. ULMFiT is based on a 3-layer AWD-LSTM (Mer, 2017) language model. The AWD-LSTM is a regular LSTM <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref> with tuned dropout hyperparameters. To enable faster training and finetuning of the model, we replace it with a QRNN <ref type="bibr" target="#b3">(Bradbury et al., 2017)</ref>. The QRNN alternates convolutional layers, which are parallel across timesteps, and a recurrent pooling function, which is parallel across channels. It has been shown to outperform LSTMs, while being up to 16√ó faster at train and test time. ULMFiT in addition is restricted to words as input. To make our model more robust across languages, we use subword tokenization based on a unigram language model <ref type="bibr" target="#b14">(Kudo, 2018)</ref>, which is more flexible compared to byte-pair encoding <ref type="bibr" target="#b27">(Sennrich et al., 2016)</ref>. We additionally employ label smoothing <ref type="bibr" target="#b29">(Szegedy et al., 2016)</ref> and a novel cosine variant of the one-cycle policy (Smith, 2018) 6 , which we found to outperform ULMFiT's slanted triangular learning rate schedule and gradual unfreezing. The full model can be seen in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-lingual Bootstrapping</head><p>Prior methods have employed cross-lingual training strategies relying on parallel data and a shared BPE vocabulary. These can be combined with our language model, but increase its training complexity. For the case where an existing pretrained cross-lingual model and source language data are available, we propose a bootstrapping method <ref type="bibr" target="#b25">(Ruder and Plank, 2018</ref>) that uses the pretrained model's zero-shot predictions as pseudo labels to fine-tune the monolingual model on target language data. The steps of the method can be seen in <ref type="figure">Figure  2</ref>. Specifically, we first fine-tune a linear classification layer on top of pretrained cross-lingual representations on source language training data. We then apply this cross-lingual classifier to the target language data and store its predicted label for every example. We now fine-tune our pretrained LM on the target language data and these pseudo labels 7 . Importantly, this method enables our monolingual LM to significantly outperform its crosslingual teacher in the zero-shot setting ( ¬ß5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head><p>This section provides an overview of our experimental setup; see the appendix for full details.</p><p>Data We evaluate our models on the Multilingual Document Classification Corpus (MLDoc; <ref type="bibr" target="#b26">Schwenk and Li, 2018)</ref>    product reviews in four languages. We provide an overview of the datasets in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Pretraining We pretrain our models on 100M tokens extracted from the Wikipedia of the corresponding language for 10 epochs. As fewer tokens might be available for some languages, we also compare against a version (no wiki) that uses no pretraining. For all models, we fine-tune the LMs on the target data of the same language for 20 epochs. We perform subword tokenization with the unigram language model <ref type="bibr" target="#b14">(Kudo, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation settings</head><p>We compare two settings based on the availability of source and target language data: supervised and zero-shot. In the supervised setting, every model is fine-tuned and evaluated on examples from the target language.</p><p>In the zero-shot setting, every model is fine-tuned on source language examples and evaluated on target language examples. In all cases, we use English as the source language.</p><p>Baselines We compare against the state-ofthe-art cross-lingual embedding models LASER <ref type="bibr" target="#b2">(Artetxe and Schwenk, 2018)</ref>, which uses a large parallel corpus, multilingual BERT (MultiB-   ERT) 10 , and monolingual BERT 11 . We also compare against the best models on each dataset, Mul-tiCCA <ref type="bibr" target="#b1">(Ammar et al., 2016)</ref>, a cross-lingual word embedding model, and BiDRL <ref type="bibr" target="#b31">(Zhou et al., 2016)</ref>, which translates source and target data.</p><p>Our methods We evaluate our monolingual LMs in the supervised setting (MultiFit) and our LMs fine-tuned with pseudo labels from LASER in the zero-shot setting (pseudo  forms the comparison methods as the shared embedding space between many languages is overly restrictive. Our monolingual LMs outperform their cross-lingual teacher LASER in almost every setting. When fine-tuned with only 100 target language examples, they are able to outperform all zero-shot approaches except MultiFiT on DE and FR. This calls into question the need for zeroshot approaches, as fine-tuning with even a small number of target examples is able to yield superior performance. When fine-tuning with 1,000 target examples, MultiFiT-even without pretrainingoutperforms all comparison methods, including monolingual BERT. <ref type="table" target="#tab_5">Table 3</ref>. Mul-tiFiT is able to outperform its zero-shot teacher LASER across all domains. Importantly, the bootstrapped monolingual model also outperforms more sophisticated models that are trained on translations across almost all domains. In the supervised setting, MultiFiT similarly outperforms multilingual BERT. For both datasets, our methods that have been pretrained on 100 million tokens outperform both multilingual BERT and LASER, models that have been trained with orders of magnitude more data and compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLS We show results for CLS in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>Speed We compare the LSTM and QRNN cell in MultiFiT based on the speed for processing a single batch for pretraining and fine-tuning in <ref type="table" target="#tab_6">Table 4</ref>. MultiFiT with a QRNN pretrains and finetunes about 2√ó and 3√ó faster respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiFiT vs. ULMFiT We compare Multi-FiT pretrained on 100M Wikipedia tokens against</head><p>ULMFiT pretrained on the same data using a 3layer LSTM and spaCy tokenization 12 as well as MultiFiT pretrained on 2M Wikipedia tokens, and MultiFiT with no pretraining in <ref type="table" target="#tab_8">Table 5</ref>. Pretraining on more data generally helps. MultiFiT outperforms ULMFiT significantly; the performance improvement is particularly pronounced in Chinese where ULMFiT's word-based tokenization underperformed.    Robustness to noise We suspect that MultiFiT is able to outperform its teacher as the information from pretraining makes it robust to label noise. To test this hypothesis, we train MultiFiT and a randomly initialized model with the same architecture on 1k and 10k examples of the Spanish ML-Doc. We randomly perturb labels with a probability ranging from 0-0.75 and show results in <ref type="figure">Figure</ref> 3. The pretrained MultiFiT is able to partially ignore the noise, up to 65% of noisy training examples. Without pretraining, the model does not exceed the theoretical baseline (the percentage of correct examples). In addition, we compare Mul-tiFiT with and without pretraining in <ref type="table" target="#tab_10">Table 6</ref>. Pretraining enables MultiFiT to achieve much better performance compared to a randomly initialised model. Both results together suggest a) that pretraining increases robustness to noise and b) that information from monolingual and cross-lingual language models is complementary.</p><p>Tokenization Subword tokenization has been found useful for language modeling with morphologically rich languages <ref type="bibr" target="#b5">(Czapla et al., 2018;</ref><ref type="bibr" target="#b17">Mielke and Eisner, 2019)</ref> and has been used in recent pretrained LMs <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>, but its concrete impact on downstream performance has not been observed. We train models with the best performing vocabulary sizes for subword (15k) and regular word-based tokenization (60k) with the Moses tokenizer <ref type="bibr" target="#b13">(Koehn et al., 2007)</ref> on German and Russian MLDoc and show results in <ref type="table" target="#tab_11">Table 7</ref>. Subword tokenization outperforms wordbased tokenization on most languages, while being faster to train due to the smaller vocabulary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have proposed novel methods for multilingual fine-tuning of languages that outperform models trained with far more data and compute on two widely studied cross-lingual text classification datasets on eight languages in both the zero-shot and supervised setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>The MultiFiT language model with classifier consisting of a subword embedding layer, four QRNN layers, an aggregation layer, and two linear layers. The steps of our cross-lingual bootstrapping method for zero-shot cross-lingual transfer. a) A monolingual language model (LM) is pretrained on target language data; b) the LM is fine-tuned on target language documents; and c) the LM is fine-tuned as a classifier using the zero-shot predictions from a linear classification layer fine-tuned on top of cross-lingual representations from LASER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of MultiFiT's robustness to label noise on MLDoc with and without pretraining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The domain, languages, and number of training, development, and test examples in each dataset.</figDesc><table><row><cell></cell><cell>DE</cell><cell>ES</cell><cell>FR</cell><cell>IT</cell><cell>JA</cell><cell>RU</cell><cell>ZH</cell></row><row><cell cols="4">Zero-shot (1,000 source language examples)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MultiCCA</cell><cell cols="7">81.20 72.50 72.38 69.38 67.63 60.80 74.73</cell></row><row><cell>LASER, paper</cell><cell cols="7">86.25 79.30 78.30 70.20 60.95 67.25 70.98</cell></row><row><cell>LASER, code</cell><cell cols="7">87.65 75.48 84.00 71.18 64.58 66.58 76.65</cell></row><row><cell>MultiBERT</cell><cell cols="7">82.35 74.98 83.03 68.27 64.58 71.58 66.17</cell></row><row><cell>MultiFiT, pseudo</cell><cell cols="7">91.62 79.10 89.42 76.02 69.57 67.83 82.48</cell></row><row><cell cols="4">Supervised (100 target language examples)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MultiFit</cell><cell cols="7">90.90 89.00 85.03 80.12 80.55 73.55 88.02</cell></row><row><cell cols="4">Supervised (1,000 target language examples)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MultiCCA</cell><cell cols="7">93.70 94.45 92.05 85.55 85.35 85.65 87.30</cell></row><row><cell>LASER, paper</cell><cell cols="7">92.70 88.75 90.80 85.93 85.15 84.65 88.98</cell></row><row><cell>MultiBERT</cell><cell cols="7">94.00 95.15 93.20 85.82 87.48 86.85 90.72</cell></row><row><cell cols="2">Monolingual BERT 94.93</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>92.17</cell></row><row><cell>MultiFiT, no wiki</cell><cell cols="7">95.23 95.07 94.65 89.30 88.63 87.52 90.03</cell></row><row><cell>MultiFiT</cell><cell cols="7">95.90 96.07 94.75 90.25 90.03 87.65 92.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison of zero-shot and supervised meth- ods on MLDoc.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>78.00 79.15 83.90 83.40 80.75 74.99 74.55 76.30 MultiBERT 72.15 70.05 73.80 75.50 74.70 76.05 65.41 64.90 70.33 MultiFiT, pseudo 89.60 81.80 84.40 87.84 83.50 85.60 80.45 77.65 81.50</figDesc><table><row><cell></cell><cell></cell><cell>DE</cell><cell>FR</cell><cell>JA</cell></row><row><cell></cell><cell></cell><cell cols="2">Books DVD Music Books DVD Music Books DVD Music</cell></row><row><cell cols="4">Zero-shot 84.15 Translat. LASER, code MT-BOW 79.68 77.92 77.22 80.76 78.83 75.78 70.22 71.30 72.02 CL-SCL 79.50 76.92 77.79 78.49 78.80 77.92 73.09 71.07 75.11 BiDRL 84.14 84.05 84.67 84.39 83.60 82.52 73.15 76.78 78.77</cell></row><row><cell>Super.</cell><cell>MultiBERT MultiFiT</cell><cell cols="2">86.05 84.90 82.00 86.15 86.90 86.65 80.87 82.83 79.95 93.19 90.54 93.00 91.25 89.55 93.40 86.29 85.75 86.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of zero-shot, translation-based and supervised methods (with 2k training examples) on all domains of CLS. MT-BOW and CL-SCL results are from<ref type="bibr" target="#b31">(Zhou et al., 2016)</ref>.</figDesc><table><row><cell></cell><cell cols="2">LSTM QRNN</cell></row><row><cell>Language model pretraining</cell><cell>143</cell><cell>71</cell></row><row><cell>Classifier fine-tuning</cell><cell>467</cell><cell>156</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of LSTM and QRNN per-batch training speed on a Tesla V100 (in ms) in MultiFiT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Comparison of MultiFiT results with different</cell></row><row><cell>pretraining corpora and ULMFiT, fine-tuned with 1k</cell></row><row><cell>labels on MLDoc.</cell></row><row><cell>5 Results</cell></row><row><cell>MLDoc We show results for MLDoc in Table</cell></row><row><cell>2. In the zero-shot setting, MultiBERT underper-</cell></row><row><cell>10 https://github.com/google-research/</cell></row><row><cell>bert/blob/master/multilingual.md</cell></row><row><cell>11 Models are available for English, Chinese, and German</cell></row><row><cell>(https://deepset.ai/german-bert).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>91.34 78.92 89.45 76.00 69.57 68.19 82.45</figDesc><table><row><cell></cell><cell>DE</cell><cell>ES</cell><cell>FR</cell><cell>IT</cell><cell>JA</cell><cell>RU</cell><cell>ZH</cell></row><row><cell>LASER, code</cell><cell cols="7">87.65 75.48 84.00 71.18 64.58 66.58 76.65</cell></row><row><cell>Random init. (1k)</cell><cell cols="7">77.80 70.50 75.65 68.52 68.50 61.37 79.19</cell></row><row><cell>Random init. (10k)</cell><cell cols="7">90.53 69.75 87.40 72.72 67.55 63.67 81.44</cell></row><row><cell>MultiFiT, pseudo (1k)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Bootstrapping results on MLDoc with and without pretraining, trained on 1k/10k LASER labels.</figDesc><table><row><cell></cell><cell>DE</cell><cell>ES</cell><cell>FR</cell><cell>IT</cell><cell>RU</cell></row><row><cell cols="6">Word-based 95.28 95.97 94.72 89.97 88.02</cell></row><row><cell>Subword</cell><cell cols="5">96.10 96.07 94.75 94.75 87.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparison of different tokenization strategies for different languages on MLDoc.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://nlp.fast.ai The first four authors contributed equally. ‚Ä† Corresponding author: eisenjulian@gmail.com ‚Ä° Sebastian is now affiliated with DeepMind. 2 http://labs.theguardian.com/ digital-language-divide/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The training cost is amortized over time as pretraining only needs to be performed once and fine-tuning is much cheaper. However, if a model needs to be applied to a new language or a domain not covered by the model, a new model needs to be trained from scratch. 4 This is similar to how word embeddings are known to underperform on low-frequency tokens<ref type="bibr" target="#b8">(Gong et al., 2018)</ref>.5  We use 'multilingual' as referring to training independent models in multiple languages. We use 'cross-lingual' to refer to training a joint model across multiple languages.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">https://spacy.io/api/tokenizer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">https://github.com/google/ sentencepiece</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyper-parameters</head><p>The MultiFit architecture has 4 QRNN layers with a hidden dimensionality of 1550, a vocabulary size of 15,000 subword tokens, and an embeding size of 400. The vocabularies were computed using the SentencePiece 13 unigram language model <ref type="bibr" target="#b14">(Kudo, 2018)</ref> with 99% character coverage for Chinese and Japanese and 100% for the rest. The encoder's and decoder's weights are shared <ref type="bibr" target="#b22">(Press and Wolf, 2017)</ref>. The output of the last QRNN layer (the last time step concatenated with an average and maximum pooled over time steps) is passed to the classifier with 2 dense layers.</p><p>Our language models were trained for 10 epochs on 100 million tokens of Wikipedia articles and then fine-tuned for 20 epochs on the corresponding dataset (MLDoc or CLS). The classifier was fine-tuned for 4 to 8 epochs. Results of the best model based on accuracy on the validation set are reported. We used a modified version of 1-cycle learning rate schedule (Smith, 2018) that uses cosine instead of linear annealing, cyclical momentum and discriminative finetuning <ref type="bibr" target="#b11">(Howard and Ruder, 2018)</ref>. Our batch size for language model training was 50 and for classification tasks 18. We were using BPTT of length 70. Due to the large amount of available training data our pretrained language models were trained without any dropout. We used the same dropout values as <ref type="bibr" target="#b11">(Howard and Ruder, 2018</ref>) multiplied by 0.3 and 0.5 for fine-tuning of language models and the classification task respectively. We used weight decay of 0.01 for both tasks. The final regularization method was label smoothing <ref type="bibr" target="#b29">(Szegedy et al., 2016)</ref> with epsilon of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Speed comparison hyper-parameters</head><p>For the speed comparison, we use the same architecture and only change the underlying RNN cell (QRNN or LSTM). We pretrain and fine-tune both models on 15k tokens on a Tesla V100. For pretraining, we use a BPTT size of 70 and a batch size of 64. For fine-tuning, we use a batch size of 32.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pointer Sentinel Mixture Models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<title level="m">Massively Multilingual Word Embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10464</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quasi-Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Are all languages equally hard to language-model?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2018</title>
		<meeting>NAACL-HLT 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="536" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning with subword tokenization for polish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Czapla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PolEval 2018 Workshop</title>
		<meeting>PolEval 2018 Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuliƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FRAGE: Frequency-Agnostic Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.4931082</idno>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the Knowledge in a Neural Network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Universal Language Model Fine-tuning for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2007</title>
		<meeting>ACL 2007</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018</title>
		<meeting>ACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Crosslingual Language Model Pretraining. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phoebe</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Polyglot Contextual Representations Improve Crosslingual Transfer</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2019</title>
		<meeting>NAACL 2019</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dissecting Contextual Word Embeddings: Architecture and Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Computer</forename><surname>Paul G Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Science</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-Language Text Classification using Structural Correspondence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1145/2036264.2036277</idno>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;10 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL &apos;10 )</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving Language Understanding by Generative Pre-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Strong Baselines for Neural Semi-supervised Learning under Domain Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Corpus for Multilingual Document Classification in Eight Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multilingual neural machine translation with soft decoupled encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-Lingual Sentiment Classification with Bilingual Document Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1403" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
