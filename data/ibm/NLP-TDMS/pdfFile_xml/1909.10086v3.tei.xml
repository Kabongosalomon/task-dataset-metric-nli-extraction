<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Universal Graph Neural Network Embeddings With Aid Of Transfer Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
							<email>verma076@cs.umn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Minnesota Twin Cities</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
							<email>zhang@cs.umn.edu</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Twin Cities</orgName>
								<orgName type="institution">University of Minnesota</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Universal Graph Neural Network Embeddings With Aid Of Transfer Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning powerful data embeddings has become a center piece in machine learning, especially in natural language processing and computer vision domains. The crux of these embeddings is that they are pretrained on huge corpus of data in a unsupervised fashion, sometimes aided with transfer learning. However currently in the graph learning domain, embeddings learned through existing graph neural networks (GNNs) are task dependent and thus cannot be shared across different datasets. In this paper, we present a first powerful and theoretically guaranteed graph neural network that is designed to learn task-independent graph embeddings, thereafter referred to as deep universal graph embedding (DUGNN). Our DUGNN model incorporates a novel graph neural network (as a universal graph encoder) and leverages rich Graph Kernels (as a multi-task graph decoder) for both unsupervised learning and (task-specific) adaptive supervised learning. By learning task-independent graph embeddings across diverse datasets, DUGNN also reaps the benefits of transfer learning. Through extensive experiments and ablation studies, we show that the proposed DUGNN model consistently outperforms both the existing state-of-art GNN models and Graph Kernels by an increased accuracy of 3% − 8% on graph classification benchmark datasets.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning powerful data embeddings has become a center piece in machine learning for producing superior results. This new trend of learning embeddings from data can be attributed to the huge success of word2vec <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b35">35]</ref> with unprecedented real-world performance in natural language processing (NLP). The importance of extracting high quality embeddings has now been realized in many other domains such as computer vision (CV) and recommendation systems <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b56">56]</ref>. The crux of these embeddings is that they are pretrained in an unsupervised fashion on huge amount of data and thus can potentially capture all kind of contextual information. Such embedding learning is further advanced by the incorporation of multi-task learning and transfer learning which allow more generalized embeddings to be learned across different datasets. These developments have brought major breakthroughs in NLP: DecaNLP <ref type="bibr" target="#b30">[30]</ref> and BERT <ref type="bibr" target="#b10">[10]</ref> are recent such prime examples.</p><p>Besides natural languages ("sequence" or 1D data) and images (2D or 3D) which are well-structured, the idea of embedding learning has been applied to (irregular) "graph-structured" data for various graph learning tasks, such as node classification or link prediction. Unlike word embeddings where vocabulary is typically finite, graph "vocabulary" is potentially infinite (i.e, count of non-isomorphic graphs). Hence learning contextual based embeddings, a la ELMo <ref type="bibr" target="#b36">[36]</ref> namely, is crucial. Building upon the success of deep learning in images and words, graph neural networks (GNNs) have been recently developed for various graph learning tasks on graph-structured datasets. Most of existing GNNs are task-specific in the sense that they are trained on datasets via supervised (or <ref type="figure">Figure 1</ref>. Side figure shows two pairs of isomorphic graphs sampled from real but more interestingly different bioinformatics and quantum mechanic datasets namely NCI1, MU-TAG, PTC, QM8; suggesting the importance of learning universal graph embedding and performing transfer learning &amp; multi-tasking (for learning more generalized embeddings).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MUTAG NCI1</head><p>QM8 PTC semi-supervised) learning with task-specific labels, and thus the trained models cannot be directly applied to other tasks. In addition, they are often limited to learning node embeddings (based on a node's local structure within an underlying graph), as opposed to graph embedding (defined as a single embedding for the whole graph), which is more general purpose, e.g., for graph classification or graph generation tasks. Besides graph embedding can serve as a node embedding by operating on node's ego-graph.</p><p>We envision a deep universal graph embedding neural network (DUGNN) , which is capable of the following: 1) It can be trained on diverse datasets (e.g., with different node features) for a variety of tasks in an unsupervised fashion to learn task-independent graph embeddings; 2) The learned graph embedding model can be shared across different datasets, thus enjoying the benefits of transfer learning;</p><p>3) The learned model can further be adapted and improved for specific tasks using adaptive supervised learning. <ref type="figure">Figure 1</ref> shows some sample graphs in four different real-world datasets from fields as diverse as bioinformatics and quantum mechanics. While the node features (and their meanings) can differ vastly but the underlying graphs governing them contain isomorphic graph structures. This suggests that learning universal graph embeddings across diverse datasets is not only possible, but can potentially offer the benefits of transfer learning. From theoretical point of view, we establishes the generalization guarantee of DUGNN model for graph classification task and discuss the role of transfer learning in helping towards reducing the generalization gap. To best of our knowledge, we are the first to propose doing transfer learning in the graph neural network domain.</p><p>In order to develop a universal graph embedding model, we need to overcome three main technical challenges. Firstly, existing GNNs operate at the node feature-matrix level. Unlike images or words where the channels or embedding layer has a fixed input size, in the context of graph learning, the initial node feature (a feature vector defined on nodes in a graph) dimension can vary across different datasets. Secondly, the model complexity of GNNs is often limited by the basic graph convolution operation, which in its purest form is the aggregation of neighboring node features and may suffer from the Laplacian smoothing problem <ref type="bibr" target="#b28">[28]</ref>. Lastly, the major technical hurdle is to devise an unsupervised graph decoder that is capable of regenerating or reconstructing the original graph directly from its graph embedding with minimal loss of information.</p><p>We propose a DUGNN model to tackle these challenges with three carefully designed core components: 1) Input Transformer, 2) Universal Graph Encoder, and 3) Multi-Task Graph Decoder.</p><p>Through extensive experiments and ablation studies, we show that the DUGNN model consistently outperforms both the existing state-of-art GNN models and Graph Kernels by an increased accuracy of 3% − 8% on graph classification benchmark datasets.</p><p>In summary, the major contributions of our paper are:</p><p>• We propose a novel theoretical guaranteed DUGNN model for universal graph embedding learning that can be trained in unsupervised fashion and also capable of doing transfer learning. • We leverage rich graphs kernels to design a multi-task graph decoder which incorporates the power of graph kernels in graph neural networks and get the best of both the worlds. • Our DUGNN model achieves superior results in comparison with existing graph neural networks and graph kernels on various types of graph classification benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently, various graph neural networks (viewed as "graph encoders") have been developed, in particular, learning task-specific node embeddings from graph-structured datasets. In contrast, we are interested in learning task-independent graph embedding. Furthermore, our problem also involves designing graph decoder -graph reconstruction using graph embedding -which is far more challenging and has not received much attention in the literature. We overview the key related in these two areas along with graph kernels.</p><p>Graph Encoders and Decoders: Under graph encoders, we consider both graph convolutional neural networks (GCNNs) and message passing neural networks (MPNNs). The early development of GNNs can be traced back to graph signal processing <ref type="bibr" target="#b42">[42]</ref> and cast in terms of learning filter parameters of the graph Fourier transform <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">17]</ref>. Various GNN models have since been proposed <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b58">58]</ref> that mainly attempt to improve the basic GNN model along two aspects: 1) enhancing the graph convolution operation by developing novel graph filters; and 2) designing appropriate graph pooling operations. For instance, <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b12">12]</ref> employs complex graph filters via Cayley and b-splines as a basis for the filtering operation respectively. For graph pooling operations, pre-computed graph coarsening layers via the graclus multilevel clustering algorithm are employed in <ref type="bibr" target="#b7">[8]</ref>, while a differential pooling operation is developed in <ref type="bibr" target="#b53">[53]</ref>. The authors in <ref type="bibr" target="#b51">[51]</ref> propose a sum aggregation pooling operation that is better justified in theory. The authors in <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">13]</ref> propose message passing neural networks (MPNNs), which are viewed as equivalent to GCNN models, as the underlying notion of the graph convolution operation is the same. A similar line of studies have been developed in <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b46">46]</ref> for learning node embedding on large graphs. In contrast, the authors in <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b48">48]</ref> propose GNNs for handling graph classification problem. The main limitations of all the aforementioned GNN models lie in that they must be trained from scratch on a new dataset and the embeddings are tuned based on a supervised (task-specific) objective function. As a result, learned embeddings are task dependent. There is no mechanism for sharing the embedding model across different datasets. Unfortunately work on designing graph decoder is currently under-explored and partially falls under graph generation area <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b54">54]</ref>. But the graph generation work do not focus on recovering the exact graph structure but rather generating graph with similar characteristics.</p><p>Graph Kernels: The literature on graph kernels is vast, we only outline a few. Some of the most popular graph kernels are Weisfeiler-Lehman kernel <ref type="bibr" target="#b41">[41]</ref>, graphlets <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b40">40]</ref>, random walk or shortest path or anonymous walk based kernels <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">19]</ref>. Several graph kernels based on more complex kernel functions have also been developed that can capture sub-structural similarities at multiple levels; these include deep graph kernels <ref type="bibr" target="#b52">[52]</ref>, graph invariant kernels <ref type="bibr" target="#b33">[33]</ref> and multiscale laplacian graph kernel <ref type="bibr" target="#b24">[24]</ref>. Instead of directly computing graph kernels, powerful graph spectrum based methods have also been developed, for example, Graphlet spectrum <ref type="bibr" target="#b25">[25]</ref> based on group theory, and a family of graph spectral distances (FGSD) based on spectral graph theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Universal Graph Embedding Model</head><p>Basic Setup and Notations: Let G = (V, E, A) be a graph where V is the vertex set, E the edge set (with no self-loops) and A the adjacency matrix, with N = |V | the graph size. We define the standard graph Laplacian L ∈ R N ×N as L = D − A, where D is the degree matrix. Let X ∈ R N ×d be the node feature matrix with d as the input dimension and h denotes the hidden dimension. Further let f (L) be a function of the graph Laplacian i.e., f (L) = Uf (Σ)U T , where Σ is the diagonal matrix of eigenvalues of L and U the eigenvector matrix. <ref type="figure">Figure 2</ref> depicts the overall architecture of DUGNN model. We describe the core components namely 1) Input Transformer 2) Universal Graph Encoder 3) Multi-Task Graph Decoder, in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Transformer</head><p>The dimension of graph input features is task-specific and may differ across different datasets. To make our universal graph encoder task-independent (or dataset-independent), we devise a specific input layer which transforms a given input node feature dimension X into a consistent hidden feature dimension, i.e., T : X ∈ R N ×d → X ∈ R N ×h and feed it to the universal graph encoder.</p><p>It is well known that GNN models aggregate feature information within a K−hop local neighborhood of a node <ref type="bibr" target="#b7">[8]</ref>. Hence these models heavily rely on the initial node features to capture crucial global structural information of a graph. However, in many datasets, the node features are absent (in other words, the inputs are merely the underlying graph structure). In these cases, many models choose X = I, the identity matrix. Instead, we propose to initialize X as a Gaussian random matrix (in absence of node features), which is justified by the following theorem.</p><p>Theorem 1 (Graph Spectral Embedding Approx. with GNN Random Feature Initialization) Let f (L) ∈ R N ×N be a function of graph Laplacian and X ∈ R N ×d ∼ N (0, σ 2 ) be a Gaussian random matrix initialize as a node feature (or embedding) matrix where d ≤ N . Then f (L)X</p><formula xml:id="formula_0">X ∈ R B×N ×D A ∈ R B×N ×N G(V, E) MLP(f (L)X)</formula><p>Input Transformer</p><formula xml:id="formula_1">X ∈ R B×N ×H GNN POOL Z ∈ R B×H ( ) Graph Embedding Y ∈ R B×N ×H ( ) Universal Graph Encoder Mutli-Task Graph Decoder WL Kernel SP Kernel . . . K (1) ∈ R B×B K (2) ∈ R B×B K (k) ∈ R B×B A A ∈ R B×N ×N NN S ∈ R B×C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax Task Specific</head><p>Model Shared For Tranfer Learning <ref type="figure">Figure 2</ref>. Above figure shows overall architecture of DUGNN model in the form of tensor transformations. Starting from the left, we have a graph G(V, E) with node feature matrix X and adjacency matrix A. X is first transform into a consistent feature dimension X via Input Transformer. Next, Universal Graph Encoder computes graph embedding z and output Y which is passed down to the decoder. Our Multi-task Graph Decoder comprises of minimizing graph kernel losses and adjacency matrix reconstruction loss along with the optional supervised task loss for joint end-to-end learning.</p><p>resultant embedding is equal to a randomly projected graph spectral embedding in</p><formula xml:id="formula_2">R d space i.e, f (L)X = R(Uf (Σ)) where R(·) is some random projection in R d space.</formula><p>Remarks: The proof relies on the fact that the projection of a Guassian random matrix X onto an eigenspace U of L preserves the Gaussian properties of X [34] with probability 1. One of the consequences of Theorem 1 is that we can approximate different spectral dimension reduction techniques with suitably chosen f (·) function. For instance, if we choose f (·) as the identity function, the transformation becomes the Laplacian eigenmaps <ref type="bibr" target="#b1">[2]</ref>. Moreover, remarkably f (L)X is nothing but a graph convolution operation and thus provides an approximation of the graph spectral embedding. This provides a theoretical explanation for the competitive performance of a randomly initialized GNN model as previously noted in <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b46">46]</ref>.</p><p>With appropriately initialized input feature vector X, our input transformer performs the following operation, T (X) = MLP f (L)X where MLP is a multi-layer perceptron neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Universal Graph Encoder</head><p>Our universal graph encoder is based on a GNN model but with several key improvements. Let g(L) be a graph filter function. Some early adopted graph filters are polynomial functions of L <ref type="bibr" target="#b7">[8]</ref>. We simply choose g(L) = D −1/2 AD −1/2 + I as in <ref type="bibr" target="#b23">[23]</ref>. The th layer output of a GNN model can be written in general as,</p><formula xml:id="formula_3">F ( ) ( X, L) = MLP g(L)F ( −1) ( X, L) where F (0) ( X, L) = X and F ( ) ( X, L) ∈ R N ×h ( )</formula><p>. To further improve the model complexity of our graph encoder, we capture the higher order statistical moment information of features during the graph convolutional operation, similar to graph capsule networks <ref type="bibr" target="#b48">[48]</ref> as follows,</p><formula xml:id="formula_4">F ( ) ( X, L) = MLP P p=1 MLP g(L) F ( −1) ( X, L) p (1)</formula><p>where P is the number of instantiation parameters. Other than boosting the model complexity, there is standing problem of Laplacian smoothing <ref type="bibr" target="#b28">[28]</ref> in GCNN models. To overcome the smoothing problem, we propose to concatenate the output of intermediate encoding layers and feed it into subsequent layers. This way our graph encoder can learn on multi-scale smooth features of a node and thus can avoid under-smoothing or over-smoothing issues in a GCNN based model. Fortunately, it also enjoy the side benefits of alleviating vanishing-gradient problem and strengthen feature propagation in deep networks as shown in Dense-CNN models <ref type="bibr" target="#b18">[18]</ref>. To obtain the final graph embedding z ∈ R h <ref type="bibr">( )</ref> , we perform sum-pooling on all nodes z = N i=0 F ( ) i ( X, L) ∈ R h ( ) and show its representational power below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2 (Deep Universal Graph Embedding Representational Power) Deep Universal</head><p>Graph Embedding model initialized with node features as graph spectral embeddings is atleast as powerful as the classical Weisfeiler-Lehman (WL) graph isomorphism test. Remarks: Theorem 2's subpart where DUGNN representation power is same as WL graph isomorphism test directly follows from Theorem 3 of the paper <ref type="bibr" target="#b51">[51]</ref>. To make DUGNN more powerful than the classical WL graph isomorphism test (where nodes features are either identical or equal to respective node degree), one can initialize DUGNN node features with graph spectral embeddings. As a result, DUGNN can now differentiate certain regular graphs where the classical WL graph isomorphism test fails. The main trick here is to initialize node features such they depend on the full graph structure rather than the local structure. Coincidently, we have Theorem 1 to approximate graph spectral embedding in our DUGNN model and that too in fast manner. Although in all fairness, the same trick can also make WL more powerful.</p><p>Next, we provide the generalization guarantee of DUGNN model and further discuss the role of transfer learning in reducing the generalization gap.</p><p>Theorem 3 (Deep Universal Graph Embedding Model Generalization Guarantee) Let A S be a single layer Universal Graph Encoder equipped with the graph convolution filter g(L) and trained on a dataset S using the SGD algorithm for T iterations. Let the loss &amp; activation functions be Lipschitz-continuous and smooth. Then the following expected generalization guarantee holds with probability at least 1 − δ having δ ∈ (0, 1),</p><formula xml:id="formula_5">E SGD [R(A S )] ≤ E SGD [R emp (A S )] + O P N T +1 (λ max G ) 2T 1 m + log 1 δ 2m + C log 1 δ 2m where E SGD [R(·)]</formula><p>is the expected risk taken over the randomness due to SGD, R emp (·) is the empirical risk, m is the number of training graph samples, N is the maximum graph-size, λ max G is the largest eigenvalue of graph filter and C is a upper bound on the loss function. Remarks: Theorem 3 relies on showing the fact that Universal Graph Encoders are uniformly stable <ref type="bibr" target="#b3">[4]</ref> and has several implications. First, normalized graph filters are theoretically more stable (besides numerically) since λ max G ≤ 1 and parameter P controls the classic bias-variance tradeoff. Also the theorem does not bear any restrictions on type of graph datasets employed for training and establishes that transfer learning remains beneficial between different datasets for graph classification task. Intuitively GNN parameters are learned based on the local (k−hop) graph-structure and having more samples will always help towards generalizing better on unseen data and reduces the generalization error at a rate O( 1 √ m ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Task Graph Decoder</head><p>Multi-task learning have shown to yield superior results in natural language processing <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b10">10]</ref>. We want to equip our DUGNN model with built-in capabilities of multi-task learning. For this, we employ multiple prediction metrics in our graph decoder to enable it to learn more generalized graph embedding useful for different learning tasks. Note that supervised task specific loss i.e., cross-entropy loss L class is not considered as the part of our multi-task decoder (see <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Adjacency Matrix Reconstruction Loss:</head><p>The major technical challenge in devising a general purpose graph decoder is reconstructing the original graph structure directly from its graph embedding vector z. We consider minimizing adjacency reconstruction loss L A as the first task of our graph decoder. Following adjacency loss L A is incurred during mini-batch process,</p><formula xml:id="formula_6">L A = λ A B i=1 CE σ(Y i Y T i ), A i where Y = F ( ) ( X, L)</formula><p>is the encoder output, CE is binary cross entropy loss corresponding to presence or absence of each edge and λ A is loss weight. There are two shortcomings of this task. First, L A does not take graph embedding z directly into account and as such graph embedding may incur significant loss of information after the pooling operation. Second, with O(N 2 ) computing L A (in every batch iteration) may become expensive on datasets with large graph-size. In our experiments, we were able to compute adjacency reconstruction loss on all datasets except D&amp;D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Kernel Loss:</head><p>We propose a novel solution which leverages rich graph kernels to overcome the shortcomings present in the first task. In essence, graph kernels are developed to capture various key sub-structural properties of a graph: for two graphs, K(G i , G j ) provides a measure of their similarity. In our decoder design, we incorporate multiple graph kernels {K (k) } to jointly learn and predict the quality of (universal) graph embedding z directly by minimizing the following unsupervised joint graph kernel loss L (unsup)</p><formula xml:id="formula_7">K , K (k) = B i=1 B j=1 MSE σ z T i W k z j , K (k) ij , L (unsup) K = λ K K k=1 λ k K (k)</formula><p>where MSE is the mean square error loss, σ z T i W k z j is a (learned) similarity function between two graph embeddings and W k ∈ R h ( ) ×h <ref type="bibr">( )</ref> is the associated kernel weight parameter. By leveraging precomputed graph kernels, our computation is cheap (in every batch iteration), but also take the graph embedding z directly into account for the joint learning of our graph encoder-decoder model.</p><p>Adaptive Supervised Graph Kernel Loss: For supervised task learning problems, we augment the loss function with an adaptive supervised graph kernel loss function. Specifically, we focus on graph kernels that are aligned with the task objective. As shown in Equation <ref type="formula">(2)</ref>, if class labels of two graphs are the same, then we choose the graph kernel with maximum similarity value and vice-versa. Thus, we are making sure to pick only those sub-structural properties that are relevant to a given specific task. For instance, in MUTAG dataset, counting the number of cycles is more important than computing the distribution of random walks (see appendix for more details). </p><formula xml:id="formula_8">L (sup) K = λ K B i=1 B j=1 MSE σ z T i W k z j , I (yi=yj ) max k (K (k) ij ) + 1 − I (yi=yj ) min k (K (k) ij ) (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment and Results</head><p>We evaluate DUGNN thoroughly on a variety of graph classification benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUGNN Model Configuration:</head><p>In the Input Transformer, we set the hidden dimension to h ∈ {16, 32, 64}. For the graph datasets without node features, we initialize X using a Gaussian random matrix and choose f (L) as the normalized symmetric Laplacian. For the Universal Graph Encoder, we build ∈ {5, 7} layer deep GNN network with the internal hidden dimensions chosen from h ( ) ∈ {16, 32, 64} and pick the instantiation parameter from p ∈ {1, 2, 4}. In the Multi-Task Decoder, we employed three graph kernels loss besides adjacency reconstruction loss 1) WL-Subtree 2) Shortest-Path 3) FGSD, as they are relatively fast to compute and all decoder losses are added with equal weights. To keep the network outputs stable, we use batch normalization between layers along with the L2 weight norm regularization and dropout techniques to prevent overfitting. In addition, we employed the ADAM optimizer with varying learning rates as proposed in <ref type="bibr" target="#b45">[45]</ref> and its parameters are set as: the max epoch 3000, warmup epoch 2, initial learning rate 10 −4 , max learning rate 10 −3 and final learning rate 10 −4 . All models are trained with early stopping criteria based on validation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Baselines:</head><p>We employed 5 bioinformatics and 3 social network benchmark datasets to evaluate the DUGNN model on the graph classification task, namely, PTC, PROTEINS, NCI1, D&amp;D, ENZYMES, COLLAB, IMDB-BINARY and IMDB-MULTI and further details are present in <ref type="bibr" target="#b52">[52]</ref>. We compare the DUGNN model performance against 7 recently proposed GNNs and 8 state-of-art Graph Kernels as shown in <ref type="table">Table 1</ref>.</p><p>Experimental Set-up: We first train DUGNN in an unsupervised fashion on all mentioned datasets together. Here the universal graph encoder is shared across all the datasets. Next, we fine tune our DUGNN for each dataset separately using a cross-entropy loss (L class ) corresponding to the graph labels and the adaptive supervised kernel loss. We report 10-fold cross validation results obtained by closely following the same experimental setup used in previous studies <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b51">51]</ref>. To make a fair comparison, we either cite the best cross validation results previously reported or run the source code if available according to author's guidelines. Further details are present in the supplementary. Graph Classification Results: <ref type="table">Table 1</ref> shows the classification results on considered datasets based on graph neural network models and graph kernel methods. It is clear that DUGNN consistently outperforms every state-of-art GNN model by a margin of 3% − 8% increase in prediction accuracy on both bioinformatics as well as social-network datasets (with the highest accuracy achieved on the PTC). On relatively small datasets such as PTC &amp; ENZYMES, the increase in accuracy is around 6% − 8%. This empirically confirms our hypothesis that transfer learning in the graph domain is quite beneficial, especially where available graph samples are limited.</p><p>Our DUGNN model also significantly outperforms all the state-of-art graph kernel methods. We again observe a consistent performance gain of 3% − 11% (again with the highest increase on the PTC dataset). Interestingly, our DUGNN integrated with graph kernels in the multi-task decoder outperforms the WL-subtree kernel, FGSD and SP kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Studies and Discussion</head><p>We now take a closer look at the performance of each component of DUGNN model by performing various ablation studies and show their individual importance and contributions.</p><p>How powerful is our Universal Graph Encoder without multi-tasking and transfer learning? We conduct an experiment where the DUGNN model is trained from scratch without the multi-task decoder and evaluate its performance on a large quantum mechanic dataset QM8 (containing 21786 compounds). For this purpose, we use the same experimental settings provided in <ref type="bibr" target="#b50">[50]</ref> including the same datasplits given by the DeepChem 1 implementation, and compare it against the three state-of-the-art GNNs: 1) MPNN <ref type="bibr" target="#b15">[15]</ref>, 2) DTNN <ref type="bibr" target="#b39">[39]</ref> and 3) GCNN 2 . From <ref type="table" target="#tab_1">Table 2</ref>, it is clear that our universal graph encoder significantly outperforms these GNNs by a large margin of 20% − 30% in terms of the mean absolute error (MAE), achieving the new state-of-art result on the QM8 dataset.</p><p>How much gain do we see from sharing pretrained DUGNN model via transfer learning? We conduct an ablation study to determine the importance of utilizing the pretrained DUGNN model. In this experiment, we pick one of the cross validation splits of NCI1 &amp; PTC datasets for training &amp; validating, and fix all the hyper-parameters including the random seeds across the full ablation experiment. We first train a DUGNN model on both NCI1 &amp; PTC datasets. Then the DUGNN models are trained on each dataset from scratch. <ref type="table" target="#tab_2">Table 3</ref> shows that training without transfer learning reduces accuracy by around 0.4% − 1% on both datasets. Also, we see a bigger accuracy jump on PTC, since its dataset size is smaller, thus benefiting more via transfer learning.</p><p>How much boost do we get with Multi-Task Graph Decoder? In this ablation study, we train DUGNN from scratch with different loss functions. <ref type="table" target="#tab_3">Table 4</ref> reveals that completely removing the graph decoder (i.e., L A and L (unsup) K ) reduces accuracy by around 3% − 4% on both datasets. While removing only the graph kernel loss (L (unsup) K ) reduces accuracy by 2% − 3%. The accuracy drops by around 1% when L A is removed. Lastly, removing the supervised loss (L class ) reduces the performance considerably; nonetheless our model remains competitive and performs better against various graph kernel methods (see <ref type="table">Table 1</ref>).</p><p>Ablation study related to effectiveness of adaptive supervised graph kernel loss is deferred to appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have presented a powerful univeral graph embedding neural network architecture with three carefully designed components that can learn task-independent graph embeddings in a unsupervised fashion. In particular, the universal graph encoder component can be re-utilized across different datasets by leveraging transfer learning, and the decoder component incorporates various graph kernels to capture rich sub-structural properties to enable multi-task learning. Through extensive experiments and ablation studies on benchmark graph classification datasets, we show that our proposed DUGNN model can significantly outperform both the existing state-of-art graph neural network models and graph kernel methods. This demonstrates the the benefit of combining the power of graph neural networks in the design of a universal graph encoder with that of graph kernels in the design of a multi-task graph decoder. The proof of Theorem 1 follows immediately from a key result in <ref type="bibr" target="#b34">[34]</ref> stated below.</p><p>Lemma 1 <ref type="bibr" target="#b34">[34]</ref> Let U ∈ R N ×N be an orthonormal matrix and R ∈ R N ×d be a Gaussian random matrix with i.i.d. entries ∼ N (0, σ 2 ). Then the entries of UR are i.i.d. Gaussian random samples with the same pdf ∼ N (0, σ 2 ).</p><p>Let f (L) be a function of graph Laplacian that operates on eigenvalues and defined as follows f (L) = Uf (Σ)U T where Σ is the eigenvalue diagonal matrix and U is the eigenvector matrix which is also an orthonormal matrix. Therefore, f (L)R = Uf (Σ)U T R = Uf (Σ) R where R ∈ R N ×d is again a Gaussian random matrix from Lemma 1. Here U = Uf (Σ) contains the eigenvector columns scaled by the f (Σ) eigenvalues. Thus U R results in a random projection (or dimension reduction) of the scaled eigenvector space of the graph Laplacian into a d-dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Proof of Theorem 2</head><p>Theorem 2's subpart where DUGNN representation power is same as Weisfeiler-Lehman (WL) graph isomorphism test directly follows from Theorem 3 of the paper <ref type="bibr" target="#b51">[51]</ref>. Note that both the conditions stated in Theorem 3 are met, since our node aggregate function and readout function is same as Graph Isomorphism Network (GIN). To show that DUGNN is atleast as powerful than WL, we provide the general condition on k-regular graphs to be hold such that their graph representations are different. It is suffice to show a single example where WL fails but DUGNN succeeds. Consider two k-regular non-isomorphic graphs of size N and let N (i) represents the neighbor indices of i th node including itself. Since node features initialized are identical (or equal to node degree) in classic WL i.e, x (0) v = c for each node v, after local aggregation operation, the new node features is given by x</p><formula xml:id="formula_9">(1) v = φ(c, f ({c, ..., c} k-times ))</formula><p>for each node in G1 where f is a multi-set function and φ is a injective function. Similarly for G2 each node representation is given by x</p><formula xml:id="formula_10">(1) v = φ(c, f ({c, ..., c} k-times</formula><p>)) after first iteration which is same as G1. As a result, the graph representation of G1 and G2 are same since sum-pool operation yields the same value i.e., v x  </p><formula xml:id="formula_11">= N i=1 j∈N (i) uj = k N i=1 ui. Similarly, for G2 the final representation is y2 = k N i=1 vi.</formula><p>As a result, for all non-isomorphic k-regular graph pairs whose row-wise sum of eigenvector matrix respectively are not equal -can be distinguished by DUGNN -but not possible in the case of classic WL. It is easy to verify numerically that the row-wise sum of eigenvector matrices of graphs shown in <ref type="figure" target="#fig_2">Figure 3</ref> are not equal which implies y1 = y2 and hence their graph representations are different. Theorem 3 is based on main result of the paper <ref type="bibr" target="#b49">[49]</ref> and the extension to graph classification as well as graph capsule networks. We show that the single layer graph capsule networks are also uniformly stable <ref type="bibr" target="#b3">[4]</ref> for solving graph classification problem. Like in <ref type="bibr" target="#b49">[49]</ref>, we assume that the activation and loss functions are Lipschitz continuous and smooth. For convenience, we borrow the same notions used in <ref type="bibr" target="#b49">[49]</ref> and similarly divide the proofs in two parts. In first part, we separate out the terms due to weight parameters and graph convolution operation in order to bound their difference independently. In second part, we bound the expected difference (due to to randomness of SGD algorithm) in weight parameters under single data perturbation.</p><p>Following is a single layer Universal Graph Encoder GNN function f (x, θ) ∈ R based on statistical moments with sum as read-out function in graph classification task where N is number of nodes in a graph, xi is the i th node feature value and θ is the capsule learning parameter. For simplicity, we show our results for the case of x ∈ R but the proof remains applicable for general x ∈ R d .</p><formula xml:id="formula_12">f (x = {x1, . . . , xN }, θ) = N n=1 σ j∈ N (xn) e·jxjθ1 + σ j∈ N (xn) e·jx 2 j θ2 + · · · + σ j∈ N (xn) e·jx 2 j θp = N n=1 P p=1 σ j∈ N (xn) e·jx p j θp<label>(3)</label></formula><p>The first order derivative with respect to p th −parameter is given as,</p><formula xml:id="formula_13">∂f (x, θ) ∂θp = N n=1 σ j∈ N (x) e·jx p j θp j∈ N (x) e·jx p j<label>(4)</label></formula><p>where P is number of instantiation parameters.</p><p>Proof Part I: Let ESGD be expectation due to SGD randomness and let θS and θ S i represent the filter weights learned on training set S and S i that differs in precisely single data point. Also, ∆θ = θS − θ S i .</p><formula xml:id="formula_14">|ESGD[ (AS, y) − (A S i , y)]| ≤ α ESGD[|f (x, θS) − f (x, θ S i )|] ≤ α ESGD N n=1 P p=1 σ j∈ N (xn) e·jx p j θp,S − N n=1 P p=1 σ j∈ N (xn) e·jx p j θ p,S i ≤ α ESGD N n=1 P p=1 σ j∈ N (xn) e·jx p j θp,S − σ j∈ N (xn) e·jx p j θ p,S i<label>(5)</label></formula><p>Since activation function is also σ−Lipschitz continuous,</p><formula xml:id="formula_15">≤ α ESGD N n=1 P p=1 j∈ N (xn) e·jx p j θp,S − j∈ N (xn) e·jx p j θ p,S i ≤ α ESGD N n=1 P p=1 j∈ N (xn) e·jx p j (θp,S − θ p,S i ) ≤ α ESGD N n=1 P p=1 j∈ N (xn) e·jx p j θp,S − θ p,S i ≤ α N n=1 P p=1 j∈ N (xn) e·jx p j ESGD ∆θp ≤ α N P p=1 g p,λ ESGD ∆θp</formula><p>where g p,λ is defined as g p,λ := sup x j∈N (x) e·jx p j . The term g p,λ will later be bounded in terms of the largest absolute eigenvalue of the graph convolution filter g(L).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof Part II:</head><p>We move on the second part of the proof where we bound the different in weights learned due to single data perturbation in Lemma 4 using Lemma 2 and Lemma 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2 (Universal Graph Encoder Same Sample Loss Stability Bound)</head><p>The loss-derivative bound difference of (single-layer) Universal Graph Encoder trained with SGD algorithm for T iterations on two training datasets S and S i respectively, with respect to the same sample is given by,</p><formula xml:id="formula_16">∇p f (x, θS,t), y − ∇p f (x, θ S i ,t ), y ≤ ν νσN g 2 p,λ |∆θp,t|</formula><p>Proof: <ref type="figure" target="#fig_7">Using Equation 4</ref> here, we get,</p><formula xml:id="formula_17">∂ f (x, θS,t), y ∂θp − ∂ f (x, θ S i ,t ), y ∂θp ≤ ν ∂f (x, θS,t) ∂θp − ∂f (x, θ S i ,t ) ∂θp ≤ ν N n=1 σ j∈ N (xn) e·jx p j θp,S,t j∈ N (xn) e·jx p j − N n=1 σ j∈ N (xn) e·jx p j θ p,S i ,t j∈ N (xn) e·jx p j ≤ ν N n=1 j∈ N (xn) e·jx p j σ j∈ N (xn) e·jx p j θp,S,t − σ j∈ N (xn) e·jx p j θ p,S i ,t</formula><p>Since the activation function is Lipschitz continuous and smooth, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3 (Universal Graph Encoder Different Sample Loss Stability Bound)</head><p>The loss-derivative bound difference of (single-layer) Universal Graph Encoder trained with SGD algorithm for T iterations on two training datasets S and S i respectively, with respect to the different samples is given by,</p><formula xml:id="formula_18">∇p f (xi, θS,t), yi − ∇p f (x i , θ S i ,t ), y i ≤ 2ν ασN g p,λ .</formula><p>Proof:</p><formula xml:id="formula_19">∂ f (x, θS,t), y ∂θp − ∂ f (x, θ S i ,t ),ỹ ∂θp ≤ ν ∂f (x, θS,t) ∂θp − ∂f (x, θ S i ,t ) ∂θp ≤ ν N n=1 σ j∈ N (xn) e·jx p j θp,S,t j∈ N (xn) e·jx p j − N n=1 σ j∈ N (xn) e·jx p j θ p,S i ,t j∈ N (xn) e·jx p j ≤ ν N n=1 σ j∈ N (x) e·jx p j θp,S,t j∈ N (x) e·jx p j + ν N n=1 σ j∈ N (x) e·jx p j θ p,S i ,t j∈ N (x) e·jx p j</formula><p>Using the fact that the first order derivative is bounded,</p><formula xml:id="formula_20">≤ 2ν ασN g p,λ</formula><p>This completes the proof of Lemma 3.</p><p>Lemma 4 (Universal Graph Encoder SGD Stability Bound) Let the loss &amp; activation functions be Lipschitzcontinuous and smooth. Let θS,T and θ S i ,T denote the graph filter parameters of (single-layer) Universal Graph Encode trained using SGD for T iterations on two training datasets S and S i , respectively. Then the expected difference in the filter parameters is bounded by,</p><formula xml:id="formula_21">ESGD θp,S,T − θ p,S i ,T | ≤ 2ην ασN g p,λ m T t=1 1 + ην νσN g 2 p,λ t−1</formula><p>Proof: Following the bounding analysis presented in <ref type="bibr" target="#b49">[49]</ref> for expected weight difference due to SGD, we have,</p><formula xml:id="formula_22">ESGD ∆θp,t+1| ≤ 1 − 1 m ESGD θp,S,t − η∇p f (x, θS,t), y − θ p,S i ,t − η∇p f (x, θ S i ,t ), y + 1 m ESGD θp,S,t − η∇p f (x, θS,t),ỹ − θ p,S i ,t − η∇p f (x, θ S i ,t ),ŷ ≤ 1 − 1 m ESGD |∆θp,t| + 1 − 1 m ηESGD ∇p f (x, θS,t), y − ∇p f (x, θ S i ,t ), y + 1 m ESGD |∆θp,t| + 1 m ηESGD ∇p f (x, θS,t),ỹ − ∇p f (x, θ S i ,t ),ŷ<label>(6)</label></formula><formula xml:id="formula_23">= ESGD |∆θp,t| + 1 − 1 m ηESGD ∇p f (x, θS,t), y − ∇p f (x, θ S i ,t ), y + 1 m ηESGD ∇p f (x, θS,t),ỹ − ∇p f (x, θ S i ,t ),ŷ</formula><p>Plugging the bounds in Lemma 2 and Lemma 3 into Equation <ref type="formula" target="#formula_22">(6)</ref> This completes the proof of Lemma 4.</p><p>Bound on g p,λ : Let q = |N (x)| and gx(L) ∈ R q×q be the submatrix of g(L) whose row and column indices are from the set {j ∈ N (x)}. We use hx,p ∈ R q to denote the p th moment graph signal (node features) on the ego-graph Gx,p. Without loss of generality, we will assume that node x is represented by index 0 in Gx,p. Thus, we can compute j∈N (x) e·jx p j = [gx(L)hx,p]0, a scalar value. Here [·]0 ∈ R represents the value of a vector at index 0, i.e., corresponding to node x. Then the following holds (assuming the graph signals are normalized, i.e., hx,1 2 = 1), |[gx(L)hx,p]0| ≤ gx(L)hx,p 1 ≤ gx(L) 2 hx,p 2 ≤ gx(L) 2 hx,1 2 = λ max Gx <ref type="bibr" target="#b6">(7)</ref> where the third inequality follows from the fact that i |x p i | ≤ i |xi| 2 ≤ 1 for p ≥ 2 and since x 2 = 1. As a result the norm inequality ( i |x p i |) 1/2 ≤ ( i |xi| 2 ) 1/2 ≤ 1 holds. Finally, plugging g p,λ ≤ λ max  We qualitatively demonstrate the significance of employing the adaptive supervised graph kernel loss on MUTAG dataset. For this purpose, we take a deeper dive into the results to find out which sub-structures are more important for prediction. <ref type="figure" target="#fig_7">Figures 4a and 4b</ref> depict some representative graph structures for class labels 1 and 2 on the MUTAG dataset. It turns out that the graph samples with label 1 have mostly three or more cycles, while the graph samples with label 2 tend to contain only one cycle. On the other hand, the graph samples with two cycles can belong to either class. By simply creating a learning rule that data samples containing 3 or more cycles belong to label 1, we can get a prediction accuracy around 84%. This simple rule alone beats the random-walk graph kernel based method, which achieves a prediction accuracy of 80.72% <ref type="bibr" target="#b41">[41]</ref>. By employing an adaptive supervised graph kernel, the model can thus learn embeddings which are biased more towards a graph kernel that better captures the count of number of cycles in graphs, and discount the graph kernels which attempt to match the random walk distributions which do not help with increasing the prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Experiment Baselines Settings</head><p>For the Weisfeiler-Lehman (WL) kernel, we vary the iteration pararmeter h ∈ {2, 3, 4, 5}. For the Random-Walk (RW) kernel, the decay factor is chosen from {10 −6 , 10 −5 ..., 10 −1 }. For the graphlet kernel (GK), we choose the graphlet size {3, 5, 7}. For the deep graph kernels (DGKs), the window size and dimension are taken from the set {2, 5, 10, 25, 50} and report the best classification accuracy obtained among i) deep graphlet kernel, ii) deep shortest path kernel and iii) deep Weisfeiler-Lehman kernel. For the Multiscale Laplacian Graph (MLG) kernel, we vary the η and γ parameters of the algorithm from {0.01, 0.1, 1}, radius size from {1, 2, 3, 4}, and level number from {1, 2, 3, 4}. For the diffusion-convolutional neural networks (DCNN), we choose the number of hops from {2, 5} and employ the AdaGrad algorithm (gradient descent) with the following parameters: the learning rate 0.05, batch size 100 and number of epochs 500. We use the node degree as the labels in the cases where node labels are unavailable. For the rest, best reported results are borrowed from their respective papers since the experimental setup is same and fair comparison can be made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">Ablation Studies and Discussion</head><p>How effective is adaptive supervised graph kernel loss? </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Computational Complexity:</head><label></label><figDesc>Precomputed graph kernel representation are typically bounded by O(N 2 ) time per graph (examples include WL, FGSD kernel) where N is number of nodes in a graph. Further computing graph kernel in each batch iteration requires O(B 2 ) time and space where B is number of graphs in batch size. As a a result, our model time and space complexity is bounded by O(B 2 N 2 ) in each batch iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1 https://deepchem.io/ 2 http://moleculenet.ai/models 7 Appendix 8 Proof of Theorem 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Two Regular Graphs G 1 and G 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>with graph spectral embeddings as node features. Let {u1, u2, ..., uN } and {v1, v2, ..., vN } be the spectral embeddings of nodes in G1and G2. Then, updated representation of i th node is given by x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>j∈N (i) uj) where φ is the universal MLP function. Let φ MLP function learns an identity function. Then the sum-pool representation of G1 is given by y1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>,t − θ p,S i ,t | ≤ ν νσN g 2 p,λ |∆θp,t| This completes the proof of Lemma 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>G 1 1 a</head><label>11</label><figDesc>and Lemma 4 into Equation (5) yields the following remaining result,2βm ≤ P p=1 α N λ max G ESGD ∆θp βm ≤ P ηα ασν N 2 (λ max G ) 2 T t=1 1 + ην νσN (λ max G ) 2 t−1 m βm ≤ 1 m O P N T +1 (λ max G ) 2T ∀T ≥ Label . Typical label 1 sample. Label 2 b. Typical label 2 sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Some MUTAG graph data samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Ablation Study of Universal</cell><cell>Model / QM8 Results</cell><cell>Val MAE (×10 −3 )</cell><cell>Test MAE (×10 −3 )</cell></row><row><cell>Graph Encoder on quantum mechanics</cell><cell>MPNN[ 2017]</cell><cell>14.60</cell><cell>14.30</cell></row><row><cell>dataset. DUGGNN -A -K (model trained from scratch without multi-task</cell><cell>DTNN[ 2017]</cell><cell>17.00</cell><cell>16.90</cell></row><row><cell>decoder) sets the new state-of-art result</cell><cell>GCNN[ 2018]</cell><cell>15.00</cell><cell>14.80</cell></row><row><cell>on QM8 dataset.</cell><cell>DUGNN -L A -L K</cell><cell>11.16</cell><cell>11.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation Study of Transfer Learning. DUGNN is the base model trained from scratch on both NCI1 and PTC datasets via transfer learning. DUGNN -NCI1 / PTC represent models trained from scratch on individual datasets.</figDesc><table><row><cell>Model / Dataset</cell><cell>NCI1</cell><cell>PTC</cell></row><row><cell>DUGNN</cell><cell>83.51</cell><cell>74.22</cell></row><row><cell>DUGNN -NCI1 / PTC</cell><cell>83.10</cell><cell>73.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation Study of Multi-Task Decoder. DUGNN -(·) represents model trained from scratch without (·) loss function. We pick one of the cross validation splits to report the accuracy and all the hyper-parameters, random seeds and data splits were kept constant across the ablation experiments.</figDesc><table><row><cell cols="2">Model / Dataset</cell><cell></cell><cell>PTC</cell><cell>ENZYMES</cell></row><row><cell>DUGNN</cell><cell></cell><cell></cell><cell>73.53</cell><cell>65.00</cell></row><row><cell cols="2">DUGNN -L A -L</cell><cell>(unsup) K</cell><cell>70.59</cell><cell>61.67</cell></row><row><cell cols="2">DUGNN -L A</cell><cell></cell><cell>72.68</cell><cell>64.10</cell></row><row><cell>DUGNN -L</cell><cell cols="2">(unsup) K</cell><cell>71.59</cell><cell>62.83</cell></row><row><cell cols="2">DUGNN -Lclass</cell><cell></cell><cell>64.71</cell><cell>56.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Lastly, solving the ESGD ∆θt| first order recursion yields,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>, we have,</cell></row><row><cell cols="5">ESGD ∆θp,t+1| ≤ ESGD |∆θp,t| + 1 −</cell><cell>1 m</cell><cell>ην νσN g 2 p,λ ESGD[|θp,t|] +</cell><cell>1 m</cell><cell>2ην ασN g p,λ</cell></row><row><cell>= 1 + 1 −</cell><cell>1 m</cell><cell cols="4">ην νσN g 2 p,λ ESGD[|θp,t|] +</cell><cell>2ην ασN g p,λ m</cell></row><row><cell cols="4">≤ 1 + ην νσN g 2 p,λ ESGD[|θp,t|] +</cell><cell cols="2">2ην ασN g p,λ m</cell></row><row><cell></cell><cell></cell><cell>ESGD ∆θp,T | ≤</cell><cell cols="3">2ην ασN g p,λ m</cell><cell>T t=1</cell><cell>p,λ 1 + ην νσN g 2</cell><cell>t−1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation Study of Supervised Adpative Graph Kernel Loss. DUGNN is the base model trained with non-adaptive kernel loss function. DUGNN -L (unsup) K + L (sup) K is trained with adaptive loss inplace of non-adaptive graph kernel loss.</figDesc><table><row><cell cols="2">Model / Dataset</cell><cell></cell><cell></cell><cell>PTC</cell><cell>ENZYMES</cell></row><row><cell>DUGNN</cell><cell></cell><cell></cell><cell></cell><cell>73.53</cell><cell>65.00</cell></row><row><cell>DUGNN -L</cell><cell>(unsup) K</cell><cell>+ L</cell><cell>(sup) K</cell><cell>76.47</cell><cell>64.13</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, Fifth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stability and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="499" to="526" />
			<date type="published" when="2002-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quantum walk neural networks for graphstructured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dernbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohseni-Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Complex Networks and their Applications</title>
		<imprint>
			<biblScope unit="page" from="182" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Splinecnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03059</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anonymous walk embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2186" to="2195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning image embeddings using convolutional neural networks for improved multi-modal semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The multiscale laplacian graph kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2982" to="2990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The graphlet spectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09037</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cayleynets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07664</idno>
		<title level="m">Graph convolutional neural networks with complex rational spectral filters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07606</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03226</idno>
		<title level="m">Adaptive graph convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual international conference on machine learning</title>
		<meeting>the 33rd annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph invariant kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3756" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fast eigenspace approximation using random signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paratte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00938</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pržulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unifying local and non-local signal processing with graph cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kitic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07759</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphvae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03480</idno>
		<title level="m">Towards generation of small graphs using variational autoencoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<title level="m">Deep graph infomax</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08090</idno>
		<title level="m">Graph capsule convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01004</idno>
		<title level="m">Stability and generalization of graph convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02473</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generating realistic graphs with deep auto-regressive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphrnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5694" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Quantum-based subgraph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="38" to="49" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">We observe that employing the adaptive supervised graph kernel loss yields a smoother decay in the validation loss and produces more stable results. However as evident from Table 5, it only increases the performance on PTC by 3%, and reducess the performance on ENZYMES by around 1%. As a result</title>
		<imprint/>
	</monogr>
	<note>we advice treating the adaptive supervised kernel loss as a hyper-parameter</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
