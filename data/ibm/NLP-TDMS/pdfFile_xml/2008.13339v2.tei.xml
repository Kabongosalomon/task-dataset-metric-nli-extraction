<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BiTT: Bidirectional Tree Tagging for Joint Extraction of Overlapping Entities and Relations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xukun</forename><surname>Luo</surname></persName>
							<email>luoxukun@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Ma</surname></persName>
							<email>mameng@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wang</surname></persName>
							<email>pwang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BiTT: Bidirectional Tree Tagging for Joint Extraction of Overlapping Entities and Relations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>We adopt the Bi-LSTM and the BERT as the encoder in our framework respectively, and obtain promising results in public English as well as Chinese datasets.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Joint extraction refers to extracting triples, composed of entities and relations, simultaneously from the text with a single model. However, most existing methods fail to extract all triples accurately and efficiently from sentences with overlapping issue, i.e., the same entity is included in multiple triples. In this paper, we propose a novel scheme called Bidirectional Tree Tagging (BiTT) to label overlapping triples in text. In BiTT, the triples with the same relation category in a sentence are especially represented as two binary trees, each of which is converted into a word-level tags sequence to label each word. Based on BiTT scheme, we develop an end-to-end extraction framework to predict the BiTT tags and further extract triples efficiently. We adopt the Bi-LSTM and the BERT as the encoder in our framework respectively, and obtain promising results in public English as well as Chinese datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) is an important task for building knowledge graphs. RE refers to extracting the relations between entity pairs contained in unstructured text based on predefined entity and relation categories. The relation between an entity pair can be formalized as a relational triple t: t = (e 1 , r, e 2 )</p><p>(1)</p><p>where we call e 1 and e 2 as the head and tail entity of t, and r represents a specific category of relation between them. The early studies on RE are mainly pipeline methods <ref type="bibr" target="#b0">(Socher et al., 2012;</ref><ref type="bibr" target="#b1">Zeng et al., 2014)</ref>. To begin with, researches proposed to identify the entities using named entity recognition (NER) module. Then they combined the entities in pairs to classify their relations with a relation classification (RC) module <ref type="bibr">(Li et al., 2016)</ref>. After these two serial steps, the valid triples are finally outputted. However, the pipeline method is faced with the error propagation problem, i.e., the error in NER module will be introduced into the subsequent RC module. The joint method <ref type="bibr" target="#b3">(Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b4">Zheng et al., 2017)</ref>, aiming to obtain triples directly through a combined module of NER and RC, was proposed to solve the problem. Compared with the pipeline method, the joint method can simultaneously extract and leverage the deep correlations between entities and relations. Unfortunately, due to the existence of overlapping triples in text, the effect of joint models fail to meet expectations. Taking the first sentence in <ref type="figure" target="#fig_0">Figure 1</ref> for example, the RC module in some joint models <ref type="bibr" target="#b3">(Miwa and Bansal, 2016)</ref> can only predict a triple for Trump and America, and the tagging scheme of <ref type="bibr" target="#b4">Zheng et al. (2017)</ref> is unable to represent both Birth-Place and President in a single tag for each word. This kind of sentences not only make the RC module in urgent need of improvement, but also put the original sequential tagging schemes in a dilemma. <ref type="bibr" target="#b5">Zeng et al. (2018)</ref> are the first to recognize the overlapping triple problem and categorized the sentences with overlapping triples into EntityPairOverlap (EPO) and SingleEntityOverlap (SEO). To solve this problem, some novel tagging scheme is proposed to handle these sentences, but their models cost much time and still cannot hold all the triples in a sentence through a limited number of tags. For example, <ref type="bibr" target="#b6">Dai et al. (2019)</ref> built a large model to fill a (sequence-length × sequence-length) table with tags, but they still predicted a single relation category for an entity pair so that fail to EPO.</p><p>In this paper, to represent more triples by less tags, we further divide SEO into Ex-cludeLoopSentences (ELS) and IncludeLoopSentences (ILS). Thus the sentences with overlapping triples are separated into three sets: EPO, ELS, and ILS. Based on the characteristics of them, we propose a scheme called Bidirectional Tree Tagging (BiTT) to improve the relation extraction performance 1 . Firstly, the triples with the same relation category in a sentence are grouped together. Secondly, the entities and relations in a group are modeled into two binary tree structures according to the order in which they appear in the sentence. Finally, we establish a mapping between a binary tree structure and word-level sequence tags so that they can be converted to each other with little loss. Thus, a new sequential tagging approach, representing most triples in ILS as well as all triples in ELS and EPO, is used for neural network training.</p><p>To enable our proposal, we build an end-to-end extraction framework to predict BiTT tags, since the relational triples can be extracted by reconstructing the relation forests based on BiTT tags in a sentence.</p><p>The key contributions are summarized as:</p><p>• We propose a novel scheme called Bidirectional Tree Tagging (BiTT) to label the overlapping triples in a sentence;</p><p>• Based on our proposed BiTT scheme, an end-to-end framework is developed for automatically extracting triples in a sentence;</p><p>• The models built on our framework outperform other benchmark models significantly on the two public datasets, which is a unique benefit obtained by effectively handling the overlap issue through the BiTT scheme.</p><p>2 Related Work RE is a core task for text mining and information extraction <ref type="bibr" target="#b7">(Zhang et al., 2017)</ref>, and its output can be used to support multiple NLP tasks, such as question answering and knowledge graph building. In recent years, with the development of neural networks, some deep learning frameworks have been proposed for this task since they can avoid the error propagation of feature extraction in classical methods <ref type="bibr" target="#b8">(Li and Ji, 2014;</ref><ref type="bibr" target="#b9">Miwa and Sasaki, 2014)</ref>. Early RE models mainly followed the pipelined ideas, i.e., first recognize the entities in a sequence, and then distinguish the relations between entities. Depending on the structural diversity of neural networks, a number of pipeline RE models have been proposed with improved effect. For example, <ref type="bibr" target="#b0">Socher et al. (2012)</ref> presented a matrix-vector RNN model (MV-RNN) to capture the compositional meaning of longer phrases. <ref type="bibr" target="#b10">Hashimoto et al. (2013)</ref> expanded the feature set and applied average parameters when training the RNN based model. <ref type="bibr" target="#b1">Zeng et al. (2014)</ref> classified relations by the word and sentence level features extracted from CDNN. <ref type="bibr" target="#b11">Vu et al. (2016)</ref> tried to combine Bi-RNN and CNN for RC module. <ref type="bibr" target="#b12">Xu et al. (2015)</ref> applied LSTM to pick up the information along the shortest dependency path (SDP) for RC module. However, due to the error propagation problem, the performance of this method has reached a bottleneck.</p><p>The NLP community began to explore a way to extract entities and relations simultaneously, i.e., joint method. <ref type="bibr" target="#b3">Miwa and Bansal (2016)</ref>; <ref type="bibr" target="#b13">Katiyar and Cardie (2016)</ref> extended the RC module to share the encoder representation with the NER module, solved the problem of error propagation. <ref type="bibr" target="#b4">Zheng et al. (2017)</ref> presented a novel tagging scheme, which turned the extracting tasks into a sequential tagging problem, bridging the information gap between the RC and NER steps. An important issue that affects the effectiveness of the joint models is overlapping triple. <ref type="bibr" target="#b5">Zeng et al. (2018)</ref> studied this problem and classified the sentences with overlapping triples into EPO and SEO, and used a sequence-to-sequence (seq-to-seq) model with copy mechanism to handle them. But its NER part relied heavily on high-precision word segmentation tools. <ref type="bibr" target="#b6">Dai et al. (2019)</ref> also proposed an improved position-attention sequential labeling scheme to deal with them, yet they cost much longer time to fill a (sequence-length × sequencelength) table with tags. Recently, <ref type="bibr" target="#b14">Fu et al. (2019)</ref> put forward a new joint model that combines LSTM and graph convolutional networks (GCNs). But their work failed to deal with EPO, and its computational burden is also heavy.</p><p>In this paper, we propose a new sequential tagging scheme called BiTT and an extraction framework for it to solve the overlapping triple problem. Our framework can extract more triples in the sentences with overlapping triples and costs less time when comparing with previous sequential tagging works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we firstly present our fine-grained division for the sentences with overlapping triples. Then we illustrate how to convert these sentences to BiTT tag sequences and extract triples from the BiTT tags. Finally, we introduce our end-to-end extraction framework for predicting BiTT tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fine-grained Division</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, <ref type="bibr" target="#b5">Zeng et al. (2018)</ref> categorized the sentences with overlapping triples into EPO and SEO. A sentence belongs to EPO if there are some triples whose entity sets ({e 1 , e 2 }) are the same, and belongs to SEO if there are some triples whose entity sets are different and contain at least one overlapping entity. Note that a sentence can pertain to the intersection of EPO and SEO. To capture more triples by a single sequential tagging scheme, we further divide SEO into ELS and ILS based on the existence of relation loops. A sentence belongs to ILS if there are some triples whose entity sets satisfy following conditions: (1) they are different; (2) each of them has at least one overlapping entity; (3) some of them contain two overlapping entities. A sentence belongs to ELS if it pertains to SEO but is not an ILS sentence. Note that there is at least one loop in the relation graph of an ILS sentence and no loop in the relation graph of an ELS sentence without considering the edges' direction. Therefore, we separate these sentences into EPO, ELS and ILS, then deal with them in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bidirectional Tree Tagging Scheme</head><p>We propose a novel tagging scheme called BiTT, which uses two binary tree structures to incorporate the three kinds of sentence mode, i.e., EPO, ELS and ILS. We show an example in <ref type="figure" target="#fig_4">Figure 2</ref> to detail our handling approach and algorithm for different sentence modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EPO Handling</head><p>Though there are some triples whose entity sets are the same in the EPO sentences, their relation categories are different from each other. Hence we group together the triples with the same relation category in a sentence and label them respectively. For example, as shown in <ref type="figure" target="#fig_4">Figure 2</ref>, (America, Capital, W ashington) with the relation category Capital is divided into an independent group, and triples with the relation category Contains are aggregated into another group. Note that although the triple groups are labeled respectively, all triples are predicted simultaneously by the extraction framework to preserve their correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELS Handling</head><p>The most remarkable feature for the triples in an ELS sentence is that, there is no loop in its relation graph, thus the triples can be completely represented as a forest. We handle the ELS sentences with a Tree Tagging scheme, which consists of two steps. Initialize l = an array of all EN i 's location pair in S;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Add the elements in l to L; 6: end for 7: Sort the element in L from small to large according to beginning index; 8: while L is not empty do 9:</p><p>Initialize a tree T where L 1 is the root and remove L 1 from L; 10:</p><formula xml:id="formula_0">for i = 2; i ≤ length(L); i + + do 11:</formula><p>if L i not in T and there is a valid relation between L i and the node in T then Add T to F ; 16: end while 17: /* Transform the forest to a binary tree. */ 18: Initialize an empty stack S t , push the root nodes in F to S t in order; 19: Initialize a binary tree B where the root of F 1 is the root; 20: for i = 2; i ≤ length(F ); i + + do 21:</p><p>Add F i 's root to B as the right child of F i−1 's root; 22: end for 23: while S t is not empty do 24:</p><p>Initialize node cur = P op(S t ), C = an array of the children of node cur in F ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>25:</head><p>Add C 1 to B as the left child of node cur ; 26:</p><formula xml:id="formula_1">for i = 2; i ≤ length(C); i + + do 27:</formula><p>Add C i to B as the right child of C i−1 ; 28: end for 29:</p><p>Push the children of node cur to S t in order; 30: end while</p><p>(1) Relation-to-Tree. The pseudo code is presented in Algorithm 1. First, we obtain the positions of all entities in a sentence and construct a relation forest according to the entities' forward appearing order. Take the first entity as the root of the first tree T 1 , and then recurrently go through other entities from left to right, add the entities having relations with T 1 's nodes to T 1 gradually. When the remaining entities can no longer be added to the current tree, we select the first of them as the root of a new tree T i and apply the previous operation on T i until there are no entities left. After that, all trees are aggregated into a forest F . Second, we convert F into a binary tree B. For an entity node in F , its first child becomes its left child in B, and its right adjacent brother becomes its right child in B. For example, the node Washington is the first child of The White House and the left brother of America in the forward forest in <ref type="figure" target="#fig_4">Figure 2</ref>, therefore it becomes the left child of The White House and America becomes its right child in the forward binary tree. Note that we add the annotation Brother on the edge in B if the entity nodes connected by it used to be two root nodes in F . Besides, the edge is directional, from e 1 to e 2 in a specific relational triple.</p><p>(2) Tree-to-Tag. We assign a tag for every single word in the sentence by parsing the binary relation tree B from the Relation-to-Tree step. If a word does not belong to any entity, its tag will be "O". If a word is a part of an entity node e, its tag will be a combination of four parts:</p><p>• Part 1 (P 1 ) indicates the position of a word in the entity node e with the "BIES" signs, i.e., P 1 ∈ {B(begin), I(in), E(end), S(single)}.</p><p>• Part 2 (P 2 ) indicates the information on the edge between e and its parent in B. P 2 = Root when e is the root of B. And P 2 = Brother when the annotation on the edge is also Brother. Except them, P 2 = (Child 2 , Role 2 ), where Child 2 ∈ {l(lef t), r(right)} indicates if e is the left or right child of its parent, and Role 2 ∈ {1(e 1 ), 2(e 2 )} shows the entity role of e pointed out by the direction of the edge.</p><p>• Part 3 (P 3 ) indicates the information on the edge between e and its left child in B. P 3 = N U LL when e has no left child. Except N U LL, P 3 = Role 3 , where Role 3 ∈ {1(e 1 ), 2(e 2 )}.</p><p>• Part 4 (P 4 ) indicates the information on the edge between e and its right child in B. P 4 = N U LL when e has no right child. P 4 = Brother when the annotation on the edge is also Brother. Except them, P 4 = Role 4 , where Role 4 ∈ {1(e 1 ), 2(e 2 )}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ILS Handling</head><p>Since there is at least one loop in the relation graph of an ILS sentence, the triples can not be absolutely represented by a single forest. To solve the problem, we simply build another forest according to the entities' backward appearing order, then convert it to a backward binary tree, and obtain the backward tree tags for a sentence. We apply the forward tags and the backward tags to accommodate more triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tags to Triples</head><p>For the forward tags, first we find all root nodes in the forest by the condition of P 1 = Root or P 1 = Brother. Then we start from these root nodes, recursively match P 3 or P 4 of the nodes already in forest with other nodes' P 2 to reconnect the edges between   the node pairs. If a parent node can match more than one node as its left (or right) child, we select the nearest node behind of it, since we take the entities appearing order into account when building the forest in the first step of the Tree Tagging scheme in Section 3.2. After reconstructing the forest, we can recursively extract the relational triples in the sentence from it.</p><p>The operation on the backward tags is the same as that on the forward tags. Except that when a parent node can match more than one node as its left (or right) child, we select the nearest node before it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">End-to-End Extraction Framework</head><p>As shown in <ref type="figure" target="#fig_5">Figure 3</ref>, an end-to-end extraction framework is built for BiTT tags prediction. In this section, we introduce the encoder module, decoder module and our loss function respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>The encoder module aims to extract a context vector representation for each word in the sentence. In this paper, we adopt the Bi-directional Long Short-Term Memory (Bi-LSTM) layers and a pre-trained BERT as the encoder respectively.</p><p>(1) Bi-LSTM Layers. A Bi-LSTM layer consists of a forward LSTM <ref type="bibr" target="#b15">(Hochreiter and Schmidhuber, 1997)</ref>  the t-th word w t from LSTM are:</p><formula xml:id="formula_2">o t , h t = lstm block(v t , h t−1 )<label>(2)</label></formula><p>where lstm block(·) is the function of a memory block in LSTM. We concatenate the two hidden states corresponding to the same word together as the Bi-LSTM hidden state. For example, the Bi-LSTM hidden stateḣ t of the t-th word w t is:</p><formula xml:id="formula_3">h t = [ − → h t , ← −−− − h s−t+1 ]<label>(3)</label></formula><p>where − → h t is the hidden state of w t in the forward LSTM, and ← −−− − h s−t+1 is the hidden state of w t in the backward one. We adopt the final hidden states from Bi-LSTM layers to represent the words.</p><p>(2) BERT Encoder. The BERT encoder <ref type="bibr" target="#b16">(Devlin et al., 2019</ref>) is a stack of Transformer <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref> layers. For a given word w, the input representation of BERT encoder is denoted as I s , whose generation formula is:</p><formula xml:id="formula_4">I s = E t + E p<label>(4)</label></formula><p>where E t is the token embedding corresponding to w in the WordPiece embeddings matrix <ref type="bibr">(Wu et al., 2016)</ref> with a 30,000 words vocabulary, E p is the positional embedding which indicates the position index of w in the input sequence. Note that though the input representation of w is generated by summing the corresponding token, segment, and position embeddings in the original work of BERT encoder, we drop the segment embedding in Eq.(4) since our input sequence is not a sentence pair but a single sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder</head><p>The decoder module is designed to parse the word representations and then predict the BiTT tags for each word. In this paper, it consists of the Bi-LSTM layers and the Linear layers. The Bi-LSTM layers part has been illustrated in the encoder part. And there are two alternative construction methods for the Linear layers to predict BiTT tags, i.e., one-head and multi-head. Take a forward tree tag as example, the former means that concatenate the four parts of the tag as one label and predict it by a single Linear layer, like the work in <ref type="bibr" target="#b4">Zheng et al. (2017)</ref>. The latter predicts these four parts separately with four Linear layers, requiring fewer parameters. In this study, we employ the multi-head mechanism to reduce the computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>The loss function for training the forward or backward tags is a weighted bias objective function defined by:</p><formula xml:id="formula_5">L = λ 1 L 1 + λ 2 L 2 + λ 3 L 3 + λ 4 L 4<label>(5)</label></formula><p>where L j (j ∈ {1, 2, 3, 4}) are the bias objective functions <ref type="bibr" target="#b4">(Zheng et al., 2017)</ref> for the four parts of the tags respectively, λ j are the weights for L j . We obtain L f for the forward tags and L b for the backward tags by Eq. <ref type="formula" target="#formula_5">(5)</ref>, then define the total loss L T of our framework as follow:</p><formula xml:id="formula_6">L T = L f + γL b (6)</formula><p>where γ is a weight hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Datasets and Metrics We evaluate BiTT and the extraction framework on two public datasets, NYT 2 <ref type="bibr" target="#b19">(Riedel et al., 2010)</ref> and DuIE 3 . NYT is an English dataset labeled by the distant supervision method <ref type="bibr" target="#b20">(Mintz et al., 2009)</ref>. It contains 1.18M sentences with 24 predefined relation categories from New York Times news. After filtering the sentences with no valid relational triple, there are finally 66,339 sentences. We divide them randomly into a training set (56,339 sentences), a validation set (5000 sentences) and a testing set (5,000 sentences). DuIE is a Chinese dataset released by Baidu Inc for information extraction, consisting of 210k Chinese sentences with 50 predefined relation categories. This dataset is divided into a training set (173,108 sentences) and a testing set (21,639 sentences). We randomly separate the training set into a validation set (20,000 sentences) and a new training set (153, 108 sentences) in our study. <ref type="table" target="#tab_2">Table  1</ref> illustrates some statistics of the training set and the testing set in the two datasets.</p><p>Considering the testing sets, the sentences with overlapping triples account for 39.2% of all of the sentences in NYT, and this number in DuIE raises to 63.3%. It indicates that the overlapping triple problem is very common in these two datasets, especially for DuIE. We employ Precision (Prec), Recall (Rec) and F1 score (F1) to evaluate the performance of our framework. We consider a predicted triple (e 1 , r, e 2 ) as a correct one only if its e 1 , e 2 and r are all correct.   <ref type="bibr" target="#b6">(Dai et al., 2019)</ref> - <ref type="bibr" target="#b5">(Zeng et al., 2018)</ref> 38.6\59.4 36.1\53.1 37.3\56.0 ---CopyRE-Mul <ref type="bibr" target="#b5">(Zeng et al., 2018)</ref> 39.1\61.0 36.5\56.6 37.8\58.7 ---GraphRel-1p <ref type="bibr" target="#b14">(Fu et al., 2019)</ref> 85.0\62.9 51.8\57.3 64.3\60.0 52.2 23.9 32.8 GraphRel-2p <ref type="bibr" target="#b14">(Fu et al., 2019)</ref> 82.5\63.9 57.9\60.0 68.1\61.9 41.1 25.8 31.8</p><formula xml:id="formula_7">\49.4 -\59.1 -\53.8 - - - CopyRE-One</formula><p>BiTT <ref type="formula">(</ref>  Framework Details We build up two models for evaluation based on the extraction framework. One called BiTT-BERT is embedded with the pre-trained BERT encoder, while the other called BiTT-LSTM contains a Bi-LSTM encoder. The maximum length of an input sentence and the batch size are simply set to 128 and 8 respectively. And the training operation is terminated when the F1 on validation set no longer rises. We implement the BERT encoder with the UER 4 , which is a Pytorch-implemented BERT framework, and conduct a further pre-training with the two evaluation datasets for 20,000 epochs respectively based on the pre-trained BERT-base 5 . The number of the transformer layers is 12 and the hidden size is 768. Besides, we apply the pre-trained Glove embedding <ref type="bibr" target="#b21">(Pennington et al., 2014)</ref> for BiTT-LSTM. The number of Bi-LSTM layers in the encoder (or decoder) module is 2 and the hidden size is also 768. In order to balance the loss of each part of the BiTT tags, we adopt λ 1 = 8/6, λ 2 = 8/8, λ 3 = 8/5 and λ 4 = 8/6 in Eq.(5), since the number of optional labels of the four parts are 6, 8, 5 and 6 after adding the tag "O" and the padding tag respectively. And we simply set γ = 1 in Eq.(6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Result and Analysis Baseline Models</head><p>• NovelTagging It is the first framework to extract relational triples by a novel sequential tagging scheme, proposed by <ref type="bibr" target="#b4">Zheng et al. (2017)</ref>.</p><p>• PA It is a unified position-attentive sequential tagging framework for joint extraction, proposed by <ref type="bibr" target="#b6">Dai et al. (2019)</ref>.</p><p>• CopyRE It is an end-to-end neural model based on seq-to-seq learning framework with copy mechanism for relation extraction, proposed by <ref type="bibr" target="#b5">Zeng et al. (2018)</ref>.</p><p>• GraphRel It is an end-to-end joint extraction model based on GCNs, proposed by <ref type="bibr" target="#b14">Fu et al. (2019)</ref>.</p><p>Note that in <ref type="table" target="#tab_4">Table 2</ref>, CopyRE-One and CopyRE-Mul are the CopyRE frameworks with one decoder and multiple decoders respectively. GraphRel-1p is the GraphRel architecture with only the 1st-phase and GraphRel-2p is the complete version. The reported results for Bi-LSTM in NYT are obtained from the original achievements in their papers. In addition, we upgrade these baseline models with the pre-trained BERT encoder for a further comparison. For Graph-Rel, we concatenate BERT embedding with POS embedding generated by spaCy 6 as the context vector representation. For other baselines, we simply substitute pre-trained BERT encoder for their word embedding layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Results</head><p>We compare our proposed method with baseline models in two aspects, i.e., quality and efficiency.</p><p>(1) Quality. <ref type="table" target="#tab_4">Table 2</ref> shows the Prec, Rec and F1 of our framework and the baseline models. BiTT-BERT achieves the best F1 scores in both NYT and DuIE, which are 88.9% and 78.0% respectively. In NYT, compared with the best baseline method NovelTagging with BERT, BiTT-BERT outperforms it by 0.7% in Prec, 32.4% in Rec and 19.6% in F1. In DuIE, the improvements are more significant, i.e., 0.7% in Prec, 42.6% in Rec and 27.6% in F1, since the sequences with overlapping triples account for more in DuIE. What's more, even without the pre-trained BERT encoder, BiTT-LSTM also performs better than all baseline models, validating the effectiveness of our BiTT scheme. And there is an impressive Rec gap between BiTT-BERT and the baselines, which verifies the utility of BiTT-BERT when dealing with overlapping triples.</p><p>Please note that since PA with BERT contains too many parameters, we do not have enough memory to train it. And the evaluation metrics in original CopyRE are much looser than that for CopyRE with BERT in our experiment. What's more, CopyRE's F1 is almost 0 in DuIE, since its original implementation only considers one-token entities that are rare in Chinese datasets.</p><p>(2) Efficiency. <ref type="figure" target="#fig_6">Figure 4(a)</ref> shows the training curves of the NYT validation set in different models with the pre-trained BERT encoder. From it, all of the models converge at a similar epoch, i.e., about the 15th epoch. Besides, <ref type="figure" target="#fig_6">Figure 4</ref>(b) provides us with the number of parameters of different models' decoder for the NYT dataset. It shows the parameters in our framework is almost the same as that in NovelTagging and much less than that in other baseline models when they are embedded by the same    <ref type="table">Table 4</ref>: Case study of BiTT-BERT and two baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>As is shown in <ref type="table" target="#tab_6">Table 3</ref>, to explore the effects of the handling methods for overlapping triples in BiTT scheme and our decoder architecture, we perform the ablation tests based on BiTT-BERT and the NYT dataset."BiTT-BERT w/o Group" means that the "EPO Handling" described in Section 3.2 is dropped, replaced by adding the relation categories information to P 2 , P 3 and P 4 of the BiTT tags. "BiTT-BERT w/o Bidirectional" implies that we only build the forward forest in a sentence and then generate the forward tags. "BiTT-BERT w/o Multi-head" indicates that the multi-head structure in our framework is substituted by the one-head structure, which is illustrated in the "Decoder" part of Section 3.4.</p><p>From <ref type="table" target="#tab_6">Table 3</ref>, we have observed some impressive facts when comparing these three ablation tests with BiTT-BERT. First, the grouping operation greatly improves the performance of our framework on not only EPO but also ELS and ILS, since it reduces a complex relation graph into several simpler ones in a sentence. And it also reduces the parameters of the decoder module. Second, the backward forest can effectively supplement the triples that can not be totally represented by a single forward forest in ILS. Last, compared with the one-head structure, the multi-head structure can decrease the parameters of the decoder module, thus lessen the computation burden and the training time in an epoch. <ref type="table">Table 4</ref> shows the case study of BiTT-BERT and two baseline models with BERT. The first sentence is a case with no overlapping triples so that these three models obtain the single gold triple easily. The second sentence is an EPO case which only BiTT-BERT can perfectly handle. The third sentence belongs to ELS. BiTT-BERT and GraphRel-2p with BERT extract all the triples in the third sentence accurately, while NovelTagging with BERT can only discover one triple due to the limitations of its tagging scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a Bidirectional Tree Tagging (BiTT) scheme to label the overlapping entities and relations in sentences with higher accuracy and efficiency. Based on BiTT, an end-to-end extraction framework is developed for predicting the BiTT tags. We adopt the Bi-LSTM layers and the pre-trained BERT as its encoder module respectively for experiment. The results on two public datasets demonstrate that our proposal outperforms state-of-the-art models, especially when processing the overlapping cases.</p><p>Our future work aims to improve the BiTT scheme and the extraction framework. (1) When extracting results from BiTT tags, we will reconstructing a binary forest instead of a binary tree to reduce the error propagation if a node is dropped. (2) We will propose more rule restrictions for BiTT to make it more robust. (3) We will apply more powerful pre-trained encoders to the extraction framework for better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Trump was born in America in 1946 and became President of the state in 2016. Examples of the sentences with overlapping triples, including EPO, ELS and ILS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Relation-to-Tree Input: A s-words sentence, S; An array of m relational triples with the same relation category in S, RT ; An array of n entities in S, EN ; Output: A binary relation tree, B; 1: /* Construct a relation forest. */ 2: Initialize L = [], F = []; 3: for each i ∈ [1, n] do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>12:Remove L i from L and add it to T ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Our Bidirectional Tree Tagging (BiTT) Scheme. We only take the triples with the relation category Contains as examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>and a backward one. Denote the word embedding vectors of a sentence with s words as V = [v 1 , . . . , v s ]. The output o t and the hidden state h t of The end-to-end extraction framework for predicting BiTT tags.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>(a) The training curves of different models with BERT for NYT. (b) The number of parameters of different models' decoder for NYT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets in our experiment.</figDesc><table><row><cell>Method</cell><cell cols="3">NYT (BERT\Bi-LSTM)</cell><cell cols="2">DuIE (BERT)</cell></row><row><cell></cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec Rec</cell><cell>F1</cell></row><row><cell>NovelTagging (Zheng et al., 2017)</cell><cell cols="3">89.0\62.4 55.6\31.7 69.3\42.0</cell><cell cols="2">75.0 38.0 50.4</cell></row><row><cell>PA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The performance comparison of different methods in NYT and DuIE datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The results of the ablation study on BiTT-BERT model. encoder, indicating that the converge speed of our framework is much faster than the baselines except for NovelTagging.</figDesc><table><row><cell>Sentence</cell><cell>Gold Triples</cell><cell>NovelTagging (Zheng et al., 2017)</cell><cell>GraphRel-2p (Fu et al., 2019)</cell><cell>BiTT-BERT (Ours)</cell></row><row><cell>Munir Said Thalib was one of Indonesia's most respected human rights lawyers.</cell><cell>(Munir Said Thalib,nationality,Indonesia)</cell><cell>(Munir Said Thalib,nationality,Indonesia)</cell><cell>(Munir Said Thalib,nationality,Indonesia)</cell><cell>(Munir Said Thalib,nationality,Indonesia)</cell></row><row><cell>OVED-WEISS -Or, 23, beloved son of InbalOved and Mark and Cathy Weiss, died suddenly and tragically in Bangkok, Thailand.</cell><cell>(Thailand,administrative divisions,Bangkok) (Thailand,capital,Bangkok) (Thailand,contains,Bangkok) (Bangkok,country,Thailand)</cell><cell>(Bangkok,country,Thailand)</cell><cell>(Thailand,capital,Bangkok) (Bangkok,country,Thailand)</cell><cell>(Thailand,administrative divisions,Bangkok) (Thailand,capital,Bangkok) (Thailand,contains,Bangkok) (Bangkok,country,Thailand)</cell></row><row><cell>In Connecticut, Storrs, Windsor Locks and North Haven are among the fastest aging.</cell><cell>(Connecticut,contains,North Haven) (Connecticut,contains,Storrs) (Connecticut,contains,Windsor Locks)</cell><cell>(Connecticut,contains,Windsor Locks)</cell><cell>(Connecticut,contains,North Haven) (Connecticut,contains,Storrs) (Connecticut,contains,Windsor Locks)</cell><cell>(Connecticut,contains,North Haven) (Connecticut,contains,Storrs) (Connecticut,contains,Windsor Locks)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code is available at https://anonymous/for/review.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Available at https://github.com/shanzhenren/CoType. 3 Available at http://ai.baidu.com/broad/download.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Available at https://github.com/dbiir/UER-py.5  The English cased and Chinese models are all available at the github repository UER.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The original model for SpaCy is proposed by<ref type="bibr" target="#b22">Honnibal and Johnson (2015)</ref>. And Chinese model for Spacy is available at https://github.com/howl-anderson/Chinese models for SpaCy, but it is still in beta.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Relation Classification via Convolutional Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Bi-LSTM-RNN Model for Relation Classification Using Low-Cost Sequence Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<idno>CoRR abs/1608.07720</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting Relational Facts by an Endto-End Neural Model with Copy Mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint Extraction of Entities and Overlapping Relations Using Position-Attentive Sequence Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6300" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICMCCE</title>
		<meeting>ICMCCE<address><addrLine>Harbin, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="178" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<title level="m">Incremental Joint Extraction of Entity Mentions and Relations</title>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling Joint Entity and Relation Extraction with Table Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple Customization of Recursive Neural Networks for Semantic Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chikayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1372" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining Recurrent and Convolutional Neural Networks for Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Investigating LSTMs for Joint Extraction of Opinion Entities and Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="919" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL, Minneapolis</title>
		<meeting>ACL, Minneapolis<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NeurIPS</title>
		<meeting><address><addrLine>Long Beach, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling Relations and Their Mentions without Labeled Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">148163</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL and AFNLP</title>
		<meeting>ACL and AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Improved Non-monotonic Transition System for Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1373" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
