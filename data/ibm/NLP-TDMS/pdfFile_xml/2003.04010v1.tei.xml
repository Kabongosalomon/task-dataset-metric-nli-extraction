<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Aware Domain Adaptation in Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas</orgName>
								<address>
									<settlement>Arlington</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>An</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas</orgName>
								<address>
									<settlement>Arlington</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochao</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas</orgName>
								<address>
									<settlement>Arlington</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas</orgName>
								<address>
									<settlement>Arlington</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Aware Domain Adaptation in Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we consider the problem of unsupervised domain adaptation in the semantic segmentation. There are two primary issues in this field, i.e., what and how to transfer domain knowledge across two domains. Existing methods mainly focus on adapting domain-invariant features (what to transfer) through adversarial learning (how to transfer). Context dependency is essential for semantic segmentation, however, its transferability is still not well understood. Furthermore, how to transfer contextual information across two domains remains unexplored. Motivated by this, we propose a cross-attention mechanism based on self-attention to capture context dependencies between two domains and adapt transferable context. To achieve this goal, we design two cross-domain attention modules to adapt context dependencies from both spatial and channel views. Specifically, the spatial attention module captures local feature dependencies between each position in the source and target image. The channel attention module models semantic dependencies between each pair of cross-domain channel maps. To adapt context dependencies, we further selectively aggregate the context information from two domains. The superiority of our method over existing state-of-the-art methods is empirically proved on "GTA5 to Cityscapes" and "SYNTHIA to Cityscapes".</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation aims to predict pixel-level labels for the given images <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>, which has been widely recognized as one of the fundamental tasks in computer vision. Unfortunately, the manual pixel-wise annotation for largescale segmentation datasets is extremely time-consuming and requires massive amounts of labor efforts. As a tradeoff, synthetic datasets <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> with freely-available labels offer a promising alternative by providing considerable data for model training. However, the domain discrepancy between synthetic (source) and real (target) images is still the central challenge to effectively transfer knowledge across domains. To overcome this limitation, the key idea of existing methods is to leverage knowledge from a source domain to enhance the learning performance of a target domain. Such a strategy is mainly inspired by the recent advances in unsupervised domain adaptation for image classification <ref type="bibr" target="#b26">[27]</ref>.</p><p>Conventional domain adaptation methods in image classification attempt to learn domain-invariant feature representations by directly minimizing the representation distance between two domains <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, encouraging a common feature space through an adversarial objective <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33]</ref>, or automatically determining what and where to transfer via meta-learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b13">14]</ref>. Motivated by this, various domain adaptation methods for semantic segmentation are proposed recently. Among them, the most common practices are based on feature alignment <ref type="bibr" target="#b12">[13]</ref>, structured output adaptation <ref type="bibr" target="#b31">[32]</ref>, curriculum adaptation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b15">16]</ref>, self training <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b14">15]</ref>, and image-to-image translation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>. Despite remarkable performance improvement achieved by these methods, they fail to explicitly consider the contextual dependencies across the source and target domains which is essential for scene understanding <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42]</ref>. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the source and target images share a much similar semantic context such as vegetation, car, and sidewalk, although their appearances (e.g., scale, texture, and illumination) are quite different. However, how to adapt context information across two domains remains unexplored.</p><p>Inspired by this, we propose a novel domain adaptation framework named cross-domain attention network (CDANet), designed for urban-scene semantic segmentation.</p><p>The key idea of CDANet is to leverage cross-domain context dependencies from both a local and global perspective. To <ref type="bibr">Figure 2</ref>. An overview of the proposed framework. It applies a feature extractor (i.e., ResNet101 or VGG16) to learn source and target features. Two cross-domain attention modules (i.e., CD-SAM and CD-CAM) are designed to adapt spatial and semantic context information across source and target domains. A classifier G is used to predict segmentation output based on the features from CD-SAM and CD-CAM. Our framework contains three discriminators (i.e., D1, D2, and D3) for output adaptation by enforcing the source output be indistinguishable from the target output.</p><p>achieve this goal, we innovatively design a cross-attention mechanism which contains two cross-domain attention modules to capture mutual context dependencies between source and target domains. Given that same objects with different appearances and scales often share similar features, we introduce a cross-domain spatial attention module (CD-SAM) to capture local feature dependencies between any two positions in a source image and a target image. The CD-SAM involves two directions (i.e., "source-to-target" and "targetto-source") to adaptively aggregate cross-domain features to learn common context information. On the forward direction (or "source-to-target"), CD-SAM updates the feature at each position in the source image as the weighted sum of features at all positions in the target image. The weights are computed based on the similarity of source and target features at each position. Similarly, the backward direction (or "target-to-source") updates the target feature at each position based on the attention to features at all positions in the source image. In consequence, spatial contexts from the source domain are encoded in the target domain, and vice versa. To model the associations between different semantic responses across two domains, we introduce a cross-domain channel attention module (CD-CAM) which has the same bidirectional structure as CD-SAM. The CD-CAM is designed for contextual information aggregation through capturing the channel feature dependencies between any two channel maps in the source and target image. In such a way, common semantic contexts are shared by both domains. CD-SAM and CD-CAM play a complementary role for context adaptation and their outputs are further merged to provide better feature representations for scene understanding.</p><p>Our main contributions are summarized as follows:</p><p>• We propose a novel cross-attention mechanism that en-ables to transfer of context dependencies across two domains. This is the first-of-its-kind study that investigates the transferability of context information in the domain adaptation.</p><p>• Two cross-domain attention modules are proposed to capture and adapt context dependencies at both spatial and channel levels. This allows us to learn the common semantic context shared by source and target domains.</p><p>• Comprehensive empirical studies demonstrate the superiority of our method over the existing state of the art on two benchmark settings, i.e., "GTA5 to Cityscapes" and "SYNTHIA to Cityscapes".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Domain Adaptation for Semantic Segmentation Inspired by the Generative Adversarial Network <ref type="bibr" target="#b9">[10]</ref>, Hoffman et al. <ref type="bibr" target="#b12">[13]</ref> propose the first domain adaptation model for semantic segmentation by learning domain-invariant features through adversarial training. To rule out taskindependent factors during feature alignment, SIBAN <ref type="bibr" target="#b21">[22]</ref> purifies significance-aware features before the adversarial adaptation to facilitate feature adaptation and stabilize the adversarial training. However, these global adversarial methods ignore to align the category-level joint distribution, which may disturb well-aligned features. To alleviate this problem, Luo et al. propose a category-level adversarial network to encourage local semantic consistency through reweighting the adversarial loss for each feature <ref type="bibr" target="#b22">[23]</ref>. Based on the hypothesis that structure information plays an essential role in semantic segmentation, Chang et al. adapt structure information by learning domain-invariant structure <ref type="bibr" target="#b0">[1]</ref>. This is achieved by disentangling the domain-invariant structure of a given image from its domain-specific texture information. AdaptSetNet moves forward by further considering structured output adaptation which is based on the observation that segmentation outputs of the source and target domains share substantial similarities <ref type="bibr" target="#b31">[32]</ref>. Different from AdaptSet-Net, we apply three domain discriminators to perform output adaptation on the segmentation outputs from CD-SAM, CD-CAM, and the aggregation of these two modules.</p><p>Most recently, image-to-image translation <ref type="bibr" target="#b42">[43]</ref> has proved its effectiveness in domain adaptation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4]</ref>. The key idea is to translate images from the source domain to the target domain by using an image translation model and use the translated images for adapting cross-domain knowledge through a segmentation adaptation model. Rather than keeping the image translation model unchanged after obtaining translated images, BDL <ref type="bibr" target="#b14">[15]</ref> applies a bidirectional learning framework to alternatively optimize the image translation model and the segmentation model. Similar to <ref type="bibr" target="#b43">[44]</ref>, a selfsupervised learning strategy is also used in BDL to generate pseudo labels for target images and re-training the segmentation model with these labels. Although BDL achieves the new state of the art, it is limited in its ability to consider the cross-domain context dependencies. To overcome this limitation, we introduce two cross-domain attention modules to adapt context information between source and target domains.</p><p>Context-Aware Embedding It has been long known that context information plays an important role in perceptual tasks such as semantic segmentation <ref type="bibr" target="#b25">[26]</ref>. Zhang et al. <ref type="bibr" target="#b38">[39]</ref> propose a context encoding module to capture the semantic context of scenes and selectively emphasize or de-emphasize class-dependent feature maps. To aggregate image-adapted context, MSCI <ref type="bibr" target="#b16">[17]</ref> further considers multi-scale context embedding and spatial relationships among super-pixels in a given image. Following the success of attention mechanism <ref type="bibr" target="#b34">[35]</ref> in image generation <ref type="bibr" target="#b39">[40]</ref> and sentence embedding <ref type="bibr" target="#b17">[18]</ref>, recent studies have highlighted the potential of self-attention in capturing context dependencies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>. Specifically, Zhao et al. <ref type="bibr" target="#b41">[42]</ref> introduce a point-wise spatial attention network to aggregate long-range contextual information. Their model mainly draws its strength from the self-adaptively predicted attention maps which can take full advantage of both nearby and distant information of each pixel. DANet <ref type="bibr" target="#b7">[8]</ref> adaptively integrates local features with their global dependencies through a position attention module and a channel attention module. These two modules are considered to be able to capture spatial and semantic interdependencies, and in turn, facilitate scene understanding. As opposed to capturing contextual information within a single domain as previously reported, we design an innovative cross-attention mechanism to model context dependencies between two different domains, which is essential for context adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we begin by briefing the key idea of our framework. We then detail the proposed cross-attention mechanism which contains two cross-domain attention modules for adapting context dependencies between a source and a target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given a set of source-domain images X s with pixel-wise labels Y s and a set of target-domain images X t without any annotation. Our goal is to train a segmentation model that can provide accurate prediction to X t . To achieve this, X s is first translated from the source domain to the target domain using CycleGAN <ref type="bibr" target="#b42">[43]</ref>. The translated images X s = F(X s ) (where F denotes the image translation model) share the same semantic labels with X s but with common visual appearance as X t . Motivated by the self-training strategy, we follow the same idea in <ref type="bibr" target="#b14">[15]</ref> to generate pseudo labels Y st t for X t with high prediction confidence. Coordinated with these translated images and pseudo labels, we introduce a cross-attention mechanism for domain adaptation of semantic segmentation by leveraging cross-domain contextual information ( <ref type="figure">Figure 2</ref>). First, a feature extractor E is applied to get source feature E(X s ) and target feature E(X t ) which are 1/8 of the corresponding input image size. Then a linear interpolation is applied to E(X s ) and E(X t ) to match their spatial size. After that, two parallel convolution layers </p><formula xml:id="formula_0">, A t } and {B s , B t }, respectively. {A s , A t } is then fed into CD-SAM to adapt spatial-level context, while CD-CAM adapts channel-level context based on {B s , B t }.</formula><p>For each module, two directions, i.e., forward direction ("source-to-target") and backward direction ("target-tosource") are involved. Take the CD-SAM as an example, an energy map is first obtained based on {A s , A t }. This energy map is further divided into two attention matrices denoted by Γ s→t and Γ t→s . During the forward direction, we perform a matrix multiplication between target features and Γ s→t . The result is then summed with the original source features in an element-wise manner. For the backward direction, a matrix multiplication is conducted between source features and Γ t→s . After that, an element-wise summation between the obtained results and original target features is carried out. The CD-CAM follows the same setting above except that the energy map is calculated in the channel dimension. The final source feature and target feature are obtained by aggregating the outputs from these two attention modules, which are then fed into a classifier G for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross-Domain Spatial Attention Module</head><p>The goal of CD-SAM is to adapt spatial contextual information across two domains. To achieve this, we introduce the forward direction ("source-to-target") to augment source features by selectively aggregating target features based on their similarities. We further introduce the backward direction ("target-to-source") to update target features by aggregating source features in the same way.</p><p>The architecture of CD-SAM is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. Given A s ∈ R C×H×W and A t ∈ R C×H×W (C denotes the channel number and H × W indicates the spatial size), two parallel convolution layers are applied to generate Q ∈ R C×H×W and K ∈ R C×H×W , respectively. A s and A t are also fed into another convolution layer to obtain V s ∈ R C×H×W and V t ∈ R C×H×W . We reshape Q, V s , K, and V t to C × N , where N = H × W . To determine spatial context relationships between each position in A s and A t , an energy map Φ ∈ R N ×N is formulated as Φ = Q T K, where Φ (i,j) measure the similarity between i th position in A s and j th position in A t . To augment A s with spatial context information from A t and vice versa, a bidirectional feature adaptation is defined as follows.</p><p>During the forward direction, we first define the "sourceto-target" spatial attention map as,</p><formula xml:id="formula_1">Γ (i,j) s→t = exp(Φ (i,j) ) Nt j=1 exp(Φ (i,j) ) ,<label>(1)</label></formula><p>where Γ (i,j) s→t indicates the impact of i th position in A s to j th position in A t . To capture spatial context in the target domain, we update A s as,</p><formula xml:id="formula_2">A s = A s + λ s V t Γ T s→t ,<label>(2)</label></formula><p>where λ s leverages the importance of target-domain context and original source features. In this regime, each position in A s has a global context view of target features. For the backward direction, the "target-to-source" spatial attention map is formulated as,</p><formula xml:id="formula_3">Γ (i,j) t→s = exp(Φ (i,j) ) Ns i=1 exp(Φ (i,j) ) ,<label>(3)</label></formula><p>where Γ (i,j)</p><p>t→s indicates to what extent the j th position in A t attends to the i th position in A s . Similarly, A t is updated by,</p><formula xml:id="formula_4">A t = A t + λ t V s Γ t→s ,<label>(4)</label></formula><p>where λ t leverages the importance of source-domain context and original target features. As a consequence, each position in A s and A t is a combination of their original feature and the weighed sum of features from the opposite domain. Therefore, A s and A t allow us to encode the spatial context of both source and target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-Domain Channel Attention Module</head><p>Given B s ∈ R C×H×W and B t ∈ R C×H×W , the CD-CAM is designed to adapt semantic context between source and target domains ( <ref type="figure" target="#fig_2">Figure 4</ref>) by following the same bidirectional structure as CD-SAM. Different from CD-SAM that applies convolution layers to obtain Q, K, V s , and V t before measuring spatial relationships. Here, B s and B t are directly used to capture their semantical context relationships, which allows us to maintain interdependencies between channel maps <ref type="bibr" target="#b7">[8]</ref>. Specifically, we reshape both B s and B t to C × N , where N = H × W . The energy map is defined as Θ = B t B T s ∈ R C×C , where Θ (i,j) denotes the similarity between i th channel in B s and j th channel in B t .</p><p>For the forward direction, the "source-to-target" attention map is given by,</p><formula xml:id="formula_5">Ψ (i,j) s→t = exp(Θ (i,j) ) C j=1 exp(Θ (i,j) ) ,<label>(5)</label></formula><p>where Ψ (i,j) s→t measures the impact of i th channel in B s to j th channel in B t . To model the cross-domain semantic context dependencies, B s is updated by,</p><formula xml:id="formula_6">B s = B s + ξ s Ψ s→t B t ,<label>(6)</label></formula><p>where ξ s leverages the associations between target-domain semantic information and original source features. As a consequence, each channel in B s is augmented by selectively aggregating semantic information from B t . During the backward direction, the "target-to-source" attention map is,</p><formula xml:id="formula_7">Ψ (i,j) t→s = exp(Θ (i,j) ) C i=1 exp(Θ (i,j) )<label>(7)</label></formula><p>To take semantic context in B s into consideration, we have</p><formula xml:id="formula_8">B t = B t + ξ t Ψ T t→s B s ,<label>(8)</label></formula><p>where ξ t leverages the associations between original target features and semantic contexts from the source domain. It is noteworthy that by considering cross-domain semantic context, our framework is able to further reduce domain discrepancy from the context perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Aggregation of Spatial and Channel Context</head><p>To take full advantage of spatial and channel context information, we aggregate the outputs from these two crossdomain attention modules. Specifically, A s and B s are concatenated and then fed into a convolution layer to generate the enhanced source feature Z s ∈ R C×H×W . Obviously, Z s is enriched by spatial and semantic context dependencies from both source and target domains. The same operation is also performed on A t and B t to obtain Z t ∈ R C×H×W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training Objective</head><p>Our framework contains a segmentation loss L seg and an adversarial loss L adv . We first feed Z s and Z t into the classifier G to predict their segmentation outputs G(Z s ) and G(Z t ). The segmentation loss of G(Z s ) is defined as:</p><formula xml:id="formula_9">L seg (G(Z s ), Y s ) = − H×W i=1 L j=1 Y (i,j) s G(Z s ) (i,j) ,<label>(9)</label></formula><p>where L is the number of label classes. L seg (G(Z t ), Y st s ) is defined in a similar way. To adapt structured output space <ref type="bibr" target="#b31">[32]</ref>, a discriminator D 1 is applied to G(Z s ) and G(Z t ) to make them be indistinguishable from each other. To achieve this, an adversarial loss L adv (G(Z s ), G(Z t )) is formulated as,</p><formula xml:id="formula_10">L adv (G(Z s ), G(Z t ), D 1 ) = E[logD1(G(Zs))]+ E[log(1 − D 1 (G(Z t )))]</formula><p>(10) To encourage A s , A t , B s and B t to encode useful information for semantic segmentation, they are also fed into the classifier G to predict their segmentation outputs. The overall segmentation loss is given by,</p><formula xml:id="formula_11">L seg = L seg (G(Z s ), Y s ) + L seg (G(Z t ), Y st t )+ L seg (G(A s ), Y s ) + L seg (G(A t ), Y st t )+ L seg (G(B s ), Y s ) + L seg (G(B t ), Y st t )<label>(11)</label></formula><p>We also encourage G(A s ) and G(A t ) to have similar structured layout, and enforce G(B s ) to be indistinguishable from G(B t ). Therefore, the overall adversarial loss can be written as,</p><formula xml:id="formula_12">L adv = L adv (G(Z s ), G(Z t ), D 1 )+ L adv (G(A s ), G(A t ), D 2 )+ L adv (G(B s ), G(B t ), D 3 ),<label>(12)</label></formula><p>where D 2 and D 3 are two discriminators. Specifically, D 2 aims to discriminate between G(A s ) and G(A t ), while D 3 attempts to distinguish between G(B s ) and G(B t ).</p><p>Taken them together, the training objective of our framework is: min</p><formula xml:id="formula_13">E,G max D1,D2,D3 L seg + λL adv<label>(13)</label></formula><p>where λ controls the importance of L seg and L adv . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our method on syntheticto-real domain adaptation for urban scene understanding problem. Extensive empirical experiments and ablation studies are performed to demonstrate out method's superiority over existing state-of-the-art models. We also visualize the cross-domain attention maps to reveal context dependencies between source and target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Two synthetic datasets, i.e., GTA5 <ref type="bibr" target="#b27">[28]</ref> and SYNTHIA-RAND-CITYSCAPES <ref type="bibr" target="#b28">[29]</ref> are used as the source domain in our study, while the Cityscapes <ref type="bibr" target="#b5">[6]</ref> is served as the target domain. Specifically, the GTA5 is collected from a photorealistic open-world game known as Grand Theft Auto V, which contains 24,966 images with pixel-accurate semantic labels. The resolution of each image is 1914 × 1052. SYNTHIA-RAND-CITYSCAPES contains 9,400 images (1280 × 760) with precise pixel-level semantic annotations, which are generated from a virtual city. Cityscapes is a large-scale street scene datasets collected from 50 cities, including 5,000 images with high-quality pixel-level annotations. These images are split into training (2,975 images), validation (500 images), and test (1,525 images) set, each of which with the resolution of 2048 × 1024. Following the same setting as previous studies, only the training set from Cityscapes is used as the target domain, and the validation set is used for performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Network Architecture The same CycleGAN architecture <ref type="bibr" target="#b42">[43]</ref> as reported in BDL <ref type="bibr" target="#b14">[15]</ref> is used to translate images from the source domain to the target domain. DeepLab-VGG16 and DeepLab-ResNet101, which are pre-trained on ImageNet <ref type="bibr" target="#b6">[7]</ref>, are used as our segmentation network by following the same setting in <ref type="bibr" target="#b31">[32]</ref>. Both of them use DeepLab-v2 <ref type="bibr" target="#b1">[2]</ref> as classifier, while DeepLab-VGG16 uses VGG16 <ref type="bibr" target="#b30">[31]</ref> and DeepLab-ResNet101 uses ResNet101 <ref type="bibr" target="#b10">[11]</ref> as the feature extractor. The three discriminators used for structured output adaptation have the identical architecture, each of which has 5 convolution layers with kernel 4×4 and  <ref type="figure">Figure 5</ref>. Qualitative comparison between our method and the baseline model BDL <ref type="bibr" target="#b14">[15]</ref>. For each given image (A), we present its segmentation output from (B) BDL, (C) our method incorporating CD-SAM only, (D) our method incorporating CD-CAM only, (E) our method considering both CD-SAM and CD-CAM, and the ground truth (F). stride of 2. The channel number of each layer is {64, 128, 256, 512, 1}. Each layer is followed by a leaky ReLU <ref type="bibr" target="#b23">[24]</ref> parameterized by 0.2 except the last one. The CD-SAM contains 3 convolution layers with kernel 1×1 and stride of 1 to obtain the query and key-value pairs. The channel number of these convolution layers are {128, 128, 1024} and {256, 256, 2048} for DeepLab-VGG16 and DeepLab-ResNet101, respectively.</p><p>Network Training To train the CycleGAN network, we follow the same setting in BDL <ref type="bibr" target="#b14">[15]</ref>. DeepLab-VGG16 is trained using Adam optimizer with initial learning rate 1e-5 and momentum (0.9, 0.99). We apply step decay to the learning rate with step size 50000 and drop factor 0.1. Both DeepLab-ResNet101 and CD-SAM use Stochastic Gradient Descent (SGD) optimizer with momentum 0.9 and weight decay 5e-4. The initial learning rate for DeepLab-ResNet101 and CD-SAM are 2.5e-4 and 1e-4, respectively, and are decreased by the same polynomial policy with power 0.9. For the discriminator, we use an Adam optimizer with momentum (0.9, 0.99). Its initial learning rate is set to 1e-6 for DeepLab-VGG16 and 1e-4 for DeepLab-ResNet101, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance Comparison</head><p>GTA5 to Cityscapes Our method is first evaluated by using GTA5 as the source domain and Cityscapes as the target domain. The performance is assessed on 19 common classes between these two datasets by following the same evaluation criterion in previous studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4]</ref>. Our method is compared with existing state-of-the-art models by using VGG16 and ResNet101 as the base architectures. As shown in <ref type="table" target="#tab_0">Table 1</ref>, our method achieves the best performance compared to other models. Specifically, we surpass the mean intersection-over-union (mIoU) of feature alignment-based <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23]</ref> and curriculum-based methods <ref type="bibr" target="#b40">[41]</ref> by a large margin. This observation indicates that simply aligning feature space and label distribution cannot fully transfer domain knowledge in semantic segmentation. Compared to the models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref> that are based on image-to-image translation, our method gains up to 9.5% improvement by using VGG16, revealing that domain discrepancy can be further reduced by considering context adaptation. Similar to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b14">15]</ref>, we also adapt structured output space in our model, but our method achieves significant performance improvement. This observation reveals the important role of context adaptation in knowledge transfer. It is noteworthy that the prediction of the "train" class is extremely challenging, owing to the limited "train" samples in the source domain. Our method enables to alleviate this limitation by adapting cross-domain context information. Compared to the CyCADA <ref type="bibr" target="#b11">[12]</ref>, we achieve 16.1% improvement on the "train" class.</p><p>SYNTHIA to Cityscapes The superiority of our method is further proved on "SYNTHIA to Cityscapes". It is noteworthy that domain adaptation on "SYNTHIA to Cityscapes" is more challenging than "GTA5 to Cityscapes", owing to the large domain gap between these two domains. Following [15], we consider the 16 and 13 common classes for VGG16 and ResNet101-based models, respectively. As summarized in <ref type="table" target="#tab_1">Table 2</ref>, our method substantially outperforms other competitive models. Notably, we achieve a performance improvement of 1.8% and 1.0% over BDL <ref type="bibr" target="#b14">[15]</ref> with VGG16 and ResNet101 base architectures. This result demonstrates the benefit of explicitly adapting cross-domain context dependencies in semantic segmentation, especially for two domains with significant differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section, we conduct extensive ablation studies to investigate the effectiveness of two cross-domain attention modules in our model. GTA5 to Cityscapes By incorporating CD-SAM and CD-CAM individually, we get 2.4% and 2.3% performance boost over the VGG16-based baseline ( <ref type="table" target="#tab_2">Table 3)</ref>. Taken them together, the mIoU is further improved to 44.9 mIoU. Similarly, 0.5% and 0.3% improvement is also observed in the ResNet101-based model by considering CD-SAM and CD-CAM. We achieve 49.2 mIoU by integrating both attention modules. To qualitatively demonstrate the superiority of our method, we showcase the examples of its segmentation outputs at different stages in <ref type="figure">Figure 5</ref>. As shown in the figure, our method enables to predict more consistent segmentation outputs than the baseline model and becomes increasingly accurate by incorporating two cross-domain attention modules.</p><p>SYNTHIA to Cityscapes For VGG16-based model, CD-SAM and CD-CAM contribute to 1.2% and 1.0% improvement compared to the baseline <ref type="table" target="#tab_3">(Table 4</ref>). Our method gains 1.8% improvement by combining them. By applying CD-SAM and CD-CAM to ResNet101, we achieve 51.8 and 52.0 mIoU with 0.4% and 0.6% improvement over the baseline, respectively. It is further boosted to 52.4 mIoU when both of them are considered. Our results reveal that the proposed cross-attention mechanism significantly contributes to domain adaptation in semantic segmentation by adapting context dependencies. Furthermore, the two cross-domain attention modules play a complementary role in capturing context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization of the Cross Attention</head><p>To fully understand the cross-attention mechanism in our model, we visualize the spatial attention maps in this section. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, two images are randomly selected from the source and target domain. Recall that each position in the source feature has a spatial attention map corresponding to all positions in the target feature, and vice versa. We, therefore, select two positions in the source image and visualize their "source-to-target" attention map. For the blue point that is marked on a building in the source image, its spatial attention map mainly corresponds to the building in the target image. Similarly, we select another two positions in the target image and conduct the visualization of the "target-to-source" attention map. For the blue point in the target image, its attention map focuses on the vegetation in the source image. These visualizations demonstrate the power of our method in capturing cross-domain spatial context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Parameter Sensitivity Analysis</head><p>In this section, we perform a sensitivity analysis of λ s , λ t , ξ s , and ξ t as shown in <ref type="table" target="#tab_4">Table 5</ref>. We investigate three different choices, i.e., 0.1, 1, and 10, indicating how much attention should pay for the context information from the opposite domain. Our results reveal that λ s = λ t = ξ s = ξ t = 1 performs best. The reason is that a small value fails to capture cross-domain context dependencies, while a large value may disturb the original feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an innovative cross-attention mechanism for domain adaptation by adapting the semantic context. Specifically, we introduce two cross-domain attention modules to capture spatial and channel context between source and target domains. The obtained contextual dependencies, which are shared across two domains, are further adapted to decrease the domain discrepancy. Empirical studies demonstrate that our method achieves the new state-of-the-art performance on "GTA5-to-Cityscapes" and "SYNTHIA-to-Cityscapes".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An example of cross-domain context. The source and target images share similar context information at the spatial and semantic level. The red line, orange line, and blue line denote vegetation, car, and sidewalk across two domains, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Cross-domain spatial attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Cross-domain channel attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>An example of the spatial attention map. Given a source image (A) and a target image (D), we present the source-to-target attention maps (B) and (C) for the blue and red point in (A), respectively. Similarly, we present the target-to-source attention maps (E) and (F) of the blue and red point in (D), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The performance comparison by adapting from GTA5 to Cityscapes. Two base architectures (i.e., VGG16 and ResNet101) are used in our study. The comparison is performed on 19 common classes between source and target domains. We use per-class IoU and mean IoU (mIoU) for the performance measurement. The best result in each column is highlighted in bold.<ref type="bibr" target="#b39">40</ref>.9 81.2 29.1 19.2 14.2 29.0 19.6 83.7 35.9 80.7 54.7 23.3 82.7 25.8 28.0 2.3 25.7 19.9 41.3 Ours 90.1 46.7 82.7 34.2 25.3 21.3 33.0 22.0 84.4 41.4 78.9 55.5 25.8 83.1 24.9 31.4 20.6 25.2 27.8 44.9 27.1 79.6 27.3 23.3 28.3 35.5 24.2 83.6 27.4 74.2 58.6 28.0 76.2 33.1 36.7 6.7 31.9 31.4 43.2 MaxSquare [25] 89.4 43.0 82.1 30.5 21.3 30.3 34.7 24.0 85.3 39.4 78.2 63.0 22.9 84.6 36.4 43.0 5.5 34.7 33.5 46.4 BDL [15] 91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5 Ours 91.3 46.0 84.5 34.4 29.7 32.6 35.8 36.4 84.5 43.2 83.0 60.0 32.2 83.2 35.0 46.7 0.0 33.7 42.2 49.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">GTA5 to Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Architecture</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorbike</cell><cell>bicycle</cell><cell>mIoU</cell></row><row><cell>FCNs wild [13]</cell><cell></cell><cell cols="20">70.4 32.4 62.1 14.9 5.4 10.9 14.2 2.7 79.2 21.3 64.6 44.1 4.2 70.4 8.0 7.3 0.0 3.5 0.0 27.1</cell></row><row><cell>CDA [41]</cell><cell></cell><cell cols="20">74.9 22.0 71.4 6.0 11.9 8.4 16.3 11.1 75.7 13.3 66.5 38.0 9.3 55.2 18.8 18.9 0.0 16.8 14.6 28.9</cell></row><row><cell>AdaptSegNet [32] CyCADA [12] LSD [30] CrDoCo [4]</cell><cell>VGG16</cell><cell cols="20">87.3 29.8 78.6 21.1 18.2 22.5 21.5 11.0 79.7 29.6 71.3 46.8 6.5 80.1 23.0 26.9 0.0 10.6 0.3 35.0 85.2 37.2 76.5 21.8 15.0 23.8 22.9 21.5 80.5 31.3 60.7 50.5 9.0 76.9 17.1 28.2 4.5 9.8 0.0 35.4 88.0 30.5 78.6 25.2 23.5 16.7 23.5 11.6 78.7 27.2 71.9 51.3 19.5 80.4 19.8 18.3 0.9 20.8 18.4 37.1 89.1 33.2 80.1 26.9 25.0 18.3 23.4 12.8 77.0 29.1 72.4 55.1 20.2 79.9 22.3 19.5 1.0 20.1 18.7 38.1</cell></row><row><cell cols="22">BDL [15] 89.2 AdaptSegNet [32] 86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4 ResNet101 CLAN [23] 87.0</cell></row></table><note>2 are applied to E(X s ) and E(X s ) to generate feature pairs {A s</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The performance comparison by adapting from SYNTHIA to Cityscapes. Two base architectures (i.e., VGG16 and ResNet101) are used in our study. The comparison is performed on 16 common classes for VGG16 and 13 common classes for ResNet101. We use per-class IoU and mean IoU (mIoU) for the performance measurement. The best result in each column is highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">SYNTHIA to Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Architecture</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>motorbike</cell><cell>bicycle</cell><cell>mIoU</cell></row><row><cell>DCAN [37]</cell><cell></cell><cell cols="17">79.9 30.4 70.8 1.6 0.6 22.3 6.7 23.0 76.9 73.9 41.9 16.7 61.7 11.5 10.3 38.6 35.4</cell></row><row><cell>DADA [36] GIO-Ada [3] TGCF-DA [5] BDL [15]</cell><cell>VGG16</cell><cell cols="17">71.1 29.8 71.4 3.7 0.3 33.2 6.4 15.6 81.2 78.9 52.7 13.1 75.9 25.5 10.0 20.5 36.8 78.3 29.2 76.9 11.4 0.3 26.5 10.8 17.2 81.7 81.9 45.8 15.4 68.0 15.9 7.5 30.4 37.3 90.1 48.6 80.7 2.2 0.2 27.2 3.2 14.3 82.1 78.4 54.4 16.4 82.5 12.3 1.7 21.8 38.5 72.0 30.3 74.5 0.1 0.3 24.6 10.2 25.2 80.5 80.0 54.7 23.2 72.7 24.0 7.5 44.9 39.0</cell></row><row><cell>Ours</cell><cell></cell><cell cols="17">73.0 31.1 77.1 0.2 0.5 27.0 11.3 27.4 81.2 81.0 59.0 25.6 75.0 26.3 10.1 47.4 40.8</cell></row><row><cell>SIBAN [22]</cell><cell></cell><cell cols="3">82.5 24.0 79.4</cell><cell></cell><cell></cell><cell></cell><cell cols="11">16.5 12.7 79.2 82.8 58.3 18.0 79.3 25.3 17.6 25.9 46.3</cell></row><row><cell>CLAN [23] MaxSquare [25] DADA [36] BDL [15]</cell><cell>ResNet101</cell><cell cols="3">81.3 37.0 80.1 82.9 40.7 80.3 89.2 44.8 81.4 86.0 46.7 80.3</cell><cell></cell><cell></cell><cell></cell><cell cols="11">16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9 22.6 30.7 47.8 12.8 18.2 82.5 82.2 53.1 18.0 79.0 31.4 10.4 35.6 48.2 8.6 11.1 81.8 84.0 54.7 19.3 79.7 40.7 14.0 38.8 49.8 14.1 11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3 51.4</cell></row><row><cell>Ours</cell><cell></cell><cell cols="3">82.5 42.2 81.3</cell><cell></cell><cell></cell><cell></cell><cell cols="11">18.3 15.9 80.6 83.5 61.4 33.2 72.9 39.3 26.6 43.9 52.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on "GTA5 to Cityscapes".</figDesc><table><row><cell></cell><cell cols="2">GTA5 to Cityscapes</cell><cell></cell></row><row><cell>Base</cell><cell>CD-SAM</cell><cell>CD-CAM</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell>41.3</cell></row><row><cell>VGG16</cell><cell></cell><cell></cell><cell>43.7 43.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>44.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>48.5</cell></row><row><cell>ResNet101</cell><cell></cell><cell></cell><cell>49.0 48.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>49.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on "SYNTHIA to Cityscapes".</figDesc><table><row><cell></cell><cell cols="2">SYNTHIA to Cityscapes</cell><cell></cell></row><row><cell>Base</cell><cell>CD-SAM</cell><cell>CD-CAM</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell>39.0</cell></row><row><cell>VGG16</cell><cell></cell><cell></cell><cell>40.2 40.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>40.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>51.4</cell></row><row><cell>ResNet101</cell><cell></cell><cell></cell><cell>51.8 52.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>52.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study of λs, λt, ξs, and ξt.</figDesc><table><row><cell>λs/λt/ξs/ξt</cell><cell>0.1</cell><cell>1</cell><cell>10</cell></row><row><cell>mIoU</cell><cell>43.7</cell><cell>44.9</cell><cell>40.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui-Po</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsiao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1900" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Crdoco: Pixel-level domain transfer with crossdomain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Selfensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00589</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning what and where to transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hankook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05901</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Constructing self-motivated pyramid curriculums for crossdomain semantic segmentation: A non-adversarial approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09547</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Lixin Duan, and Boqing Gong</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-scale context intertwining for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<title level="m">Learning transferable features with deep adaptation networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Significance-aware information bottleneck for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00876</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain adaptation for semantic segmentation with maximum squares loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hongyang Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision(ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10349</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dada: Depth-aware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01886</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><forename type="middle">Gokhan</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transfer learning via learning to transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5072" to="5081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
