<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
							<email>shaojing@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">â€ </forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pedestrian analysis plays a vital role in intelligent video surveillance and is a key component for security-centric computer vision systems. Despite that the convolutional neural networks are remarkable in learning discriminative features from images, the learning of comprehensive features of pedestrians for fine-grained tasks remains an open problem. In this study, we propose a new attentionbased deep neural network, named as HydraPlus-Net (HPnet), that multi-directionally feeds the multi-level attention maps to different feature layers. The attentive deep features learned from the proposed HP-net bring unique advantages: (1) the model is capable of capturing multiple attentions from low-level to semantic-level, and (2) it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image. We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attribute recognition and person reidentification. Intensive experimental results have been provided to prove that the HP-net outperforms the state-of-theart methods on various datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pedestrian analysis is a long-lasting research topic because of the continuing demands for intelligent video surveillance and psychological social behavior researches. Particularly, with the explosion of researches about the deep convolutional neural networks in recent computer vision community, a variety of applications categorized as the pedestrian analysis, e.g. pedestrian attribute recognition, person re-identification and etc., have received remarkable improvements and presented potentialities for practical us-  <ref type="figure">Figure 1</ref>. Pedestrian analysis needs a comprehensive feature representation from multi-levels and scales. (a) Semantic-level: attending features around local regions facilitates distinguishing persons that own similar appearances at a glance, such as "long hair" vs. "short hair" and "long-sleeves" vs. "short-sleeves". (b) Lowlevel: some patterns like "clothing stride" can be well captured by low-level features rather than those in high-level. (c-d) Scales: multi-scale attentive features benefit describing person's characteristics, where the small-scale attention map in (c) corresponds to the "phone" and a large-scale one in (d) for a global understanding.</p><p>age in modern surveillance system. However, the learning of feature representation for pedestrian images, as the backbone for all those applications, still confronts critical challenges and needs profound studies. At first, most traditional deep architectures have not extracted the detailed and localized features complementary to the high-level global features, which are especially effective for fine-grained tasks in pedestrian analysis. For example, it is difficult to distinguish two instances if no semantic features are extracted around hair and shoulders, as shown in <ref type="figure">Fig. 1(a)</ref>. Also in <ref type="figure">Fig. 1(c)</ref>, the effective features should be located within a small-scale head-shoulder region if we want to detect the attribute "calling". However, existing arts merely extract global features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref> and are hardly effective to location-aware semantic pattern extraction. Furthermore, it is well-known that multi-level features aid diverse vision tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6]</ref>. Similar phenomenon has also happened in the pedestrian analysis, such as the pattern "clothing stride" shown in <ref type="figure">Fig. 1(b)</ref> should be inferred from low-level features, while the attribute "gender" in <ref type="figure">Fig. 1(d)</ref> is judged by semantic understanding of the whole pedestrian image. Unlike previous approaches that mainly generate the global feature representations, the proposed feature representation encodes multiple levels of feature patterns as well as a mixture of global and local information, and thus it owns a potential capability for multi-level pedestrian attribute recognition and person re-identification.</p><p>Facing the drawbacks of recent methods for pedestrian analysis, we try to tackle the general feature learning paradigm for pedestrian analysis by a multi-directional network, called HydraPlus-Net, which is proposed to better exploit the global and local contents with multi-level feature fusion of a single pedestrian image. Specifically, we propose a multi-directional attention (MDA) module that aggregates multiple feature layers within the attentive regions extracted from multiple layers in the network. Since the attention maps are extracted from different semantic layers, they naturally abstract different levels of visual patterns of the same pedestrian image. Moreover, filtering multiple levels of features by the same attention map results in an effective fusion of multi-level features from a certain local attention distribution. After applying the MDA to different layers of the network, the multi-level attentive features are fused together to form the final feature representation.</p><p>The proposed framework is evaluated on two representatives among the pedestrian analysis tasks, i.e. pedestrian attribute recognition and person re-identification (ReID), in which attribute recognition focuses on assigning a set of attribute labels to each pedestrian image while ReID aims to associate the images of one person across multiple cameras and/or temporal shots. Although pedestrian attribute recognition and ReID pay attention to different aspects of the input pedestrian image, these two tasks can be solved by learning a similar feature representation, since they are inherently correlated with similar semantic features and the success of one task will improve the performance of the other. Compared with existing approaches, our framework achieves the state-of-the-art performance on most datasets.</p><p>The contributions of this work are three-fold:</p><p>(1) A HydraPlus Network (HP-net) is proposed with the novel multi-directional attention modules to train multilevel and multi-scale attention-strengthened features for fine-grained tasks of pedestrian analysis.</p><p>(2) The HP-net is comprehensively evaluated on pedestrian attribute recognition and person re-identification. State-of-the-art performances have been achieved with significant improvements against the prior methods.</p><p>(3) A new large-scale pedestrian attribute dataset (PA-100K dataset) is collected with the most diverse scenes and Attentive Feature Net</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv layers</head><p>Incep <ref type="table" target="#tab_3">-1 Incep-2 Incep-3   Main Net  GAP  FC  Output   Incep-1 Incep-2 Incep-3   Incep-1 Incep-2 Incep-3</ref> Incep- the largest number of samples and instances up-to-date. The PA-100K dataset is more informative than the previous collections and helpful for various pedestrian analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Attention models In computer vision, attention models have been used in tasks such as image caption generation <ref type="bibr" target="#b33">[34]</ref>, visual question answering <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref> and object detection <ref type="bibr" target="#b1">[2]</ref>. Mnih et al. <ref type="bibr" target="#b19">[20]</ref> and Xiao et al. <ref type="bibr" target="#b31">[32]</ref> explored hard attention, in which the network attends to a certain region of the image or feature map. Compared to non-differentiable hard attention trained by reinforce algorithms <ref type="bibr" target="#b27">[28]</ref>, soft attention which weights the feature maps is differentiable and can be trained by back propagation. Chen et al. <ref type="bibr" target="#b3">[4]</ref> introduced an attention to the multi-scale features, and Zagoruyko et al. <ref type="bibr" target="#b34">[35]</ref> exploited attention in knowledge transfer. In this work, we design a multi-directional attention network for better pedestrian feature representation and apply it to both pedestrian attribute recognition and re-identification tasks. To the best of our knowledge, this is the first work to adopt attention idea in the aforementioned two tasks. Pedestrian attribute recognition Pedestrian attribute has been an important research topic recently, due to its prospective application in video surveillance systems. Convolutional neural networks have achieved great success in pedestrian attribute recognition. Sudowe et al. <ref type="bibr" target="#b23">[24]</ref> and Li et al. <ref type="bibr" target="#b12">[13]</ref> proposed that jointly training multiple attributes can improve the performance of attribute recognition. Previous work also investigated the effectiveness of  utilizing pose and body part information in attribute recognition. Zhang et al. <ref type="bibr" target="#b36">[37]</ref> proposed a pose aligned network to capture the pose-normalized appearance differences. Different from previous works, we propose an attention structure which can attend to important areas and align body parts without prior knowledge on body parts or poselets. Person re-identification Feature extraction and metric learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref> are two main components for person reidentification. The success of deep learning in image classification inspired lots of studies on person ReID <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31]</ref>. The filter pairing neural network (FPNN) proposed by Li et al. <ref type="bibr" target="#b15">[16]</ref> jointly handles misalignment, transforms, occlusions and background clutters. Cheng et al. <ref type="bibr" target="#b4">[5]</ref> presented a multi-channel parts-based CNN to learn body features from the input image. In this paper, we mainly target on feature extraction and cosine distance is directly adopted for metric learning. Moreover, attention masks are utilized in our pipeline to locate discriminative regions which can better describe each individual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HydraPlus-Net Architecture</head><p>The design of the HydraPlus network 2 (HP-net) is motivated by the necessity to extract multi-scale features from multiple levels, so as not only to capture both global and local contents of the input image but also assemble its features with different levels of semantics. As shown in <ref type="figure">Fig. 2</ref>, the HP-net consists of two parts, one is the Main Net (M-net) that is a plain CNN architecture, the other is the Attentive Feature Net (AF-net) including multiple branches of multidirectional attention (MDA) modules applied to different semantic feature levels. The AF-net shares the same basic convolution architectures as the M-net except the added <ref type="bibr" target="#b1">2</ref> Hydra is a water monster with nine heads. In this work, the network consists of a 9-branch in AF-net (3 MDA modules and 3 attention subbranches for each MDA), plus an M-net, so it is called HydraPlus Net. MDA modules. Their outputs are concatenated and then fused by global average pooling (GAP) and fully connected (FC) layers. The final output can be projected as the attribute logits for attribute recognition or feature vectors for re-identification. In principle, any kind of CNN structure can be applied to construct the HP-net. But in our implementation, we design a new end-to-end model based on inception v2 architecture <ref type="bibr" target="#b9">[10]</ref> because of its excellent performance in general image-related recognition tasks. As sketched in <ref type="figure">Fig. 2</ref>, each network of the proposed framework contains several low-level convolutional layers and is followed by three inception blocks. This model seems simple but is non trivial as it achieves all required abilities and brings them together to boost the recognition capability.</p><formula xml:id="formula_0">(a) (b) (c) â†µ 1 â†µ 2 â†µ 3 â†µ 3 1 â†µ 3 2 â†µ 3 3 â†µ 3 4 â†µ 3 5 â†µ 3 7 â†µ 3 6 â†µ 3 8</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attentive Feature Network</head><p>The Attentive Feature Network (AF-net) in <ref type="figure">Fig. 2</ref> comprises three branches of sub-networks augmented by the multi-directional attention (MDA) modules, namely F(Î± i ), i âˆˆ â„¦ = {1, 2, 3}, where Î± i are the attention maps generated from the output features of the inception block i marked by black solid lines, and are applied to the output of the k th block (k âˆˆ â„¦ = {1, 2, 3}) in hot dash lines. For each MDA module, there is one link of attention generation and three links for attentive feature construction. Different MDA modules have their attention maps generated from different inception blocks and then been multiplied to feature maps of different levels to produce multi-level at-tentive features. An example of a MDA module F(Î± 2 ) is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. The main stream network of each AF-net branch is initialized exactly as the M-net, and thus the attention maps approximately distill similar features as what the M-net extracts.</p><p>It is well known that the attention maps learned from different blocks vary in scale and detailed structure. For example, the attention maps from higher blocks (e.g. Î± 3 ) tend to be coarser but usually figure out the semantic regions like Î± 3 highlights the handbag in <ref type="figure" target="#fig_2">Fig. 4(a)</ref>. But those from lower blocks (e.g. Î± 1 ) often respond to local feature patterns and can catch detailed local information like edges and textures, just as the examples visualized in <ref type="figure" target="#fig_2">Fig. 4(a)</ref>. Therefore, if fusing the multi-level attentive features by MDA modules, we enable the output features to gather information across different levels of semantics, thus offering more selective representations. Moreover, the MDA module also differs from the traditional attention-based models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref> that push the attention map back to the same block, and it extends this mechanism by applying the attention maps to adjacent blocks, as shown in lines with varying hot colors in <ref type="figure" target="#fig_1">Fig. 3</ref>. Applying one single attention map to multiple blocks naturally let the fused features encode multi-level information within the same spatial distribution which is illustrated in Section 4.2.</p><p>More specifically, for a given inception block i, its output feature map is denoted as F i âˆˆ R CÃ—HÃ—W with the width W , height H and C channels. The attention map Î± i is generated from F i by a 1 Ã— 1 conv layer with BN and ReLU activation function afterwards, noted as</p><formula xml:id="formula_1">Î± i = g att (F i ; Î¸ i att ) âˆˆ R LÃ—HÃ—W ,<label>(1)</label></formula><p>where L means the channels of the attention map. In this paper, we fix L = 8 for both tasks. And the attentive feature map to the inception block k is an element-wise multiplicatioÃ±</p><formula xml:id="formula_2">F i,k l = Î± i l â€¢ F k , l âˆˆ {1, . . . , L}.<label>(2)</label></formula><p>Each attentive feature mapF i,k l is then passed through the following blocks thereafter, and at the end of MDA module we concatenate the L attentive feature maps as the final feature representation. We visualized the detailed structure of an MDA module F(Î± 2 ) in <ref type="figure" target="#fig_1">Fig. 3</ref>. Î± 2 is generated from the inception block 2 and then applied to feature maps indexed by k âˆˆ â„¦ = {1, 2, 3}, as shown in <ref type="figure" target="#fig_1">Fig. 3(b)</ref>. Note that we prefer the ReLU activation function rather than the sigmoid function to constrain the attention maps so that the attentive regions receive more weights, and the contrast of the attention map is enlarged. More examples and analyses are shown in Sec. 4 to illustrate the MDA's effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">HP-Net Stage-wise Training</head><p>We train the HP-net in a stage-wise fashion. Initially, a plain M-net is trained to learn the fundamental pedes-</p><formula xml:id="formula_3">(a) (b) F 1 â†µ 3 8 F 2 â†µ 2 8 â†µ 3 3 â†µ 1 3 F 1,1 3F 3,1 3F 2,2 8F</formula><p>3,2 8 trian features. Then the M-net is duplicated three times to construct the AF-net with adjacent MDA modules, each of which following the framework shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Since each MDA module consists of three branches where the attention map masks adjacent inception blocks, thus in each branch we only fine-tune the blocks after the attentionoperated block. After separately fine-tuning three MDA modules in AF-net, we fix both the M-net and AF-net and train the remaining GAP and FC layers. The output layer to minimize losses defined by different tasks, in which the cross-entropy loss L att is applied for pedestrian attribute recognition, and softmax loss for person re-identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Ablation Study On Attentive Deep Features</head><p>The advantages of HP-net are its capability of learning both multi-level attentions and multi-scale attentive features for a comprehensive feature representation of a pedestrian image. To better understand these advantages, we analyze the effectiveness of each component in the network with qualitative visualization and quantitative comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-level Attention Maps</head><p>The level of attention maps. The compared exemplars of attention maps from three layers (i.e. the outputs of the inception blocks i âˆˆ â„¦ = {1, 2, 3}) are shown in <ref type="figure" target="#fig_2">Fig. 4(a)</ref>. We observe that the attention map from earlier layer i = 1 prefers grasping low-level patterns like edges or textures, while those from higher layers i = 2 or 3 are more likely to capture semantic visual patterns corresponding to a specific object (e.g. handbag) or human identity. The quantity of attention maps. Most previous studies <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b20">21]</ref>  (i.e., L = 1 or 2). In this study, we explore the potential performance of an attention model with increasing channels in both diversity and consistency. 1) Attention Diversity. <ref type="figure" target="#fig_2">Fig. 4(b)</ref> shows two images of one single pedestrian captured by two cameras, alongside with L = 8 attention channels of Î± 3 are presented. From the raw image, it is hard to distinguish these images due to the large intra-class variations from cluttered background, varying illumination, viewpoint changes and etc. Nevertheless, benefited from the discriminative localization ability of multiple attention channels from one level, the entire features can be captured separately with respect to different attentive areas. Compared to a single attention channel, the diversity of multiple attention channels enriches the feature representations and improves the chance to accurately analyze both the attributes and identity of one pedestrian.</p><p>2) Attention Consistency. We also observe that one attention map generated upon different input samples might be similarly distributed in spatial domain since they highlight the same semantic parts of a pedestrian. Notwithstanding different pedestrians, shown in <ref type="figure" target="#fig_2">Fig. 4(b-c)</ref>, their attention channels Î± <ref type="bibr" target="#b2">3</ref> 3 capture the head-shoulder regions and the channels Î± 3 5 infer the background area. Since the consistent attention maps are usually linked to salient objects, the selectiveness of these attention maps is thus essential on identifying the pedestrian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-Directional Attentive Features</head><p>Apart from the benefits of the multi-level attention maps, the effectiveness of the proposed method also lies on the novel transition scheme. For instance, the pedestrian in <ref type="figure" target="#fig_3">Fig. 5(b)</ref> holds a phone near the right ear that cannot be directly captured neither by the feature map F 2 in a lower layer i = 2, nor by the naÃ¯ve attentive feature mapsF 2,2 <ref type="bibr" target="#b7">8</ref> . Surprisingly, with the help of a higher level attention map Î± <ref type="bibr">3 8</ref> , the attentive feature mapF 2,3 8 can precisely attend the region around the phone. On the other hand, the high-level attention map Î± 3 3 might not be able to capture lower-level visual patterns related to attributes like "upper-clothing pattern". For example, the attention map Î± 3 3 shown in <ref type="figure" target="#fig_3">Fig. 5</ref>(a) does not point out the local patterns onto the T-shirt, while on the contrary, the low-level attention map Î± 1 3 filters out F 1,1 3 that typically reflects these texture patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Component Analysis</head><p>We also demonstrate the cases when dropping partial attention modules or connections in comparison with the complete AF-net. As an example, the person ReID on VIPeR dataset <ref type="bibr" target="#b7">[8]</ref> with six typical configurations are compared in <ref type="figure" target="#fig_4">Fig. 6(a)</ref>. The orange bar shown in its bottom indicates the performance with the complete AF-net, while the yellow one is the M-net which is considered as the baseline model without the attention modules. The rest four bars are configured as: (1) Blue: naÃ¯ve attention modules per branch. In each branch of AF-net, a naÃ¯ve attention module is applied to extract the attentive featuresF i,i , i âˆˆ â„¦ = {1, 2, 3}.</p><p>(2) Cyan: discarding the middle-level attention maps and attentive features. We discard both the attention maps and attentive features of the block 2 nd , i.e. prune the modules that produceF 2,k andF i,2 , âˆ€i, k âˆˆ {1, 2, 3}.</p><p>(3) Purple: pruning one branch. It discards the first MDA module F(Î± 1 ). (4) Light purple: pruning two branches. The first two MDA modules F(Î± 1 ) and F(Î± 2 ) are discarded.</p><p>The results clearly prove that either cutting down the number of MDA modules or connections within this module will pull down the performance, and it is reasonable that these attention components complement each other to generate the comprehensive feature representation and thus gain a higher accuracy. Two examples with Top-5 identification results shown in <ref type="figure" target="#fig_4">Fig. 6(b-c)</ref> further demonstrate the  effectiveness and indispensability of each component of the entire AF-net. The complete network is superior to both the multi-level naÃ¯ve attention modules ( <ref type="figure" target="#fig_4">Fig. 6(b)</ref>) and the single MDA module <ref type="figure" target="#fig_4">(Fig. 6(c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Pedestrian Attribute Recognition</head><p>We evaluate our HP-net on two public datasets comparing the state-of-the-art methods. In addition, we further propose a new large-scale pedestrian attribute dataset PA-100K with larger scene diversities and amount of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">PA-100K Dataset</head><p>Most of existing public pedestrian attribute datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> only contain a limited number of scenes (at most <ref type="bibr" target="#b25">26)</ref> with no more than 50, 000 annotated pedestrians. To further evaluate the generality of the proposed method, we construct a new large-scale pedestrian attribute (PA) dataset named as PA-100K with 100, 000 pedestrian images from 598 scenes, and therefore offer a superiorly comprehensive dataset for pedestrian attribute recognition. To our best knowledge, it is to-date the largest dataset for pedestrian attribute recognition. We compare our PA-100K dataset with the other two publicly available datasets in <ref type="table">Table 1</ref>.</p><p>The samples of one person in PETA dataset <ref type="bibr" target="#b6">[7]</ref> are only annotated once by randomly picking one exemplar image, and therefore share the same annotated attributes even though some of them might not be visible and some other attributes are ignored. Another limitation is that the random partition of the training, validation and test sets are conducted in the whole dataset with no consideration of the person's identity across images, which leads to unfair image assignment of one person in different sets. In RAP dataset <ref type="bibr" target="#b13">[14]</ref>, the high-quality indoor images with controlled lighting conditions contain much lower variances than those under unconstrained real scenarios. Moreover, some attributes are even highly imbalanced.</p><p>PA-100K dataset surpasses the the previous datasets both in quantity and diversity, as shown in <ref type="table">Table 1</ref>. We define 26 commonly used attributes including global attributes like gender, age, and object level attributes like handbag, phone, upper-clothing and etc. The PA-100K dataset was constructed by images captured from real outdoor surveillance cameras which is more challenging. Different from the existing datasets, the images were collected by sampling the frames from the surveillance videos, which makes some future applications available, such as video-based attribute recognition and frame-level pedestrian quality estimation. We annotated all pedestrians in each image and abandoned pedestrians with blurred motion or extreme low resolution (lower than 50 Ã— 100). The whole dataset is randomly split into training, validation and test sets with a ratio of 8 : 1 : 1. The samples of one person was extracted along its tracklets in a surveillance video, and they are randomly assigned to one of these sets, in which case PA-100K dataset ensures the attributes are learned independent of the person's identity. All these sets are guaranteed to have positives and negatives of the 26 attributes. Note that this partition based on tracklets is fairer than the partition that randomly shuffles the images in PETA dataset.</p><p>In the following experiments, we employ five evaluation criteria 3 including a label-based metric mean accuracy (mA), and four instance-based metrics, i.e. accuracy, precision, recall and F1-score. To address the issue of imbalanced classes, we adopt a weighted cross-entropy loss func-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with the Prior Arts</head><p>We quantitatively and qualitatively compare the performance of the proposed method with the previous state-ofthe-art methods on the previously mentioned three datasets. The following comparisons keep the same settings as the prior arts on different datasets respectively. Quantitative Evaluation. We list the results of each method on RAP, PETA and PA-100K datasets in <ref type="table" target="#tab_3">Table 2</ref>. Six reference methods are selected to be compared with the proposed model. The first three models are based on SVM classifier with hand-crafted features (ELF-mm <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>) and deep-learned features (FC7-mm and FC6-mm) respectively. ACN <ref type="bibr" target="#b23">[24]</ref> and DeepMar <ref type="bibr" target="#b12">[13]</ref> are CNN models that achieved good performances by joint training the multiple attributes.</p><p>The baseline M-net and the proposed final model significantly outperform the state-of-the-art methods. We are also interested in the performance of each attribute. The bar in <ref type="figure" target="#fig_5">Fig. 7</ref> shows the overlapped histograms of the mean accuracy (mA) for all attributes by DeepMar and HP-net. The bars are sorted in descending order according to the larger mA between these methods at one attribute. We find that the envelope superimposing the histogram is always supported by the HP-net with prominent performance gain against DeepMar, and is extremely superior on attributes which require fine-grained localization, like glasses and handbags. Qualitative Evaluation. Besides the quantitative results in  <ref type="table">Table 3</ref>. The specifications of three evaluated ReID datasets. <ref type="table" target="#tab_3">Table 2</ref>, we also conduct qualitative evaluations for exemplar pedestrian images. As shown in the examples in <ref type="figure" target="#fig_5">Fig. 7</ref>, sample images from RAP dataset and their attention maps demonstrate the localizability of the learned attention maps. Especially in the first image, the attention map highlights two bags simultaneously. We also notice a failure case on the attribute "talking" which is irrelevant to a certain region but requires a global understanding of the whole image. For the PA-100K dataset, we show attribute recognition results for several exemplar pedestrian images in <ref type="figure" target="#fig_6">Fig. 8</ref>. The bars indicate the prediction probabilities. Although the probabilities of one attribute do not directly imply its actual recognition confidences, they uncover the discriminative power of different methods as the lower probability corresponds to ambiguity or difficulty in correctly predicting one attribute. The proposed HP-net reliably predicts these attributes with region-based saliency, like "glasses", "back-pack", "hat", "shorts" and "handcarry".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Person Re-identification</head><p>Referring to the person re-identification, we also evaluate the HP-net with several reference methods on three publicly available datasets, quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets and Setups</head><p>The proposed approach is evaluated on three publicly standard datasets, including CUHK03 <ref type="bibr" target="#b15">[16]</ref>, VIPeR <ref type="bibr" target="#b7">[8]</ref>, and Market-1501 <ref type="bibr" target="#b37">[38]</ref>. A summary about the statistical information of the three datasets are listed in <ref type="table">Table 3</ref>. For the Market-1501 dataset, the same data separation strategy is used as <ref type="bibr" target="#b37">[38]</ref>. For the other datasets, the training, validation and testing images are sampled based on the strategy introduced in <ref type="bibr" target="#b29">[30]</ref>. The training and validation identities are guaranteed to have no overlaps with the testing ones for all The widely applied cumulative match curve (CMC) metric is adopted for quantitative evaluation. While in the matching process, the cosine distance is computed between each query image and all the gallery images, and the ranked gallery list is returned. All the experiments are conducted under the setting of single query and the testing procedure is repeated 100 times to get an average result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Performance Comparisons</head><p>Quantitative Evaluation. As shown in <ref type="table">Table 4</ref>, the proposed approach is compared with a series of the deep neural networks like PersonNet <ref type="bibr" target="#b28">[29]</ref>, the multi-domain CNN JSTL <ref type="bibr" target="#b29">[30]</ref>, the Joint ReID method <ref type="bibr" target="#b0">[1]</ref>, and the horizontal occurrence model LOMO-XQDA <ref type="bibr" target="#b16">[17]</ref> on CUHK03 <ref type="bibr" target="#b15">[16]</ref>. As for the VIPeR <ref type="bibr" target="#b7">[8]</ref> dataset, the null space semisupervised learning method NFST <ref type="bibr" target="#b35">[36]</ref>, the similarity learning method SCSP <ref type="bibr" target="#b2">[3]</ref>, the hierarchical Gaussian model GOG+XQDA <ref type="bibr" target="#b18">[19]</ref>, and the triplet loss model TCP <ref type="bibr" target="#b4">[5]</ref> are selected for comparison. The Market-1501 <ref type="bibr" target="#b37">[38]</ref> dataset is also evaluated with the metric learning WARCA-L <ref type="bibr" target="#b10">[11]</ref>, a novel Siamese LSTM architecture LOMO+CN <ref type="bibr" target="#b26">[27]</ref>, the Siamese CNN with learnable gate S-CNN <ref type="bibr" target="#b25">[26]</ref>, and the bag of words model BoW-best <ref type="bibr" target="#b37">[38]</ref>.</p><p>Besides the results of the proposed approach with complete HP-net, the results of the M-net are also listed as the baseline for the three datasets. From <ref type="table">Table 4</ref>, we can ob-Probe Top1 Top2 Top3 Top4 Top5 <ref type="figure">Figure 9</ref>. Comparison results between HP-net and M-net. For the probe images, the Top-5 retrieval results of HP-net together with attention maps are shown in the first row, and the results of M-net are shown in the second row.</p><p>serve that the proposed approach achieves the Top-1 accuracies of 91.8%, 56.6% and 76.9% on the CUHK03, ViPeR and Market-1501 datasets, respectively, and it achieves the state-of-the-art performance on all the three datasets. Moreover, even though the M-net can achieve quite satisfactory results on all datasets, the proposed pipeline can further improve the Top-1 accuracies by 3.6%, 5.0%, and 3.8% for each dataset, respectively.</p><p>Qualitative Evaluation. To highlight the performance of the proposed method on extracting localized semantic features, one query image together with its Top-5 gallery results by the proposed method and the M-net are visualized in <ref type="figure">Fig. 9</ref>. We observe that the proposed approach improves the rankings of the M-net and gets the correct results. By visualizing the attention maps from HP-Net of the query images and the Top-5 gallery images of both methods, we observe that the proposed attention modules can successfully locate the T-shirt patterns, in which the fine-grained features are extracted and discriminatingly identify the query person against the other identities with similar dressing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we present a new deep architecture called HydraPlus network with a novel multi-directional attention mechanism. Extensive ablation studies and experimental evaluations have manifested the effectiveness of the HP-net to learn multi-level and multi-scale attentive feature representations for fine-grained tasks in pedestrian analysis, like pedestrian attribute recognition and person re-identification. In the end, a new large-scale attribute dataset PA-100K is introduced to facilitate various pedestrian analysis tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>An example of the multi-directional attention (MDA) module F(Î± 2 ). The M-net in (a) is presented with simplified layers (indexed by i âˆˆ â„¦) representing the output layers of three inception blocks. The attention map Î± 2 is generated from block 2 and multi-directionally masks three adjacent blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The diversity and semantic selectiveness of attention maps. (a) The attention maps generated from three adjacent blocks respond to visual patterns in different scales and levels, in which Î± 3 owns the power to highlight semantic patterns in object level. (b-c) Different channels in attention maps capture different visual patterns related to body parts, salient objects and background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Examples of multi-directional attentive features. (a) The identification of low-level attributes like "upper-clothing pattern" requires the low-level attention connections, for example, applying Î± 1 3 to extractF 1,1 3 indicating textures onto the T-shirt. (b) But the semantic or object-level attributes like "phone" require high-level attention connections such as applying Î± 3 8 to extractF 3,2 8for the detection of the phone near the ear.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FFigure 6 .</head><label>6</label><figDesc>merely demonstrated the effectiveness of an attention-based model with a limited number of channels Results of discarding partial attention modules or connections compared with that of the complete network fed with all MDA modules on VIPeR dataset. The 3 Ã— 3 boxes in (a) indicates the indices of different attention maps and their mask directions. The hollow white in each box means the corresponding attentions or directional links have been cut down. Bars are plot by the Top-1 accuracy. (b) and (c) present the qualitative results by the complete network compared with two kinds of partial networks in (a). For a query image shown in the middle, Top-5 results are shown aside with the correct marked by green and the false alarm are red. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Mean accuracy scores for all attributes of RAP dataset by HP-net and DeepMar marked with red and blue bars respectively. The bars are sorted according to the larger mAs between two methods. The HP-net outperforms DeepMar especially on "glasses" and "hat" which have the exemplar sample listed aside. The sample in orange provides a failure case of predicting the attribute "talking".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FemaleFigure 8 .</head><label>8</label><figDesc>Comparison results between DeepMar, M-net and HPnet on partial ground truth attributes annotated for the given examples. Different colors represent different methods. Bars are plot by the prediction probabilities. tion introduced by [13].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>72.28 73.32 69.66 73.79 74.44 76.12 75.21 76.65 77.96 81.15 82.6 80.58 81.77 72.7 72.3 74.21 Accu 29.29 31.72 33.37 62.61 62.02 64.99 65.39 43.68 45.41 48.13 73.66 75.07 75.68 76.13 70.39 70.44 72.19 Prec 32.84 35.75 37.57 80.12 74.92 77.83 77.33 49.45 51.33 54.06 84.06 83.68 84.81 84.92 Quantitative results(%) on three datasets for pedestrian attribute recognition, compared with previous benchmark methods.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PETA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PA-100K</cell></row><row><cell>Method</cell><cell>ELF-mm</cell><cell>FC7-mm</cell><cell cols="2">FC6-mm</cell><cell>ACN</cell><cell>Deep-Mar</cell><cell>M-net</cell><cell>HP-net</cell><cell>ELF-mm</cell><cell>FC7-mm</cell><cell>FC6-mm</cell><cell>ACN</cell><cell>Deep-Mar</cell><cell>M-net</cell><cell>HP-net</cell><cell>Deep-Mar</cell><cell>M-net</cell><cell>HP-net</cell></row><row><cell>mA</cell><cell cols="17">69.94 82.24 81.7</cell><cell>82.97</cell></row><row><cell>Recall</cell><cell cols="8">71.18 71.78 73.23 72.26 76.21 77.89 78.79</cell><cell cols="6">74.24 75.14 76.49 81.26 83.14 82.9</cell><cell>83.24</cell><cell cols="3">80.42 81.05 82.09</cell></row><row><cell>F1</cell><cell cols="8">44.95 47.73 49.66 75.98 75.56 77.86 78.05</cell><cell>59.36</cell><cell>61</cell><cell cols="5">63.35 82.64 83.41 83.85 84.07</cell><cell cols="3">81.32 81.38 82.53</cell></row><row><cell>0</cell><cell>0.5</cell><cell>1</cell><cell>0</cell><cell></cell><cell>0.5</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Glasses</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Backpack</cell><cell></cell><cell></cell><cell cols="2">Long Sleeves</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Short Sleeves</cell><cell></cell><cell></cell><cell cols="2">Long Coat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Upperbody Splice</cell><cell></cell><cell></cell><cell cols="2">Trousers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell cols="2">0.5</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Female</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Side</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Long Coat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">DeepMar</cell><cell cols="2">M-net</cell><cell>HP-net</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Criterion definitions are the same as those in<ref type="bibr" target="#b13">[14]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7755</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A deep multi-level network for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01064</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pedestrian attribute recognition at far distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</title>
		<meeting>IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Scalable metric learning via weighted approximate rank component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00370</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-attribute learning for pedestrian attribute recognition in surveillance scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ACPR), 2015 3rd IAPR Asian Conference on</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A richly annotated dataset for pedestrian attribute recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07054</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian descriptor for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Person re-identification by support vector ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Prosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep attributes driven multi-camera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Person attribute recognition with a jointly-trained holistic cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sudowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multiregion bilinear convolutional neural networks for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07255</idno>
		<title level="m">Personnet: person re-identification with deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.02139</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
