<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense-Captioning Events in Videos: SYSU Submission to ActivityNet Challenge 2020</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Data and Computer Science</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep3">Ministry of Education</orgName>
								<orgName type="department" key="dep4">Guangdong Province Key Laboratory of Information Security Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huicheng</forename><surname>Zheng</surname></persName>
							<email>zhenghch@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Data and Computer Science</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep3">Ministry of Education</orgName>
								<orgName type="department" key="dep4">Guangdong Province Key Laboratory of Information Security Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjing</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Data and Computer Science</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep3">Ministry of Education</orgName>
								<orgName type="department" key="dep4">Guangdong Province Key Laboratory of Information Security Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dense-Captioning Events in Videos: SYSU Submission to ActivityNet Challenge 2020</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This technical report presents a brief description of our submission to the dense video captioning task of Activi-tyNet Challenge 2020. Our approach follows a two-stage pipeline: first, we extract a set of temporal event proposals; then we propose a multi-event captioning model to capture the event-level temporal relationships and effectively fuse the multi-modal information. Our approach achieves a 9.28 METEOR score on the test set. Code is available at https://github.com/ttengwang/ dense-video-captioning-pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dense video captioning attracts increasing attention in recent years, whose goal is to localize and describe all events in an untrimmed video. Different from traditional video captioning which only described a single event, dense video captioning requires a comprehensive understanding of the long-term temporal structure and the semantic relationships among a sequence of events. Previous methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7]</ref> have employed different types of contexts for constructing event representation, e.g., neighboring regions within an expanded receptive field <ref type="bibr" target="#b0">[1]</ref>, event-level semantic attention <ref type="bibr" target="#b1">[2]</ref>, and clip-level recurrent features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b9">10]</ref>. Although promising progress has been achieved, their context modeling missed the perception of the temporal structure of the event sequence, i.e. the temporal positions and lengths of other events. As a consequence, the temporal relationship between events is not fully exploited in the captioning stage. In this work, we propose to explore the temporal relation between events in the encoding phase. Furthermore, we also design a cross-modal gating (CMG) block for hierarchical RNN, which can adaptively estimate the weight of linguistic information and visual information for better caption generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>The overall framework contains three parts, i.e., the feature extractor, the temporal event proposal model, and the event captioning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature Extractor</head><p>We divide the video into several non-overlapping clips with a stride of 0.5s and extract the frame-level features by a TSN <ref type="bibr" target="#b8">[9]</ref> pretrained on ActivityNet datasets for the action recognition task. We concatenate the feature vectors in optical flow modality and RGB modality to construct the frame-level representation, which is utilized for the temporal event proposal module (TEP) and the event captioning module (EC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Temporal Event Proposal</head><p>Accurate event proposals generation is the basis for further captioning. For TEP, we adopt an off-the-shell DBG <ref type="bibr" target="#b2">[3]</ref> to detect the top 100 proposals for each video. Since the number of proposals in the ground-truth annotation is usually small (around 3.7 per video) in average, we follow Chen et al. <ref type="bibr" target="#b0">[1]</ref> to perform a modified event sequence selection network (ESGN) <ref type="bibr" target="#b3">[4]</ref> to predict a subset of candidate proposals. The selection process can achieve a good balance between precision and recall, especially when the number of proposals is relatively small. After selection, the number of output proposals per video is around 2.4 on average. The average precision and recall on the validation set across tIOU∈ {0.3, 0.5, 0.7, 0.9} is 66.63% and 40.09%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Event Captioning</head><p>The event captioning model follows an encoder-decoder architecture. For the visual encoder, we follow Wang et al. <ref type="bibr" target="#b7">[8]</ref> to adopt the temporal-semantic relation module (TSRM) to capture rich relationships between events in terms of both temporal structure and semantic meaning. For the language decoder, we develop a gated hierarchical RNN, aided by a cross-modal gate to strike a balance of visual information and linguistic information when captioning the next sentence. The encoder and the decoder are illustrated in <ref type="figure">Fig. 1 and Fig. 2</ref>, respectively.</p><p>Visual Encoder. TSRM contains two branches, i.e., a temporal relation branch and a semantic relation branch. Each branch estimates the relation score between the target proposal p i and the other proposals {p j }, and then we fuse the two types of scores by addition. The relational features of an event are obtained by weighted summation of features of all proposals conditioned on the relation scores.</p><p>For the temporal relation branch, we encode the relative length and the relative distance between each pair of proposals, and then put the position encoding into a nonlinear embedding <ref type="bibr" target="#b5">[6]</ref> and an FC layers with an output size of 1. For the semantic relation branch, we first obtain the proposals' initial representation by the mean pooling of the frame-level features, then we adopt scaled dot-product attention <ref type="bibr" target="#b5">[6]</ref> to calculate the semantic similarity between proposals.</p><p>Language Decoder. The function of decoder is to translate the visual representation produced by the encoder into target modality. Different from models in traditional image/video captioning, the target output of the decoder in dense video captioning is a set of sentences instead of one. To increase the coherence among sentences, hierarchical RNN (a sentence RNN plus a word RNN) based deocder is widely used for multi-sentence captioning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref>. The sentence RNN stores multi-modal information of all previous events and guides the generation of the next sentence. To enhance the multi-modal fusion, we propose a crossmodal gating (CMG) block to adaptively balance the visual and linguistic information. Specifically, the inputs of cross- modal gating block are four folds: 1) the position embedding l i of proposal p i , 2) the proposal's feature vector z i , which is the concatenation of the output of TSRM module and the mean pooling of the frame-level features within p i , 3) the last hidden state s i−1 in the word RNN of the previous sentence, and 4) the previous hidden state h i−1 of the sentence RNN. Motivated by Wang et al. <ref type="bibr" target="#b6">[7]</ref>, we use gating mechanism to balance the linguistic information s i−1 and the visual information z i . The gate g i is calculated by an FC layer with sigmoid activation function. We use g t and 1 − g t to gate the information of visual features and linguistic features, respectively. The word RNN is implemented as an attention-enhanced RNN, which adaptively select the salient frames within the proposal p i for word prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Setting</head><p>Inplementation Details. For data processing, we build a vocabulary that only takes into consideration those words that occurred at least 5 times. Sentences longer than 30 words have been truncated. For the event captioning model, we adopt LSTM as the RNN in the decoder. The hidden units of LSTMs and all FC layers are set to be 512.</p><p>We first train the event captioning module based on ground-truth proposals using cross-entropy loss. Afterwards, to address the exposure bias problem and boost the performance, we continually train the model by selfcritical sequence training (SCST) <ref type="bibr" target="#b4">[5]</ref> based on learnt event sequences. For each video, we sample 24 event sequences from the output results of DBG <ref type="bibr" target="#b2">[3]</ref> for SCST. The reward is set to be the METEOR score.</p><p>Dataset. We use the ActivityNet Captions dataset to evaluate the performance of our method. We follow the official split, which assigns 10,009/4,917/5,044 videos for training/validation/testing. In our final submission, we train the event captioning model using SCST with an enlarged training set. Specifically, we randomly select ∼80% videos from the validation set and add them to the official training set. The modified split contains 13,926/1,000 videos for training/validation. Evaluation Metrics. We use the official evaluation tool to measure the ability of our model in both localizing and captioning events. Specifically, the average precision is computed across tIoU thresholds of 0.3, 0.5, 0.7, and 0.9. The precision of generated captions is measured by traditional evaluation metrics in video captioning: BLEU, ME-TEOR, and CIDEr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Performance Evaluation</head><p>We show the ablation study for event captioning in Table 1. The first row in the table shows that the lacking of event-event interaction leads to poor performance of generated captions. When the sentence RNN or TSRM is incorporated, the generated sentence has the perception of previous events, thus a significant performance improvement can be achieved. The proposed CMG brings the model a big advantage at effective multi-modal fusion, which further boosts the captioning capability of the hierarchical RNN.</p><p>We also investigate the performance of training schemes and model ensemble in <ref type="table">Table 2</ref>. When using SCST after the cross-entropy training, the METEOR score increases considerably from 11.49/7.65 to 14.18/9.71. When using the enlarged training set, the performance can obtain further improvement. Our single model achieves a 9.17 METEOR score on the challenging test set. In our final submission, we use an ensemble of three models with different seeds, which achieves a 9.28 METEOR score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Conclusion</head><p>In this paper, we present a dense video captioning system with two plug-and-play modules, i.e. TSRM and CMG. TSRM aims to enhance the event-level representation by capturing rich relationships between events in terms of both temporal structure and semantic meaning. CMG is designed to effectively fuse the linguistic features and visual features in hierarchical RNN. Experimental results on ActivityNet Captions verify the effectiveness of our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .p1:OutputFigure 2 .</head><label>12</label><figDesc>The proposed encoder. Person drives the red riding lawnmower across the yard. p3 : The person decides to turn off the red mower. p2 : The dogs run across the yard. The proposed Decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation study for event captioning model on validation set with ground-truth proposals. Note that the validation set has two independent annotations, we only use the ground-truth proposal in val 1 as the input proposals. The performance is evaluated on both annotations according to the official evaluation code.</figDesc><table><row><cell></cell><cell cols="6">TSRM CMG sent RNN METEOR BLEU@4 CIDEr</cell></row><row><cell></cell><cell>× √ × × √</cell><cell>× × × √ √</cell><cell>× × √ √ √</cell><cell>9.96 11.16 10.76 11.31 11.49</cell><cell>1.67 2.73 2.28 2.74 2.85</cell><cell>32.67 49.15 40.51 48.37 49.34</cell></row><row><cell></cell><cell cols="2">Validation set</cell><cell>Test set</cell><cell></cell><cell></cell></row><row><cell></cell><cell>GT prop.</cell><cell cols="2">Learnt prop. Learnt prop.</cell><cell></cell><cell></cell></row><row><cell>Cross-entropy</cell><cell>11.49</cell><cell>7.65</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>+SCST</cell><cell cols="2">14.18(+2.69) 9.71(+2.06)</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell cols="3">+ Larger train. set 14.64  *  (+0.46) 10.26  *  (+0.55)</cell><cell>9.17</cell><cell></cell><cell></cell></row><row><cell>+ ensemble</cell><cell>14.85</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* (+0.21) 10.31* (+0.05) 9.28(+0.11) Table 2. Performance of different training scheme and model en- semble on validation set.* incicates evaluation results on the small validation set.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Acknowledgement</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61976231, Grant U1611461, Grant 61172141, Grant 61673402, and Grant 61573387.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05092</idno>
		<title level="m">Activitynet 2019 task 3: Exploring contexts for dense captioning events in videos</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Int Conf Comput Vis</title>
		<meeting>IEEE Int Conf Comput Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04127</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Streamlined dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf Comput Vis Pattern Recognit</title>
		<meeting>IEEE Conf Comput Vis Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6588" to="6597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf Comput Vis Pattern Recognit</title>
		<meeting>IEEE Conf Comput Vis Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Adv Neural Inf Proc Sys</title>
		<meeting>Adv Neural Inf Sys</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bidirectional attentive fusion with context gating for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf Comput Vis Pattern Recognit</title>
		<meeting>IEEE Conf Comput Vis Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7190" to="7198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Event-centric hierarchical representation for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00797</idno>
		<title level="m">Cuhk &amp; ethz &amp; siat submission to activitynet challenge 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical context encoding for events captioning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Int Conf Img Process</title>
		<meeting>IEEE Int Conf Img ess</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1288" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf Comput Vis Pattern Recognit</title>
		<meeting>IEEE Conf Comput Vis Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4584" to="4593" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
