<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Human Activities and Object Affordances from RGB-D Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><forename type="middle">Swetha</forename><surname>Koppula</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudhir</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
							<email>asaxena@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Human Activities and Object Affordances from RGB-D Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding human activities and object affordances are two very important skills, especially for personal robots which operate in human environments. In this work, we consider the problem of extracting a descriptive labeling of the sequence of sub-activities being performed by a human, and more importantly, of their interactions with the objects in the form of associated affordances. Given a RGB-D video, we jointly model the human activities and object affordances as a Markov random field where the nodes represent objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural support vector machine (SSVM) approach, where labelings over various alternate temporal segmentations are considered as latent variables. We tested our method on a challenging dataset comprising 120 activity videos collected from 4 subjects, and obtained an accuracy of 79.4% for affordance, 63.4% for sub-activity and 75.0% for high-level activity labeling. We then demonstrate the use of such descriptive labeling in performing assistive tasks by a PR2 robot. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>It is indispensable for a personal robot to perceive the environment in order to perform assistive tasks. Recent works in this area have addressed tasks such as estimating geometry <ref type="bibr" target="#b18">(Henry et al., 2012)</ref>, tracking objects <ref type="bibr" target="#b6">(Choi and Christensen, 2012)</ref>, recognizing objects <ref type="bibr" target="#b7">(Collet et al., 2011)</ref>, placing objects <ref type="bibr" target="#b24">(Jiang et al., 2012b)</ref> and labeling geometric classes <ref type="bibr" target="#b31">(Koppula et al., 2011;</ref><ref type="bibr" target="#b4">Anand et al., 2012)</ref>. Beyond geometry and objects, humans are an important part of the indoor environments. Building upon the recent advances in human pose detection from an RGB-D sensor <ref type="bibr" target="#b63">(Shotton et al., 2011)</ref>, in this paper we present learning algorithms to detect the human activities and object affordances. This information can then be used by assistive robots as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Most prior works in human activity detection have focussed on activity detection from still images or from 2D videos. Estimating the human pose is the primary focus of these works, and they consider actions taking place over shorter time scales (see Section II). Having access to a 3D camera, which provides RGB-D videos, enables us to robustly estimate human poses and use this information for learning complex human activities.</p><p>Our focus in this work is to obtain a descriptive labeling of the complex human activities that take place over long time scales and consist of a long sequence of sub-activities, such as making cereal and arranging objects in a room (see <ref type="figure">Fig. 9</ref>). For example, the making cereal activity consists of around 12 sub-activities on average, which includes reaching the pitcher, moving the pitcher to the bowl, and then pouring the milk into the bowl. This proves to be a very challenging task given the variability across individuals in performing each sub-activity, and other environment induced conditions such as cluttered background and viewpoint changes. (See <ref type="figure">Fig. 2</ref> for some examples.)</p><p>In most previous works, object detection and activity recognition have been addressed as separate tasks. Only recently, some works have shown that modeling mutual context is beneficial <ref type="bibr" target="#b16">(Gupta et al., 2009;</ref><ref type="bibr" target="#b71">Yao and Fei-Fei, 2010)</ref>. The key idea in our work is to note that, in activity detection, it is sometimes more informative to know how an object is being used (associated affordances, <ref type="bibr" target="#b15">Gibson, 1979)</ref> rather than knowing what the object is (i.e. the object category). For example, both chair and sofa might be categorized as 'sittable,' and a cup might be categorized as both 'drinkable' and 'pourable.' Note that the affordances of an object change over time depending on its use, e.g., a pitcher may first be reachable, then movable and finally pourable. In addition to helping activity recognition, recognizing object affordances is important by itself because of their use in robotic applications (e.g., <ref type="bibr" target="#b33">Kormushev et al., 2010;</ref><ref type="bibr" target="#b23">Jiang et al., 2012a;</ref><ref type="bibr" target="#b26">Jiang and Saxena, 2012)</ref>.</p><p>We propose a method to learn human activities by mod- <ref type="figure">Fig. 2</ref>. Significant Variations, Clutter and Occlusions: Example shots of reaching sub-activity from our dataset. First and third rows show the RGB images, and the second and bottom rows show the corresponding depth images from the RGB-D camera. Note that there are significant variations in the way the subjects perform the sub-activity. In addition, there is significant background clutter and subjects are partially occluded (e.g., column 1) or not facing the camera (e.g., row 1 column 4) in many instances.</p><p>eling the sub-activities and affordances of the objects, how they change over time, and how they relate to each other. More formally, we define a Markov random field (MRF) over two kinds of nodes: object and sub-activity nodes. The edges in the graph model the pairwise relations among interacting nodes, namely the object-object interactions, object-subactivity interactions, and the temporal interactions. This model is built with each spatio-temporal segment being a node. The parameters of this model are learnt using a structural support vector machine (SSVM) formulation <ref type="bibr" target="#b11">(Finley and Joachims, 2008)</ref>. Given a new sequence of frames, we label the high-level activity, all the sub-activities and the object affordances using our learned model. The activities take place over a long time scale, and different people execute sub-activities differently and for different periods of time. Furthermore, people also often merge two consecutive sub-activities together. Thus, segmentations in time are noisy and, in fact, there may not be one 'correct' segmentation, especially at the boundaries. One approach could be to consider all possible segmentations, and marginalize the segmentation; however, this is computationally infeasible. In this work, we perform sampling of several segmentations, and consider labelings over these temporal segments as latent variables in our learning algorithm.</p><p>We first demonstrate significant improvement over previous work on Cornell Activity Dataset (CAD-60). We then contribute a new dataset comprising 120 videos collected from four subjects (CAD-120). These datasets along with our code are available at http://pr.cs.cornell.edu/ humanactivities/. In extensive experiments, we show that our approach outperforms the baselines in both the tasks of activity as well as affordance detection. We achieved an accuracy of 91.8% for affordance, 86.0% for sub-activity labeling and 84.7% for high-level activities respectively when given the ground truth segmentation, and an accuracy of 79.4%, 63.4% and 75.0% on these respective tasks using our multiple segmentation algorithm.</p><p>In summary, our contributions in this paper are five-fold:</p><p>• We provide a fully annotated RGB-D human activity dataset containing 120 long term activities such as making cereal, microwaving food, etc. Each video is annotated with the human skeleton tracks, object tracks, object affordance labels, sub-activity labels, and highlevel activities.</p><p>• We propose a method for joint sub-activity and affordance labeling of RGB-D videos by modeling temporal and spatial interactions between humans and objects.</p><p>• We address the problem of temporal segmentation by learning the optimal labeling from multiple temporal segmentation hypotheses.</p><p>• We provide extensive analysis of our algorithms on two datasets and also demonstrate how our algorithm can be used by assistive robots.</p><p>• We release full source code along with ROS and PCL integration.</p><p>The rest of the paper is organized as follows. We start with a review of the related work in Section II. We describe the overview of our methodology in Section III and describe the model in Section IV. We then describe the object tracking and segmentation methods in Section V and VI respectively and describe the features used in our model in Section VII. We present our learning, inference and temporal segmentation algorithms in Section VIII. We present the experimental results along with robotic demonstrations in Section IX and finally conclude the paper in Section X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There is a lot of recent work in improving robotic perception in order to enable the robots to perform many useful tasks. These works includes 3D modeling of indoor environments <ref type="bibr" target="#b18">(Henry et al., 2012)</ref>, semantic labeling of environments by modeling objects and their relations to other objects in the scene <ref type="bibr" target="#b31">(Koppula et al., 2011;</ref><ref type="bibr" target="#b36">Lai et al., 2011b;</ref><ref type="bibr" target="#b54">Rosman and Ramamoorthy, 2011;</ref><ref type="bibr" target="#b4">Anand et al., 2012)</ref>, developing frameworks for object recognition and pose estimation for manipulation <ref type="bibr" target="#b7">(Collet et al., 2011)</ref>, object tracking for 3D object modeling <ref type="bibr">(Krainin et al., 2011), etc.</ref> Robots are now becoming more integrated in human environments and are being used in assistive tasks such as automatically interpreting and executing cooking recipes <ref type="bibr" target="#b5">(Bollini et al., 2012)</ref>, robotic laundry folding <ref type="bibr" target="#b43">(Miller et al., 2011)</ref> and arranging a disorganized house <ref type="bibr" target="#b24">(Jiang et al., 2012b;</ref><ref type="bibr" target="#b26">Jiang and Saxena, 2012)</ref>. Such applications makes it critical for the robots to understand both object affordances as well as human activities in order to work alongside with humans. We describe the recent advances in the various aspects of this problem here.</p><p>Object affordances. An important accept of the human environment a robot needs to understand is the object affordances. Most of the work within the robotics community related to affordances has focused on predicting opportunities for interaction with an object either by using visual clues <ref type="bibr" target="#b19">Hermans et al., 2011;</ref><ref type="bibr" target="#b3">Aldoma et al., 2012)</ref> or through observation of the effects of exploratory behaviors <ref type="bibr" target="#b45">(Montesano et al., 2008;</ref><ref type="bibr" target="#b52">Ridge et al., 2009;</ref><ref type="bibr" target="#b44">Moldovan et al., 2012)</ref>. For instance, <ref type="bibr" target="#b64">Sun et al. (2009)</ref> proposed a probabilistic graphical model that leverages visual object categorization for learning affordances and <ref type="bibr" target="#b19">Hermans et al. (2011)</ref> proposed the use of physical and visual attributes as a mid-level representation for affordance prediction. <ref type="bibr" target="#b3">Aldoma et al. (2012)</ref> proposed a method to find affordances which depends solely on the objects of interest and their position and orientation in the scene. These methods, do not consider the object affordances in human context, i.e. how the objects are usable by humans. We show that human-actor based affordances are essential for robots working in human spaces in order for them to interact with objects in a humandesirable way.</p><p>There is some recent work in interpreting human actions and interaction with objects <ref type="bibr" target="#b41">(Lopes and Santos-Victor, 2005;</ref><ref type="bibr" target="#b60">Saxena et al., 2008;</ref><ref type="bibr" target="#b2">Aksoy et al., 2011;</ref><ref type="bibr" target="#b30">Konidaris et al., 2012;</ref><ref type="bibr" target="#b38">Li et al., 2012)</ref> in context of learning to perform actions from demonstrations. <ref type="bibr" target="#b41">Lopes and Santos-Victor (2005)</ref> use context from objects in terms of possible grasp affordances to focus the attention of their action recognition system and reduce ambiguities. In contrast to these methods, we propose a model to learn human activities spanning over long durations and action-dependent affordances which make robots more capable in performing assistive tasks as we later describe in Section IX-E. <ref type="bibr" target="#b60">Saxena et al. (2008)</ref> used supervised learning to detect grasp affordances for grasping novel objects. <ref type="bibr" target="#b38">Li et al. (2012)</ref> used a cascaded classification model to model the interaction between objects, geometry and depths. However, their work is limited to 2D images. In recent work, <ref type="bibr" target="#b23">Jiang et al. (2012a)</ref> used a data-driven technique for learning spatial affordance maps for objects. This work is different from ours in that we consider semantic affordances with spatio-temporal grounding useful for activity detection. <ref type="bibr" target="#b49">Pandey and Alami (2010;</ref> proposed mightability maps and taskability graphs that capture affordances such as reachability and visibility, while considering efforts required to be performed by the agents. While this work manually defines object affordances in terms of kinematic and dynamic constraints, our approach learns them from observed data.</p><p>Human activity detection from 2D videos. There has been a lot of work on human activity detection from images <ref type="bibr" target="#b70">(Yang et al., 2010;</ref><ref type="bibr" target="#b72">Yao et al., 2011)</ref> and from videos <ref type="bibr" target="#b37">(Laptev et al., 2008;</ref><ref type="bibr" target="#b40">Liu et al., 2009;</ref><ref type="bibr" target="#b21">Hoai et al., 2011;</ref><ref type="bibr" target="#b62">Shi et al., 2011;</ref><ref type="bibr" target="#b42">Matikainen et al., 2012;</ref><ref type="bibr" target="#b51">Pirsiavash and Ramanan, 2012;</ref><ref type="bibr" target="#b53">Rohrbach et al., 2012;</ref><ref type="bibr" target="#b59">Sadanand and Corso, 2012;</ref><ref type="bibr" target="#b67">Tang et al., 2012)</ref>. Here, we discuss works that are closely related to ours, and refer the reader to <ref type="bibr" target="#b0">Aggarwal and Ryoo (2011)</ref> for a survey of the field. Most works (e.g. <ref type="bibr" target="#b21">Hoai et al., 2011;</ref><ref type="bibr" target="#b62">Shi et al., 2011;</ref><ref type="bibr" target="#b42">Matikainen et al., 2012)</ref> consider detecting actions at a 'sub-activity' level (e.g. walk, bend, and draw) instead of considering high-level activities. Their methods range from discriminative learning techniques for joint segmentation and recognition <ref type="bibr" target="#b62">(Shi et al., 2011;</ref><ref type="bibr" target="#b21">Hoai et al., 2011)</ref> to combining multiple models <ref type="bibr" target="#b42">(Matikainen et al., 2012)</ref>. Some works, such as <ref type="bibr" target="#b67">Tang et al. (2012)</ref>, consider high-level activities. <ref type="bibr" target="#b67">Tang et al. (2012)</ref> proposed a latent model for high-level activity classification and have the advantage of requiring only high-level activity labels for learning. None of these methods explicitly consider the role of objects or object affordances that not only help in identifying sub-activities and high-level activities, but are also important for several robotic applications (e.g. <ref type="bibr" target="#b33">Kormushev et al., 2010)</ref>.</p><p>Some recent works <ref type="bibr" target="#b16">(Gupta et al., 2009;</ref><ref type="bibr" target="#b71">Yao and Fei-Fei, 2010;</ref><ref type="bibr" target="#b1">Aksoy et al., 2010;</ref><ref type="bibr" target="#b22">Jiang et al., 2011a;</ref><ref type="bibr" target="#b51">Pirsiavash and Ramanan, 2012)</ref> show that modeling the interaction between human poses and objects in 2D videos results in a better performance on the tasks of object detection and activity recognition. However, these works cannot capture the rich 3D relations between the activities and objects, and are also fundamentally limited by the quality of the human pose inferred from the 2D data. More importantly, for activity recognition, the object affordance matters more <ref type="bibr">Fig. 3</ref>. Pictorial representation of the different types of nodes and relationships modeled in part of the cleaning objects activity comprising three sub-activities: reaching, opening and scrubbing. (See Section III.) than its category. <ref type="bibr" target="#b28">Kjellström et al. (2011)</ref> used a factorial CRF to simultaneously segment and classify human hand actions, as well as classify the object affordances involved in the activity from 2D videos. However, this work is limited to classifying only hand actions and does not model interactions between the objects. We consider complex full-body activities and show that modeling object-object interactions is important as objects have affordances even if they are not directly interacted with human hands.</p><p>Human activity detection from RGB-D videos. Recently, with the availability of inexpensive RGB-D sensors, some works <ref type="bibr" target="#b39">(Li et al., 2010;</ref><ref type="bibr" target="#b46">Ni et al., 2011;</ref><ref type="bibr" target="#b66">Sung et al., 2011;</ref><ref type="bibr" target="#b74">Zhang and Parker, 2011;</ref><ref type="bibr" target="#b65">Sung et al., 2012)</ref> consider detecting human activities from RGB-D videos. <ref type="bibr" target="#b39">Li et al. (2010)</ref> proposed an expandable graphical model, to model the temporal dynamics of actions and use a bag of 3D points to model postures. They use their method to classify 20 different actions which are used in context of interacting with a game console, such as draw tick, draw circle, hand clap, etc. <ref type="bibr" target="#b74">Zhang and Parker (2011)</ref> designed 4D local spatio-temporal features and use an LDA classifier to identify six human actions such as lifting, removing, waving, etc., from a sequence of RGB-D images. However, both these works only address detecting actions which span short time periods. <ref type="bibr" target="#b46">Ni et al. (2011)</ref> also designed feature representations such as spatiotemporal interest points and motion history images which incorporate depth information in order to achieve better recognition performance. <ref type="bibr" target="#b47">Panangadan et al. (2010)</ref> used data from laser rangefinder to model observed movement patterns and interactions between persons. They segment tracks into activities based on difference in displacement distributions in each segment, and use a Markov model for capturing the transition probabilities. None of these works model interactions with objects which provide useful information for recognizing complex activities.</p><p>In recent previous work from our lab, <ref type="bibr" target="#b66">Sung et al. (2011</ref><ref type="bibr" target="#b65">Sung et al. ( , 2012</ref> proposed a hierarchical maximum entropy Markov model to detect activities from RGB-D videos and treat the sub-activities as hidden nodes in their model. However, they use only human pose information for detecting activities and also constrain the number of sub-activities in each activity.</p><p>In contrast, we model context from object interactions along with human pose, and also present a better learning algorithm. (See Section IX for further comparisons.) <ref type="bibr" target="#b14">Gall et al. (2011)</ref> also use depth data to perform sub-activity (referred to as action) classification and functional categorization of objects. Their method first detects the sub-activity being performed using the estimated human pose from depth data, and then performs object localization and clustering of the objects into functional categories based on the detected subactivity. In contrast, our proposed method performs joint sub-activity and affordance labeling and uses these labels to perform high-level activity detection. All of the above works lack a unified framework that combines all of the information available in human interaction activities and therefore we propose a model that captures both the spatial and temporal relations between object affordances and human poses to perform joint object affordance and activity detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERVIEW</head><p>Over the course of a video, a human may interact with several objects and perform several sub-activities over time.</p><p>In this section we describe at a high level how we process the RGB-D videos and model the various properties for affordance and activity labeling.</p><p>Given the raw data containing the color and depth values for every pixel in the video, we first track the human skeleton using Openni's skeleton tracker 2 for obtaining the locations of the various joints of the human skeleton. However these values are not very accurate, as the Openni's skeleton tracker is only designed to track human skeletons in clutter-free environments and without any occlusion of the body parts. In real-world human activity videos, some body parts are often occluded and the interaction with the objects hinders accurate skeleton tracking. We show that even with such noisy data, our method gets high accuracies by modeling the mutual context between the affordances and sub-activities.</p><p>We then segment the object being used in the activity and track them through out the 3D video, as explained in detail in Section V. We model the activities and affordances by defining a MRF over the spatio-temporal sequence we get from an RGB-D video, as illustrated in <ref type="figure">Fig. 3</ref>. MRFs are a workhorse of machine learning, and have been successfully applied to many applications (e.g. <ref type="bibr" target="#b61">Saxena et al., 2009)</ref>. Please see <ref type="bibr" target="#b29">Koller and Friedman (2009)</ref> for a review. If we build our graph with nodes for objects and sub-activities for each time instant (at 30 fps), then we will end up with quite a large graph. Furthermore, such a graph would not be able to model meaningful transitions between the sub-activities because they take place over a long-time (e.g. a few seconds). Therefore, in our approach we first segment the video into small temporal segments, and our goal is to label each segment with appropriate labels. We try to over-segment, so that we end up with more segments and avoid merging two subactivities into one segment. Each of these segments occupies a small length of time and therefore, considering nodes per segment gives us a meaningful and concise representation for the graph G. With such a representation, we can model meaningful transitions of a sub-activity following another, e.g. pouring followed by moving. Our temporal segmentation algorithms are described in Section VI. The outputs from the skeleton and object tracking along with the segmentation information and RGB-D videos are then used to generate the features described in Section VII.</p><p>Given the tracks and segmentation, the graph structure (G) is constructed with a node for each object and a node for the sub-activity of a temporal segment, as shown in <ref type="figure">Fig. 3</ref>. The nodes are connected to each other within a temporal segment and each node is connected to its temporal neighbors by edges as further described in Section IV. The learning and inference algorithms for our model are presented in Section VIII. We capture the following properties in our model:</p><p>• Affordance-sub-activity relations. At any given time, the affordance of the object depends on the sub-activity it is involved in. For example, a cup has the affordance of 'pour-to' in a pouring sub-action and has the affordance of 'drinkable' in a drinking sub-action. We compute relative geometric features between the object and the human's skeletal joints to capture this. These features are incorporated in the energy function as described in Eq. (6).</p><p>• Affordance-affordance relations. Objects have affordances even if they are not interacted directly with by the human, and their affordances depend on the affordances of other objects around them. For example, in the case of pouring from a pitcher to a cup, the cup object and the human's skeletal joints to capture this. These features are incorporated in the energy function as described in Eq. (6).</p><p>• Affordance -affordance relations. Objects have affordances even if they are not interacted directly with by the human, and their affordances depend on the affordances of other objects around them. E.g., in the case of pouring from a pitcher to a cup, the cup is not interacted by the human directly but has the affordance 'pour-to'. We therefore use relative geometric features such as "on top of", "nearby", "in front of", etc., to model the affordance -affordance relations. These features are incorporated in the energy function as described in Eq. (5).</p><p>• Sub-activity change over time. Each activity consists of a sequence of sub-activities that change over the course of performing the activity. We model this by incorporating temporal edges in G. Features capturing the change in human pose across the temporal segments are used to model the sub-activity change over time and the corresponding energy term is given in Eq. (8).</p><p>• Affordance change over time. The object affordances depend on the sub-activity being performed and hence change along with the sub-activity over time. We model the temporal change in affordances of each object using features such as change in appearance and location of the object over time. These features are incorporated in the energy function as described in Eq. <ref type="formula" target="#formula_11">(7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODEL</head><p>Our goal is to perform joint activity and object affordance labeling of RGB-D videos. We model the spatio-temporal structure of an activity using a model isomorphic to a Markov Random Field with log-linear node and pairwise o be a binary variable representing th where k ∈ K o for object nodes and nodes. All k binary variables togeth a node. Let V s o denote set of object v s a denote the sub-activity node of s The energy term associated with l is denoted by E o and is defined as potentials ψ o (i) as:</p><formula xml:id="formula_0">E o = i∈V o ψ o (i) = i∈V o k∈K o</formula><p>where φ o (i) denotes the feature m affordance of the object node i in its segment through a vector of features vector for each affordance class in K energy term, E a , for labeling the s is defined as the sum of sub-activity</p><formula xml:id="formula_1">E a = i∈V a ψ a (i) = i∈V a k∈K a</formula><p>where φ a (i) denotes the feature m activity node i through a vector o one weight vector for each sub-acti For all segments s, there is an nodes in V s o to each other, denoted activity node v s a , denoted by E oa . relationships within the objects, a and the human pose within a segm as 'object -object interactions' an interactions' in the <ref type="figure">Fig. 3</ref> respective</p><p>The sub-activity node of segmen sub-activity nodes in segments</p><formula xml:id="formula_2">(s − v s o 1 v s o 2 v s o 3 v s−1 a v s+1 a 1 v s o 1 v s o 2 v s o 3 v s−1 a v s+1 a v s o 1 v s o 2 v s o 3 v s−1 a v s+1 a</formula><p>• Affordance -sub-activity relations. At any given time, the affordance of the object depends on the subactivity it is involved in. For example, a cup has the affordance of 'pour-to' in a pouring sub-action and has the affordance of 'drinkable' in a drinking sub-action. We compute relative geometric features between the object and the human's skeletal joints to capture this. These features are incorporated in the energy function as described in Eq. (6).</p><p>• Affordance -affordance relations. Objects have affordances even if they are not interacted directly with by the human, and their affordances depend on the affordances of other objects around them. E.g., in the case of pouring from a pitcher to a cup, the cup is not interacted by the human directly but has the affordance 'pour-to'. We therefore use relative geometric features such as "on top of", "nearby", "in front of", etc., to model the affordance -affordance relations. These features are incorporated in the energy function as described in Eq. (5).</p><p>• Sub-activity change over time. Each activity consists of a sequence of sub-activities that change over the course of performing the activity. We model this by incorporating temporal edges in G. Features capturing the change in human pose across the temporal segments are used to model the sub-activity change over time and the corresponding energy term is given in Eq. (8).</p><p>• Affordance change over time. The object affordances depend on the sub-activity being performed and hence change along with the sub-activity over time. We model the temporal change in affordances of each object using features such as change in appearance and location of the object over time. These features are incorporated in the energy function as described in Eq. (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODEL</head><p>Our goal is to perform joint activity and object affordance labeling of RGB-D videos. We model the spatio-temporal structure of an activity using a model isomorphic to a Markov Random Field with log-linear node and pairwise defined by an undirected graph G = (V,</p><p>We now describe the structure of thi the corresponding potentials. There are in G: object nodes denoted by V o and denoted by V a . Let K a denote the set of and K o denote the set of object afford be a binary variable representing the no where k ∈ K o for object nodes and k ∈ nodes. All k binary variables together re a node. Let V s o denote set of object node v s a denote the sub-activity node of segm The energy term associated with label is denoted by E o and is defined as the potentials ψ o (i) as:</p><formula xml:id="formula_3">E o = i∈V o ψ o (i) = i∈V o k∈K o y k i w</formula><p>where φ o (i) denotes the feature map d affordance of the object node i in its corr segment through a vector of features, and vector for each affordance class in K o . S energy term, E a , for labeling the sub-a is defined as the sum of sub-activity nod</p><formula xml:id="formula_4">E a = i∈V a ψ a (i) = i∈V a k∈K a y k i w</formula><p>where φ a (i) denotes the feature map activity node i through a vector of fea one weight vector for each sub-activity For all segments s, there is an edge nodes in V s o to each other, denoted by E activity node v s a , denoted by E oa . Thes relationships within the objects, and b and the human pose within a segment as 'object -object interactions' and 's interactions' in the <ref type="figure">Fig. 3</ref> respectively.</p><p>The sub-activity node of segment s sub-activity nodes in segments (s − 1) Temporal Segment 's' <ref type="figure">Fig. 4</ref>. An illustrative example of our Markov random field (MRF) for three temporal segments, with one activity node, v s a , and three object nodes,</p><formula xml:id="formula_5">{v s o 1 ,v s o 2 ,v s o 3 }, per temporal segment.</formula><p>is not interacted with by the human directly but has the affordance 'pour-to'. We therefore use relative geometric features such as 'on top of', 'nearby', 'in front of', etc., to model the affordance-affordance relations. These features are incorporated in the energy function as described in Eq. (5).</p><p>• Sub-activity change over time. Each activity consists of a sequence of sub-activities that change over the course of performing the activity. We model this by incorporating temporal edges in G. Features capturing the change in human pose across the temporal segments are used to model the sub-activity change over time and the corresponding energy term is given in Eq. (8).</p><p>• Affordance change over time. The object affordances depend on the sub-activity being performed and hence change along with the sub-activity over time. We model the temporal change in affordances of each object using features such as change in appearance and location of the object over time. These features are incorporated in the energy function as described in Eq. <ref type="formula" target="#formula_11">(7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODEL</head><p>Our goal is to perform joint activity and object affordance labeling of RGB-D videos. We model the spatio-temporal structure of an activity using a model isomorphic to a MRF with log-linear node and pairwise edge potentials (see <ref type="figure">Fig. 3</ref> and 4 for an illustration). The MRF is represented as a graph G = (V, E). Given a temporally segmented 3D video, with temporal segments s ∈ {1, ..., N }, our goal is to predict a labeling y = (y 1 , ..., y N ) for the video, where y s is the set of sub-activity and object affordance labels for the temporal segment s. Our input is a set of features x extracted from the segmented 3D video as described in Section VII. The predictionŷ is computed as the argmax of a energy function E w (x, y) that is parameterized by weights w.</p><formula xml:id="formula_6">y = argmax y E w (x, y) (1) E w (x, y) = E o + E a + E oo + E oa + E t oo + E t aa<label>(2)</label></formula><p>The energy function consists of six types of potentials that define the energy of a particular assignment of sub-activity and object affordance labels to the sequence of segments in the given video. The various potentials capture the dependencies between the sub-activity and object affordance labels as defined by an undirected graph G = (V, E). We now describe the structure of this graph along with the corresponding potentials. There are two types of nodes in G: object nodes denoted by V o and sub-activity nodes denoted by V a . Let K a denote the set of sub-activity labels, and K o denote the set of object affordance labels. Let y k i be a binary variable representing the node i having label k, where k ∈ K o for object nodes and k ∈ K a for sub-activity nodes. All k binary variables together represent the label of a node. Let V s o denote set of object nodes of segment s, and v s a denote the sub-activity node of segment s. <ref type="figure">Figure 4</ref> shows the graph structure for three temporal segments for an activity with three objects.</p><p>The energy term associated with labeling the object nodes is denoted by E o and is defined as the sum of object node potentials ψ o (i) as:</p><formula xml:id="formula_7">Eo = i∈Vo ψo(i) = i∈Vo k∈Ko y k i w k o · φo(i) ,<label>(3)</label></formula><p>where φ o (i) denotes the feature map describing the object affordance of the object node i in its corresponding temporal segment through a vector of features, and there is one weight vector for each affordance class in K o . Similarly, we have an energy term, E a , for labeling the sub-activity nodes which is defined as the sum of sub-activity node potentials as</p><formula xml:id="formula_8">Ea = i∈Va ψa(i) = i∈Va k∈Ka y k i w k a · φa(i) ,<label>(4)</label></formula><p>where φ a (i) denotes the feature map describing the subactivity node i through a vector of features, and there is one weight vector for each sub-activity class in K a . For all segments s, there is an edge connecting all the nodes in V s o to each other, denoted by E oo , and to the subactivity node v s a , denoted by E oa . These edges signify the relationships within the objects, and between the objects and the human pose within a segment and are referred to as 'object-object interactions' and 'sub-activity-object interactions' in the <ref type="figure">Fig. 3</ref>, respectively.</p><p>The sub-activity node of segment s is connected to the sub-activity nodes in segments (s − 1) and (s + 1). These temporal edges are denoted by E t aa . Similarly every object node of segment s is connected to the corresponding object nodes in segments (s−1) and (s+1), denoted by E t oo . These edges model the temporal interactions between the human poses and the objects, respectively, and are represented by dotted edges in the <ref type="figure">Fig. 3</ref>.</p><p>We have one energy term for each of the four interaction types and are defined as:</p><formula xml:id="formula_9">Eoo = (i,j)∈Eoo (l,k)∈Ko×Ko y l i y k j w lk oo · φoo(i, j) ,<label>(5)</label></formula><formula xml:id="formula_10">Eoa = (i,j)∈Eoa (l,k)∈Ko×Ka y l i y k j w lk oa · φoa(i, j) ,<label>(6)</label></formula><formula xml:id="formula_11">E t oo = (i,j)∈E t oo (l,k)∈Ko×Ko y l i y k j w t lk oo · φ t oo (i, j) ,<label>(7)</label></formula><formula xml:id="formula_12">E t aa = (i,j)∈E t aa (l,k)∈Ka×Ka y l i y k j w t lk aa · φ t aa (i, j) . (8)</formula><p>The feature maps φ oo (i, j) and φ oa (i, j) describe the interactions between pair of objects and interactions between an object and the human skeleton within a temporal segment, respectively, and the feature maps φ t oo (i, j) and φ t aa (i, j) describe the temporal interactions between objects and subactivities, respectively. Also, note that there is one weight vector for every pair of labels in each energy term.</p><p>Given G, we can rewrite the energy function based on individual node potentials and edge potentials compactly as follows:</p><formula xml:id="formula_13">Ew(x, y) = i∈Va k∈Ka y k i w k a · φa(i) + i∈Vo k∈Ko y k i w k o · φo(i) + t∈T (i,j)∈E t (l,k)∈T t y l i y k j w lk t · φt(i, j)<label>(9)</label></formula><p>where T is the set of the four edge types described above.</p><p>Writing the energy function in this form allows us to apply efficient inference and learning algorithms as described later in Section VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. OBJECT DETECTION AND TRACKING</head><p>For producing our graph G, we need as input the segments corresponding to the objects (but not their labels) and their tracks over time. In order to do so, we run pre-trained object detectors on a set of frames sampled from the video and then use particle filter tracker to obtain tracks of the detected objects. We then find consistent tracks that connect the various detected objects in order to find reliable object tracks. We present the details below.</p><p>Object Detection: We first train a set of 2D object detectors for the common objects present in our dataset (e.g. mugs, bowls). We use features that capture the inherent local and global properties of the object encompassing the appearance and the shape/geometry. Local features includes color histogram and the histogram of oriented gradients (HoG, <ref type="bibr" target="#b8">Dalal and Triggs, 2005)</ref> which provide the intrinsic properties of the target object while viewpoint features histogram (VFH, <ref type="bibr" target="#b56">Rusu et al., 2010)</ref> provides the global orientation of the normals from the object's surface. For training we used the RGB-D object dataset by <ref type="bibr" target="#b35">Lai et al. (2011a)</ref> and built a one-vs-all SVM classifier using the local features for each object class in order to obtain the probability estimates of each object class. We also build a k-nearest neighbor (kNN) classifier over VFH features. The kNN classifier provides the detection score as inverse of the distance between training and the test instance. We obtain a final classification score by adding the scores from these two classifiers.</p><p>At test time, for a given point cloud, we first reduce the set of 3D bounding boxes by only considering those that belong to a volume around the hands of the skeleton. This reduces the number of false detections as well as the detection time. We then run our SVM-based object detectors on the RGB image. This gives us the exact x and y coordinates of the possible detections. The predictions with score above a certain threshold are further examined by calculating the kNN score based on VFH features. To find the exact depth of the object, we do pyramidal window search inside the current 3D bounding box and get the highest scoring box. In order to remove the empty space and any outlier points within a bounding box, we shrink it towards the highestdensity region that captures 90% of the object points. These bounding box detections are ordered according to their final classification score.</p><p>Object Tracking: We used the particle filter tracker implementation 3 provided under the PCL library for tracking our target object. The tracker uses the color values and the normals to find the next probable state of the object.</p><p>Combining Object Detections with Tracking: We take the top detections, track them, and assign a score to each of them in order to compute the potential nodes in the graph G.</p><p>We start with building a graph with the initial bounding boxes as the nodes. In our current implementation, this method needs an initial guess on the 2D bounding boxes of the objects to keep the algorithm tractable. We can obtain this by considering only the tabletop objects by using a tabletop object segmenter (e.g. <ref type="bibr" target="#b58">Rusu et al., 2009)</ref>. We initialize the graph with these guesses. We then perform tracking through the video and grow the graph by adding a node for every object detection and connect two nodes with an edge if a track exists between their corresponding bounding boxes. Our object detection algorithm is run after every fixed number of frames, and the frames on which it is run are referred to as the detection frames. Each edge is assigned a weight corresponding to its track score as defined in Eq. (10). After the whole video is processed, the best track for every initial node in the graph is found by taking the highest weighted path starting at that node.</p><p>The object detections at the current frame are categorized into one of the following categories: {merged detection, isolated detection, ignored detection} based on their vicinity and similarity to the currently tracked objects as shown in <ref type="figure" target="#fig_1">Figure 5</ref>. If a detection occurs close to a currently tracked object and has high similarity with it, the detection can be merged with the current object track. Such detections are called merged detections. The detections which have high detection score but do not occur close to the current tracks are labeled as isolated detections and are tracked in both 3 http://www.willowgarage.com/blog/2012/01/17/tracking-3d-objectspoint-cloud-library directions. This helps in correcting the tracks which have gone wrong due to partial occlusions or missing due to full occlusions of the objects. The rest of the detections are labeled as ignored detections and are not tracked.</p><p>More formally, let d i j represent the bounding box of the j th detection in the i th detection frame and let D i j represent its tracking score. Letd i j represent the tracked bounding box at the current frame with the track starting from d i j . We define a similarity score, S(a, b), between two image bounding boxes, a and b, as the correlation score of the color histograms of the two bounding boxes. The track score of an edge e connecting the detections d i−1 k and d i j is given by</p><formula xml:id="formula_14">ts(e) = S(d i−1 k , d i j ) + S(d i−1 k , d i j ) + λD i j<label>(10)</label></formula><p>Finally, the best track of a given object bounding box b is the path having the highest cumulative track score from all paths originating at node corresponding to b in the graph, represented by the set P b</p><formula xml:id="formula_15">t(b) = argmax p∈P b e∈p ts(e).<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. TEMPORAL SEGMENTATION</head><p>We perform temporal segmentation of the frames in an activity in order to obtain groups of frames representing atomic movements of the human skeleton and objects in the activity. This will group similar frames into one segment, thus reducing the total number of nodes to be considered by the learning algorithm significantly.</p><p>There are several problems with naively performing one temporal segmentation-if we make a mistake here, then our followup activity detection would perform poorly. In certain cases, when the features are additive, methods based on dynamic programming <ref type="bibr" target="#b21">(Hoai et al., 2011;</ref><ref type="bibr" target="#b62">Shi et al., 2011;</ref><ref type="bibr" target="#b20">Hoai and De la Torre, 2012)</ref> could be used to search for an optimal segmentation. However, in our case, we have the following three challenges. First, the feature maps we consider are non-additive in nature, and the feature computation cost is exponential in the number of frames if we want to consider all the possible segmentations. Therefore, we cannot apply dynamic programming techniques to find the optimal segmentation. Second, the complex human-object interactions are poorly approximated with a linear dynamical system, therefore techniques such as <ref type="bibr" target="#b12">Fox et al. (2011)</ref> cannot be directly applied. Third, the boundary between two sub-activities is often not very clear, as people often start performing the next sub-activity before finishing the current sub-activity. The amount of overlap might also depend on which subactivities are being performed. Therefore, there may not be one optimal segmentation. In our work, we consider several temporal segmentations and propose a method to combine them in Section VIII-C.</p><p>We consider three basic methods for temporal segmentation of the video frames and generate a number of temporal segmentations by varying the parameters of these methods. The first method is uniform segmentation, in which we consider a set of continuous frames of fixed size as the temporal segment. There are two parameters for this method: the segment size and the offset (the size of the first segment). The other two segmentation methods use the graph-based segmentation proposed by <ref type="bibr" target="#b10">Felzenszwalb and Huttenlocher (2004)</ref> adapted to temporally segment the videos. The second method uses the sum of the Euclidean distances between the skeleton joints as the edge weights, whereas the third method uses the rate of change of the Euclidean distance as the edge weights. These methods consider smooth movements of the skeleton joints to belong to one segment and identify sudden changes in skeletal motion as the sub-activity boundaries.</p><p>In detail, we have one node per frame representing the skeleton in the graph based methods. Each node is connected to its temporal neighbor, therefore giving a chain graph. The algorithm begins with having each node as a separate segmentation, and iteratively merges the components if the edge weight is less than a certain threshold (computed based on the current segment size and a constant parameter). We obtain different segmentations by varying the parameter. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. FEATURES</head><p>For a given object node i, the node feature map φ o (i) is a vector of features representing the object's location in the scene and how it changes within the temporal segment. These features include the (x, y, z) coordinates of the object's centroid and the coordinates of the object's bounding box at the middle frame of the temporal segment. We also run a SIFT feature based object tracker <ref type="bibr" target="#b50">(Pele and Werman, 2008)</ref> to find the corresponding points between the adjacent frames and then compute the transformation matrix based on the matched image points. We add the transformation matrix corresponding to the object in the middle frame with respect to its previous frame to the features in order to capture the object's motion information. In addition to the above 4 Details: In order to handle occlusions, we only use the upper body skeleton joints for computing the edge weights that are estimated more reliably by the skeleton tracker. When changing the parameters for the three segmentation methods for obtaining multiple segmentations, we select the parameters such that we always err on the side of over-segmentation instead of under-segmentation. This is because our learning model can handle oversegmentation by assigning the same label to the consecutive segments for the same sub-activity, but under-segmentation is is bad as the model can only assign one label to that segment. features, we also compute the total displacement and the total distance moved by the object's centroid in the set of frames belonging to the temporal segment. We then perform cumulative binning of the feature values into 10 bins. In our experiments, we have φ o (i) ∈ R 180 . Similarly, for a given sub-activity node i, the node feature map φ a (i) gives a vector of features computed using the human skeleton information obtained from running Openni's skeleton tracker 5 on the RGB-D video. We compute the features described above for each of the upper-skeleton joint (neck, torso, left shoulder, left elbow, left palm, right shoulder, right elbow and right palm) locations relative to the subject's head location. In addition to these, we also consider the body pose and hand position features as described by <ref type="bibr" target="#b65">Sung et al. (2012)</ref>, thus giving us φ a (i) ∈ R 1030 .</p><p>The edge feature maps φ t (i, j) describe the relationship between node i and j. They are used for modeling four types of interactions: object-object within a temporal segment, object-sub-activity within a temporal segment, objectobject between two temporal segments, and sub-activitysub-activity between two temporal segments. For capturing the object-object relations within a temporal segment, we compute relative geometric features such as the difference in (x, y, z) coordinates of the object centroids and the distance between them. These features are computed at the first, middle and last frames of the temporal segment along with minimum and maximim of their values across all frames in the temporal segment to capture the relative motion information. This gives us φ 1 (i, j) ∈ R 200 . Similarly for object-sub-activity relation features φ 2 (i, j) ∈ R 400 , we use the same features as for the object-object relation features, but we compute them between the upper-skeleton joint locations and each object's centroid. The temporal relational features capture the change across temporal segments and we use the vertical change in position and the distance between the corresponding object and the joint locations. This gives us φ 3 (i, j) ∈ R 40 and φ 4 (i, j) ∈ R 160 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. INFERENCE AND LEARNING ALGORITHM</head><p>A. Inference.</p><p>Given the model parameters w, the inference problem is to find the best labelingŷ for a new video x, i.e. solving the argmax in Eq. (1) for the discriminant function in Eq. (9). This is a NP hard problem. However, its equivalent formulation as the following mixed-integer program has a linear relaxation which can be solved efficiently as a quadratic pseudo-Boolean optimization problem using a graph-cut method <ref type="bibr" target="#b55">(Rother et al., 2007)</ref>.</p><formula xml:id="formula_16">y = argmax y max z i∈Va k∈Ka y k i w k a · φa(i) + i∈Vo k∈Ko y k i w k o · φo(i) + t∈T (i,j)∈E t (l,k)∈T t z lk ij w lk t · φt(i, j) (12) ∀i, j, l, k : z lk ij ≤ y l i , z lk ij ≤ y k j , y l i + y k j ≤ z lk ij + 1, z lk ij , y l i ∈ {0, 1}<label>(13)</label></formula><p>Note that the products y l i y k j have been replaced by auxiliary variables z lk ij . Relaxing the variables z lk ij and y l i to the interval [0, 1] results in a linear program that can be shown to always have half-integral solutions (i.e. y l i only take values {0, 0.5, 1} at the solution) <ref type="bibr" target="#b17">(Hammer et al., 1984)</ref>. Since every node in our experiments has exactly one class label, we also consider the linear relaxation from above with the additional constraints ∀i ∈ V a : l∈Ka y l i = 1 and ∀i ∈ V o : l∈Ko y l i = 1. This problem can no longer be solved via graph cuts. We compute the exact mixed integer solution including these additional constraint using a generalpurpose MIP solver 6 during inference.</p><p>In our experiments, we obtain a processing rate of 74.9 frames/second for inference and 16.0 frames/second end-toend (including feature computation cost) on a 2.93 GHz Intel processor with 16 GB of RAM on Linux. In detail, the MIP solver takes 6.94 seconds for a typical video with 520 frames and the corresponding graph has 12 sub-activity nodes and 36 object nodes, i.e. 15908 variables. This is the time corresponding to solving the argmax in Eq. (12-13) and does not involve the feature computation time. The time taken for end-to-end classification including feature generation is 32.5 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning.</head><p>We take a large-margin approach to learning the parameter vector w of Eq. (9) from labeled training examples 6 http://www.tfinley.net/software/pyglpk/readme.html (x 1 , y 1 ), ..., (x M , y M ) <ref type="bibr" target="#b68">(Taskar et al., 2004;</ref><ref type="bibr" target="#b69">Tsochantaridis et al., 2004)</ref>. Our method optimizes a regularized upper bound on the training error</p><formula xml:id="formula_17">R(h) = 1 M M m=1 ∆(ym,ŷm),</formula><p>whereŷ m is the optimal solution of Eq. <ref type="figure" target="#fig_0">(1) and ∆(y,ŷ)</ref> is the loss function defined as ∆(y,ŷ) = i∈Vo k∈Ko |y k i −ŷ k i | + i∈Va k∈Ka |y k i −ŷ k i |.</p><p>To simplify notation, note that Eq. (12) can be equivalently written as w T Ψ(x, y) by appropriately stacking the w k a , w k o and w lk t into w and the y k i φ a (i), y k i φ o (i) and z lk ij φ t (i, j) into Ψ(x, y), where each z lk ij is consistent with Eq. (13) given y. Training can then be formulated as the following convex quadratic program :</p><formula xml:id="formula_18">min w,ξ 1 2 w T w + Cξ (14) s.t. ∀ȳ1, ...,ȳM ∈ {0, 0.5, 1} N ·K : 1 M w T M m=1 [Ψ(xm, ym) − Ψ(xm,ȳm)] ≥ ∆(ym,ȳm) − ξ</formula><p>While the number of constraints in this QP is exponential in M , N and K, it can nevertheless be solved efficiently using the cutting-plane algorithm . The algorithm needs access to an efficient method for computinḡ ym = argmax y∈{0,0.5,1} N ·K w T Ψ(xm, y) + ∆(ym, y) .</p><p>Due to the structure of ∆(., .), this problem is identical to the relaxed prediction problem in Eqs. (12)-(13) and can be solved efficiently using graph cuts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multiple Segmentations</head><p>Segmenting an RGB-D video in time can be noisy, and multiple segmentations may be valid. Therefore, we perform multiple segmentations by using different methods and criterion of segmentation (see Section VI for details). Thus, we get a set H of multiple segmentations, and let h n be the n th segmentation. A discriminant function E w hn (x hn , y hn ) can now be defined for each h n as in Eq. (9). We now define a score function g θ (y hn , y) which gives a score for assigning the labels of the segments from y hn to y, where K = K o ∪ K a . Here, θ k n can be interpreted as the confidence of labeling the segments of label k correctly in the n th segmentation hypothesis. We want to find the labeling that maximizes the assignment score across all the <ref type="figure">Fig. 6</ref>. Example shots of reaching (first row), placing (second row), moving (third row), drinking (fourth row) and eating (fourth row) sub-activities from our dataset. There are significant variations in the way the subjects perform the sub-activity. This formulation is equivalent to considering the labelings y hn over the segmentations as unobserved variables. It is possible to use the latent structural SVM  to solve this, but it becomes intractable if the size of the segmentation hypothesis space is large. Therefore we propose an approximate two-step learning procedure to address this. For a given set of segmentations H, we first learn the parameters w hn independently as described in Section IV. We then train the parameters θ on a separate held-out training dataset. This can now be formulated as a QP</p><formula xml:id="formula_20">min θ 1 2 θ T θ − hn∈H g θn (y hn , y) s.t. ∀k ∈ K : |H| n=1 θ k n = 1<label>(18)</label></formula><p>Using the fact that the objective function defined in Eq. (17) is convex, we design an iterative two-step procedure where we solve for y hn , ∀h n ∈ H in parallel and then solve for y. This method is guaranteed to converge, and when the number of variables scales linearly with the number of segmentation hypothesis considered, the original problem in Eq. (17) will become considerably slow, but our method will still scale. More formally, we iterate between the following two problems:</p><formula xml:id="formula_21">y hn = argmax y hn E w hn (x hn , y hn ) + g θn (y hn ,ŷ) (19) y = argmax y g θn (ŷ hn , y)<label>(20)</label></formula><p>D. High-level Activity Classification.</p><p>For classifying the high-level activity, we compute the histograms of sub-activity and affordance labels and use them as features. However, some high-level activities, such as stacking objects and unstacking objects, have the same subactivity and affordance sequences. Occlusion of objects plays a major role in being able to differentiate such activities. Therefore, we compute additional occlusion features by dividing the video into n uniform length segments and finding the fraction of objects that are occluded fully or partially in the temporal segments. We then train a multi-class SVM classifier on training data using these features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data</head><p>We test our model on two 3D activity datasets: Cornell Activity Dataset -60 (CAD-60, <ref type="bibr" target="#b65">Sung et al., 2012)</ref> and one that we collected. The CAD-60 dataset has 60 RGB-D videos of four different subjects performing 12 high-level activity classes. However, some of these activity classes contain only one sub-activity (e.g. working on a computer, cooking (stirring), etc.) and do not contain object interactions (e.g. talking on couch, relaxing on couch).</p><p>We collected the CAD-120 dataset (available at: http://pr.cs.cornell.edu/humanactivities, along with the code), which contains 120 activity sequences of ten different highlevel activities performed by four different subjects, where each high-level activity was performed three times. We thus have a total of 61,585 RGB-D video frames in our dataset. The high-level activities are {making cereal, taking medicine, stacking objects, unstacking objects, microwaving food, picking objects, cleaning objects, taking food, arranging objects, having a meal}. The subjects were only given a high-level description of the task, 7 and were asked to perform the activities multiple times with different objects. For example, the stacking and unstacking activities were performed with pizza boxes, plates and bowls. They performed the activities through a long sequence of sub-activities, which varied from subject to subject significantly in terms of length of the subactivities, order of the sub-activities as well as in the way they executed the task. <ref type="table" target="#tab_0">Table II</ref> specifies the set of subactivities involved in each high-level activity. The camera was mounted so that the subject was in view (although the subject may not be facing the camera), but often there were significant occlusions of the body parts. See <ref type="figure">Fig. 2 and Fig. 6</ref> for some examples.</p><p>We labeled our CAD-120 dataset with the sub-activity and the object affordance labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object Tracking Results</head><p>In order to evaluate our object detection and tracking method, we have generated the ground-truth bounding boxes of the objects involved in the activities. We do this by manually labeling the object bounding boxes in the images corresponding to every 50 th frame. We compute the bounding boxes in the rest of the frames by tracking using SIFT feature matching <ref type="bibr" target="#b50">(Pele and Werman, 2008)</ref>, while enforcing depth consistency across the time frames for obtaining reliable object tracks. <ref type="figure" target="#fig_3">Fig. 7</ref> shows the visual output of our tracking algorithm. The center of the bounding box for each frame of the output is marked with a blue dot and that of the ground-truth is marked with a red dot. We compute the overlap of the bounding boxes obtained from our tracking method with the generated ground-truth bounding boxes. <ref type="table" target="#tab_0">Table III</ref> shows the percentage overlap with the ground-truth when considering tracking from the given bounding box in the first frame both with and without object detections. As can be seen from <ref type="table" target="#tab_0">Table III</ref>, our tracking algorithm produces greater than 10% overlap with the ground truth bounding boxes for 77.8% of the frames. Since, we only require that an approximate bounding box of the objects are given, 10% overlap is sufficient. We study the effect of the errors in tracking on the performance of our algorithm in Section IX-D.  <ref type="table" target="#tab_0">Table IV</ref> shows the precision and recall of the highlevel activities on the CAD-60 dataset <ref type="bibr" target="#b65">(Sung et al., 2012)</ref>. Following <ref type="bibr">Sung et al.'s (2012)</ref> experiments, we considered the same five groups of activities based on their location, and learnt a separate model for each location. To make it a fair comparison, we do not assume perfect segmentation of subactivities and do not use any object information. Therefore, we train our model with only sub-activity nodes and consider segments of uniform size (20 frames per segments). We consider only a subset of our features described in Section IV that are possible to compute from the tracked human skeleton and RGB-D data provided in this dataset. <ref type="table" target="#tab_0">Table IV</ref> shows that our model significantly outperforms Sung et al.'s MEMM model even when using only the sub-activity nodes and a simple segmentation algorithm.    74.2 ± 0.7 15.9 ± 2.7 16.0 ± 2.5 56.2 ± 0.4 39.6 ± 0.5 41.0 ± 0.6 34.7 ± 2.9 24.2 ± 1.5 35.8 ± 2.2 SVM multiclass 75.6 ± 1.8 40.6 ± 2.4 37.9 ± 2.0 58.0 ± 1.2 47.0 ± 0.6 41.6 ± 2.6 30.6 ± 3.5 27.4 ± 3.6 31.2 ± 3.7 MEMM <ref type="bibr" target="#b65">(Sung et al., 2012)</ref> ------26.4 ± 2.0 23.7 ± 1.0 23.7 ± 1.0 object only 86.9 ± 1.0 72.7 ± 3.8 63.1 ± 4.3 ---59.7 ± 1.8 56.3 ± 2.2 58.3 ± 1.9 sub-activity only ---71.9 ± 0.8 60.9 ± 2.2 51.9 ± 0.9 27.4 ± 5.2 31.8 ± 6.3 27.7 ± 5.3 no temporal interactions 87.0 ± 0.8 79.8 ± 3.6 66.1 ± 1.5 76.0 ± 0.6 74.5 ± 3.5 66.  <ref type="table">Table V</ref> shows the performance of various models on object affordance, sub-activity and high-level activity labeling. These results are obtained using 4-fold cross-validation and averaging performance across the folds. Each fold constitutes the activities performed by one subject, therefore the model is trained on activities of three subjects and tested on a new subject. We report both the micro and macro averaged precision and recall over various classes along with standard error. Since our algorithm can only predict one label for each segment, micro precision and recall are same as the percent-      age of correctly classified segments. Macro precision and recall are the averages of precision and recall respectively for all classes.</p><p>Assuming ground-truth temporal segmentation is given, the results for our full model are shown in <ref type="table">Table V</ref> on line 10, its variations on lines 5-9 and the baselines on lines 1-4. The results in lines 11-14 correspond to the case when temporal segmentation is not assumed. In comparison to a basic SVM multiclass model ) (referred to as SVM multiclass when using all features and image only when using only image features), which is equivalent to only considering the nodes in our MRF without any edges, our model performs significantly better. We also compare with the high-level activity classification results obtained from the method presented in <ref type="bibr" target="#b65">Sung et al. (2012)</ref>. We ran their code on our dataset and obtain accuracy of 26.4%, whereas our method gives an accuracy of 84.7% when ground truth segmentation is available and 80.6% otherwise. <ref type="figure">Figure 9</ref> shows a sequence of images from taking food activity along with the inferred labels. <ref type="figure" target="#fig_7">Figure 8</ref> shows the confusion matrix for labeling affordances, sub-activities and high-level activities with our proposed method. We can see that there is a strong diagonal with a few errors such as scrubbing misclassified as placing, and picking objects misclassified as arranging objects.</p><p>We analyze our model to gain insight into which interactions provide useful information by comparing our full model to variants of our model.</p><p>How important is object context for activity detection? We show the importance of object context for sub-activity labeling by learning a variant of our model without the object nodes (referred to as sub-activity only). With object context, the micro precision increased by 14.1% and both macro precision and recall increased by around 23.3% over sub-activity only. Considering object information (affordance labels and occlusions) also improved the high-level activity accuracy by three-fold.</p><p>How important is activity context for affordance detection? We also show the importance of context from sub-activity for affordance detection by learning our model without the sub-activity nodes (referred to as object only).</p><p>With sub-activity context, the micro precision increased by 4.9% and the macro precision and recall increased by 17.7% and 11.1% respectively for affordance labeling over object only. The relative gain is less compared with that obtained in sub-activity detection as the object only model still has object-object context which helps in affordance detection.</p><p>How important is object-object context for affordance detection? In order to study the effect of the object-object interactions for affordance detection, we learnt our model without the object-object edge potentials (referred to as no object interactions). We see a considerable improvement in affordance detection when the object interactions are modeled, the macro recall increased by 14.9% and the macro precision by about 10.9%. This shows that sometimes just the context from the human activity alone is not sufficient to determine the affordance of an object.</p><p>How important is temporal context? We also learn our model without the temporal edges (referred to as no temporal interactions). Modeling temporal interactions increased the micro precision by 4.8% and 10.0% for affordances and subactivities respectively and increased the micro precision for high-level activity by 3.3%.</p><p>How important is reliable human pose detection? In order understand the effect of the errors in human pose tracking, we consider the affordances that require direct contact by human hands, such as movable, openable, closable, drinkable, etc. The distance of the predicted hand locations to the object should be zero at the time of contact. We found that for the correct predictions, these distances had a mean of 3.8 cm and variance of 48.1 cm. However, for the incorrect predictions, these distances had a mean that was 43.3% higher and a variance that was 53.8% higher. This indicates that the prediction accuracies can potentially be improved with more robust human pose tracking.</p><p>How important is reliable object tracking? We show the effect of having reliable object tracking by comparing to the results obtained from using our object tracking algorithm mentioned in Section V. We see that using the object tracks generated by our algorithm gives slightly lower micro precision/recall values compared with using ground-truth object tracks, around 3.5% drop in affordance and sub-activity detection, and 5.7% drop in high-level activity detection. The drop in macro precision and recall are higher, which shows that the performance of few classes are effected more than the others. In future work, one can increase accuracy  <ref type="figure" target="#fig_0">10</ref>. Comparison of the sub-activity labeling of various segmentations. This activity involves the sub-activities: reaching, moving, pouring and placing as colored in red, green, blue and magenta respectively. The x-axis denotes the time axis numbered with frame numbers. It can be seen that the various individual segmentation labelings are not perfect and make different mistakes, but our method for merging these segmentations selects the correct label for many frames.</p><p>by improving object tracking.</p><p>Results with multiple segmentations. Given the RGB-D video and initial bounding boxes for objects in the first frame, we obtain the final labeling using our method described in Section VIII-C. To generate the segmentation hypothesis set H we consider three different segmentation algorithms, and generate multiple segmentations by changing their parameters as described in Section VI. The lines 11-13 of <ref type="table">Table V</ref> show the results of the best performing segmentation, average performance all the segmentations considered, and our proposed method for combining the segmentations respectively. We see that our method improves the performance over considering a single best performing segmentation: macro precision increased by 5.8% and 9.1% for affordance and sub-activity labeling respectively. <ref type="figure" target="#fig_0">Fig. 10</ref> shows the comparison of the sub-activity labeling of various segmentations, our end-to-end labeling and the ground-truth labeling for one making cereal high-level activity video. It can be seen that the various individual segmentation labelings are not perfect and make different mistakes, but our method for merging these segmentations selects the correct label for many frames. Line 14 of <ref type="table">Table V</ref> show the results of our proposed method for combining the segmentations along with using our object tracking algorithm. The numbers show a drop compared with the case of using ground-truth tracks, therefore providing a scope for improvement by using more reliable tracking algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robotic Applications</head><p>We demonstrate the use of our learning algorithm in two robotics applications. First, we show that the knowledge of the activities currently being performed enables a robot to assist the human by performing an appropriate response action. Second, we show that the knowledge of the affordances of the objects enables a robot to use them appropriately when manipulating them.</p><p>We use Cornell's Kodiak, a PR2 robot, in our experiments. Kodiak is mounted with a Microsoft Kinect, which is used as the main input sensor to obtain the RGB-D video stream. We used the OpenRAVE libraries <ref type="bibr" target="#b9">(Diankov, 2010)</ref> for programming the robot to perform the pre-programmed assistive tasks.</p><p>Assisting Humans. There are several modes of operation for a robot performing assistive tasks. For example, the robot can perform some tasks completely autonomous, independent of the humans. For some other tasks, the robot needs to act more reactively. That is, depending on the task and current human activity taking place, perform a complementary sub-task. For example, bring a glass of water when a person is attempting to take medicine (and there is no glass within person's reach). Such a behavior is possible only when the activities are successfully detected. In this experiment, we demonstrate that our algorithm for detecting the human activities enables a robot to take such (pre-programmed) reactive actions. <ref type="bibr">8</ref> We consider the following three scenarios:</p><p>• Having Meal: The subject eats food from a bowl and drinks water from a cup in this activity. On detecting the having meal activity, the robot assists by clearing the table (i.e. move the cup and the bowl to another place) after the subject finishes eating.</p><p>• Taking Medicine: The subjects opens the medicine container, takes the medicine, and waits as there is no water nearby. The robot assists the subject by bringing a glass of water on detecting the taking medicine activity.</p><p>• Making Cereal: The subject prepares cereal by pouring cereal and milk in to a bowl. On detecting the activity, the robot responds by taking the milk and putting it into the refrigerator.</p><p>Our robot was placed in a kitchen environment so that it can observe the activity being performed by the subject. We found that our robot successfully detected the activities and performed the above described reactive actions. <ref type="figure" target="#fig_0">Fig. 11</ref> shows the sequence of images of the robot detecting the activity being performed, planning the response in simulation and then performing the appropriate response for all three activities described above.</p><p>Using Affordances. An important component of our work 8 Our goal in this paper is on activity detection, therefore we pre-program the response actions using existing open-source tools in ROS. In future, one would need to make significant advances in several fields to make this useful in practice, e.g., object detection <ref type="bibr" target="#b31">Koppula et al. (2011);</ref><ref type="bibr" target="#b4">Anand et al. (2012)</ref>, grasping <ref type="bibr" target="#b60">Saxena et al. (2008)</ref>; <ref type="bibr" target="#b25">Jiang et al. (2011b)</ref>, human-robot interaction, and so on. is to learn affordances. In particular, by observing how the humans interact with the objects, a robot can figure out the affordances of the objects. Therefore, it can use these inferred affordances to interact with objects in a meaningful way. For example, given an instruction of 'clear the table', the robot should be able to perform the response in a desirable way: move the bowl with cereal without tilting it, and not move the microwave. In this experiment, we demonstrate that our algorithm for labeling the affordances explicitly helps in manipulation.</p><p>In our setting, we directly infer the object affordances (movable, pourable, drinkable, etc.). Therefore, we only need to encode the low-level control actions of each affordance, e.g. to move only movable objects, and to execute constrained movement, i.e. no rotation in the xy plane, for objects with affordances such as pour-to, pourable or drinkable, etc. The robot is allowed to observe various activities performed with the objects and it uses our learning algorithms to infer the affordances associated with the objects. When an instruction is given to the robot, such as 'clear the table' or 'move object x', it uses the inferred affordances to perform the response.</p><p>We demonstrate this in two scenarios for the task of 'clearing the table': detecting movable objects and detecting constrained movement. We consider a total of seven activities with nine unique objects. Some objects were used in multiple activities, with a total of 19 object instances. Two of these activities were other high-level activities that were not seen during training, but comprise sequences of the learned affordances and sub-activities. The results are summarized in <ref type="table" target="#tab_0">Table VI</ref>. In the scenario of detecting movable objects, the robot was programmed to move only objects with inferred movable affordance, to a specified location. There were a total of 15 instances of movable objects. The robot was able to correctly identify all movable objects using our model and could perform the moving task with 100% accuracy.</p><p>In the scenario of constrained movement, i.e. the robot should not tilt the objects which contain food items or liquids when moving them. In order to achieve this, we have programmed the robot to perform constrained movement without tilting the objects if it has inferred at least one of the following affordances: {drinkable, pourable, pour-to}. The robot was able to correctly identify constraint movement for 80% of the movable instances. Also, if we let the robot observe the activities for a longer time, i.e. let the subject <ref type="figure" target="#fig_0">Fig. 11</ref>. Robot performing the task of assisting humans: (top row) robot clearing the table after detecting having a meal activity, (middle row) robot fetching a bottle of water after detecting taking a medicine activity and (third row) robot putting milk in the fridge after detecting making cereal activity. First two columns show the robot observing the activity, third row shows the robot planning the response in simulation and the last three columns show the robot performing the response action. perform multiple activities with the objects and aggregate the affordances associated with the objects before performing the task, the robot is able to perform the task with 100% accuracy.</p><p>These experiments show that robot can use the affordances for manipulating the objects in a more meaningful way. Some affordances such as moving are easy to detect, where as some complicated affordances such as pouring might need more observations to be detected correctly. Also, by considering other high-level activities in addition to those used for learning, we have also demonstrated the generalizability of our algorithm for affordance detection.</p><p>We have made the videos of our results, along with the CAD-120 dataset and code, available at http://pr.cs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>cornell.edu/humanactivities</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. CONCLUSION AND DISCUSSION</head><p>In this paper, we have considered the task of jointly labeling human sub-activities and object affordances in order to obtain a descriptive labeling of the activities being performed in the RGB-D videos. The activities we consider happen over a long time period, and comprise several sub-activities performed in a sequence. We formulated this problem as a MRF, and learned the parameters of the model using a structural SVM formulation. Our model also incorporates the temporal segmentation problem by computing multiple segmentations and considering labeling over these segmentations as latent variables. In extensive experiments over a challenging dataset, we show that our method achieves an accuracy of 79.4% for affordance, 63.4% for sub-activity and 75.0% for high-level activity labeling on the activities performed by a different subject than those in the training set. We also showed that it is important to model the different properties (object affordances, object-object interaction, temporal interactions, etc.) in order to achieve good performance.</p><p>We also demonstrate the use of our activity and affordance labeling by a PR2 robot in the task of assisting humans with their daily activities. We have shown that being able to infer affordance labels enables the robot to perform the tasks in a more meaningful way.</p><p>In this growing area of RGB-D activity recognition, we have presented algorithms for activity and affordance detection and also demonstrated their use in assistive robots, where our robot responds with pre-programmed actions. We have focused on the algorithms for temporal segmentation and labeling while using simple bounding-box detection and tracking algorithms. However, improvements to object perception and task-planning, while taking into consideration the human-robot interaction aspects, are needed for making assistive robots working efficiently alongside humans.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An assistive robot observes human activities (making cereal, top left). Using RGB-D images (top right) as input, our algorithm detects the activity being performed as well as the object affordances. This enables the robot to figure out how to interact with objects and plan actions (bottom left), and to respond appropriately to the activities being performed (cleaning up the table, bottom right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Pictorial representation of our algorithm for combining object detections with tracking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Tracking Results: Blue dots represent the trajectory of the center of tracked bounding box and red dots represent the trajectory of the center of ground-truth bounding box. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>V</head><label></label><figDesc>Results on our CAD-120 dataset, SHOWING AVERAGE MICRO PRECISION/RECALL, AND AVERAGE MACRO PRECISION AND RECALL FOR AFFORDANCE, SUB-ACTIVITIES AND HIGH-LEVEL ACTIVITIES. STANDARD ERROR IS ALSO REPORTED. Full model, assuming ground-truth temporal segmentation is given. ± 0.2 29.2 ± 0.2 10.0 ± 0.0 10.0 ± 0.0 10.0 ± 0.0 10.0 ± 0.0 image only</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>MFig. 8 .</head><label>8</label><figDesc>a k in g C e re a l T a k in g M e d ic in e M ic ro w a v in g F o o d S ta c k in g O b je c ts U n s ta c k in g O b je c ts P ic k in g O b je c ts C le a n in g O b je c ts T a k in g F o o d A rr a n g in g O b je c ts H a v in g M e a l Confusion matrix for affordance labeling (left), sub-activity labeling (middle) and high-level activity labeling (right) of the test RGB-D videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF THE FEATURES USED IN THE ENERGY FUNCTION.</figDesc><table><row><cell>Description Object Features N1. Centroid location N2. 2D bounding box N3. Transformation matrix of SIFT matches between adjacent frames N4. Distance moved by the centroid N5. Displacement of centroid Sub-activity Features N6. Location of each joint (8 joints) N7. Distance moved by each joint (8 joints) N8. Displacement of each joint (8 joints) N9. Body pose features N10. Hand position features Object-object Features (computed at start frame, middle frame, end frame, max and min) E1. Difference in centroid locations (∆x, ∆y, ∆z) E2. Distance between centroids Object-sub-activity Features (computed at start frame, middle frame, end frame, max and min) E3. Distance between each joint location and object centroid Object Temporal Features E4. Total and normalized vertical displacement E5. Total and normalized distance between centroids Sub-activity Temporal Features E6. Total and normalized distance between each corresponding joint locations (8 joints)</cell><cell>Count 18 3 4 6 1 1 103 24 8 8 47 16 20 3 1 40 8 4 2 2 16 16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc>Description of high-level activities in terms of sub-activities. NOTE THAT SOME ACTIVITIES CONSIST OF SAME SUB-ACTIVITIES BUT ARE EXECUTED IN DIFFERENT ORDER. THE HIGH-LEVEL ACTIVITIES (ROWS) ARE LEARNT USING THE ALGORITHM IN SECTION VIII-D AND THE SUB-ACTIVITIES (COLUMNS) ARE LEARNT USING THE ALGORITHM IN SECTION VIII-B. hn ∀hn∈H hn∈H [E w hn (x hn , y hn ) + g θn (y hn , y)] (17)</figDesc><table><row><cell>reaching moving placing opening closing</cell><cell>eating drinking pouring scrubbing</cell><cell>null</cell></row><row><cell>Making Cereal Taking Medicine Stacking Objects Unstacking Objects Microwaving Food Picking Objects Cleaning Objects Taking Food Arranging Objects Having a Meal</cell><cell></cell><cell></cell></row><row><cell>segmentations. Therefore we can write inference in terms of a joint objective function as followŝ</cell><cell></cell><cell></cell></row><row><cell>y = argmax</cell><cell></cell><cell></cell></row></table><note>y maxy</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Specifically, our sub-activity labels are {reaching, moving, pouring, eating, drinking, opening, placing, closing, scrubbing, null} and our affordance labels are {reachable, movable, pourable, pourto, containable, drinkable, openable, placeable, closable, scrubbable, scrubber, stationary}.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III Object</head><label>III</label><figDesc>Tracking Results, SHOWING THE % OF FRAMES WHICH HAVE ≥40%, ≥20% AND ≥10% OVERLAP WITH THE GROUND-TRUTH OBJECT BOUNDING BOXES.</figDesc><table><row><cell></cell><cell>≥40%</cell><cell>≥20%</cell><cell>≥10%</cell></row><row><cell>tracking w/o detection tracking + detection</cell><cell>49.2 53.5</cell><cell>65.7 69.4</cell><cell>75 77.8</cell></row><row><cell cols="4">C. Labeling results on the Cornell Activity Dataset 60 (CAD-</cell></row><row><cell>60)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV</head><label>IV</label><figDesc>Results on Cornell Activity Dataset<ref type="bibr" target="#b65">(Sung et al., 2012)</ref>, TESTED ON "New Person" DATA FOR 12 ACTIVITY CLASSES.</figDesc><table><row><cell>Sung et al. (2012) Our method</cell><cell>bathroom prec. (%) rec. (%) 72.7 65.0 88.9 61.1</cell><cell>bedroom prec. (%) rec. (%) 76.1 59.2 73.0 66.7</cell><cell>kitchen prec. (%) rec. (%) 64.4 47.9 96.4 85.4</cell><cell>living room prec. (%) rec. (%) 52.6 45.7 69.2 68.7</cell><cell>office prec. (%) rec. (%) 73.8 59.8 76.7 75.0</cell><cell>Average prec. (%) rec. (%) 67.9 55.5 80.8 71.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Descriptive output of our algorithm: Sequence of images from the taking food (Top Row), having meal (Middle Row) and cleaning objects (Bottom Row) activities labeled with sub-activity and object affordance labels. A single frame is sampled from the temporal segment to represent it.</figDesc><table><row><cell>Subject opening openable</cell><cell>Subject reaching reachable</cell><cell>Subject moving movable</cell><cell>Subject placing placable</cell><cell>Subject reaching reachable</cell><cell>Subject closing closable</cell></row><row><cell>object1</cell><cell>object2</cell><cell>object2</cell><cell>object2</cell><cell>object1</cell><cell>object1</cell></row><row><cell>Subject moving movable</cell><cell>Subject eating</cell><cell>Subject moving movable</cell><cell>Subject drinking from</cell><cell>Subject moving movable</cell><cell>Subject placing</cell></row><row><cell>object1</cell><cell></cell><cell>object1</cell><cell>drinkable object1</cell><cell>object1</cell><cell>placeable object1</cell></row><row><cell>Subject reaching</cell><cell>Subject opening</cell><cell>Subject reaching</cell><cell>Subject moving movable</cell><cell>Subject scrubbing</cell><cell>Subject moving movable</cell></row><row><cell>reachable object1</cell><cell>openable object1</cell><cell></cell><cell>object2</cell><cell>scrubbable object1 with</cell><cell>object2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>scrubber object2</cell><cell></cell></row><row><cell>Fig. 9.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Fig.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VI Robot</head><label>VI</label><figDesc>Object Manipulation Results, SHOWING THE ACCURACY ACHIEVED BY THE KODIAK PR2 PERFORMING THE TWO MANIPULATION TASKS, WITH AND WITHOUT MULTIPLE OBSERVATIONS.</figDesc><table><row><cell>task object movement constrained movement</cell><cell># instance accuracy (%) accuracy (%) (multi. obsrv.) 19 100 100 15 80 100</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A first version of this work was made available on arXiv for faster dissemination of scientific work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://openni.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://openni.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">For example, the instructions for making cereal were: 1) Place bowl on table, 2) Pour cereal, 3) Pour milk. For microwaving food, they were: 1) Open microwave door, 2) Place food inside, 3) Close microwave door.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XI. ACKNOWLEDGEMENTS</head><p>We thank Li Wang for his significant contributions to the robotic experiments. We also thank Yun Jiang, Jerry Yeh, Vaibhav Aggarwal, and Thorsten Joachims for useful discussions. This research was funded in part by ARO award W911NF-12-1-0267, and by Microsoft Faculty Fellowship and Alfred P. Sloan Research Fellowship to one of us (Saxena).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CSUR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Categorizing object-action relations from semantic scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abramov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Worgotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dellen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning the semantics of objectaction relations by observation. IJRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abramov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dörr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wörgötter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1229" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised learning of hidden and non-hidden 0-order affordances and detection in real scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aldoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Contextually guided semantic labeling and search for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>IJRR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interpreting and executing recipes with a cooking robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bollini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISER</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust 3d visual tracking using particle filtering on the special euclidean group: A combined approach of keypoint and edge features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="519" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The MOPED framework: Object Recognition and Pose Estimation for Manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Automated Construction of Robotic Manipulation Programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Diankov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University, Robotics Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training structural svms when exact inference is intractable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Willsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian Nonparametric Inference of Switching Dynamic Linear Models</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Functional categorization of objects using real-time markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fossati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Ecological Approach to Visual Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<pubPlace>Houghton Mifflin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Observing human-object interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1775" to="1789" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Roof duality, complementation and persistency in quadratic 0-1 optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Simeone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Prog</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="155" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rgb-d mapping: Using kinect-style depth cameras for dense 3d modeling of indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="647" to="663" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Affordance prediction via learned object attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA: Workshop on Semantic Perception, Mapping, and Exploration</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maximum margin temporal clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint segmentation and classification of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling scene and object contexts for human action retrieval with few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Circuits &amp; Sys Video Tech</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning object arrangements in 3d scenes using human context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning to place new objects in a scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>IJRR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient grasping from rgbd images: Learning using a new rectangle representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moseson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Hallucinating humans for learning robotic placement of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cutting-plane training of structural SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual object-action recognition: Inferring object affordances from human demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="90" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robot learning from demonstration by constructing skill trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kuindersma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grupen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="360" to="375" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic labeling of 3d point clouds for indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Human activity learning using object affordances from rgb-d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<idno>abs/1208.0967</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robot motor skill coordination with EM-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kormushev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calinon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Caldwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Manipulator and object tracking for in-hand 3d object modeling. IJRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1311" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Large-Scale Hierarchical Multi-View RGB-D Object Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparse Distance Learning for Object Recognition Combining RGB and Depth Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards holistic scene understanding: Feedback enabled cascaded classification models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1394" to="1408" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3d points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on CVPR for Human Communicative Behavior Analysis</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos &quot;in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual learning by imitation with motor representations. Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="438" to="449" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Model recommendation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A geometric approach to robotic laundry folding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>IJRR</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Statistical relational learning of object affordances for robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Otterlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Latest Advances in Inductive Logic Programming</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning object affordances: From sensory-motor coordination to imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos-Victor</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rgbd-hudaact: A color-depth video database for human daily activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on Consumer Depth Cameras for Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Tracking and modeling of human activity using laser rangefinders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panangadan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mataric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Sukhatme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>IJSR</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Taskability graph: Towards analyzing effort based agent-agent affordances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alami</surname></persName>
		</author>
		<editor>RO-MAN</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mightability maps: A perceptual level decisional framework for co-operative and competitive human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A linear time histogram metric for improved sift matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised learning of basic object affordances from object properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Skočaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Computer Vision Winter Workshop</title>
		<meeting>the Fourteenth Computer Vision Winter Workshop</meeting>
		<imprint>
			<publisher>CVWW</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Learning spatial relationships between objects. IJRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramamoorthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1328" to="1342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Optimizing binary mrfs via extended roof duality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fast 3d recognition and pose using the viewpoint feature histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Close-range scene segmentation and reconstruction of 3d point cloud maps for mobile manipulation in domestic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Action bank: A high-level representation of activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Robotic grasping of novel objects using vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Driemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">157</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Human action segmentation and recognition using discriminative semi-markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Realtime human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Learning visual object categories for robot affordance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>IJRR</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unstructured human activity detection from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Human activity detection from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI workshop on Pattern, Activity and Intent Recognition (PAIR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning associative markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Recognizing human actions from still images with latent poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">4-dimensional local spatio-temporal features for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
