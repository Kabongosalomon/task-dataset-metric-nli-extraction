<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Correctness Proofs with Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sanchez-Stern</surname></persName>
							<email>alexss@eng.ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousef</forename><surname>Alhessi</surname></persName>
							<email>yalhessi@eng.ucsd.edu</email>
							<affiliation key="aff1">
								<address>
									<settlement>San Diego</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Saul</surname></persName>
							<email>saul@cs.ucsd.edu</email>
							<affiliation key="aff2">
								<address>
									<settlement>San Diego</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorin</forename><surname>Lerner</surname></persName>
							<email>lerner@cs.ucsd.edu</email>
							<affiliation key="aff3">
								<address>
									<settlement>San Diego</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Correctness Proofs with Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS Concepts: • Computing methodologies → Symbolic and algebraic manipulation; Machine learning Keywords: Machine-learning, Theorem proving</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Foundational verification allows programmers to build software which has been empirically shown to have high levels of assurance in a variety of important domains. However, the cost of producing foundationally verified software remains prohibitively high for most projects, as it requires significant manual effort by highly trained experts. In this paper we present Proverbot9001, a proof search system using machine learning techniques to produce proofs of software correctness in interactive theorem provers. We demonstrate Proverbot9001 on the proof obligations from a large practical proof project, the CompCert verified C compiler, and show that it can effectively automate what were previously manual proofs, automatically producing proofs for 28% of theorem statements in our test dataset, when combined with solver-based tooling. Without any additional solvers, we exhibit a proof completion rate that is a 4X improvement over prior state-of-the-art machine learning models for generating proofs in Coq.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A promising approach to software verification is foundational verification. In this approach, programmers use an interactive theorem prover, such as Coq <ref type="bibr" target="#b14">[14]</ref> or Isabelle/HOL <ref type="bibr" target="#b35">[35]</ref>, to state and prove properties about their programs. Foundational verification has shown increasing promise over the past two decades; it has been used to prove properties of programs in a variety of settings, including compilers <ref type="bibr" target="#b28">[28]</ref>, operating systems <ref type="bibr" target="#b23">[23]</ref>, database systems <ref type="bibr" target="#b31">[31]</ref>, file systems <ref type="bibr" target="#b7">[8]</ref>, distributed systems <ref type="bibr" target="#b39">[39]</ref>, and cryptographic primitives <ref type="bibr" target="#b2">[3]</ref>.</p><p>One of the main benefits of foundational verification is that it provides high levels of assurance. The interactive theorem prover makes sure that proofs of program properties are done in full and complete detail, without any implicit assumptions or forgotten proof obligations. Furthermore, once a proof is completed, foundational proof assistants can generate a representation of the proof in a foundational logic; these proofs can be checked with a small kernel. In this setting only the kernel needs to be trusted (as opposed to the entire proof assistant), leading to a small trusted computing base. As an example of this high-level of assurance, a study of compilers <ref type="bibr" target="#b41">[41]</ref> has shown that CompCert <ref type="bibr" target="#b28">[28]</ref>, a compiler proved correct in the Coq proof assistant, is significantly more robust than its non-verified counterparts.</p><p>Unfortunately, the benefits of foundational verification come at a great cost. The process of performing proofs in a proof assistant is extremely laborious. CompCert <ref type="bibr" target="#b28">[28]</ref> took 6 person-years and 100,000 lines of Coq to write and verify, and seL4 <ref type="bibr" target="#b23">[23]</ref>, which is a verified version of arXiv:1907.07794v4 [cs.PL] 28 May 2020 a 10,000 line operating system, took 22 person-years to verify. The sort of manual effort is one of the main impediments to the broader adoption of proof assistants.</p><p>In this paper, we present Proverbot9001, a novel system that uses machine learning to help alleviate the manual effort required to complete proofs in an interactive theorem prover. Proverbot9001 trains on existing proofs to learn models. Proverbot9001 then incorporates these learned models in a tree search process to complete proofs. The source of Proverbot9001 is publicly available on GitHub 1 .</p><p>The main contribution of this paper is bringing domain knowledge to the feature engineering, model architecture, and search procedures of machine-learning based systems for interactive theorem proving. In particular, our work distinguishes itself from prior work on machine learning for proofs in three ways:</p><p>1. A two part tactic-prediction model, in which prediction of tactic arguments is primary and informs prediction of tactics themselves. 2. An argument prediction architecture which makes use of recurrent neural networks over sequential representations of terms. 3. Several effective tree pruning techniques inside of a prediction-guided proof search. We tested Proverbot9001 end-to-end by training on the proofs from 162 files from CompCert, and testing on the proofs from 13 files 2 . When combined with solverbased tooling (which alone can only solve 7% of proofs), Proverbot9001 can automatically produce proofs for 28% of the theorem statements in our test dataset (138/501). In our default configuration without external solvers, Proverbot9001 solves (produces a checkable proof for) <ref type="bibr" target="#b19">19</ref>.36% (97/501) of the proofs in our test set, which is a nearly 4X improvement over the previous state of the art system that attempts the same task <ref type="bibr" target="#b40">[40]</ref>. Our model is able to reproduce the tactic name from the solution 32% of the time; and when the tactic name is correct, our model is able to predict the solution argument 89% of the time. We also show that Proverbot9001 can be trained on one project and then effectively predict on another project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Foundational Verification</head><p>Program verification is a well studied problem in the programming languages community. Most work in this field falls into one of two categories: solver-backed automated (or semi-automated) techniques, where a simple proof is checked by a complex procedure; and foundational logic based techniques, where a complex proof is checked by a simple procedure.</p><p>While research into solver-backed techniques has produced fully-automated tools in many domains, these approaches are generally incomplete, failing to prove some desirable propositions. When these procedures fail, it is often difficult or impossible for a user to complete the proof, requiring a deep knowledge of the automation. In contrast, foundational verification techniques require a heavy initial proof burden, but scale to any proposition without requiring a change in proof technique. However, the proof burden of foundational techniques can be prohibitive; CompCert, a large and well-known foundationally verified compiler, took 6 person-years of work to verify <ref type="bibr" target="#b27">[27]</ref>, with other large verification projects sporting similar proof burdens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interactive Theorem Provers</head><p>Most foundational (and some solver-backed) verification is done in an interactive theorem prover. Interactive theorem provers allow the user to define proof goals alongside data and program definitions, and then prove those goals interactively, by entering commands which manipulate the proof context. The name and nature of these commands varies by the proof assistant, but in many foundational assistants, these commands are called "tactics", and coorespond to primitive proof techniques like "induction", as well as search procedures like "omega" (which searches for proofs over ring-like structures). Proof obligations in such proof assistants take the form of a set of hypotheses (in a Curry-Howard compatible proof theory, bound variables in a context), and a goal (a target type); proof contexts may consist of multiple proof obligations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Machine Learning and Neural Networks</head><p>Machine learning is an area of computer science dating back to the 1950s. In problems of supervised learning, the goal is to learn a function from labeled examples of input-output pairs. Models for supervised learning parameterize a function from inputs to outputs and have  <ref type="figure">Figure 2</ref>. A recurrent neural network. Inputs are in blue boxes at the bottom, and each iteration produces an output value, as well as a new state value for the next iteration.</p><p>a procedure to update the parameters from a data set of labeled examples. Machine learning has traditionally been applied to problems such as handwriting recognition, natural language processing, and recommendation systems. Neural Networks are a particular class of learned model where layers of nodes are connected together by a linear combination and a non-linear activation function, to form general function approximators. Neural Networks have a variety of structures, some forming a straightforward "stack" of nodes with some connections removed (convolutional), and others, such as those used for natural language processing, using more complex structures like loops.</p><p>We will make use of two different kinds of neural networks: feed-forward networks and recurrent neural networks. <ref type="figure" target="#fig_0">Figure 1(a)</ref> shows the structure of a feed-forward network, where each gray circle is a perceptron, and <ref type="figure" target="#fig_0">Figure 1</ref>(b) shows individual structure of a perceptron. <ref type="figure">Figure 2</ref> shows the structure of a recurrent neural network (RNN). Inputs are shown in blue, outputs in green and computational nodes in gray. The computational nodes are Gated Recurrent Network nodes, GRU for short, a commonly used network component with two inputs and two outputs <ref type="bibr" target="#b10">[10]</ref>. The network is recurrent because it feeds back into itself, with the state output from the previous iteration feeding into the state input of the next iteration. When we display an RNN receiving data, we visually unfold the RNN, as shown on the right side of <ref type="figure">Figure 2</ref>, even though in practice there is still only one GRU node. The right side of <ref type="figure">Figure 2</ref> shows an example RNN that processes tokens of a Coq goal, and produces some output values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview</head><p>In this section, we'll present Proverbot9001's prediction and search process with an example from CompCert. You can see the top-level structure of Proverbot9001 in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Consider the following theorem from the CompCert compiler:  <ref type="figure">Figure 4</ref>. A graph of a Proverbot9001 search. In green are the tactics that formed part of the discovered solution, as well as the lemma name and the QED. In orange are nodes that resulted in a context that is at least as hard as one previously found (see <ref type="bibr">Section 7)</ref>. This theorem states that the mulhs expression constructor is sound with respect to the specification Val.mulhs.</p><p>At the beginning of the proof of eval_mulhs, Prover-bot9001 predicts three candidate tactics, econstructor, eauto, and unfold binary_constructor_sound. Once these predictions are made, Proverbot9001 tries running all three, which results in three new states of the proof assistant. In each of these three states, Proverbot9001 again makes predictions for what the most likely tactics are to apply next. These repeated predictions create a  search tree, which Proverbot9001 explores in a depth first way. The proof command predictions that Prover-bot9001 makes are ordered by likelihood, and the search explores more likely branches first. <ref type="figure">Figure 4</ref> shows the resulting search tree for eval_mulhs. The nodes in green are the nodes that produce the final proof. Orange nodes are predictions that fail to make progress on the proof (see Section 7); these nodes are not expanded further. All the white nodes to the right of the green path are not explored, because the proof in the green path is found first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Definitions</head><p>In the rest of the paper, we will describe the details of how Proverbot9001 works. We start with a set of definitions that will be used throughout. In particular, <ref type="figure" target="#fig_4">Figure 5</ref> shows the formalism we will use to represent the state of an in-progress proof. A tactic ∈ is a tactic name. An argument ∈ is a tactic argument. For simplicity of the formalism, we assume that all tactics take zero or one arguments. We use ℐ for the set of Coq identifiers, and for the set of Coq propositions. A proof state ∈ is a state of the proof assistant, which consists of a list of obligations along with their proof command history. We use [ ] to denote the set of lists of elements from . An obligation is a pair of: (1) a set of hypotheses (2) a goal to prove. A hypothesis is a proposition named by an identifier, and a goal is a proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Predicting a Single Proof Step</head><p>We start by explaining how we predict individual steps in the proof. Once we have done this, we will explain how we use these proof command predictions to guide a proof search procedure.</p><p>We define [ ] to be a scoring function over , where larger scores are preferred over smaller ones:</p><formula xml:id="formula_0">[ ] = → R</formula><p>We define a -predictor ℛ[ ] to be a function that takes a proof state ∈ (i.e. a state of the proof assistant under which we want to make a prediction) and returns a scoring function over . In particular, we have:</p><formula xml:id="formula_1">ℛ[ ] = → [ ]</formula><p>Our main predictor will be a predictor of the next step in the proof, i.e. a predictor for proof commands:</p><formula xml:id="formula_2">: ℛ[ × ]</formula><p>We divide our main predictor into two predictors, one for tactics, and one for arguments:</p><formula xml:id="formula_3">tac : ℛ[ ] arg : → ℛ[ ]</formula><p>Our main predictor combines tac and arg as follows:</p><formula xml:id="formula_4">( ) = ( , ) . tac ( )( ) ⊗ arg ( )( )( )</formula><p>where ⊗ is an operator that combines the scores of the tactic and the argument predictors. We now describe the three parts of this prediction architecture in turn: tac , arg , and ⊗.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Predicting Tactics ( tac )</head><p>To predict tactics, Proverbot9001 uses of a set of manually engineered features to reflect important aspects of proof prediction: (1) the head of the goal as an integer (2) the name of the previously run tactic as an integer (3) a hypothesis that is heuristically chosen (based on string similarity to goal) as being the most relevant to the goal (4) the similarity score of this most relevant hypothesis.</p><p>These features are embedded into a continuous vector of 128 floats using a standard word embedding, and then fed into a fully connected feed-forward neural network (3 layers, 128 nodes-wide) with a softmax (normalizing) layer at the end, to compute a probability distribution over possible tactic names. This architecture is trained on 153402 samples with a stochastic gradient descent optimizer.</p><p>The architecture of this model is shown in <ref type="figure">Figure 6</ref>. Blue boxes represent input; purple boxes represent intermediate encoded values; green boxes represent outputs; and gray circles represent computations. The NN circle is the feed-forward Neural Network mentioned above. The Enc circle is a word embedding module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Predicting Tactic Arguments ( arg )</head><p>Once a tactic is predicted, Proverbot9001 next predicts arguments. Recall that the argument predictor is a function arg : ℛ[ ]. In contrast to previous work, our argument model is a prediction architecture in its own right.</p><p>Proverbot9001 currently predicts zero or one tactic arguments; However, since the most often-used multiargument Coq tactics can be desugared to sequences of single argument tactics (for example "unfold a, b" to  <ref type="figure">Figure 6</ref>. Proverbot9001's model for predicting tactics. Takes as input three features for each data point: the previous tactic run, the head token of the goal, and of the most relevant hypothesis (see Section 5.1). We restrict the previous tactic feature to the 50 most common tactics, and head tokens on goal and hypothesis to the 100 most common head tokens.</p><p>"unfold a. unfold b."), this limitation does not significantly restrict our expressivity in practice. Proverbot9001 makes three kinds of predictions for arguments: goal-token arguments, hypothesis arguments, lemma arguments:</p><p>Goal-token arguments are arguments that are a single token in the goal; for instance, if the goal is not (eq x y), we might predict unfold not, where not refers to the first token in the goal. In the case of tactics like unfold and destruct, the argument is often (though not always) a token in the goal.</p><p>Hypothesis arguments are identifiers referring to a hypothesis in context. For instance, if we have a hypothesis H in context, with type is_path (cons (pair s d) m), we might predict inversion H, where H refers to the hypothesis, and inversion breaks it down. In the case of tactics like inversion and destruct, the argument is often a hypothesis identifier.</p><p>Finally, lemma arguments are identifiers referring to a previously defined proof. These can be basic facts in the standard library, like plus_n_0 : forall n : nat, n = n + 0 or a lemma from the current project, such as the eval_mulhs described in the overview. In Proverbot9001, lemmas are considered from a subset of the possible lemma arguments available in the global context, in order to make training tractable. Proverbot9001 supports several different modes for determining this subset; by default we consider lemmas defined previously in the current file.</p><p>The architecture of the scoring functions for these argument types is shown in <ref type="figure">Figure 7</ref>. One recurrent neural network (RNN) is used to give scores to each hypothesis and lemma by processing the type of the term, and outputting a final score. A different RNN is then used to process the goal, assigning a score to each token in processes. Hypothesis/ Lemma Output Score</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token Output Scores</head><formula xml:id="formula_5">G G G G G G G G G G G G G G G G G G G Figure 7.</formula><p>The model for scoring possible arguments.</p><p>As before, blue boxes are inputs; purple boxes are encoded values; green diamonds are outputs, in this case scores for each individual possible argument; and gray circles are computational nodes. The GRU nodes are Gated Recurrent Units <ref type="bibr" target="#b10">[10]</ref>. The NN node is a feedforward neural network.</p><p>For illustration purposes, <ref type="figure">Figure 7</ref> uses an example to provide sample values. Each token in the goal is an input -in <ref type="figure">Figure 7</ref> the goal is "not (eq x y)". The tactic predicted by tac is also an input -in <ref type="figure">Figure 7</ref> this tactic is "unfold". The hypothesis that is heuristically closest to the goal (according to our heuristic from Section 5.1) is also an input, one token at a time being fed to a GRU. In our example, let's assume this closest hypothesis is "y &gt; (x+1)". The similarity score of this most relevant hypothesis is an additional input -in <ref type="figure">Figure 7</ref> this score is 5.2.</p><p>There is an additional RNN (the middle row of GRUs in <ref type="figure">Figure 7</ref>) which encodes the goal as a vector of reals. The initial state of this RNN is set to some arbitrary constant, in this case 0.</p><p>The initial state of the hypothesis RNN (the third row of GRUs in <ref type="figure">Figure 7</ref>) is computed using a feedforward Neural Network (NN). This feed-forward Neural Network takes as input the tactic predicted by tac , the goal encoded as a vector of reals, and the similarity score of the hypothesis.</p><p>The architecture in <ref type="figure">Figure 7</ref> produces one output score for each token in the goal and one output score for the hypothesis. The highest scoring element will be chosen as the argument to the tactic. In <ref type="figure">Figure 7</ref>, the highest scoring element is the "not" token, resulting in the proof command "unfold not". If the hypothesis score (in our example this score is 8) would have been the highest score, then the chosen argument would be the identifier of that hypothesis in the Coq context. For example, if the identifier was IHn (as is sometimes the case for inductive hypotheses), then the resulting proof command would be "unfold IHn".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Combining Tactic and Argument Scores</head><p>(⊗) The ⊗ operator attempts to provide a balanced combination of tactic and argument prediction, taking both into account even across different tactics. The operator works as follows. We pick the highest-scoring tactics and for each tactic the highest-scoring arguments. We then score each proof command by multiplying the tactic score and the argument score, without any normalization. Formally, we can implement this approach by defining ⊗ to be multiplication, and by not normalizing the probabilities produced by arg until all possibilities are considered together.</p><p>Because we don't normalize the probabilities of tactics, the potential arguments for a tactic are used in determining the eligibility of the tactic itself (as long as that tactic is in the top ). This forms one of the most important contributions of our work: the argument selection is primary, with the tactic prediction mostly serving to help prune its search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Putting It All Together</head><p>The overall architecture that we have described is shown in <ref type="figure" target="#fig_8">Figure 8</ref>. The tac predictor (whose detailed structure is shown in <ref type="figure">Figure 6</ref>) computes a distribution over tactic using three features as input: the previous tactic, head constructor of goal, and head constructor of the hypothesis deemed most relevant. Then, for each of the top tactic predicted by tac , the arg predictor (whose detailed structure is shown in <ref type="figure">Figure 7</ref>) is invoked. In addition to the tactic name, the arg predictor takes several additional inputs: the goal, the hypotheses in context, and the similarity between each of those hypotheses and the goal. The arg predictor produces scores for each possible argument (in our case one score for each token in the goal, and one score the single hypothesis). These scores are combined with ⊗ to produce an overall scoring of proof commands. <ref type="figure" target="#fig_9">Figure 9</ref> shows the training architecture for the tactic predictor, tac (recall that the detailed architecture of tac is shown in <ref type="figure">Figure 6</ref>). The goal of training is to find weights for the neural network that is found inside the gray tac circle. Proverbot9001 processes all the Coq theorems in the training set, and steps through the proof of each of these theorems. <ref type="figure" target="#fig_9">Figure 9</ref> shows what happens at each step in the proof. In particular, at each step in the proof, Proverbot9001 computes the three features we are training with, and passes these features to the current tactic model to get a distribution over tactics. This distribution over tactics, along with the correct tactic name (from the actual proof), are passed to a module that computes changes to the weights based on the NLLLoss criterion. These changes are batched together over several steps of the proof, and then applied to update the tactic model. Running over all the training data to update the weights is called an epoch, and we run our training over 20 epochs. <ref type="figure" target="#fig_0">Figure 10</ref> shows the training architecture for the argument predictor, arg (recall that the detailed architecture of arg is shown in <ref type="figure">Figure 7</ref>). The goal of training is to find weights for the GRU components in arg . Here again, Proverbot9001 processes all the Coq theorems in the training set, and steps through the proof of each of these theorems. <ref type="figure" target="#fig_0">Figure 10</ref> shows what happens at each step in the proof. In particular, at each step in the proof, the current tac predictor is run to produce the top predictions for tactic. These predicted tactic, along with the correct tactic, are passed to the argument model arg . To make <ref type="figure" target="#fig_0">Figure 10</ref> more readable, we do not show the additional parameters to arg that where displayed in <ref type="figure" target="#fig_8">Figure 8</ref>, but these parameters are in fact also passed to arg during training. Note that it is very important for us to inject the tactics predicted by tac into the input of the argument model arg , instead of using just the correct tactic name. This allows the scores produced by the argument model to be comparable across different predicted tactics. Once the argument model arg computes a score for each possible argument, we combine these predictions using ⊗ to get a distribution of scores over tactic/argument pairs. Finally, this distribution, along with the correct tactic/argument pair is passed to a module that computes changes to the weights based on the NLLLoss criterion. In our main CompCert benchmark the 153402 tactic samples from the training set are processed for 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Training Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Learning From Higher-Order Proof</head><p>Commands Proof assistants generally have higher-order proof commands, which are tactics that take other proof commands as arguments; in Coq, these are called tacticals. One of the most common examples is the (;) infix operator which runs the proof command on the right on every sub-goal produced by the tactic on the left. Another example is the repeat tactical, which repeats a provided tactic until it fails.</p><p>While higher-order proof commands are extremely important for human proof engineers, they are harder to predict automatically because of their generality. While some previous work <ref type="bibr" target="#b40">[40]</ref> attempts to learn directly on data which uses these higher-order proof commands, we instead take the approach of desugaring higher-order proof commands into first-order ones as much as possible;    <ref type="figure" target="#fig_0">Figure 10</ref>. The architecture for training the argument models. Note that we inject predicted tactics into the input of the argument model, instead of just using the correct tactic, so that argument scores will be comparable.</p><p>this makes the data more learnable, without restricting the set of expressible proofs. For example, instead of trying to learn and predict (;) directly, Proverbot9001 has a system which attempts to desugar (;) into linear sequences of proof commands. This is not always possible (without using explicit subgoal switching commands), due to propagation of existential variables across proof branches. Proverbot9001 desugars the cases that can be sequenced, and the remaining commands containing (;) are filtered out of the training set.</p><p>In addition to the (;) tactical, there are other tacticals in common use in Coq. Some can be desugared into simpler forms. For example:</p><p>• "now &lt;tac&gt;" becomes "&lt;tac&gt;;easy". • "rewrite &lt;term&gt; by &lt;tac&gt;" becomes "rewrite &lt;term&gt; ; [ | &lt;tac&gt;]" • "assert &lt;term&gt; by &lt;tac&gt;" becomes "assert &lt;term&gt; ; [ | &lt;tac&gt;]"</p><p>In other cases, like try &lt;tac&gt; or solve &lt;tac&gt;, the tactical changes the behavior of the proof command in a way that cannot be desugared; for these we simply treat the prefixed tactic as a separate, learned tactic. For example, we would treat try eauto as a new tactic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Prediction-Guided Search</head><p>Now that we have explained how we predict a single step in the proof, we describe how Proverbot9001 uses these predictions in a proof search.</p><p>In general, proof search works by transitioning the proof assistant into different states by applying proof commands, and backtracking when a given part of the search space has either been exhausted, or deemed unviable. Exhaustive proof search in proof assistants is untenable because the number of possible proof commands to apply is large. Instead, we use the predictor described above to guide the search. Aside from using these predictions, the algorithm is a straightforward depth-limited search, with three subtleties.</p><p>First. we stop the search when we find a proof goal that is at least as hard (by a syntactic definition) as a goal earlier in the history. While in general it is hard to formally define what makes one proof state harder than another, there are some obvious cases which we can detect. A proof state with a superset of the original obligations will be harder to prove, and a proof state with the same goal, but fewer assumptions, will be harder to prove.</p><p>To formalize this intuition, we define a relation ≥ between states such that 1 ≥ 2 is meant to capture "Proof state 1 is at least as hard as proof state 2 ". We say that 1 ≥ 2 if and only if for all obligations 2 in 2 there exists an obligation 1 in 1 such that 1 ≥ 2 . For obligations 1 and 2 , we say that 1 ≥ 2 if and only if each hypothesis in 1 is also a hypothesis in 2 , and the goals of 1 and 2 are the same.</p><p>Since ≥ is reflexive, this notion allows us to generalize all the cases above to a single pruning criteria: "proof command prediction produces a proof state which is ≥ than a proof state in the history".</p><p>Second. when backtracking, we do not attempt to find a different proof for an already proven sub-obligation. While in general this can lead to missed proofs because of existential variables (typed holes filled based on context), this has not been an issue for the kinds of proofs we have worked with so far.</p><p>Third. we had to adapt our notion of search "depth" to the structure of Coq proofs (in which a tactic can produce multiple sub-obligations). A naïve tree search through the Coq proof space will fail to exploit some of the structure of sub-proofs in Coq.</p><p>Consider for example the following two proofs:</p><p>1. intros. simpl. eauto. 2. induction n. eauto. simpl.</p><p>At first glance, it seems that both of these proofs have a depth of three. This means that a straightforward tree search (which is blind to the structure of subproofs) would not find either of these proofs if the depth limit were set to two. However, there is a subtlety in the second proof above which is important (and yet not visible syntactically). Indeed, the induction n proof command actually produces two obligations ("sub-goals" in the Coq terminology). These correspond to the base case and the inductive case for the induction on n. Then eauto discharges the first obligation (the base case), and simpl discharges the second obligation (the inductive case). So in reality, the second proof above really only has a depth of two, not three.</p><p>Taking this sub-proof structure into account is important because it allows Proverbot9001 to discover more proofs for a fixed depth. In the example above, if the depth were set to two, and we used a naïve search, we would not find either of the proofs. However, at the same depth of two, a search which takes the sub-proof structure into account would be able to find the second proof (since this second proof would essentially be considered to have a depth of two, not three).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Evaluation</head><p>This section shows that Proverbot9001 is able to successfully solve many proofs. We also experimentally show that Proverbot9001 improves significantly on the stateof-the-art presented in previous work.</p><p>First, in Section 8.2, we compare experimentally to previous work, by running both Proverbot9001 and the CoqGym <ref type="bibr" target="#b40">[40]</ref> project on CompCert, in several configurations outlined in the CoqGym paper. Next, in Section 8.3, we experiment with using the weights learned from one project to produce proofs in another. Then, in Section 8.4, we show the "hardness" of proofs that Proverbot9001 is generally able to complete, using the length of the original solution as proxy for proof difficulty. Finally, in Appendix A.1, we measure the predictor subsystem, without proof search. Additional evaluation can be found in the appendix.</p><p>Experiments were run on two machines. Machine A is an Intel i7 machine with 4 cores, a NVIDIA Quadro P4000 8BG 256-bit, and 20 gigabytes of memory. Machine B is Intel Xeon E5-2686 v4 machine with 8 cores, a Nvidia Tesla v100 16GB 4096-bit, and 61 gigabytes of memory. Experiments were run using GNU Parallel <ref type="bibr" target="#b38">[38]</ref>.</p><p>During the development of Proverbot9001, we explored many alternatives, including n-gram/bag-of-words representations of terms, a variety of features, and several core models including k-nearest neighbors, support vector machines, and several neural architectures. While we include here some experiments that explore high-level design decisions (such as training and testing on the same projects vs cross project, working with and without solver-based tooling, modifying the search depth and width, and running with and without pre-processing), we also note that in the development of a large system tackling a hard problem, it becomes intractable to evaluate against every possible permutation of every design decision. In this setting, we are still confident in having demonstrated a system that works for the specific problem of generating correctness proof with performance that outperforms the state-of-the-art techniques by many folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Summary of Results</head><p>Proverbot9001, run using CoqHammer <ref type="bibr" target="#b11">[11]</ref> and the default configuration, is able to produce proofs for 28% of the theorem statements in CompCert. This represents a 2.4X improvement over the previous state-of-the-art. Without any external tooling, Proverbot9001 can produce proofs for 19.36%, an almost 4X improvement over previous state-of-the-art prediction-based proofs. Our core prediction model is able to reproduce the tactic name from the solution 32% of the time; and when the tactic name is correct, our model is able to predict the solution argument 89% of the time. We also show that Proverbot9001 can be trained on one project and then effectively predict on another project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Experimental Comparison to Previous</head><p>Work We tested Proverbot9001 end-to-end by training on the proofs from 162 files from CompCert, and testing on the proofs from 13 different files. On our default configuration, Proverbot9001 solves 19.36% (97/501) of the proofs in our test set.</p><p>In addition to running Proverbot9001 on CompCert, we ran the CoqGym <ref type="bibr" target="#b40">[40]</ref> tool, which represents the state of the art in this area, on the same dataset in several configurations.</p><p>To account for differences in training dataset, we ran CoqGym with their original training schema, and also our training schema, and reported the best of the two numbers. CoqGym is intended to be combined with a solver based proof-procedure, CoqHammer <ref type="bibr" target="#b11">[11]</ref>, which is run after every proof command invocation. While our system was not originally designed this way, we compare both systems using CoqHammer, as well as both systems without. We also compared our system to using CoqHammer on the initial goal directly, which simultaneously invokes Z3 <ref type="bibr" target="#b13">[13]</ref>, CVC4 <ref type="bibr" target="#b5">[6]</ref>, Vampire <ref type="bibr" target="#b26">[26]</ref>, and E Prover <ref type="bibr" target="#b36">[36]</ref>, in addition to attempting to solve the goal using a crush-like tactic <ref type="bibr" target="#b9">[9]</ref>. <ref type="figure" target="#fig_0">Figure 11</ref> shows the proofs solved by various configurations. The configurations are described in the caption. For all configurations, we ran Proverbot9001 with a search depth of 6 and a search width of 3 (see Appendix A.5). Note that in <ref type="figure" target="#fig_0">Figure 11</ref> the bars for H, G, and G H are prior work. The bars P, G+P and G H +P H are the ones made possible by our work.</p><p>When CoqHammer is not used, Proverbot9001 can complete nearly 4 times the number of proofs that are completed by CoqGym. In fact, even when CoqGym is augmented with CoqHammer Proverbot9001 by itself (without CoqHammer) still completes 39 more proofs, which is a 67% improvement (and corresponds to about 8% of the test set). When enabling CoqHammer in both CoqGym and Proverbot9001, we see that CoqGym solves 48 proofs whereas Proverbot9001 solves 138 proofs, which is a 2.88X improvement over the state of art.</p><p>Finally, CoqGym and Proverbot9001 approaches are complementary; both can complete proofs which the other cannot. Therefore, one can combine both tools to produce more solutions than either alone. Combining Co-qGym and Proverbot9001, without CoqHammer, allows us to complete 100/501 proofs, a proof success rate of  <ref type="figure" target="#fig_0">Figure 11</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Cross-Project Predictions</head><p>To test Proverbot9001's ability to make use of training across projects, we used the weights learned from CompCert, and ran Proverbot9001 in its default configuration on three other Coq projects from the Coq Contrib collection, concat, float, and zfc. concat is a library of constructive category theory proofs, which showcases Coq proofs of mathematical concepts instead of program correctness. The concat library is made of 514 proofs across 105 files; Prover-bot9001 was able to successfully produce a proof for 91 (17.7%) of the extracted theorem statements, without the use of CoqHammer.</p><p>float is a formalization of floating point numbers, made of 742 proofs across 38 files; Proverbot9001 was able to successfully produce a proof for 100 (13.48%) proofs.</p><p>zfc is a formalization of set theory made of 241 proofs across 78 files; 41 (17.01%) were successfully completed.</p><p>The comparable number for CompCert was 19.36%. <ref type="figure" target="#fig_0">Figure 12</ref>. A histogram plotting the original proof lengths in proof commands vs number of proofs of that length, in three classes, for proofs with length 10 or less. From bottom to top: proofs solved, proofs unsolved because of depth limit, and proofs where our search space was exhausted without finding a solution. <ref type="figure" target="#fig_0">Figure 13</ref>. A histogram plotting the original proof lengths in proof commands vs number of proofs of that length, in three classes. From bottom to top: proofs solved, proofs unsolved because of depth limit, and proofs where our search space was exhausted without finding a solution. Note that most proofs are between 0 and 10 proof commands long, with a long tail of much longer proofs.</p><p>These results demonstrate not only that Proverbot9001 can operate on proof projects in a variety of domains, but more importantly that it can effectively transfer training from one project to another. This would allow programmers to use Proverbot9001 even in the initial development of a project, if it had been previously trained on other projects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Original Proof Length vs Completion Rate</head><p>In <ref type="figure" target="#fig_0">Figure 12</ref> and <ref type="figure" target="#fig_0">Figure 13</ref>, we plot a histogram of the original proof lengths (in proof commands) vs the number of proofs of that length. We break down the proofs by (from bottom to top) number we solve, number we cannot solve but still have unexplored nodes, and number run out of unexplored nodes before finding a solution. Note that for the second class (middle bar), it's possible that increasing the search depth would allow us to complete the proof. <ref type="figure" target="#fig_0">Figure 12</ref> shows proofs of length 10 or below, and <ref type="figure" target="#fig_0">Figure 13</ref> shows all proofs, binned in sets of 10.</p><p>There are several observations that can be made. First, most original proofs in our test set are less than 20 steps long, with a heavy tail of longer proofs. Second, we do better on shorter proofs. Indeed, 51% (256/501) of the original proofs in our test set are ten proof commands or shorter, and of those proofs, we can solve 35% (89/256), compared to our overall solve rate of 19.36% (97/501). Third, we are in some cases able to handle proofs whose original length is longer than 10. Indeed, 7 of the proofs we solve (out of 79 solved) had an original length longer than 10. In fact, the longest proof we solve is originally 25 proof commands long; linearized it's 256 proof commands long. Our solution proof is 267 (linear) proof commands long, comparable to the original proof, with frequent case splits. The depth limit for individual obligations in our search was 6 in all of these runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Program Synthesis</head><p>Program Synthesis is the automatic generation of programs from a high-level specification <ref type="bibr" target="#b18">[18]</ref>. This specification can come in many forms, the most common being a logical formula over inputs and outputs, or a set of inputoutput examples. Programs generated can be in a variety of paradigms and languages, often domain-specific. Our tool, Proverbot9001, is a program synthesis tool that focuses on synthesis of proof command programs.</p><p>Several program synthesis works have used types extensively to guide search. Some work synthesizes programs purely from their types <ref type="bibr" target="#b19">[19]</ref>, while other work uses both a type and a set of examples to synthesize programs <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b33">33]</ref>. In Proverbot9001, the programs being synthesized use a term type as their specification, however, the proof command program itself isn't typed using that type, rather it must generate a term of that type (through search).</p><p>Further work in <ref type="bibr" target="#b29">[29]</ref> attempts to learn from a set of patches on GitHub, general rules for inferring patches to software. This work does not use traditional machine learning techniques, but nevertheless learns from data, albeit in a restricted way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Machine Learning for Code</head><p>Machine learning for modeling code is a well explored area <ref type="bibr" target="#b1">[2]</ref>, as an alternative to more structured methods of modeling code. Several models have been proposed for learning code, such as AST-like trees <ref type="bibr" target="#b32">[32]</ref>, long-term language models <ref type="bibr" target="#b12">[12]</ref>, and probabilistic grammars <ref type="bibr" target="#b6">[7]</ref>. Proverbot9001 does not attempt to be so general, using a model of programs that is specific to its domain, allowing us to capture the unique dependencies of proof command languages. While the model is simple, it is able to model real proofs better than more general models in similar domains (see <ref type="bibr">Section 8.2)</ref>. Machine learning has been used for various tasks such as code and patch generation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">12]</ref>, program classification <ref type="bibr" target="#b32">[32]</ref>, and learning loop invariants <ref type="bibr" target="#b16">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Machine Learning for Proofs</head><p>While machine learning has previously been explored for various aspects of proof writing, we believe there are still significant opportunities for improving on the state-ofthe-art, getting closer and closer to making foundational verification broadly applicable.</p><p>More concretely, work on machine learning for proofs includes: using machine learning to speed up automated solvers <ref type="bibr" target="#b3">[4]</ref>, developing data sets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b40">40]</ref>, doing premise selection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">30]</ref>, pattern recognition <ref type="bibr" target="#b25">[25]</ref>, clustering proof data <ref type="bibr" target="#b24">[24]</ref>, learning from synthetic data <ref type="bibr" target="#b21">[21]</ref>, interactively suggesting tactics <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b24">24]</ref>.</p><p>Finally, CoqGym attempts to model proofs with a fully general proof command and term model expressing arbitrary AST's. We experimentally compare Prover-bot9001's ability to complete proofs to that of CoqGym in detail in Section 8.2 There are also several important conceptual differences. First, the argument model in Co-qGym is not as expressive as the one in Proverbot9001. CoqGym's argument model can predict a hypothesis name, a number between 1 and 4 (which many tactics in Coq interpret as referring to binders, for example induction 2 performs induction on the second quantified variable), or a random (not predicted using machine learning) quantified variable in the goal. In contrast, the argument model in Proverbot9001 can predict any token in the goal, which subsumes the numbers and the quantified variables that CoqGym can predict. Most importantly because Proverbot9001's model can predict symbols in the goal, which allows effective unfolding, for example "unfold eq". Second, in contrast to Coq-Gym, Proverbot9001 uses several hand-tuned features for predicting proof commands. One key example is the previous tactic, which CoqGym does not even encode as part of the context. Third, CoqGym's treatment of higher-order proof commands like ";" is not as effective as Proverbot9001's. While neither system can predict ";", Proverbot9001 learns from ";" by linearizing them, whereas CoqGym does not.</p><p>There is also a recent line of work on doing end-toend proofs in Isabelle/HOL and HOL4 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b34">34]</ref>. This work is hard to experimentally compare to ours, since they use different benchmark sets, proof styles, and proof languages. Their most recent work <ref type="bibr" target="#b34">[34]</ref> uses graph representations of terms, which is a technique that we have not yet used, and could adapt if proven successful.</p><p>Finally, there is also another approach to proof generation, which is to generate the term directly using language translation models <ref type="bibr" target="#b37">[37]</ref>, instead of using tactics; however this technique has only been applied to small proofs due to its direct generation of low-level proof term syntax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix: Additional Evaluation</head><p>We now explore more detailed measurements about proof production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Individual Prediction Accuracy</head><p>We want to measure the effectiveness of the predictor subsystem that predicts proof command pairs (the function defined in Section 5). To do this, we broke the test dataset down into individual (linearized) proof commands, and ran to just before each proof command to get its prediction context. Then we fed that context into our predictor, and compared the result to the proof command in the original solution. Of all the proof commands in our test dataset, we are able to predict 28.66% (3784/13203) accurately. This includes the correct tactic and the correct argument. If we only test on the proof commands which are in Proverbot9001's prediction domain, we are able to predict 39.25% (3210/8178) accurately.</p><p>During search, our proof command predictor returns the top N tactics for various values of N, and all of these proof commands are tried. Therefore, we also measured how often the proof command in the original proof is in the top 3 predictions, and the top 5 predictions. For all proof commands in the data set, the tactic in the original proof is in our top 3 predictions 38.93% of the time, and in our top 5 predictions 42.66% of the time. If we restrict to proof commands in Proverbot9001's prediction domain, those numbers are 52.17% and 60.39%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Argument Accuracy</head><p>Our argument prediction model is crucial to the success of our system, and forms one of the main contributions of our work. To measure its efficacy at improving search is hard, because it's impossible to separate its success in progressing a proof from the success of the tactic predictor. However, we can measure how it contributes to individual prediction accuracy.</p><p>On our test dataset, where we can predict the full proof command in the original proof correctly 28.66% of the time, we predict the tactic correctly but the argument wrong 32.24% of the time. Put another way, when we successfully predict the tactic, we can predict the argument successfully with 89% accuracy. If we only test on proof commands within Proverbot9001's prediction domain, where we correctly predict the entire proof command 39.25% of the time, we predict the name correctly 41.01% of the time; that is, our argument accuracy is 96% when we get the tactic right. It's important to note, however, that many common tactics don't take any arguments, and thus predicting their arguments is trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Completion Rate in Proverbot9001's</head><p>Prediction Domain Proverbot9001 has a restricted model of proof commands: it only captures proof commands with a single argument that is a hypothesis identifier or a token in the goal. As result, it makes sense to consider Proverbot9001 within the context of proofs that were originally solved with these types of proof commands. We will call proofs that were originally solved using these types of proof commands proofs that are in Proverbot9001's prediction domain.</p><p>There are 79 such proofs in our test dataset (15.77% of the proofs in the test dataset), and Proverbot9001 was able to solve 48 of them.</p><p>What is interesting is that Proverbot9001 is able to solve proofs that are not in its prediction domain: these are proofs that were originally performed with proof commands that are not in Proverbot9001's domain, but Proverbot9001 found another proof of the theorem that is in its domain. This happened for 49 proofs (out of a total of 97 solved proofs). Sometimes this is because Prover-bot9001 is able to find a simpler proof command which fills the exact role of a more complex one in the original proof; for instance, destruct (find_symbol ge id) in an original proof is replaced by destruct find_symbol in Proverbot9001's solution. Other times it is because Proverbot9001 finds a proof which takes an entirely different path than the original. In fact, 31 of Proverbot9001's 97 found solutions are shorter than the original. It's useful to note that while previous work had a more expressive proof command model, in practice it was unable to solve as many proofs as Proverbot9001 could in our more restricted model.</p><p>Together, these numbers indicate that the restricted tactic model used by Proverbot9001 does not inhibit its ability to solve proofs in practice, even when the original proof solution used tactics outside of that model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Data Transformation</head><p>Crucial to Proverbot9001's performance is its ability to learn from data which is not initially in its proof command model, but can be transformed into data which is. This includes desugaring tacticals like now, splitting up multi-argument tactics like unfold a, b into single argument ones, and rearranging proofs with semicolons into linear series of proof commands. To evaluate how much this data transformation contributes to the overall performance of Proverbot9001, we disabled it, and instead filtered the proof commands in the dataset which did not fit into our proof command model.</p><p>With data transformation disabled, and the default search width (5) and depth <ref type="bibr" target="#b5">(6)</ref>, the proof completion accuracy of Proverbot9001 is 15.57% (78/501 proofs). Recall that with data transformation enabled as usual, this accuracy is 19.36%. This shows that the end-to-end performance of Proverbot9001 benefits greatly from the transformation of input data, although it still outperforms prior work (CoqGym) without it.</p><p>When we measure the individual prediction accuracy of our model, trained without data transformation, we see that its performance significantly decreases (16.32% instead of 26.77%), demonstrating that the extra data produced by preprocessing is crucial to training a good tactic predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Search Widths and Depths</head><p>Our search procedure has two main parameters, a search width, and a search depth. The search width is how many predictions are explored at each context. The search depth is the longest path from the root a single proof obligation state can have.</p><p>To explore the space of possible depths and widths, we varied the depth and width, on our default configuration without external tooling. With a search width of 1 (no search, just running the first prediction), and a depth of 6, we can solve 5.59% (28/501) of proofs in our test dataset. With a search width of 2, and a depth of 6, we're able to solve 16.17% (81/501) of proofs, as opposed to a width of 3 and depth of 6, where we can solve 19.36% of proofs.</p><p>To explore variations in depth, we set the width at 3, and varied depth. With a depth of 2, we were able to solve 5.19% (26/501) of the proofs in our test set. By increasing the depth to 4, we were able to solve 13.97% (70/501) of the proofs in our test set. At a depth of 6 (our default), that amount goes up to 19.36% (97/501).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) A feed-forward neural network, where each individual gray circle is a perceptron (b) An individual perceptron, which multiplies all the inputs by weights, sums up the results, and then applies a nonlinear function .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The overall architecture of Proverbot9001, built using CoqSerapi, Python, and PyTroch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Definition binary_constructor_sound (cstr: expr -&gt; expr -&gt; expr) (sem: val -&gt; val -&gt; val) : Prop := forall le a x b y, eval_expr ge sp e m le a x -&gt; eval_expr ge sp e m le b y -&gt; exists v, eval_expr ge sp e m le (cstr a b) v /\ Val.lessdef (sem x y) v.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Formalism to model a Proof Assistant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>The overall prediction model, combining the tactic prediction and argument prediction models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>The architecture for training the tactic models.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/UCSD-PL/proverbot9001 2 This training/test split comes from splitting the dataset 90/10, and then removing from the test set files that don't contain proofs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Joseph Redmon for his invaluable help building the first Proverbot9001 version, Prover-bot9000.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">DeepMath -Deep Sequence Models for Premise Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04442</idno>
		<ptr target="http://arxiv.org/abs/1606.04442" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Survey of Machine Learning for Big Code and Naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earl</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><forename type="middle">T</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06182</idno>
		<ptr target="http://arxiv.org/abs/1709.06182" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Verification of a Cryptographic Primitive: SHA-256</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Appel</surname></persName>
		</author>
		<idno type="DOI">10.1145/2701415</idno>
		<ptr target="https://doi.org/10.1145/2701415" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Program. Lang. Syst</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to Solve SMT Formulas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mislav</forename><surname>Balunoviundefined</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavol</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS&apos;18)</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems (NIPS&apos;18)<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10338" to="10349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">HOList: An Environment for Machine Learning of Higher-Order Theorem Proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><surname>Wilcox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03241</idno>
		<ptr target="http://arxiv.org/abs/1904.03241" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>extended version</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">L</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Deters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liana</forename><surname>Hadarean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejan</forename><surname>Jovanović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Tinelli</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2032305.2032319" />
		<title level="m">Proceedings of the 23rd International Conference on Computer Aided Verification (CAV&apos;11)</title>
		<meeting>the 23rd International Conference on Computer Aided Verification (CAV&apos;11)<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="171" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PHOG: Probabilistic Model for Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavol</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/bielik16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>Maria Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning ( Machine Learning Research<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Verifying a High-performance Crash-safe File System Using a Tree Specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haogang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tej</forename><surname>Chajed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Konradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atalay</forename><surname>İleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Chlipala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Frans</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nickolai</forename><surname>Zeldovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles (SOSP &apos;17)</title>
		<meeting>the 26th Symposium on Operating Systems Principles (SOSP &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3132747.3132776</idno>
		<ptr target="https://doi.org/10.1145/3132747.3132776" />
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="270" to="286" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Certified Programming with Dependent Types: A Pragmatic Introduction to the Coq Proof Assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Chlipala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<ptr target="http://arxiv.org/abs/1406.1078" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hammer for Coq: Automation for Dependent Type Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Czajka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10817-018-9458-4</idno>
		<ptr target="https://doi.org/10.1007/s10817-018-9458-4" />
	</analytic>
	<monogr>
		<title level="j">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="423" to="453" />
			<date type="published" when="2018-06-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A deep language model for software code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Hoa Khanh Dam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trang</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02715</idno>
		<ptr target="http://arxiv.org/abs/1608.02715" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Z3: An Efficient SMT Solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>De Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaj</forename><surname>Bjørner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools and Algorithms for the Construction and Analysis of Systems</title>
		<editor>C. R. Ramakrishnan and Jakob Rehof</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="337" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Coq Proof Assistant -Reference Manual Version 6</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Christophe</forename><surname>Filliâtre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Herbelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Barras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Barras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Boutin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Giménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Boutin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">César</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Cornes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Cornes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judicaël</forename><surname>Courant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judicael</forename><surname>Courant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Murthy</surname></persName>
		</author>
		<editor>Christine Paulin-mohring, Christine Paulin-mohring, Amokrane Saibi, Amokrane Saibi, Benjamin Werner, and Benjamin Werner</editor>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Catherine Parent, Catherine Parent</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Example-directed synthesis: a type-theoretic interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter-Michael</forename><surname>Osera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zdancewic</surname></persName>
		</author>
		<idno type="DOI">10.1145/2914770.2837629</idno>
		<ptr target="https://doi.org/10.1145/2914770.2837629" />
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="802" to="815" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Invariants Using Decision Trees and Implication Counterexamples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Madhusudan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1145/2914770.2837664</idno>
		<ptr target="https://doi.org/10.1145/2914770.2837664" />
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="499" to="512" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TacticToe: Learning to Reason with HOL4 Tactics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<idno type="DOI">10.29007/ntlb</idno>
		<ptr target="https://doi.org/10.29007/ntlb" />
	</analytic>
	<monogr>
		<title level="m">LPAR-21. 21st International Conference on Logic for Programming, Artificial Intelligence and Reasoning (EPiC Series in Computing)</title>
		<editor>Thomas Eiter and David Sands</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="125" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dimensions in Program Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/dimensions-program-synthesis/" />
	</analytic>
	<monogr>
		<title level="m">PPDP &apos;10 Hagenberg, Austria (ppdp &apos;10 hagenberg</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Complete Completion using Types and Weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tihomir</forename><surname>Gvero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Kuncak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kuraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruzica</forename><surname>Piskac</surname></persName>
		</author>
		<ptr target="http://infoscience.epfl.ch/record/188990" />
	</analytic>
	<monogr>
		<title level="j">PLDI</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ACL2(ml): Machine-Learning for ACL2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jónathan</forename><surname>Heras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Komendantskaya</surname></persName>
		</author>
		<idno type="DOI">10.4204/EPTCS.152.5</idno>
		<ptr target="https://doi.org/10.4204/EPTCS.152.5" />
	</analytic>
	<monogr>
		<title level="m">Proceedings Twelfth International Workshop on the ACL2 Theorem Prover and its Applications</title>
		<meeting>Twelfth International Workshop on the ACL2 Theorem Prover and its Applications<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-07" />
			<biblScope unit="page" from="61" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">GamePad: A Learning Environment for Theorem Proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00608</idno>
		<ptr target="http://arxiv.org/abs/1806.00608" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Dawn Song, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">HolStep: A Machine Learning Dataset for Higherorder Logic Theorem Proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00426</idno>
		<ptr target="http://arxiv.org/abs/1703.00426" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">seL4: Formal Verification of an OS Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerwin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Elphinstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Heiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June</forename><surname>Andronick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Derrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhammika</forename><surname>Elkaduwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Engelhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Kolanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Norrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harvey</forename><surname>Tuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Winwood</surname></persName>
		</author>
		<idno type="DOI">10.1145/1629575.1629596</idno>
		<ptr target="https://doi.org/10.1145/1629575.1629596" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22Nd Symposium on Operating Systems Principles (SOSP &apos;09)</title>
		<meeting>the ACM SIGOPS 22Nd Symposium on Operating Systems Principles (SOSP &apos;09)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="207" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Komendantskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jónathan</forename><surname>Heras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gudmund</forename><surname>Grov</surname></persName>
		</author>
		<idno type="DOI">10.4204/EPTCS.118.2</idno>
		<ptr target="https://doi.org/10.4204/EPTCS.118.2" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning in Proof General: Interfacing Interfaces. Electronic Proceedings in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Komendantskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kacper</forename><surname>Lichota</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33266-1_53</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-33266-1_53" />
		<title level="m">Neural Networks for Proof-Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7553</biblScope>
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">First-Order Theorem Proving and Vampire</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Kovács</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Voronkov</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-39799-8_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-39799-8_1" />
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8044</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kästner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joerg</forename><surname>Barrho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Wünsche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schlickling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ferdinand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandrine</forename><surname>Blazy</surname></persName>
		</author>
		<title level="m">CompCert: Practical Experience on Integrating and Qualifying a Formally Verified Optimizing Compiler</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Formal verification of a realistic compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Leroy</surname></persName>
		</author>
		<ptr target="http://xavierleroy.org/publi/compcert-CACM.pdf" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic inference of code transforms for patch generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Amidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rinard</surname></persName>
		</author>
		<idno type="DOI">10.1145/3106237.3106253</idno>
		<ptr target="https://doi.org/10.1145/3106237.3106253" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="727" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep Network Guided Proof Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06972</idno>
		<ptr target="http://arxiv.org/abs/1701.06972" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Toward a Verified Relational Database Management System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Malecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Morrisett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avraham</forename><surname>Shinnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Wisnesky</surname></persName>
		</author>
		<idno type="DOI">10.1145/1706299.1706329</idno>
		<ptr target="https://doi.org/10.1145/1706299.1706329" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL &apos;10)</title>
		<meeting>the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">TBCNN: A Tree-Based Convolutional Neural Network for Programming Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5718</idno>
		<ptr target="http://arxiv.org/abs/1409.5718" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Type-andexample-directed Program Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Osera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zdancewic</surname></persName>
		</author>
		<idno type="DOI">10.1145/2813885.2738007</idno>
		<ptr target="https://doi.org/10.1145/2813885.2738007" />
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="619" to="630" />
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graph Representations for Higher-Order Logic and Theorem Proving. CoRR abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10006</idno>
		<ptr target="http://arxiv.org/abs/1905.10006" />
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Natural Deduction as Higher-Order Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paulson</surname></persName>
		</author>
		<idno>cs.LO/9301104</idno>
		<ptr target="http://arxiv.org/abs/cs.LO/9301104" />
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">System Description: E 1.8</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 19th LPAR, Stellenbosch (LNCS)</title>
		<editor>Ken McMillan, Aart Middeldorp, and Andrei Voronkov</editor>
		<meeting>of the 19th LPAR, Stellenbosch (LNCS)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">8312</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Towards Proof Synthesis Guided by Neural Machine Translation for Intuitionistic Propositional Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Sekiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akifumi</forename><surname>Imanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Suenaga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06462</idno>
		<ptr target="http://arxiv.org/abs/1706.06462" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tange</surname></persName>
		</author>
		<ptr target="http://www.gnu.org/s/parallel" />
		<title level="m">GNU Parallel -The Command-Line Power Tool. ;login: The USENIX Magazine</title>
		<imprint>
			<date type="published" when="2011-02" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Verdi: A Framework for Implementing and Formally Verifying Distributed Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Woos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Panchekha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Tatlock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1145/2737924.2737958</idno>
		<ptr target="https://doi.org/10.1145/2737924.2737958" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI &apos;15)</title>
		<meeting>the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="357" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to Prove Theorems via Interacting with Proof Assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09381</idno>
		<ptr target="http://arxiv.org/abs/1905.09381" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Finding and Understanding Bugs in C Compilers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Regehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLDI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
