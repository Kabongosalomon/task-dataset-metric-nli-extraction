<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">U-Net: Machine Reading Comprehension with Unanswerable Questions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<email>yang.liu@liulishuo.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Liulishuo Silicon Valley AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Shanghai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">U-Net: Machine Reading Comprehension with Unanswerable Questions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing. A key subtask is to reliably predict whether the question is unanswerable. In this paper, we propose a unified model, called U-Net, with three important components: answer pointer, no-answer pointer, and answer verifier. We introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens. The universal node encodes the fused information from both the question and passage, and plays an important role to predict whether the question is answerable and also greatly improves the conciseness of the U-Net. Different from the stateof-art pipeline models, U-Net can be learned in an end-to-end fashion. The experimental results on the SQuAD 2.0 dataset show that U-Net can effectively predict the unanswerability of questions and achieves an F1 score of 71.7 on SQuAD 2.0.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Machine reading comprehension (MRC) is a challenging task in natural language processing, which requires that machine can read, understand, and answer questions about a text. Benefiting from the rapid development of deep learning techniques and large-scale benchmarks <ref type="bibr" target="#b1">(Hermann et al. 2015;</ref><ref type="bibr" target="#b2">Hill et al. 2015;</ref><ref type="bibr" target="#b7">Rajpurkar et al. 2016)</ref>, the end-to-end neural methods have achieved promising results on MRC task <ref type="bibr" target="#b4">Huang et al. 2017;</ref><ref type="bibr" target="#b1">Clark and Gardner 2017;</ref>. The best systems have even surpassed human performance on the Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b7">(Rajpurkar et al. 2016)</ref>, one of the most widely used MRC benchmarks. However, one of the limitations of the SQuAD task is that each question has a correct answer in the context passage, therefore most models just need to select the most relevant text span as the answer, without necessarily checking whether it is indeed the answer to the question.</p><p>To remedy the deficiency of SQuAD, <ref type="bibr" target="#b8">Rajpurkar, Jia, and Liang (2018)</ref> developed SQuAD 2.0 that combines SQuAD with new unanswerable questions. <ref type="table" target="#tab_0">Table 1</ref> shows two examples of unanswerable questions. The new dataset requires the MRC systems to know what they don't know.</p><p>To do well on MRC with unanswerable questions, the model needs to comprehend the question, reason among the * Corresponding Author.</p><p>Article: Endangered Species Act Paragraph: "... Other legislation followed, including the Migratory Bird Conservation Act of 1929, a 1937 treaty prohibiting the hunting of right and gray whales, and the Bald Eagle Protection Act of 1940. These later laws had a low cost to societythe species were relatively rareand little opposition was raised. Question 1: Which laws faced significant opposition? Plausible Answer: later laws Question 2: What was the name of the 1937 treaty? Plausible Answer: Bald Eagle Protection Act passage, judge the unanswerability and then identify the answer span. Since extensive work has been done on how to correctly predict the answer span when the question is answerable (e.g., SQuAD 1.1), the main challenge of this task lies in how to reliably determine whether a question is not answerable from the passage.</p><p>There are two kinds of approaches to model the answerability of a question. One approach is to directly extend previous MRC models by introducing a no-answer score to the score vector of the answer span <ref type="bibr" target="#b6">(Levy et al. 2017;</ref><ref type="bibr" target="#b1">Clark and Gardner 2017)</ref>. But this kind of approaches is relatively simple and cannot effectively model the answerability of a question. Another approach introduces an answer verifier to determine whether the question is unanswerable <ref type="bibr" target="#b3">(Hu et al. 2018;</ref><ref type="bibr" target="#b10">Tan et al. 2018</ref>). However, this kind of approaches usually has a pipeline structure. The answer pointer and answer verifier have their respective models, which are trained separately. Intuitively, it is unnecessary since the underlying comprehension and reasoning of language for these components is the same.</p><p>In this paper, we decompose the problem of MRC with unanswerable questions into three sub-tasks: answer pointer, no-answer pointer, and answer verifier. Since these three sub-tasks are highly related, we regard the MRC with unanswerable questions as a multi-task learning problem (Caruana 1997) by sharing some meta-knowledge.</p><p>We propose the U-Net to incorporate these three sub-tasks into a unified model: 1) an answer pointer to predict a can-didate answer span for a question; 2) a no-answer pointer to avoid selecting any text span when a question has no answer; and 3) an answer verifier to determine the probability of the "unanswerability" of a question with candidate answer information. Additionally, we also introduce a universal node and process the question and its context passage as a single contiguous sequence of tokens, which greatly improves the conciseness of U-Net. The universal node acts on both question and passage to learn whether the question is answerable. Different from the previous pipeline models, U-Net can be learned in an end-to-end fashion. Our experimental results on the SQuAD 2.0 dataset show that U-Net effectively predicts the unanswerability of questions and achieves an F1 score of 72.6.</p><p>The contributions of this paper can be summarized as follows.</p><p>• We decompose the problem of MRC with unanswerable questions into three sub-tasks and combine them into a unified model, which uses the shared encoding and interaction layers. Thus, the three-tasks can be trained simultaneously in an end-to-end fashion. • We introduce a universal node to encode the common information of the question and passage. Thus, we can use a unified representation to model the question and passage, which makes our model more condensed. • U-Net is very easy to implement yet effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Model</head><p>Formally, we can represent the MRC problem as: given a set of tuples (Q, P, A), where Q = (q 1 , q 2 , · · · , q m ) is the question with m words, P = (p 1 , p 2 , · · · , p n ) is the context passage with n words, and A = p rs:re is the answer with r s and r e indicating the start and end points, the task is to estimate the conditional probability P (A|Q, P ).</p><p>The architecture of our proposed U-Net is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>U-Net consists of four major blocks: Unified Encoding, Multi-Level Attention, Final Fusion, and Prediction. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we first combine the embedded representation of the question and passage with a universal node u and pass them through a BiLSTM to encode the whole text. We then use the encoded representation to do the information interaction. Then we use the encoded and interacted representation to fuse the full representation and feed them into the final prediction layers to do the multi-task training.</p><p>We will describe our model in details in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(A) Unified Encoding</head><p>Embedding Following the successful models on SQuAD 1.1, we first embed both the question and the passage with the following features. Glove embedding (Pennington, Socher, and Manning 2014) and Elmo embedding <ref type="bibr" target="#b7">(Peters et al. 2018</ref>) are used as basic embeddings. Besides, we use POS embedding, NER embedding, and a feature embedding that includes the exact match, lower-case match, lemma match, and a TF-IDF feature ). represented as a d-dim embedding by combining the features/embedding described above.</p><p>Universal Node We create a universal node u, which is a key factor in our model and has several roles in predicting the unanswerability of question Q. We expect this node to learn universal information from both passage and question. This universal node is added and connects the passage and question at the phase of embedding, and then goes along with the whole representation, so it is a key factor in information representation. Since the universal node is in between and later shared between passage and question, it has an abstract semantic meaning rather than just a word embedding.</p><p>Also, the universal node is later shared in the attention interaction mechanism and used in both the answer boundary detection and classification tasks, so this node carries massive information and has several important roles in our whole model construction.</p><p>The universal node u is first represented by a d-dim randomly-initialized vector. We concatenated question representation, universal node representation, passage representation together as:</p><formula xml:id="formula_0">V = [Q, u, P ] = [q 1 , q 2 . . . q m , u, p 1 , p 2 , · · · , p n ], (1) V ∈ R d×(m+n+1)</formula><p>is a joint representation of question and passage.</p><p>Word-level Fusion Then we first use two-layer bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber 1997) to fuse the joint representation of question, universal node, and passage.</p><formula xml:id="formula_1">H l = BiLSTM(V ),<label>(2)</label></formula><formula xml:id="formula_2">H h = BiLSTM(H l ),<label>(3)</label></formula><p>where H l is the hidden states of the first BiLSTM, representing the low-level semantic information, and H h is the hidden states of the second BiLSTM, representing the highlevel semantic information. Finally, we concatenate H l and H h together and pass them through the third BiLSTM and obtain a full representation H f .</p><formula xml:id="formula_3">H f = BiLSTM([H l ; H h ]).<label>(4)</label></formula><p>Thus, H = [H l ; H h ; H f ] represents the deep fusion information of the question and passage on word-level. When a BiLSTM is applied to encode representations, it learns the semantic information bi-directionally. Since the universal node u is between the question and passage, its hidden states h m+1 can learn both question and passage information. When the passage-question pair was encoded as a unified representation and information flows via the BiLSTM, the universal node has an important role in information representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(B) Multi-Level Attention</head><p>To fully fuse the semantic representation of the question and passage, we use the attention mechanism (Bahdanau, Cho, and Bengio 2014) to capture their interactions on different levels.</p><p>We expected that we could simply use self-attention on the encoded representation H for interaction between question and passage, which contains both bi-attention <ref type="bibr" target="#b9">(Seo et al. 2016</ref>) and self-attention ) of the question and passage. But we found that it performed slightly worse than the traditional bi-directional attention with the universal node included. Therefore, we use a bi-directional attention between the question and passage.</p><p>We first divide H into two representations: attached passage H q and attached question H p , and let the universal node representation h m+1 attached to both the passage and question, i.e.,</p><formula xml:id="formula_4">H q = [h 1 , h 2 , · · · , h m+1 ],<label>(5)</label></formula><formula xml:id="formula_5">H p = [h m+1 , h m+2 , · · · , h m+n+1 ],<label>(6)</label></formula><p>Note h m+1 is shared by H q and H p . Here the universal node works as a special information carrier, and both passage and question can focus attention information on this node so that the connection between them is closer than a traditional biattention interaction.</p><formula xml:id="formula_6">Since both H q = [H l q ; H h q ; H f q ] and H p = [H l p ; H h p ; H f p ]<label>are</label></formula><p>concatenated by three-level representations, we followed previous work FusionNet  to construct their iterations on three levels.</p><p>Take the first level as an example. We first compute the affine matrix of H l q and H l p by</p><formula xml:id="formula_7">S = ReLU(W 1 H l q ) T ReLU(W 2 H l p ),<label>(7)</label></formula><p>where S ∈ R (m+1)×(n+1) ; W 1 and W 2 are learnable parameters. Next, a bi-directional attention is used to compute the interacted representation H l q and H l p .</p><formula xml:id="formula_8">H l q = H l p × softmax(S T ),<label>(8)</label></formula><formula xml:id="formula_9">H l p = H l q × softmax(S),<label>(9)</label></formula><p>where softmax(·) is column-wise normalized function. We use the same attention layer to model the interactions for all the three levels, and get the final fused representation H l , H h , H f for the question and passage respectively.</p><p>Note that while dealing with the attention output of the universal node, we added two outputs from passage-toquestion attention and question-to-passage attention. So after the interaction, the fused representation H l , H h , H f still have the same length as the encoded representation H l , H h and H f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(C) Final Fusion</head><p>After the three-level attentive interaction, we generate the final fused information for the question and passage. We concatenate all the history information: we first concatenate the encoded representation H and the representation after attention H (again, we use H l , H h , H f , and H l , H h , H f to represent 3 different levels of representation for the two previous steps respectively).</p><p>Following the success of DenseNet (Huang, Liu, and Weinberger 2016), we concatenate the input and output of each layer as the input of the next layer.</p><p>First, we pass the concatenated representation H through a BiLSTM to get H A .</p><formula xml:id="formula_10">H A = BiLSTM [H l ; H h ; H f ; H l ; H h ; H f ] ,<label>(10)</label></formula><p>where the representation H A is a fusion of information from different levels. Then we concatenate the original embedded representation V and H A for better representation of the fused information of passage, universal node, and question.</p><formula xml:id="formula_11">A = [V ; H A ].<label>(11)</label></formula><p>Finally, we use a self-attention layer to get the attention information within the fused information. The self-attention layer is constructed the same way as <ref type="bibr" target="#b10">(Vaswani et al. 2017)</ref>:</p><formula xml:id="formula_12">A = A × softmax(A T A),<label>(12)</label></formula><p>where A is the representation after self-attention of the fused information A. Next we concatenated representation H A and A and pass them through another BiLSTM layer:</p><formula xml:id="formula_13">O = BiLSTM[H A ; A].<label>(13)</label></formula><p>Now O is the final fused representation of all the information. At this point, we divide O into two parts: O P , O Q , representing the fused information of the question and passage respectively.</p><formula xml:id="formula_14">O P = [o 1 , o 2 , · · · , o m ],<label>(14)</label></formula><formula xml:id="formula_15">O Q = [o m+1 , o m+2 , · · · , o m+n+1 ],<label>(15)</label></formula><p>Note for the final representation, we attach the universal node only in the passage representation O P . This is because we need the universal node as a focus for the pointer when the question is unanswerable. These will be fed into the next decoder prediction layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(D) Prediction</head><p>The prediction layer receives fused information of passage O P and question O Q , and tackles three prediction tasks:</p><p>(1) answer pointer, (2) no-answer pointer and (3) answer verifier. First, we use a function shown below to summarize the question information O Q into a fixed-dim representation c q .</p><formula xml:id="formula_16">c q = i exp(W q o Q i ) j exp(W o Q j ) o Q i ,<label>(16)</label></formula><p>where W q is a learnable weight matrix and o Q i represents the i th word in the question representation. Then we feed c q into the answer pointer to find boundaries of answers <ref type="bibr" target="#b11">(Wang and Jiang 2016)</ref>, and the classification layer to distinguish whether the question is answerable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(i) Answer Pointer</head><p>We use this answer pointer to detect the answer boundaries from the passage when the question is answerable (i.e., the answer is a span in the passage). This layer is a classic pointer net structure <ref type="bibr" target="#b11">(Vinyals, Fortunato, and Jaitly 2015)</ref>. We use two trainable matrices W s and W e to estimate the probability of the answer start and end boundaries of the i th word in the passage, α i and β i .</p><formula xml:id="formula_17">α i ∝ exp(c q W s o P i ),<label>(17)</label></formula><formula xml:id="formula_18">β i ∝ exp(c q W e o P i ),<label>(18)</label></formula><p>Note here when the question is answerable, we do not consider the universal node in answer boundary detection, so we have i &gt; 0 (i = 0 is the universal node in the passage representation). The loss function for the answerable question pairs is:</p><formula xml:id="formula_19">L A = − log α a + log β b ,<label>(19)</label></formula><p>where a and b are the ground-truth of the start and end boundary of the answer.</p><p>(ii) No-Answer Pointer Then we use the same pointer for questions that are not answerable. Here the loss L N A is:</p><formula xml:id="formula_20">L N A = − log α 0 + log β 0 ,<label>(20)</label></formula><p>α 0 and β 0 correspond to the position of the universal node, which is at the front of the passage representation O p . For this scenario, the loss is calculated for the universal node. Additionally, since there exits a plausible answer for each unanswerable question in SQuAD 2.0, we introduce an auxiliary plausible answer pointer to predict the boundaries of the plausible answers. The plausible answer pointer has the same structure as the answer pointer, but with different parameters. Thus, the total loss function is:</p><formula xml:id="formula_21">L N A = − log α 0 + log β 0 − log α a * + log β b * ,<label>(21)</label></formula><p>where α and β are the output of the plausible answer pointer; a * and b * are the start and end boundary of the unanswerable answer.</p><p>The no-answer pointer and plausible answer pointer are removed at test phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(iii) Answer Verifier</head><p>We use the answer verifier to distinguish whether the question is answerable.</p><p>Answer verifier applies a weighted summary layer to summarize the passage information into a fixed-dim representation c q (as shown in Eq. <ref type="formula" target="#formula_5">(16)</ref>).</p><p>And we use the weight matrix obtained from the answer pointer to get two representations of the passage.</p><formula xml:id="formula_22">c s = i α i · o P i (22) c e = i β i · o P i<label>(23)</label></formula><p>Then we use the universal node o m+1 and concatenate it with the summary of question and passage to make a fixed vector</p><formula xml:id="formula_23">F = [c q ; o m+1 ; c s ; c e ].<label>(24)</label></formula><p>This fixed F includes the representation c q representing the question information, and c s and c e representing the passage information. Since these representations are highly summarized specially for classification, we believe that this passage-question pair contains information to distinguish whether this question is answerable. In addition, we include the universal node as a supplement. Since the universal node is pointed at when the question is unanswerable and this node itself already contains information collected from both the passage and question during encoding and information interaction, we believe that this node is important in distinguishing whether the question is answerable. Finally, we pass this fixed vector F through a linear layer to obtain the prediction whether the question is answerable.</p><formula xml:id="formula_24">p c = σ(W T f F )<label>(25)</label></formula><p>where σ is a sigmoid function, W f is a learnable weight matrix.</p><p>Here we use the cross-entropy loss in training.</p><formula xml:id="formula_25">L AV = − δ · log p c + (1 − δ) · (log (1 − p c )) ,<label>(26)</label></formula><p>where δ ∈ {0, 1} indicates whether the question has an answer in the passage.</p><p>Compared with other relatively complex structures developped for this MRC task, our U-Net model passes the original question and passage pair through embedding and encoding, which then interacts with each other, yielding fused information merged from all the levels. The entire architecture is very easy to construct. After we have the fused representation of the question and passage, we pass them through the pointer layer and a fused information classification layer in a multi-task setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>We jointly train the three tasks by combining the three loss functions. The final loss function is:</p><formula xml:id="formula_26">L = δL A + (1 − δ)L N A + L AV ,<label>(27)</label></formula><p>where δ ∈ {0, 1} indicates whether the question has an answer in the passage, L A , L N A and L AV are the three loss functions of the answer pointer, no-answer pointer, and answer verifier. Although the three tasks could have different weights in the final loss function and be further fine-tuned after joint training, here we just consider them in the same weight and do not fine-tune them individually.</p><p>At the test phase, we first use the answer pointer to find a potential answer to the question, while the verifier layer judges whether the question is answerable. If the classifier predicts the question is unanswerable, we consider the answer extracted by the answer pointer as plausible. In this way, we get the system result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Datasets</head><p>Recently, machine reading comprehension and question answering have progressed rapidly, owing to the computation ability and publicly available high-quality datasets such as SQuAD. Now new research efforts have been devoted to the newly released answer extraction test with unanswerable questions, SQuAD 2.0 <ref type="bibr" target="#b8">(Rajpurkar, Jia, and Liang 2018)</ref>. It is constructed by combining question-answer pairs selected from SQuAD 1.0 and newly crafted unanswerable questions. These unanswerable questions are created by workers that were asked to pose questions that cannot be answered based on the paragraph alone but are similar to the answerable questions. It is very difficult to distinguish these questions from the answerable ones. We evaluate our model using this data set. It contains over 100,000+ questions on 500+ wikipedia articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We use Spacy to process each question and passage to obtain tokens, POS tags, NER tags and lemmas tags of each text. We use 12 dimensions to embed POS tags, 8 for NER tags . We use 3 binary features: exact match, lower-case match and lemma match between the question and passage . We use 100-dim Glove pretrained word embeddings and 1024-dim Elmo embeddings. All the LSTM blocks are bi-directional with one single layer.</p><p>We set the hidden layer dimension as 125, attention layer dimension as 250. We added a dropout layer over all the modeling layers, including the embedding layer, at a dropout rate of 0.3 <ref type="bibr" target="#b9">(Srivastava et al. 2014)</ref>. We use Adam optimizer with a learning rate of 0.002 (Kingma and Ba 2014).</p><p>During training, we omit passage with over 400 words and question with more than 50 words. For testing, when the passage has over 600 words and the question is over 100 words, we simply label these questions as unanswerable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>Our model achieves an F1 score of 74.0 and an EM score of 70.3 on the development set, and an F1 score of 72.6 and an EM score of 69.2 on Test set 1 , as shown in <ref type="table" target="#tab_2">Table  2</ref>. Our model outperforms most of the previous approaches. Comparing to the best-performing systems, our model has a simple architecture and is an end-to-end model. In fact, among all the end-to-end models, we achieve the best F1 scores. We believe that the performance of the U-Net can be boosted with an additional post-processing step to verify answers using approaches such as <ref type="bibr" target="#b3">(Hu et al. 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We also do an ablation study on the SQuAD 2.0 development set to further test the effectiveness of different components in our model. In <ref type="table" target="#tab_3">Table 3</ref>, we show four different configurations.</p><p>First, we remove the universal node U . We let the negative examples focus on the plausible answer spans instead of focusing on the universal node U . This results in a loss of 2.6% F1 score on the development set, showing that the universal node U indeed learns information about whether the question is answerable.</p><p>We also tried to make the universal node U only attached to the passage representation when passing the attention layer. Our results showed that when node U is shared, as it is called 'universal', it learns information interaction between the question and passage, and when it is not shared, the performance slightly degraded.</p><p>As for the approaches to encode the representations, we pass both the question and passage through a shared BiL-STM. To test the effectiveness of this, we ran the experiment using separate BiLSTMs on embedded question and passage representations. Results show that the performance dropped slightly, suggesting sharing BiLSTM is an effective method to improve the quality of the encoder.</p><p>After removing the plausible answer pointer, the performance also dropped, indicating the plausible answers are useful to improve the model even though they are incorrect.</p><p>After removing the answer verifier, the performance dropped greatly, indicating it is vital for our model.</p><p>Lastly, we run a test using a more concise configuration. In the second block (multi-level attention) of the U-Net, we do not split the output of the encoded presentation and let it pass through a self-attention layer. The bidirectional attention is removed. In this way, our model uses only one unified  -0.7 -1.1 no classification 63.5 68.5 -6.8 -5.5</p><p>Self-Attn Only 69.7 73.5 -0.5 -0.5 representation of the question and passage at all time. We simply pass this representation layer by layer to get the final result. Compared to the bi-attention model, the F1-score decreases 0.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task Study</head><p>We also run an experiment to test the performance of our multi-task model. We select different losses that participate in the training procedure to observe the performance affected by answer boundary detect or classification. <ref type="table" target="#tab_4">Table 4</ref> shows the performance. Here we use EM * and F 1 * to represent the EM and F1 score when the classification is not part of the task, which makes it very much like the task in SQuAD 1.1.</p><p>Loss EM * F1 * Classification Acc.  To test our classifier performance, we do not use backward propagation over the loss of answer boundary detection and simply run a classification task. Results (the first two rows in <ref type="table" target="#tab_4">Table 4)</ref> show that there is a large gain when using the multi-task model. The answer boundary detection task helps the encoder learn information between the passage and question and also feed information into the universal node, therefore we can use a summarized representation of the passage and question as well as the universal node to distinguish whether the question is answerable, i.e., help improve classification.</p><p>For the answer boundary detection task, we find that the multi-task setup (i.e., the classification layer participates in the training process) does not help its performance. Since the classifier and pointer layer shared the encoding process, we originally expected that classification information can help detect answer boundaries. But this is not the case. We think this is also reasonable since distinguishing whether the question is answerable is mainly focusing on the interactions between the passage-question pair, so once the question is predicted as answerable or not, it has nothing to do with the answer boundaries. This is consistent with how human-beings do this classification task.</p><p>We also run the test over SQuAD 1.1 development test to evaluate the performance. Due to a condensed structure, our model achieves an F 1 * score of less than 86%, which is not a very competitive score on SQuAD 1.1 test. But as shown above, our model achieves a good score in SQuAD 2.0 test, which shows this model has the potential to achieve higher performance by making progress on both the answer detection and classification tasks.</p><p>Overall, we can conclude that our multi-task model works well since the performance of unanswerability classification improves significantly when the answer pointer and answer verifier work simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study on the Different Thresholds of Unanswerability Classification</head><p>The output b of the answer verifier is the probability of a question being unanswerable. The smaller the output, the lower the probability of unanswerability is. In SQuAD 2.0, the proportions of unanswerable questions are different in the training and test sets. The default threshold 0.5 is optimized on the training set, but not suitable for the test set. Therefore, it is reasonable to set a proper threshold to manually adapt to the test set.</p><p>As mentioned in SQuAD 2.0 paper <ref type="bibr" target="#b8">(Rajpurkar, Jia, and Liang 2018)</ref>, different thresholds for answerability prediction result in fluctuated scores between answerable and unanswerable questions. Here we show the variation of the F1 score with different thresholds in <ref type="figure">Figure .</ref> The threshold between [0, 1] is used to decide whether a question can be answered. When the threshold is set to 0, all questions are considered as answerable. As we can see, when the threshold is set to 0.5, F1 score of answerable questions is similar to that of unanswerable questions. When we increase the threshold (i.e., more likely to predict the question as unanswerable), performance for answerable questions degrades, and improves for unanswerable questions. This is as expected. We can see that the overall F 1 score is slightly better, which is consistent with the idea from SQuAD 2.0. In addition, we find that for larger thresholds, the variance between EM and F 1 is narrowed since EM and F 1 scores for unanswerable questions are the same.</p><p>Finally, we set the threshold to be 0.7 for the submission system to SQuAD evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>End-to-end Models for MRC Currently, end-to-end neural network models have achieved great successes for machine reading comprehension <ref type="bibr" target="#b9">(Seo et al. 2016;</ref><ref type="bibr" target="#b5">Kumar et al. 2015;</ref><ref type="bibr" target="#b10">Sukhbaatar et al. 2015;</ref><ref type="bibr" target="#b1">Cui et al. 2016;</ref><ref type="bibr" target="#b13">Xiong, Zhong, and Socher 2016;</ref><ref type="bibr" target="#b1">Dhingra et al. 2016;</ref><ref type="bibr" target="#b12">Wang, Yan, and Wu 2018)</ref>. Most of these models consist of three components: encoder, interaction, and pointer. The BiLSTM is widely used for encoding the embedded representation. For the interaction, bidirectional attention mechanism is very effective to fuse information of the question and passage. Finally, a pointer network <ref type="bibr" target="#b11">(Vinyals, Fortunato, and Jaitly 2015)</ref> is used to predict the span boundaries of the answer. Specifically, in SQuAD test <ref type="bibr" target="#b7">(Rajpurkar et al. 2016)</ref>, there are approaches to combine match-LSTM and pointer networks to produce boundaries of the answer and employ variant bidirectional attention mechanism to match the question and passage mutually.</p><p>In our model, we learn from previous work and develop a condensed end-to-end model for the SQuAD 2.0 task. Different from the previous models, we use a unified representation to encode the question and passage simultaneously, and introduce a universal node to encode the fused information of the question and passage, which also plays an important role to predict the unanswerability of a question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MRC with Unanswerable Questions</head><p>MRC with unanswerable questions is a more challenging task. Previous work <ref type="bibr" target="#b6">Levy et al.;</ref><ref type="bibr" target="#b1">Clark and Gardner (2017;</ref> has attempted to normalize a no-answer score depending on the probability of all answer spans and still detect boundaries at the same time. But the scores of the answer span predictions are not very discriminative in distinguishing whether the question is answerable. Therefore, this kind of approaches, though relatively simple, cannot effectively deal with the answerability of a question. <ref type="bibr" target="#b10">Tan et al. (2018;</ref>) introduced an answer verifier idea to construct a classification layer. However, this kind of approaches usually has a pipeline structure. The answer pointer and answer verifier have their respective models that are trained separately.</p><p>Multi-task models Different from existing work, we regard the MRC with unanswerable questions as a multi-task learning problem (Caruana 1997) by sharing some metaknowledge. Intuitively, answer prediction and answer verification are related tasks since the underlying comprehension and reasoning of language for these components is the same. Therefore, we construct a multi-task model to solve three sub-tasks: answer pointer, no-answer pointer, and answer verifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper, we regard the MRC with unanswerable questions as multi-task learning problems and propose the U-Net, a simple end-to-end model for MRC challenges. U-Net has good performance on SQuAD 2.0. We first add a universal node to learn a fused representation from both the question and passage, then use a concatenated representation to pass through encoding layers. We only treat question and passage differently during attention interactions. In the rest blocks of U-Net, we still use the unified representation containing both the question and passage representation. Finally, we train the U-Net as a multi-task framework to determine the final answer boundaries as well as whether the question is answerable. Our model has very simple structure yet achieves good results on SQuAD 2.0 test.</p><p>Our future work is to reconstruct the structure of U-Net by replacing the current multi-level attention block with a simpler self-attention mechanism, which we believe can capture the question and passage information, and intuitively is also coherent with the rest of our U-Net model. In addition, we will improve the answer boundary detection performance based on some of the previous successful models. Since our model actually does not achieve very competitive performance in the boundary detection task yet still has a good overall performance on SQuAD 2.0 test, we are optimistic that our U-Net model is potentially capable of achieving better performance. Furthermore, our model has a simple structure and is easy to implement, therefore we believe that our model can be easily modified for various datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Now we get the question representation Q = q m i=1 and the passage representation P = p n i=1 , where each word is u Architecture of the U-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>F1 score variation with different thresholds. "NoAns F1" is the recall of unanswerable questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Unanswerable Questions from SQUAD 2.0 (Rajpurkar, Jia, and Liang 2018).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results on the SQuAD 2.0 (extracted on Sep 9, 2018). means the BiDAF with No Answer.</figDesc><table><row><cell>Configuration</cell><cell>EM</cell><cell>F1</cell><cell cols="2">∆EM ∆ F1</cell></row><row><cell>U-Net</cell><cell cols="2">70.3 74.0</cell><cell>-</cell><cell>-</cell></row><row><cell>no node U</cell><cell cols="2">67.9 71.4</cell><cell>-2.4</cell><cell>-2.6</cell></row><row><cell>no share U</cell><cell cols="2">69.7 73.5</cell><cell>-0.6</cell><cell>-0.5</cell></row><row><cell>no concatenate P &amp; Q</cell><cell cols="2">69.0 72.8</cell><cell>-1.3</cell><cell>-1.2</cell></row><row><cell cols="3">no plausible answer pointer 69.6 72.9</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Comparison of different configurations for our U-</cell></row><row><cell>Net model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Multi-task performance on the development set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://rajpurkar.github.io/ SQuAD-explorer/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Robin Jia, Pranav Rajpurkar for their help with SQuAD 2.0 submissions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multitask learning. Machine Learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention-over-attention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10723</idno>
		<idno>arXiv:1606.01549</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Teaching machines to read and comprehend</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<idno type="arXiv">arXiv:1511.02301</idno>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reinforced mnemonic reader for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<idno>arXiv:1808.05759</idno>
	</analytic>
	<monogr>
		<title level="m">Read+ verify: Machine reading comprehension with unanswerable questions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fusionnet: Fusing via fully-aware attention with application to machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno>abs/1711.07341</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
	</analytic>
	<monogr>
		<title level="m">Ask me anything: Dynamic memory networks for natural language processing</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Densely connected convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning recurrent span representations for extractive question answering</title>
		<idno type="arXiv">arXiv:1706.04115</idno>
		<idno>CoRR abs/1712.03556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Pennington, Socher, and Manning</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Glove: Global vectors for word representation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
	</analytic>
	<monogr>
		<title level="m">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proc. of NAACL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang ; Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<title level="m">Know What You Don&apos;t Know: Unanswerable Questions for SQuAD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Seo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<idno>arXiv:1609.05284</idno>
	</analytic>
	<monogr>
		<title level="m">Reasonet: Learning to stop reading in machine comprehension</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Bidirectional attention flow for machine comprehension</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">I know there is no answer: Modeling answer validation for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sukhbaatar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCF International Conference on Natural Language Processing and Chinese Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="85" to="97" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems. Attention is all you need. CoRR abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fortunato</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
	<note>Machine comprehension using match-lstm and answer pointer</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01604</idno>
		<title level="m">Dynamic coattention networks for question answering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
