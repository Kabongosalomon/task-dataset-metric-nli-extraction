<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometry-Aware Symmetric Domain Adaptation for Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Center</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Center</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Philosophy</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Center</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Geometry-Aware Symmetric Domain Adaptation for Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised depth estimation has achieved high accuracy due to the advanced deep network architectures. Since the groundtruth depth labels are hard to obtain, recent methods try to learn depth estimation networks in an unsupervised way by exploring unsupervised cues, which are effective but less reliable than true labels. An emerging way to resolve this dilemma is to transfer knowledge from synthetic images with ground truth depth via domain adaptation techniques. However, these approaches overlook specific geometric structure of the natural images in the target domain (i.e., real data), which is important for highperforming depth prediction. Motivated by the observation, we propose a geometry-aware symmetric domain adaptation framework (GASDA) to explore the labels in the synthetic data and epipolar geometry in the real data jointly. Moreover, by training two image style translators and depth estimators symmetrically in an end-to-end network, our model achieves better image style transfer and generates high-quality depth maps. The experimental results demonstrate the effectiveness of our proposed method and comparable performance against the state-of-the-art. Code will be publicly available at: https://github.com/ sshan-zhao/GASDA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Monocular depth estimation <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28]</ref> has been an active research area in the field of computer vision. Recent years have witnessed the great strides in this task, especially after deep convolutional neural networks (DCNNs) were exploited to estimate depth from a single image successfully <ref type="bibr" target="#b8">[9]</ref>. Until now, there have been lots of follow-up works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b9">10]</ref> improving or extending this work. However, since the proposed deep models are trained  <ref type="bibr" target="#b37">[38]</ref>) and synthetic image for training (vKITTI dataset <ref type="bibr" target="#b10">[11]</ref>), intermediate generated images in our approach, ground truth depth map and estimated depth map using proposed GASDA.</p><p>in a fully supervised fashion, they require a large amount of data with ground truth depth, which is expensive to acquire in practice. To address this issue, unsupervised monocular depth estimation has been proposed <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b52">53]</ref>, using geometry-based cues and without the need of image-depth pairs during training. Unfortunately, this kind of method tends to be vulnerable to illumination change, occlusion and blurring and so on. Compared to real-world data, synthetic data is much easier to obtain the depth map. As a result, some works propose to exploit synthetic data for visual tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b6">7]</ref>. However, due to domain shift from synthetic to real, the model trained on synthetic data often fails to perform well on real data. To deal with this issue, domain adaptation techniques are utilized to reduce the discrepancy between datasets/domains 1 <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Existing works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b58">59]</ref> using synthetic data via domain adaptation have achieved impressive performance for monocular depth estimation. These approaches typically perform domain adaptation either based on synthetic-torealistic translation or inversely. However, due to the lack of paired images, the image translation function usually introduces undesirable distortions in addition to the style change. The distorted image structures significantly degrade the performance of successive depth prediction. Fortunately, the unsupervised cues in the real images, for example, stereo pairs, produces additional constraints on the possible depth predictions. Therefore, it is essential to simultaneously explore both synthetic and real images and the corresponding depth cues for generating higher-quality depth maps.</p><p>Motivated by the above analysis, we propose a Geometry-Aware Symmetric Domain Adaptation Network (GASDA) for unsupervised monocular depth estimation. This framework consists of two main parts, namely symmetric style translation and monocular depth estimation. Inspired by CycleGAN <ref type="bibr" target="#b60">[61]</ref>, our GASDA employs both synthetic-to-realistic and realistic-to-synthetic translations coupled with a geometry consistency loss based on the epipolar geomery of the real stereo images. Our network is learned by groundtruth labels from the synthetic domain as well as the epipolar geometry of the real domain. Additionally, the learning process in the real and synthetic domains can be regularized by enforcing consistency on the depth predictions. By training the style translation and depth prediction networks in an end-to-end fashion, our model is able to translate images without distorting the geometric and semantic content, and thus achieves better depth prediction performance. Our contributions can be summarized as follows:</p><p>â€¢ We propose an end-to-end domain adaptation framework for monocular depth estimation. The model can generate high-quality results for both image style translation and depth estimation.</p><p>â€¢ We show that training the monocular depth estimator using ground truth depth in the synthetic domain coupled with the epipolar geometry in the real domain can boost the performance.</p><p>â€¢ We demonstrate the effectiveness of our method on KITTI dataset <ref type="bibr" target="#b37">[38]</ref> and the generalization performance on Make3D dataset <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Monocular Depth Estimation has been intensively studied over the past decade due to its crucial role in 3D scene understanding. Typical approaches sought the solution by exploiting probabilistic graphical models (e.g., MRFs) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b32">33]</ref>, and non-parametric techniques <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34]</ref>. However, these methods showed some limitations in performance and efficiency because of the employment of hand-crafted features and the low inference speed.</p><p>Recent studies demonstrated that high-performing depth estimators can be obtained relying on deep convolutional neural networks (DCNNs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b3">4]</ref>. Eigen et al. <ref type="bibr" target="#b8">[9]</ref> developed the first end-to-end deep model for depth estimation, which consists of a coarse-scale network and a fine-scale network. To exploit the relationships among image features, Liu et al. <ref type="bibr" target="#b34">[35]</ref> proposed to integrate continuous CRFs with DCNNs at super-pixel level. While previous works considered depth estimation as a regression task, Fu et al. <ref type="bibr" target="#b9">[10]</ref> solved depth estimation in the discrete paradigm by proposing an ordinal regression loss to encourage the ordinal competition among depth values.</p><p>A weakness of supervised depth estimation is the heavy requirement of annotated training images. To mitigate the issue, several notable attempts have investigated depth estimation in an unsupervised manner by means of stereo correspondence. Xie et al. <ref type="bibr" target="#b52">[53]</ref> proposed the Deep3D network for 2D-to-3D conversion by minimizing the pixelwise reconstruction error. This work motivated the development of subsequent unsupervised depth estimation networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b59">60]</ref>. In specific, Garg et al. <ref type="bibr" target="#b13">[14]</ref> showed that unsupervised depth estimation could be recast as an image reconstruction problem according to the epipolar geometry. Following Garg et al. <ref type="bibr" target="#b13">[14]</ref>, several later works improved the structure by exploiting left-right consistency <ref type="bibr" target="#b15">[16]</ref>, learning depth in a semi-supervised way <ref type="bibr" target="#b26">[27]</ref>, and introducing temporal photometric constraints <ref type="bibr" target="#b56">[57]</ref>.</p><p>Domain Adaptation <ref type="bibr" target="#b38">[39]</ref> aims to address the problem that the model trained on one dataset fails to generalize to another due to dataset bias <ref type="bibr" target="#b48">[49]</ref>. In this community, previous works either learn the domain-invariant representations on a feature space <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref> or learn a mapping between the source and target domains at feature or pixel level <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b57">58]</ref>. For example, Long et al. <ref type="bibr" target="#b36">[37]</ref> aligned feature distribution across the source and target domains by minimizing a Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b20">[21]</ref>. Tzeng et al. <ref type="bibr" target="#b49">[50]</ref> proposed to minimize MMD and the classification error jointly in a DCNN framework. Sun et al. <ref type="bibr" target="#b46">[47]</ref> proposed to match the mean and covariance of the two domain's deep features using the Correlation Alignment (CORAL) loss <ref type="bibr" target="#b45">[46]</ref>.</p><p>Coming to domain adaptation for depth estimation, Atapour et al. <ref type="bibr" target="#b1">[2]</ref> developed a two-stage framework. In specific, they first learned a translator to stylize the natural images so as to make them indistinguishable with the synthetic images, and then trained a depth estimation network using the original synthetic images in a supervised manner. Kundu et al. <ref type="bibr" target="#b25">[26]</ref> proposed a content congruent regularization method to tackle the model collapse issue caused by  <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b58">[59]</ref> and this work respectively. S, T, F, S2T (T2S) and D represent the synthetic data, real data, extracted feature, generated data, and estimated depth. AL and MDE mean adversarial loss and monocular depth estimation, respectively. Compared with existing methods, our approach utilizes real stereo data and takes into account synthetic-to-real as well as real-to-synthetic during translation.</p><p>domain adaptation in high dimensional feature space. Recently, Zheng et al. <ref type="bibr" target="#b58">[59]</ref> developed an end-to-end adaptation network, i.e. T 2 Net, where the translation network and the depth estimation network are optimized jointly so that they can improve each other. However, these works overlooked the geometric structure of the natural images from the target domain, which has been demonstrated significant for depth estimation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14]</ref>. Motivated by the observation, we propose a novel geometry-aware symmetric domain adaptation network, i.e., GASDA, by exploiting the epipolar geometry of the stereo images. The differences between GASDA and previous depth adaptation approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b58">59]</ref> are shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Method Overview</head><p>Given a set of N synthetic image-depth pairs {(x i s , y i s )} N i=1 (i.e., source domain X s ), our goal here is to learn a monocular depth estimation model which can accurately predict depth for natural images contained in X t (i.e., target domain). It is difficult to guarantee the model generalize well to the real data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b58">59]</ref> due to the domain shift. We thus provide a remedy by exploiting the epipolar geometry between stereo images and developing a geometry-aware symmetric domain adaptation network (GASDA). Our GASDA consists of two main parts like existing works, including the style transfer network and the monocular depth estimation network.</p><p>Specifically, unlike <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b25">26]</ref>, we consider both synthetic-to-real <ref type="bibr" target="#b58">[59]</ref> and real-to-synthetic translations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>. As a result, we can train two depth estimators F s and F t on the original synthetic data (X s ) and the generated realistic data (G s2t (X s )) using the generator G s2t in supervised manners, respectively. These two models are complementary, since F s has clean training set X s but dirty test set G t2s (X t ) generated by the generator G t2s with noises, such as distortion and blurs, caused by unsatisfied translation, and vise verse for F t . Nevertheless, because the depth information is rather relevant to specific scene geometry which might be different between source and target domains, the models trained on X s or G s2t (X s ) still could fail to perform well on G t2s (X t ) or X t . To provide a solution, we exploit the epipolar geometry of real stereo pairs {(</p><formula xml:id="formula_0">x i t l , x i tr )} M i=1</formula><p>(x i t l and x i tr represent the left and right image respectively 2 ) during training to encourage F t and F s to capture the relevant geometric structure of target/real data. In addition, we introduce an additional depth consistency loss to enforce the predictions from F t and F s are consistent in local regions. The overall framework of GASDA is illustrated in <ref type="figure">Figure 3</ref>. For simplicity, we will omit the superscript i in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GASDA</head><p>Bidirectional Style Transfer Loss Our goal here is to learn the bidirectional translators G s2t and G t2s to bridge the gap between the source domain (synthetic) X s and the target domain (real) X t . Specifically, taking G s2t as an example, we expect the G s2t (x s ) to be indistinguishable from real images in X t . We thus employ a discriminator D t , and train G s2t and D t in an adversarial fashion by performing a minimax game following <ref type="bibr" target="#b19">[20]</ref>. The adversarial losses are expressed as:</p><formula xml:id="formula_1">L gan (G s2t , D t , X t , X s ) =E xtâˆ¼Xt [D t (x t ) âˆ’ 1]+ E xsâˆ¼Xs [D t (G s2t (x s ))], L gan (G t2s , D s , X t , X s ) =E xsâˆ¼Xs [D s (x s ) âˆ’ 1]+ E xtâˆ¼Xt [D s (G t2s (x t ))].<label>(1)</label></formula><p>Unluckily, the vanilla GANs suffer from mode collapse. To provide a remedy and ensure the input images and the output images paired up in a meaningful way, we utilize the cycle-consistency loss <ref type="bibr" target="#b60">[61]</ref>. Specifically, when feeding an image x s to G s2t and G t2s orderly, the output should be a reconstruction of x s , and vice versa for</p><formula xml:id="formula_2">x t , i.e. G t2s (G s2t (x s )) â‰ˆ x s and G s2t (G t2s (x t )) â‰ˆ x t .</formula><p>The cycle consistency loss has the form as:  <ref type="figure">Figure 3</ref>: The proposed framework in this paper. It consists of two main parts: image style translation and monocular depth estimation. i) Style translation network, incorporating two generators (i.e., Gs2t and Gt2s) and two discriminators (i.e., Dt and Ds), is based on CycleGAN <ref type="bibr" target="#b60">[61]</ref>. ii) Monocular depth estimation network contains two complementary sub-networks (i.e., Fs and Ft). We omit the side outputs, for brevity. More details can be found in Section 3, Section 4.1.</p><formula xml:id="formula_3">L cyc (G t2s , G s2t ) = E xsâˆ¼Xs [||G t2s (G s2t (x s )) âˆ’ x s || 1 ] + E xtâˆ¼Xt [||G s2t (G t2s (x t )) âˆ’ x t || 1 ].<label>(2)</label></formula><p>Apart from the adversarial loss and cycle consistency loss, we also employ an identity mapping loss <ref type="bibr" target="#b47">[48]</ref> to encourage the generators to preserve geometric content. The identity mapping loss is given by:</p><formula xml:id="formula_4">L idt (G t2s , G s2t , X s , X t ) = E xsâˆ¼Xs [||G t2s (x s ) âˆ’ x s || 1 ] + E xtâˆ¼Xt [||G s2t (x t ) âˆ’ x t || 1 ].</formula><p>(3) The full objective for the bidirectional style transfer is as follow:</p><formula xml:id="formula_5">L trans (G t2s , G s2t , D t , D s ) = L gan (G s2t , D t , X t , X s ) + L gan (G t2s , D s , X t , X s ) + Î» 1 L cyc (G t2s , G s2t ) + Î» 2 L idt (G t2s , G s2t , X t , X s )<label>(4)</label></formula><p>where Î» 1 and Î» 2 are the trade-off parameters. Depth Estimation Loss We can now render the synthetic images to the "style" of the target domain (KITTI), and then capture a new dataset X s2t = G s2t (X s ). We train a depth estimation network F t on X s2t in a supervised manner using the provided ground truth depth in the synthetic domain X s . Here, we minimize the 1 distance between the predicted depthá»¹ ts and ground truth depth y s :</p><formula xml:id="formula_6">L tde (F t , G s2t ) = ||y s âˆ’á»¹ ts ||.<label>(5)</label></formula><p>In addition to F t , we also train a complementary depth estimator F s on X s directly with the 1 loss:</p><formula xml:id="formula_7">L sde (F s ) = ||y s âˆ’á»¹ ss ||<label>(6)</label></formula><p>whereá»¹ ss = F s (x s ) is the output of F s . Both the F s and F t are important backbones to alleviate the issue of geometry and semantic inconsistency coupled with the subsequent losses. The full depth estimation loss is expressed as:</p><formula xml:id="formula_8">L de (F t , F s , G s2t ) = L sde (F s ) + L tde (F t , G s2t ).<label>(7)</label></formula><p>Geometry Consistency Loss Combining the components above, we have already formulated a naive depth adversarial adaptation framework. However, the G s2t and G t2s are usually imperfect, which would make the predictions y st = F s (G t2s (x t )) andá»¹ tt = F t (x t ) unsatisfied. Besides, previous depth adaptation approaches overlook the specific physical geometric structure which may vary from scenes/datasets. Our main objective is to accurately estimate depth for real scenes, so we consider the geometric structure of the target data in the training phase. To this end, we present a geometric constraint on F t and F s by exploiting the epipolar geometry of real stereo images and unsupervised cues. Specifically, we generate an inverse warped image from the right image using the predicted depth, to reconstruct the left. We thus combine an 1 with single scale SSIM <ref type="bibr" target="#b51">[52]</ref> term as the geometry consistency loss to align the stereo images: where L gc represents the full geometry consistency loss, L tgc and L sgc denote the geometry consistency loss of F t and F s respectively. x tt (x st ) is the inverse warp of x tr using bilinear sampling <ref type="bibr" target="#b22">[23]</ref> based on the estimated depth map y tt (y st ), the baseline distance between the cameras and the camera focal length <ref type="bibr" target="#b15">[16]</ref>. In our experiments, Î· is set to be 0.85, and Âµ is 0.15.</p><formula xml:id="formula_9">L tgc (F t ) = Î· 1 âˆ’ SSIM (x t , x tt ) 2 + Âµ||x t âˆ’ x tt ||, L sgc (F s , G t2s ) = Î· 1 âˆ’ SSIM (x t , x st ) 2 + Âµ||x t âˆ’ x st ||, L gc (F t , F s , G t2s ) = L tgc (F t ) + L sgc (F s , G t2s )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth Smoothness Loss</head><p>To encourage depths to be consistent in local homogeneous regions, we exploit an edgeaware depth smoothness loss:</p><formula xml:id="formula_10">L ds (F t , F s , G t2s ) = e âˆ’âˆ‡xt ||âˆ‡á»¹ tt || + e âˆ’âˆ‡xt ||âˆ‡á»¹ st || (9)</formula><p>where âˆ‡ is the first derivative along spatial directions. We only apply the smoothness loss to X t and X t2s (real data), since X s and X s2t (synthetic data) have full supervision. Depth Consistency Loss We find that the predictions for x t , i.e., F t (x t ) and F s (G t2s (x t )), show inconsistency in many regions, which is in contrast to our intuition. One of the possible reason is that G t2s might fail to translate x t with details. To enforce such coherence, we introduce an 1 depth consistency loss with respect toá»¹ tt andá»¹ st as follows:</p><formula xml:id="formula_11">L dc (F t , F s , G t2s ) = ||á»¹ tt âˆ’á»¹ st ||.<label>(10)</label></formula><p>Full Objective Our final loss function has the form as:</p><formula xml:id="formula_12">L(G s2t , G t2s , D t , D s , F t , F s ) = L trans (G s2t , G t2s , D t , D s ) + Î³ 1 L de (F t , F s , G s2t ) + Î³ 2 L gc (F t , F s , G t2s ) + Î³ 3 L dc (F t , F s , G t2s ) + Î³ 4 L ds (F t , F s , G t2s )<label>(11)</label></formula><p>where Î³ n (n âˆˆ {1, 2, 3, 4}) are trade-off factors. We optimize this objective function in an end-to-end deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference</head><p>In the inference phase, we aim to predict the depth map for a given image in real domain (e.g. KITTI dataset <ref type="bibr" target="#b37">[38]</ref>) using the resultant models. In fact, there are two paths acquiring predicted depth maps: x t â†’ F t (x t ) â†’á»¹ tt and x t â†’ G t2s (x t ) â†’ x t2s â†’ F s (x t2s ) â†’á»¹ st , as shown in <ref type="figure">Figure 4</ref>, and the final prediction is the average ofá»¹ tt and y st :á»¹ t = 1 2</p><p>(á»¹ tt +á»¹ st ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first present the details about our network architecture and the learning strategy. Then, we perform GASDA on one of the largest dataset in the context of autonomous driving, i.e., KITTI dataset <ref type="bibr" target="#b37">[38]</ref>. We also demonstrate the generalization capabilities of our model to other real-world scenes contained in Make3D <ref type="bibr" target="#b44">[45]</ref>. Finally, we conduct various ablations to analyze GASDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Network Architecture Our proposed framework consists of six sub-networks, which can be divided into three groups: G s2t and G t2s for image style translation, D t and D s for discrimination, F t and F s for monocular depth estimation. The networks in each group share the identical network architecture but are with different parameters. Specifically, we employ generators (G s2t and G t2s ) and discriminators (D s and D t ) provided by CycleGAN <ref type="bibr" target="#b60">[61]</ref>. For monocular depth estimators F t and F s , we utilize the standard encoderdecoder structures with skip-connections and side outputs as <ref type="bibr" target="#b58">[59]</ref>. Datasets The target domain is KITTI <ref type="bibr" target="#b37">[38]</ref>, which is a realworld computer vision benchmark consisting of 42, 382 rectified stereo pairs in the resolution about 375 Ã— 1242. In our experiments, the ground truth depth maps provided by KITTI are only for evaluation purpose. The source domain is Virtual KITTI (vKITTI) <ref type="bibr" target="#b10">[11]</ref>, which contains 50 photorealistic synthetic videos with 21, 260 image-depth pairs of size 375 Ã— 1242. Additionally, in order to study the generalization performance of our approach, we also apply the trained model to Make3D dataset <ref type="bibr" target="#b44">[45]</ref>. Since Make3D does not offer stereo images, we directly evaluate our model on the test split without training or further fine-tuning. Training Details We implement GASDA in PyTorch. We train our model in a two-stage manner, i.e., a warming up stage and end-to-end iteratively updating stage. In the warming up stage, we first optimize the style transfer networks for 10 epochs with the momentum of Î² 1 = 0.5, Î² 2 = 0.999, and the initial learning rate of Î± = 0.0002 using the ADAM solver <ref type="bibr" target="#b24">[25]</ref>. Then we train F t on {X t , G s2t (X s )}, and F s on {X s , G t2s (X t )} for around 20 epochs by setting Î² 1 = 0.9, Î² 2 = 0.999, and Î± = 0.0001. To make style translators generate high-quality images, so as to improve the subsequent depth estimators, we fine-tune the network in an end-to-end iteratively updating fashion as shown in <ref type="figure">Figure 6</ref>. In specific, we optimize G s2t and G t2s with the supervision of F t and F s for m epochs, and then train F s and F t for n epochs. We set m = 3 and n = 7 in our experiments, and repeat this process until the network converges (around 40 epochs). In this stage, we employ the same momentum and solver as the first stage with the learning rates of 2e âˆ’ 6 and 1e âˆ’ 5 for the two respectively. The trade-off factors are set to Î» 1 = 10, Î» 2 = 30, Î³ 1 = 50, Î³ 2 = 50  and Î³ 3 = 50 and Î³ 4 = 0.5. In the training phase, we downsample all the images to 192Ã—640, and increase the training set size using some common data augmentation strategies, including random horizontal flipping, rotation with the degrees of [âˆ’5 â€¢ , 5 â€¢ ], and brightness adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">KITTI Dataset</head><p>We test our models on the 697 images extracted from 29 scenes, and use all the 23, 488 images contained in other 32 scenes for training <ref type="bibr" target="#b21">(22,</ref><ref type="bibr">600)</ref> and validation (888) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>. To make a comparison with previous works, we evaluate our results in the regions with the ground truth depth less than 80m or 50m using standard error and accuracy metrics <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b58">59]</ref>. Note that, the maximum depth value in vKITTI is 655.35m instead of 80m in KITTI, but unlike <ref type="bibr" target="#b58">[59]</ref>, we do not clip the depth maps of vKITTI to 80m during training. In <ref type="table" target="#tab_2">Table 1</ref>, we report the benchmark scores on the Eigen split <ref type="bibr" target="#b8">[9]</ref> where the training sets are only KITTI and vKITTI. GASDA obtains a convincible improvement over previous state-of-the-art methods. Specifically, we make the comparisons with two baselines, i.e., All synthetic (baseline1, trained on labeled synthetic data) and All real (baseline2, trained on real stereo pairs), and the latest domain adaptation methods <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b25">26]</ref> and (semi-)supervised/unsupervised methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b59">60]</ref>. The significant improvements in all the metrics demonstrate the superiority of our method. Note that, GASDA yields   <ref type="bibr" target="#b14">[15]</ref>. S * is captured from GTA5, and more similar to real data than vKITTI. Our approach yields lower errors than state-of-the-art approaches, and achieve competitive accuracy compared with <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real Image</head><p>CycleGAN <ref type="bibr" target="#b60">[61]</ref> GASDA Synthetic Image CycleGAN [61] GASDA <ref type="figure">Figure 7</ref>: Qualitative image style translation results of our approach and CycleGAN <ref type="bibr" target="#b60">[61]</ref>. Left: real-to-synthetic translation; Right: synthetic-to-real translation. Our method can preserve geometric and semantic content better for both synthetic-to-real translation and the inverse one. Note that, the translation result is a by-product of GASDA. The improvement is marked by the yellow box.</p><p>Input Image Ground Truth GASDA <ref type="figure">Figure 8</ref>: Qualitative results on Make3D dataset <ref type="bibr" target="#b44">[45]</ref>. Left to right: input image, ground truth depth, and our result.</p><p>higher scores than <ref type="bibr" target="#b25">[26]</ref> which employs additional ground truth depth maps for natural images contained in KITTI. GASDA cannot outperform <ref type="bibr" target="#b1">[2]</ref> in the Eigen split. The main reason is that the synthetic images employed in <ref type="bibr" target="#b1">[2]</ref> are captured from GTA5 3 , and the domain shift between GTA5 and KITTI is not that significant than the one between vKITTI and KITTI. In addition, the training set size in <ref type="bibr" target="#b1">[2]</ref> is about three times than ours. However, GASDA performs competitively on the official KITTI stereo 2015 dataset and Make3D compared with <ref type="bibr" target="#b1">[2]</ref>, as reported in <ref type="table" target="#tab_4">Table 2 and Table 4</ref>. Apart from quantitative results, we also show some example outputs in <ref type="figure">Figure 5</ref>. Our approach preserves more details, and is able to recover depth information of small objects, such as the distant cars and rails, and generate clear boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Make3D Dataset</head><p>To discuss the generalization capabilities of GASDA, we evaluate our approach on Make3D dataset <ref type="bibr" target="#b44">[45]</ref> quantita-3 https://github.com/aitorzip/DeepGTAV. tively and qualitatively. We do not train or further fine-tune our model using the images provide by Make3D. As shown in <ref type="table">Table 4</ref> and <ref type="figure">Figure 8</ref>, although the domain shift between Make3D and KITTI is large, our model still performs well. Compared with state-of-the-art models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref> trained on Make3D in a supervised manner and others using domain adaptation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b1">2]</ref>, GASDA obtains impressive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Here, we conduct a series of ablations to analyze our approach. Quantitative results are shown in <ref type="table" target="#tab_6">Table 3</ref>, and some sampled results for style transfer are shown in <ref type="figure">Figure 7</ref>. Domain Adaptation We first demonstrate the effectiveness of domain adaptation by comparing two simple models, i.e. SYN (F s trained on X s ) and SYN2REAL (F t trained on G s2t (X s )). As shown in <ref type="table" target="#tab_6">Table 3</ref>, SYN cannot capture satisfied scores on KITTI due to the domain shift. After the translation, the domain shift is reduced which means that the synthetic data distribution is relative closer to real data distribution. Thus, SYN2REAL is able to generalize better to real images. Further, we train the style translators (G s2t and G t2s ) and the depth estimation network (F t ) in an end-to-end fashion (SYN2REAL-E2E), which guides to a further improvement as compared to SYN2REAL. As a conclusion, the depth estimation network can improve the style transfer by providing a pixel-wise semantic constraint to the translation networks. Moreover, we can also observe the improvement in <ref type="figure">Figure 7</ref> by comparing the translation results of original CycleGAN <ref type="bibr" target="#b60">[61]</ref> with ours. Geometry Consistency We then study the significance of the geometric constraint coming from stereo images   <ref type="table">Table 4</ref>: Results on 134 test images of Make3D <ref type="bibr" target="#b44">[45]</ref>. Trained * indicates whether the model is trained on Make3D or not. Errors are computed for depths less than 70m in a central image crop <ref type="bibr" target="#b15">[16]</ref>. It can be observed that our approach is comparable with those trained on Make3D.</p><p>based on the epipolar geometry. In specific, we employ the stereo images provided by KITTI when optimizing F t in SYN2REAL-E2E. We enforce the geometry consistency between the stereo images as a constraint as stated in Eq. 8. The model SYN2REAL-GC-E2E outperforms SYN2REAL-E2E by a large margin, which demonstrates that the geometry consistency constraint can significantly improve standard domain adaptation frameworks.</p><p>On the other hand, the comparisons among SYN2REAL-GC, SYN-GC (trained on real data and synthetic data without domain adaptation) and REAL (F t trained on real stereo images without extra data) can show the significance of synthetic data with ground truth depth and domain adaptation. Symmetric Domain Adaptation In contrast to previous works, we expect to fully take advantage of the bidirectional style translators G s2t and G t2s . Thus, we learn REAL2SYN-SYN-GC-E2E whose network architecture is symmetrical to the aforementioned SYN2REAL-GC-E2E. We jointly optimized the two coupled with a depth consistency loss. As shown in <ref type="table" target="#tab_6">Table 3</ref>, GASDA is superior than GASDA-w/oDC which demonstrates the effectiveness of the depth consistency loss. In addition, the comparisons (GASDA-F t v.s. SYN2ERAL-GC-E2E and GASDA-F s v.s. REAL2SYN-GC-E2E) show that the two can benefit each other in the jointly training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present an unsupervised monocular depth estimation framework GASDA, which trains the monocular depth estimation model using the labelled synthetic data coupled with the epipolar geometry of real stereo data in a unified and symmetric deep learning network. Our main motivation is learning a depth estimation model from synthetic image-depth pairs in a supervised fashion, and at the same time taking into account the specific scene geometry information of the target data. Moreover, to alleviate the issues caused by domain shift, we reduce the domain discrepancy using the bidirectional image style transfer. Finally, we implement image translation and depth estimation in an end-to-end network so that then can improve each other. Experiments on KITTI and Make3D datasets show GASDA is able to generate desirable results quantitatively and qualitatively, and generalize well to unseen datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Estimated Depth by GASDA. Top to bottom: input real image in the target domain (KITTI dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Different frameworks for monocular depth estimation using domain adaptation. Left to right: approach proposed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 Figure 4 :</head><label>24</label><figDesc>Inference Phase (Section 3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on KITTI dataset using the test split suggested in<ref type="bibr" target="#b8">[9]</ref>. For the training data, K represents KITTI dataset, CS is CityScapes dataset<ref type="bibr" target="#b5">[6]</ref>, and S is vKITTI dataset. Methods, which apply domain adaptation techniques, are marked by the gray. Qualitative comparison of our results against methods proposed by Eigen et al.<ref type="bibr" target="#b8">[9]</ref> and Zheng et al.<ref type="bibr" target="#b58">[59]</ref> on KITTI. Ground truth has been interpolated for visualization. To facilitate comparison, we mask out the top regions, where ground truth depth is not available. Our approach preserves more details and yields high-quality depth maps. Iteratively updating stage. We learn our model by iteratively updating image style translators and depth estimators, i.e., freezing the module with dashed box while updating the one with solidline box. See main text for details. We omit Dt and Ds for brevity.</figDesc><table><row><cell>Input Image</cell><cell></cell><cell></cell><cell cols="2">Ground Truth</cell><cell></cell><cell>Eigen et.al. [9]</cell><cell>Zheng et.al. [59]</cell><cell>GASDA</cell></row><row><cell>2 Figure 5: backward backward 2 â„’ 2 2 â„’</cell><cell>â„’ â„’ â„’ â„’</cell><cell>â„’ â„’ â„’ â„’</cell><cell>2 2</cell><cell>2 2</cell><cell>backward backward â„’ â„’ â„’ â„’</cell><cell>â„’ â„’ â„’ â„’</cell></row><row><cell cols="3">Updating Gs2t and Gt2s</cell><cell cols="3">Updating Ft and Fs</cell><cell></cell></row><row><cell>Figure 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Abs Rel Sq Rel RMSE RMSE log Î´ &lt; 1.25 Î´ &lt; 1.25 2 Î´ &lt; 1.25 3</figDesc><table><row><cell>Method</cell><cell>Supervised</cell><cell>Dataset</cell><cell></cell><cell cols="3">Error Metrics (lower, better)</cell><cell cols="3">Accuracy Metrics (higher, better)</cell></row><row><cell>Godard et al. [16]</cell><cell>No</cell><cell>K</cell><cell>0.124</cell><cell>1.388</cell><cell>6.125</cell><cell>0.217</cell><cell>0.841</cell><cell>0.936</cell><cell>0.975</cell></row><row><cell>Godard et al. [16]</cell><cell>No</cell><cell>K+CS</cell><cell>0.104</cell><cell>1.070</cell><cell>5.417</cell><cell>0.188</cell><cell>0.875</cell><cell>0.956</cell><cell>0.983</cell></row><row><cell>Atapour et al. [2]</cell><cell>No</cell><cell>K+S  *  (DA)</cell><cell>0.101</cell><cell>1.048</cell><cell>5.308</cell><cell>0.184</cell><cell>0.903</cell><cell>0.988</cell><cell>0.992</cell></row><row><cell>GASDA</cell><cell>No</cell><cell>K+S(DA)</cell><cell>0.106</cell><cell>0.987</cell><cell>5.215</cell><cell>0.176</cell><cell>0.885</cell><cell>0.963</cell><cell>0.986</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results on 200 training images of KITTI stereo 2015 benchmark</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Method Error Metrics (lower, better)Accuracy Metrics (higher, better) Abs Rel Sq Rel RMSE RMSE log Î´ &lt; 1.25 Î´ &lt; 1.25 2 Î´ &lt; 1.25<ref type="bibr" target="#b2">3</ref> </figDesc><table><row><cell></cell><cell></cell><cell cols="3">Domain Adaptation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SYN</cell><cell>0.253</cell><cell>2.303</cell><cell>6.953</cell><cell>0.328</cell><cell>0.635</cell><cell>0.856</cell><cell>0.937</cell></row><row><cell>SYN2REAL</cell><cell>0.229</cell><cell>2.094</cell><cell>6.530</cell><cell>0.294</cell><cell>0.691</cell><cell>0.886</cell><cell>0.951</cell></row><row><cell>SYN2REAL-E2E</cell><cell>0.220</cell><cell>1.969</cell><cell>6.377</cell><cell>0.284</cell><cell>0.703</cell><cell>0.895</cell><cell>0.956</cell></row><row><cell></cell><cell></cell><cell cols="3">Geometry Consistency</cell><cell></cell><cell></cell><cell></cell></row><row><cell>REAL</cell><cell>0.158</cell><cell>1.151</cell><cell>5.285</cell><cell>0.238</cell><cell>0.811</cell><cell>0.934</cell><cell>0.970</cell></row><row><cell>SYN-GC</cell><cell>0.156</cell><cell>1.123</cell><cell>5.255</cell><cell>0.235</cell><cell>0.814</cell><cell>0.937</cell><cell>0.971</cell></row><row><cell>SYN2REAL-GC</cell><cell>0.153</cell><cell>1.112</cell><cell>5.213</cell><cell>0.233</cell><cell>0.819</cell><cell>0.938</cell><cell>0.972</cell></row><row><cell>SYN2REAL-GC-E2E</cell><cell>0.152</cell><cell>1.130</cell><cell>5.227</cell><cell>0.231</cell><cell>0.821</cell><cell>0.939</cell><cell>0.972</cell></row><row><cell></cell><cell></cell><cell cols="3">Symmetric Domain Adaptation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>REAL2SYN-SYN-GC-E2E</cell><cell>0.160</cell><cell>1.226</cell><cell>5.412</cell><cell>0.240</cell><cell>0.806</cell><cell>0.933</cell><cell>0.969</cell></row><row><cell>GASDA-w/oDC</cell><cell>0.151</cell><cell>1.098</cell><cell>5.136</cell><cell>0.230</cell><cell>0.822</cell><cell>0.940</cell><cell>0.972</cell></row><row><cell>GASDA-Ft</cell><cell>0.150</cell><cell>1.014</cell><cell>5.041</cell><cell>0.228</cell><cell>0.824</cell><cell>0.941</cell><cell>0.973</cell></row><row><cell>GASDA-Fs</cell><cell>0.156</cell><cell>1.087</cell><cell>5.157</cell><cell>0.235</cell><cell>0.813</cell><cell>0.936</cell><cell>0.971</cell></row><row><cell>GASDA</cell><cell>0.149</cell><cell>1.003</cell><cell>4.995</cell><cell>0.227</cell><cell>0.824</cell><cell>0.941</cell><cell>0.973</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results for ablation study on KITTI dataset using the test split suggested in<ref type="bibr" target="#b8">[9]</ref>. SYN, REAL, REAL2SYN, and SYN2REAL represent the model trained on Xs, Xt, Gt2s(Xt), and Gs2t(Xs); E2E represents the end-to-end training; GC and DC denote the geometry consistency and depth consistency, respectively; GASDA-Ft (Fs) represents the output of Ft (Fs) in GASDA.</figDesc><table><row><cell>Method</cell><cell>Trained  *</cell><cell cols="3">Error Metrics (lower, better) Abs Rel Sq Rel RMSE</cell></row><row><cell>Karsch et al. [24]</cell><cell>Yes</cell><cell>0.398</cell><cell>4.723</cell><cell>7.801</cell></row><row><cell>Laina et al. [30]</cell><cell>Yes</cell><cell>0.198</cell><cell>1.665</cell><cell>5.461</cell></row><row><cell>Kundu et al. [26]</cell><cell>Yes</cell><cell>0.452</cell><cell>5.71</cell><cell>9.559</cell></row><row><cell>Godard et al. [16]</cell><cell>No</cell><cell>0.505</cell><cell cols="2">10.172 10.936</cell></row><row><cell>Kundu et al. [26]</cell><cell>No</cell><cell>0.647</cell><cell cols="2">12.341 11.567</cell></row><row><cell>Atapour et al. [2]</cell><cell>No</cell><cell>0.423</cell><cell>9.343</cell><cell>9.002</cell></row><row><cell>GASDA</cell><cell>No</cell><cell>0.403</cell><cell>6.709</cell><cell>10.424</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We will use domain and dataset interchangeably for the same meaning in most cases.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We will omit the subscript l of t l for the left image in most cases.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This research was supported by Australian Research</head><p>Council Projects FL-170100117, DP-180103424 and IH-180100002.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">FranÃ§ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4446</idno>
		<title level="m">Domain-adversarial neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Abarghouei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhouhan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02305</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Singleimage depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3339" to="3348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">FranÃ§ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">ClÃ©ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04333</idno>
		<title level="m">Causal generative domain adaptation networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain adaptation with conditional transferable components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2839" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning depth from single images with deep neural network embedding focal length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyi</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sing Bing</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: Amethod for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Learn. Representations</title>
		<meeting>3rd Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adadepth: Unsupervised content congruent adaptation for depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phani</forename><forename type="middle">Krishna</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Babu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01599</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JÃ¶rg</forename><surname>StÃ¼ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for optical flow with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="354" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Anton Van Den Hengel, and Mingyi He. Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="624" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1253" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="978" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Transfer sparse coding for robust image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Geonet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dual cnn models for unsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Vamshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv Ram</forename><surname>Repala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06324</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5506" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="842" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-scale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3917" to="3925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="819" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">T2net: Synthetic-to-realistic translation for solving single-image depth estimation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="767" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
