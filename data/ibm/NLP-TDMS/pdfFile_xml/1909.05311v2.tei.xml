<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
							<email>lvshangwen@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
							<email>jingjingxu@pku.edu.cndutang</email>
							<affiliation key="aff3">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Cao</surname></persName>
							<email>gucao@microsoft.com</email>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Hu</surname></persName>
							<email>husonglin@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Commonsense question answering aims to answer questions which require background knowledge that is not explicitly expressed in the question. The key challenge is how to obtain evidence from external knowledge and make predictions based on the evidence. Recent studies either learn to generate evidence from human-annotated evidence which is expensive to collect, or extract evidence from either structured or unstructured knowledge bases which fails to take advantages of both sources simultaneously. In this work, we propose to automatically extract evidence from heterogeneous knowledge sources, and answer questions based on the extracted evidence. Specifically, we extract evidence from both structured knowledge base (i.e. ConceptNet) and Wikipedia plain texts. We construct graphs for both sources to obtain the relational structures of evidence. Based on these graphs, we propose a graph-based approach consisting of a graph-based contextual word representation learning module and a graph-based inference module. The first module utilizes graph structural information to re-define the distance between words for learning better contextual word representations. The second module adopts graph convolutional network to encode neighbor information into the representations of nodes, and aggregates evidence with graph attention mechanism for predicting the final answer. Experimental results on CommonsenseQA dataset illustrate that our graph-based approach over both knowledge sources brings improvement over strong baselines. Our approach achieves the state-of-the-art accuracy (75.3%) on the CommonsenseQA dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Reasoning is an important and challenging task in artificial intelligence and natural language processing, which is "the process of drawing conclusions from the principles and evidence" <ref type="bibr" target="#b20">(Wason and Johnson-Laird 1972)</ref>. The "evidence" is the fuel and the "principle" is the machine that operates on the fuel to make predictions. The majority of studies typically only take the current datapoint as the input, in which Figure 1: An example from the CommonsenseQA dataset which requires multiple external knowledge to make the correct prediction. ConceptNet evidence helps pick up choices (A, C) and Wikipedia evidence helps pick up choices (C, E). Combining both evidence will derive the right answer C. Words in blue are the concepts in the question. Words in green are the relations from ConceptNet. Words in red are the choices picked up by evidence. case the important "evidence" of the datapoint from background knowledge is ignored.</p><p>In this work, we study commonsense question answering, a challenging task which requires machines to collect background knowledge and reason over the knowledge to answer questions. For example, an influential dataset Com-monsenseQA <ref type="bibr" target="#b19">(Talmor et al. 2019</ref>) is built in a way that the answer choices share the same relation with the concept in the question while annotators are asked to use their background knowledge to create questions so that only one choice is the correct answer. <ref type="figure">Figure 1</ref> shows an example which requires multiple external knowledge sources to make the correct predictions. The structured evidence from Con-cepNet can help pick up the choices (A, C), while evidence from Wikipedia can help pick up the choices (C, E). Com-bining both evidence will derive the correct answer (C).</p><p>Approaches have been proposed in recent years for extracting evidence and reasoning over evidence. Typically, they either generate evidence from human-annotated evidence <ref type="bibr" target="#b14">(Rajani et al. 2019)</ref> or extract evidence from a homogeneous knowledge source like structured knowledge Con-ceptNet <ref type="bibr" target="#b8">(Lin et al. 2019;</ref><ref type="bibr" target="#b0">Bauer, Wang, and Bansal 2018;</ref><ref type="bibr" target="#b11">Mihaylov and Frank 2018)</ref> or Wikipedia plain texts <ref type="bibr" target="#b16">(Ryu, Jang, and Kim 2014;</ref><ref type="bibr" target="#b22">Yang, Yih, and Meek 2015;</ref><ref type="bibr" target="#b1">Chen et al. 2017</ref>), but they fail to take advantages of both knowledge sources simultaneously. Structured knowledge sources contain valuable structural relations between concepts, which are beneficial for reasoning. However, they suffer from low coverage. Plain texts can provide abundant and highcoverage evidence, which is complementary to the structured knowledge.</p><p>In this work, we study commonsense question answering by using automatically collected evidence from heterogeneous external knowledge. Our approach consists of two parts: knowledge extraction and graph-based reasoning. In the knowledge extraction part, we automatically extract graph paths from ConceptNet and sentences from Wikipedia. To better use the relational structure of the evidence, we construct graphs for both sources, including extracted graph paths from ConceptNet and triples derived from Wikipedia sentences by Semantic Role Labeling (SRL). In the graph-based reasoning part, we propose a graph-based approach to make better use of the graph information. We contribute by developing two graph-based modules, including (1) a graph-based contextual word representation learning module, which utilizes graph structural information to re-define the distance between words for learning better contextual word representations, and (2) a graphbased inference module, which first adopts Graph Convolutional Network <ref type="bibr" target="#b6">(Kipf and Welling 2016)</ref> to encode neighbor information into the representations of nodes, followed by a graph attention mechanism for evidence aggregation.</p><p>We conduct experiments on the CommonsenseQA benchmark dataset. Results show that both the graph-based contextual representation learning module and the graph-based inference module boost the performance. We also demonstrate that incorporating both knowledge sources can bring further improvements. Our approach achieves the state-ofthe-art accuracy (75.3%) on the CommonsenseQA dataset.</p><p>Our contributions of this paper can be summarized as follows:</p><p>• We introduce a graph-based approach to leverage evidence from heterogeneous knowledge sources for commonsense question answering.</p><p>• We propose a graph-based contextual representation learning module and a graph-based inference module to make better use of the graph information for commonsense question answering.</p><p>• Results show that our model achieves a new state-of-theart performance on the CommonsenseQA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Definition and Dataset</head><p>This paper utilizes CommonsenseQA <ref type="bibr" target="#b19">(Talmor et al. 2019)</ref>, an influential dataset for commonsense question answering task for experiments. Formally, given a natural language question Q containing m tokens {q 1 , q 2 , · · · , q m }, and 5 choices {a 1 , a 2 , · · · , a 5 }, the target is to distinguish the right answer from the wrong ones and accuracy is adopted as the metric. Annotators are required to utilize their background knowledge to write questions in which only one of them is correct, thus making the task more challenging. The lack of evidence requires the model to have strong commonsense knowledge extraction and reasoning ability to get the right results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach Overview</head><p>In this section, we give an overview of our approach. As shown in <ref type="figure">Figure 2</ref>, our approach contains two parts: knowledge extraction and graph-based reasoning. In the knowledge extraction part, we extract knowledge from structured knowledge base ConcpetNet and Wikipedia plain texts according to the given question and choices. We construct graphs to utilize the relational structures of both sources.</p><p>In the graph-based reasoning part, we propose two graphbased modules which consists of a graph-based contextual word representation learning module and a graph-based inference module to infer final answers. We will describe each part in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Extraction</head><p>Question + Choice Graph-Based Reasoning Output <ref type="figure">Figure 2</ref>: An overview of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Extraction</head><p>In this section, we provide the methods to extract evidence from ConceptNet and Wikipedia given the question and choices. Furthermore, we describe the details of constructing graphs for both sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Extraction from ConceptNet</head><p>ConceptNet is a large-scale commonsense knowledge base, containing millions of nodes and relations. The triple in ConceptNet contains four parts: two nodes, one relation, and a relation weight. For each question and choice, we first identify their entities in the given ConceptNet graph. Then we search for the paths (less than 3 hops) from question entities to choice entities and merge the covered triples into a graph where nodes are triples and edges are the relation between triples. If two triples s i , s j contain the same entity, we will add an edge from the previous triple s i to the next triple s j . In order to obtain contextual word representations for ConceptNet nodes, we transfer the triple into a natural language sequence according to the relation template in ConceptNet. An example is shown in <ref type="figure" target="#fig_0">Figure 3</ref>. We denote the graph as Concept-Graph.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Extraction from Wikipedia</head><p>We extract 107M sentences from Wikipedia 1 by Spacy 2 and adopt Elastic Search tools 3 to index the Wikipedia sentences. We first remove stopwords in the given question and choices then concatenate the words as queries to search from the Elastic Search engine. The engine ranks the matching scores between queries and all the Wikipedia sentences. We select top K sentences as the Wikipedia evidence. Here we adopt K=10 in experiments.</p><p>To discover the structure information in Wikipedia evidence, we construct a graph for Wikipedia evidence. We utilize Semantic Role Labeling (SRL) to extract triples (subjective, predicate, objective) in one sentence. Both arguments and predicates are the nodes in the graph. We add two edges &lt;subjective, predicate&gt; and &lt;predicate, objective&gt; in the graph. In order to enhance the connectivity of the graph. We remove stopwords and add an edge from node a to node b according to the following enhanced rules: (1) Node a is contained in node b and the number of words in a is more than 3; (2) Node a and node b only have one different word and the numbers of words in a and b are both more than 3. An example is shown in <ref type="figure">Figure 4</ref>. We denote the graph as Wiki-Graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-Based Reasoning</head><p>In this section, we present the model architecture of graphbased reasoning over the extracted evidence, shown in  <ref type="figure">Figure 4</ref>: An example of constructed Wiki-Graph from the Wikipedia evidence "He began making music when he started guitar lessons" and "Making music and playing guitar are his hobbies". The dotted line denotes the added edge according to our enhanced rules (1).</p><p>graph-based contextual representation learning module and a graph-based inference module. The first module learns better contextual word representations by using graph information to re-define the distance between words. The second module gets node representations via Graph Convolutional Network (Kipf and Welling 2016) by using neighbor information and aggregates graph representations to make final predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-Based Contextual Representation Learning Module</head><p>It is well accepted that pre-trained models have a strong text understanding ability and have achieved state-of-the-art results on a variety of natural language processing tasks. We use XLNet <ref type="bibr" target="#b21">(Yang et al. 2019)</ref> as the backbone here, which is a successful pre-trained model with the advantage of capturing long-distance dependency. A simple way to get the representation of each word is to concatenate all the evidence as a single sequence and feed the raw input into XLNet. However, this would assign a long distance for the words mentioned in different evidence sentences, even though they are semantically related. Therefore, we use the graph structure to re-define the relative position between evidence words. In this way, semantically related words will have shorter relative position and the internal relational structures in evidence are used to obtain better contextual word representations. Specifically, we develop an efficient way of utilizing topology sort algorithm 4 to re-order the input evidence according to the constructed graphs. For structured knowledge, ConceptNet triples are not represented as natural language. We use the relation template provided by ConceptNet to transfer a triple into a natural language text sentence. For example, "mammals HasA hair" will be transferred to "mammals has hair". In this way, we can get a set of sentences S T based on the triples in the extracted graph. Then we can get the re-ordered evidence for ConceptNet S T with the method shown in Algorithm 1. The output of <ref type="figure" target="#fig_0">Figure 3</ref> is &lt;"people has eyes", "eyes is related to cry", "people can do singing", "cry is a kind of sound", "singsing requires sound", "sound is related to playing guitar"&gt;, which will shorten the distances between triples which are more similar to each other. For Wikipedia sentences, we construct a sentence graph. The evidence sentences S are nodes in the graph. For two sentences s i and s j , if there is an edge &lt;p, q&gt; in Wiki-Graph where p, q are in s i and s j respectively, there will be an edge &lt;s i , s j &gt; in the sentence graph. We can get a sorted evidence sequence S by the method in Algorithm 1. In Algorithm 1, the relations R is a set of edges, and an edge r=&lt;x,y&gt; means the edge from node x to node y. The incident edges for s i represent edges from other nodes to the node s i .</p><p>Formally, the input of XLNet is the concatenation of sorted ConceptNet evidence sentences S T , sorted Wikipedia evidence sentences S , question q, and choice c. The output of XLNet is contextual word piece representations and the input representation &lt;cls&gt;. By transferring the extracted graph into natural language texts, we can fuse these two different heterogeneous knowledge sources into the same representation space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-Based Inference Module</head><p>The XLNet-based model mentioned in the previous subsection provides effective word-level clues for making predictions. Beyond that, the graph provides more semantic-level information of evidence at a more abstract layer, such as the subject/object of a relation. A more desirable way is to aggregate evidence at the graph-level to make final predictions.</p><p>Specifically, we regard the two evidence graphs Concept-Graph and Wiki-Graph as one graph and adopt Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b6">(Kipf and Welling 2016)</ref> to obtain node representations by encoding graph-structural information.</p><p>To propagate information among evidence and reason over the graph, GCNs update node representations by pooling features of their adjacent nodes. Because relational GCNs usually over-parameterize the model (Marcheggiani Algorithm 1 Topology Sort Algorithm.</p><p>Require: A sequence of nodes S = {si, s2, · · · , sn}; A set of relations R = {r1, r2, · · · , rm}. 1: function DFS(node, visited, sorted sequence) 2:</p><p>for each child sc in node's children do 3:</p><p>if sc has no incident edges and visited[sc]==0 then 4:</p><p>visited[sc]=1 5: sorted sequence.append(0, sc) 6:</p><p>Remove the incident edges of sc 7:</p><p>DFS <ref type="formula">(</ref> and Titov 2017; Zhang, Qi, and Manning 2018), we apply GCNs on the undirected graph.</p><p>The i-th node representation h 0 i is obtained by averaging hidden states of the corresponding evidence in the output of XLNet and reducing dimension via a non-linear transformation:</p><formula xml:id="formula_0">h 0 i = σ(W wj ∈si 1 |s i | h wj ) .<label>(1)</label></formula><p>where s i = {w 0 , · · · , w t } is the corresponding evidence to the i-th node, h wj is the contextual token representation of XLNet for the token w j , W ∈ R d×k is to reduce high dimension d into low dimension k, and σ is an activation function. In order to reason over the graph, we propagate information across evidence via two steps: aggregation and combination <ref type="bibr" target="#b3">(Hamilton, Ying, and Leskovec 2017)</ref>. The first step aggregates information from neighbors of each node. The aggregated information z l i for i-th node can be formulated as Equation 2, where N i is the neighbors of i-th node and h l j is the j-th node representation at the layer l. The representation z l i contains neighbors information for i-th node at the layer l, and we can combine it with the transformed i-th node representation to get the updated node representation h l+1 i :</p><formula xml:id="formula_1">z l i = j∈Ni 1 |N i | V l h l j ,<label>(2)</label></formula><formula xml:id="formula_2">h l+1 i = σ(W l h l i + z l i ) .</formula><p>(3) We utilize graph attention to aggregate graph-level representations to make the prediction. The graph representation is computed the same as the multiplicative attention <ref type="bibr" target="#b9">(Luong, Pham, and Manning 2015)</ref>, where h L i is the i-th node representation at the last layer, h c is the input representation &lt;cls&gt;, α i is the importance of the i-th node, and h g is the graph representation:</p><formula xml:id="formula_3">α i = h c σ(W 1 h L i ) j∈N h c σ(W 1 h L j ) ,<label>(4)</label></formula><formula xml:id="formula_4">h g = j∈N α L j h L j .<label>(5)</label></formula><p>We concatenate the input representation h c with the graph representation h g as the input of a Multi-Layer Perceptron (MLP) to compute the confidence score score(q, a). The probability of the answer candidate a to the question a can be computed as follows, where A is the set of candidate answers: p(q, a) = e score(q,a)</p><p>a ∈A e score(q,a )</p><p>.</p><p>Finally, we select the answer with the highest confidence score as the predicted answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we conduct experiments to prove the effectiveness of our proposed approach. To dig into our approach, we perform ablation studies to explore the different effects of heterogeneous knowledge sources and graph-based reasoning models. We study a case to show how our model can utilize the extracted evidence to get the right answer. We also show some error cases to point directions to improve our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings</head><p>The CommonsenseQA <ref type="bibr" target="#b19">(Talmor et al. 2019</ref>) dataset contains 12,102 examples, include 9,741 for training, 1,221 for development and 1,140 for test.</p><p>We select XLNet large cased <ref type="bibr" target="#b21">(Yang et al. 2019)</ref> as the pre-trained model. We concatenate "The answer is" before each choice to change each choice to a sentence. The input format for each choice is "&lt;evidence&gt; &lt;sep&gt; question &lt;sep&gt; The answer is &lt;choice&gt; &lt;cls&gt;". Totally, we get 5 confidences scores for all the choices then we adopt the softmax function to calculate the loss between the predictions and the ground truth. We adopt cross-entropy loss as our loss function. In our best model on the development dataset, we set the batch size to 4 and learning rate to 5e-6. We set max length of input to 256. We use Adam (Kingma and Ba 2014) with β 1 = 0.9, β 2 = 0.999 for optimization. We set GCN layer to 1. We train our model for 2,800 steps (about one epoch) and get the results 79.3% on development dataset and 75.3% on blind test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>For the compared methods, we select models and classify them into 4 groups. Group 1: models without descriptions or papers, Group 2: models without extracted knowledge, Group 3: models with extracted structured knowledge and Group 4: models with extracted unstructured knowledge.</p><p>• Group 1: models without description or papers. These models include SGN-lite, BECON (single), BECON (ensemble), CSR-KG and CSR-KG (AI2 IR). • Group 2: models without extracted knowledge, including BERT-large <ref type="bibr" target="#b2">(Devlin et al. 2019)</ref>, XLNet-large <ref type="bibr" target="#b21">(Yang et al. 2019)</ref> and RoBERTa <ref type="bibr" target="#b9">(Liu et al. 2019</ref>). These models adopt pre-trained language models to finetune on the training data and make predictions directly on the test dataset without extracted knowledge. • Group 3: models with extracted structured knowledge, including KagNet <ref type="bibr" target="#b8">(Lin et al. 2019)</ref>, BERT + AMS <ref type="bibr" target="#b23">(Ye et al. 2019)</ref>  It should be noted that these methods either utilize evidence from structured or unstructured knowledge sources, failing to take advantages of both sources simultaneously. RoBERT + CSPT adopts knowledge from ConceptNet and OMCS, but the model pre-trains on the sources without explicit reasoning over the evidence, which is different from our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Results and Analysis</head><p>The results on CommonsenseQA development dataset and blind test dataset are shown in <ref type="table">Table 1</ref>. Our model achieves the best performance on both datasets. In the following comparisons we focus on the results on test dataset. Compared with the model in group 1, we can get more than 10% absolute accuracy than these methods. Compared with models without extracted knowledge in group 2, our model also enjoys 2.8% absolute gain over the strong baseline RoBERTa (ensemble). XLNet-large is our baseline model and our approach can get 12.4% absolute improvement over the baseline and this approves the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev Acc Test Acc Compared to models with extracted structured knowledge in group 3, our model extracts graph paths from Concept-Net for graph-based reasoning rather than for pre-training, and we also extract evidence from Wikipedia plain texts, which brings 13.1% and 5.7% gains over BERT + AMS and ROBERTa + CSPT respectively. Group 4 contains model which utilizes unstructured knowledge such as Wikipedia or OMCS, etc. Compared with these methods, we not only utilize Wikipedia to provide unstructured evidences but also construct graphs to get the structural information. We also utilize the evidence from structure knowledge base Concept-Net. Our model achieves 3.2% absolute improvement over the best model RoBERTa + IR in this part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In this section, we perform ablation studies on the development dataset 5 to dive into the effectiveness of different components in our model. We first explore the effect of different components in graph-based reasoning. Then we dive into the heterogeneous knowledge sources and see their effects.</p><p>In the graph-based reasoning part, we dive into the effect of topology sort algorithm for learning contextual word representations and graph inferences with GCN and graph attention. We select XLNet + Evidence as the baseline. In the baseline, we simply concatenate all the evidence into XL-Net and adopt the contextual representation for prediction. By adding topology sort, we can obtain a 1.9% gain over the baseline. This proves that topology sort algorithm can fuse the graph structure information and change the relative position between words for better contextual word representation. The graph inference module brings 1.4% benefit, showing that GCN can obtain proper node representations and graph attention can aggregate both word and node representations to infer answers. Finally, we add topology sort, graph inference module together to get a 3.5% improvement, proving these models can be complementary and achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev Acc  Then we perform ablations studies on knowledge sources to see the effectiveness of ConceptNet and Wikipedia sources. The results are shown in <ref type="table" target="#tab_7">Table 3</ref>, "None" represents we only adopts the XLNet <ref type="bibr" target="#b21">(Yang et al. 2019</ref>) large model as the baseline. When we add one knowledge source, the corresponding graph-based reasoning models are also added. From the results, we see that the structured knowledge ConceptNet can bring 6.4% absolute improvement and the Wikipedia source can bring 4.6% absolute improvement. This proves the benefits of ConceptNet or Wikipedia source. When combining ConceptNet and Wikipedia, we can enjoy a 9.4% absolute gain over the baseline. This proves that heterogeneous knowledge sources can achieve better performance than single one and different sources in our model and they are complementary to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Sources</head><p>Dev Acc  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>In this section, we select a case to show that our model can utilize the heterogeneous knowledge sources to answer questions. As shown in <ref type="figure" target="#fig_3">Figure 6</ref>, the question is "Animals who have hair and don't lay eggs are what?" and the answer is "mammals". The first three nodes are from ConceptNet evidence graph. We can see that "mammals is animals" and "mammals has hair" can provide information about the relation between "mammals" and two concepts "animals" and "hair". More evidence is needed to show the relation between "lay eggs" and "mammals". The last three nodes are from Wikipedia evidence graph and they can provide the information that "very few mammals lay eggs". The examples also show that both sources are necessary to infer the right answer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Analysis</head><p>We randomly select 50 error examples from the development dataset and the reasons are classified into three categories: the lack of evidence, similar evidence and dataset noise. There are 10 examples which are lack of evidence. For example, the first example in <ref type="figure" target="#fig_4">Figure 7</ref> extracts no triples from ConceptNet and the evidence from Wikipedia does not contain enough information to get the right answer. This problem can be alleviated by utilizing more advanced extraction strategies and adding more knowledge sources. There are 38 examples which extract enough evidence but the evidence are too similar to distinguish between choices. For example, the second example in <ref type="figure" target="#fig_4">Figure 7</ref> has two choices "injury" and "puncture wound", the evidence from both sources provides similar information. More evidence from other knowledge sources is needed to alleviate this problem. We also find there are 2 error examples which have 2 same choices 6 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Commonsense Reasoning Commonsense reasoning is a challenging direction since it requires reasoning over ex-6 example id: e5ad2184e37ae88b2bf46bf6bc0ed2f4, fa1f17ca535c7e875f4f58510dc2f430 ternal knowledge beside the inputs to predict the right answer. Various downstream tasks have been released to address this problem like ATOMIC(Sap et al. 2019), Event2Mind <ref type="bibr" target="#b15">(Rashkin et al. 2018)</ref>, MCScript 2.0(Ostermann, Roth, and Pinkal 2019), SWAG , HellaSWAG  and Story Cloze Test <ref type="bibr" target="#b12">(Mostafazadeh et al. 2016)</ref>.</p><p>Recently proposed CommonsenseQA <ref type="bibr" target="#b19">(Talmor et al. 2019</ref>) dataset derived from ConceptNet <ref type="bibr" target="#b17">(Speer, Chin, and Havasi 2017)</ref>   <ref type="bibr" target="#b9">(Liu et al. 2019)</ref> have achieved significant improvements on many tasks. This paper utilizes XLNet <ref type="bibr" target="#b21">(Yang et al. 2019)</ref> as the backend and propose our approach to study the commonsense question answering problem.</p><p>Graph Neural Networks for NLP Recently, Graph Neural Networks (GNN) has been utilized widely in NLP. For example, <ref type="bibr" target="#b18">Sun et al. (2019)</ref> utilizes Graph Convolutional Networks (GCN) to jointly extract entity and relation. Zhang, Qi, and Manning (2018) applies GNN to relation extraction over pruned dependency trees and achieves remarkable improvements. GNN has also been applied into muli-hop reading comprehension tasks <ref type="bibr" target="#b20">(Tu et al. 2019;</ref><ref type="bibr" target="#b6">Kundu et al. 2019;</ref>. This paper utilizes GCN to represent graph nodes by utilizing the graph structure information, followed by graph attention which aggregates the graph representations to make the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we focus on commonsense question answering task and select CommonsenseQA <ref type="bibr" target="#b19">(Talmor et al. 2019)</ref> dataset as the testbed. We propose an approach consisting of knowledge extraction and graph-based reasoning. In the knowledge extraction part, we extract evidence from heterogeneous external knowledge including structured knowledge source ConceptNet and Wikipedia plain texts. In the graphbased reasoning part, we propose a graph-based approach consisting of graph-based contextual word representation learning module and graph-based inference module to select the right answer. Results show that our model achieves stateof-the-art on CommonsenseQA <ref type="bibr" target="#b19">(Talmor et al. 2019)</ref> dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>An example of constructed Concept-Graph from the ConceptNet evidence. The underlined words are the concepts in ConceptNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 5. Our graph-based model consists of two modules: a 1 Wikipedia version enwiki-20190301 2 https://spacy.io/ 3 https://www.elastic.co/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>An overview of our proposed graph-based reasoning model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>An attention heat-map for the question "Animals who have hair and don't lay eggs are what?" and the answer "mammals". The nodes in ConpcetNet are in natural language format and the template is: IsA (is a kind of), HasA (has).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Error cases of our model on the development dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>and the choices have the same relation with the concept in the question. Recently, Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019) extracts evidence from ConceptNet to study this problem. This paper focuses on automatically extracting evidence from heterogeneous external knowledge and reasoning over the extracted evidence to study this problem. Knowledge Transfer in NLP Transfer learning has played a vital role in the NLP community. Pre-trained language models from large-scale unstructured data like ELMo (Peters et al. 2018), GPT (Radford et al. 2018), BERT (Devlin et al. 2019), XLNet (Yang et al. 2019), RoBERTa</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on reasoning components in our model. E denotes evidence.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies on heterogeneous knowledge sources. "None" represents we only use XLNet baseline and do not utilize knowledge sources.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We also try to re-define the relative positions between two word tokens and get a position matrix according to the token distances in the graph. However, it consumes too much memory and cannot be executed efficiently.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The dataset restricts to submit the results no more than every two weeks.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Songlin Hu is the corresponding author. We thank the anonymous reviewers for providing valuable suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Commonsense for generative multi-hop question answering tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4220" to="4230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer open-domain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explore, propose, and assemble: An interpretable model for multi-hop reading comprehension</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2714" to="2725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Ba</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2737" to="2747" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Exploiting explicit paths for multi-hop reading comprehension</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RACE: large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">KagNet: Knowledge-aware graph networks for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2822" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized BERT pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="821" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mostafazadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mcscript2. 0: A machine comprehension corpus focused on script events and participants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename><surname>Ostermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ostermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09531</idno>
		<idno>Peters et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep contextualized word representations</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explain yourself! leveraging language models for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4932" to="4942" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Improving language understanding with unsupervised learning</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Event2mind: Commonsense inference on events, intents, and reactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="463" to="473" />
		</imprint>
	</monogr>
	<note>Rashkin et al. 2018</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open domain question answering using wikipedia-based knowledge model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang</forename><surname>Kim ; Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="3027" to="3035" />
		</imprint>
	</monogr>
	<note>Atomic: an atlas of machine commonsense for if-then reasoning</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4444" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint type inference on entities and relations via graph convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1361" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Talmor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<publisher>Harvard University Press</publisher>
			<date type="published" when="1972" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
	<note>Psychology of reasoning: Structure and content</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CoRR abs/1906.08237</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meek ; Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Align, mask and select: A simple method for incorporating commonsense knowledge into language representation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ye</surname></persName>
		</author>
		<idno>abs/1908.06725</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
	<note>SWAG: A large-scale adversarial dataset for grounded commonsense inference</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zellers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
