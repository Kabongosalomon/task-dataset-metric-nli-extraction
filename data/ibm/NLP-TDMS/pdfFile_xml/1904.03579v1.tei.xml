<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptively Connected Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University Guangzhou</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
							<email>kezewang@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff2">
								<orgName type="department">Sun Yat-sen University Guangzhou</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptively Connected Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel adaptively connected neural network (ACNet) to improve the traditional convolutional neural networks (CNNs) in two aspects. First, ACNet employs a flexible way to switch global and local inference in processing the internal feature representations by adaptively determining the connection status among the feature nodes (e.g., pixels of the feature maps) 1 . We can show that existing CNNs, the classical multilayer perceptron (MLP), and the recently proposed non-local network (NLN) <ref type="bibr" target="#b49">[48]</ref> are all special cases of ACNet. Second, ACNet is also capable of handling non-Euclidean data. Extensive experimental analyses on a variety of benchmarks (i.e., ImageNet-1k classification, COCO 2017 detection and segmentation, CUHK03 person re-identification, CIFAR analysis, and Cora document categorization) demonstrate that ACNet cannot only achieve state-of-the-art performance but also overcome the limitation of the conventional MLP and CNN 2 . The code is available at https://github.com/wanggrun/ Adaptively-Connected-Neural-Networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Artificial neural networks have been extensively studied and applied over the past three decades, achieving remarkable accomplishments in artificial intelligence and computer vision. Among such networks, two types of neural networks have had a large impact on the research community. The first type is the multi-layer perceptron (MLP), which first became popular and effective via the development of the backpropagation training algorithm <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b16">17]</ref>. However, since each neuron of the hidden layer in MLP is assigned with a private weight, the network parameters of MLP usually have a huge number and can be easily overfitted during the training phase. Moreover, MLP has difficulty in representing the spatial structure of 2D data (e.g., images). The second type <ref type="bibr" target="#b0">1</ref> In a computer vision domain, a node refers to a pixel of a feature map, while in the graph domain, a node denotes a graph node. <ref type="bibr" target="#b1">2</ref> Corresponding author: Liang Lin (linliang@ieee.org) is convolutional neural networks (CNNs) <ref type="bibr" target="#b18">[18]</ref>. Motivated by the biological visual cortex model, CNNs propose to group adjacent neurons to share identical weights and represent 2D data by capturing the local pattern (i.e., receptive field) of each neuron. Although CNNs have been proven to be significantly superior over MLP, they have two drawbacks, as highlighted by <ref type="bibr" target="#b37">[36]</ref>. On one hand, due to only abstracting information from local neighborhood pixels, the convolution operation inside each layer of CNNs does not have the ability of global inference. Consequently, convolution operations have difficulties in recognizing objects with similar appearances. For example, a convolution operation cannot distinguish the difference between the chair and the sofa in <ref type="figure" target="#fig_0">Fig.1 (a)</ref> which share the same appearance. In practice, CNN captures the global dependencies by stacking a number of local convolution operations, which still have several limitations, such as computational inefficiency, optimization difficulty, and message passing inefficiency <ref type="bibr" target="#b49">[48]</ref>. On the other hand, unlike MLP, conventional CNNs cannot be directly applied for non-Euclidean data (e.g., graph data), which are quite common in the area of machine learning.</p><p>To tackle the locality problem in CNNs, the recently proposed non-local network <ref type="bibr" target="#b49">[48]</ref> (denoted as fully non-local networks) imposes global dependencies to all the feature nodes. However, empirically we observe degradations in fully non-local networks: as the non-locality of the network increases, both the training and validation accuracies degrade for the ImageNet-1k classification. We conjecture the degradation due to over-globalization. Specifically, the dog in <ref type="figure" target="#fig_0">Fig.1 (b)</ref> is easy to recognize if we only perform the local inference, while it can be misclassified as a cow when only performing the global inference. Intuitively, although quite challenging, it is necessary to jointly consider the global and local inference from image-aware ( <ref type="figure" target="#fig_0">Fig.1(a)</ref>) or even node-aware (pixel-aware) ( <ref type="figure" target="#fig_0">Fig.1(b)</ref>) perspective.</p><p>There have been many other recent attempts to address the aforementioned issues raised by CNNs and have achieved promising results <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b11">12]</ref>. However, all of these methods are either over-localized or over-globalized. In contrast, this work focuses on developing a simple and general adaptivelyconnected neural network (ACNet) to adaptively capture the global and local dependencies, which inherits the strengths of both MLP and CNNs and overcomes their drawbacks. Thanks to the adaptively determining the global/local inference, our ACNet achieves lower top-1 training/validation error than ResNet on ImageNet-1k (see <ref type="figure" target="#fig_0">Fig. 1</ref>(c) and (d)).</p><p>ACNet first defines a simple yet basic unit named "node", which is a unit of vectors in meta-data. As depicted in <ref type="figure" target="#fig_1">Fig.2</ref>, a node may be seen as a pixel of an image ( <ref type="figure" target="#fig_1">Fig. 2(a)</ref>), a sampling of an audio ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>), and a node of a general graph ( <ref type="figure" target="#fig_1">Fig. 2(c)</ref>). Given the input data, ACNet adaptively is trained to search the optimal connection for each node, i.e., the connection ⊆ connecting {the node itself, its neighbor nodes, all possible nodes}. Keep in mind that different nodes are connected adaptively, i.e., some nodes may be conjectured to themselves, some nodes may relate to its neighborhood, while other nodes have the global vision. Therefore, our ACNet can be considered as a generalization of CNN and MLP ( <ref type="figure" target="#fig_1">Fig. 2 (d)</ref>). Note that, searching the optimal connections is differential by learning the importance degrees for different kinds of connections, which can be optimized via back-propagation.</p><p>The main contributions of this paper are summarized as follows. Firstly, we propose a conceptually general yet powerful network, which learns to switch global and local inference for general data (i.e. both Euclidean and non-Euclidean data) in a flexible parameter saving manner. Secondly, to the best of our knowledge, our proposed ACNet is the first one who is capable of inheriting the strength of both MLP and CNN while overcoming their drawbacks on a variety of computer vision and machine learning tasks, i.e., image classification on ImageNet-1k/CIFARs, object detection and segmentation on COCO 2017, person re-identification on CUHK03, and document categorization on Cora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Although significant progress has been achieved in the architecture design of CNNs from LeNet <ref type="bibr" target="#b20">[19]</ref> to more recent deep and powerful networks (e.g., ResNet <ref type="bibr" target="#b9">[10]</ref>), evolving the structure of CNNs to overcome their drawbacks is also quite crucial and a long-standing problem in machine learning (e.g. <ref type="bibr" target="#b22">[21]</ref>). This issue motivates many researchers to extend CNNs to obtain different receptive fields <ref type="bibr" target="#b4">[5]</ref>. Specifically, Dai et al. <ref type="bibr" target="#b4">[5]</ref> proposed to enhance the transformation modeling capability of CNNs by introducing learnable offsets to augment the spatial sampling locations within the feature map. Chen et al. <ref type="bibr" target="#b1">[2]</ref> revisited atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by DNNs, in the application of semantic image segmentation. Peng et al. <ref type="bibr" target="#b32">[31]</ref> proposed to use the large kernel filter and effective receptive field for semantic segmentation. Sabour et al. <ref type="bibr" target="#b37">[36]</ref> proposed employing a group of neurons named a capsule to represent the instantiation parameters of a specific type of entity, such as an object and an object part. Building upon the work of <ref type="bibr" target="#b37">[36]</ref>, Hinton et al. <ref type="bibr" target="#b11">[12]</ref> further presented a new type of capsule that has a logistic unit to represent the presence of an entity and a 4×4 pose matrix to represent the pose of that entity. Motivated by the self-attention mechanism <ref type="bibr" target="#b41">[40]</ref>, Wang et al. <ref type="bibr" target="#b49">[48]</ref> incorporated non-local operations into CNNs as a generic family of building blocks for capturing long-range dependencies. Similarly, PSANet <ref type="bibr" target="#b53">[52]</ref> is built upon NLN by introducing a position encoding to each pixel; GloRe <ref type="bibr" target="#b2">[3]</ref> improves on NLN in a way of using a graph-CNN to capture the global dependencies. Although these methods achieved promising results, their performances are limited due to the over-localization or over-globalization of the internal feature representation.</p><p>Moreover, several limited attempts <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b60">59]</ref> have been made to extend CNNs for handling graph data. For instance, Kipf et al. <ref type="bibr" target="#b14">[15]</ref> presented a layer-wise propagation rule for CNNs to operate directly on graph-structured data. Such et al. <ref type="bibr" target="#b39">[38]</ref> defined filters as polynomials of functions of the graph adjacency matrix for unstructured graph data. However, these variants of CNNs pay close attention to bridge the gap between the graph structure of network inputs and the general graph data. The global inference inside the internal representation are ignored.</p><p>Our work is also related to the fully-connected neural networks (i.e. multilayer perceptron, or MLP), the densely connected neural networks <ref type="bibr" target="#b12">[13]</ref>, and the skip-connection neural networks (e.g. UNet <ref type="bibr" target="#b34">[33]</ref>, ResNet <ref type="bibr" target="#b10">[11]</ref>), sharing the goal of finding an effective connection for the neural networks. However, the connections in our ACNet are automatically learned and adaptative to the data, while the connections in existing methods are fixed and handcrafted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adaptive-Connected Neural Networks</head><p>In this section, we first present the formulation of our proposed ACNet. Then, we discuss the relations between our ACNet and three most representative prior works, i.e., MLP, CNN, and NLN. Actually, they are special cases of our ANN. Moreover, we have also generalized our ANN for non-Euclidean data. Finally, we present the details of training, testing, and implementing our ACNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>Suppose x denotes the input signal (e.g., images, voices, graph matrices or their features). We propose to obtain the corresponding output signal as follows:</p><formula xml:id="formula_0">yi = αi j=i xjuij + βi j⊆N (i) xjvij + γi ∀j xjwij,<label>(1)</label></formula><p>where y i implies the i-th output node (e.g., the i-th pixel of a feature map) of the output signal, and j is the index of some possible nodes related to the i-th node. Actually, the j-th node belongs to three different sets, including {the i-th node itself}, {the neighborhood N (i) of the i-th node}, and {all possible nodes}. These three sets indicate three different modes of inference: self transformation, local inference, and global inference, respectively. Moreover, u ij , v ij and w ij refer to the learnable weights between the i-th and j-th nodes for the three different sets, respectively. Note that the biases are omitted for notation simplification. ACNet switches among different inference modes by adaptively learning α, β and γ in Eqn.1, which are importance degrees used to weighted average the modes. Note that, α, β and γ can be simple scalar variables, which are shared across all channels. We force α + β + γ = 1, and α, β, γ ∈ [0, 1], and define α = e λα e λα + e λ β + e λγ .</p><p>(2)</p><p>Here α is computed by using a softmax function with λ α as the control parameter, which can be learned by the standard back-propagation (BP). Similarly, β and γ are defined by using another parameters λ β and λ γ , respectively. Note that the third term ∀j x j w ij in Eqn.1 is quite computational consuming, because it equals to a fully-connected layer with large feature maps as input, leading to potential overfitting. To overcome this shortcoming, the x is first transformed by an average pooling operation for downsampling in practice before being fed to calculate ∀j x j w ij . Finally, the obtained y in Eqn.1 can be activated by a non-linear function f (·), such as BatchNorm+ReLU. Actually, if α, β, γ are formulated as scalar variables, the connection for adaptively determining the global/local inference is an average connection over the whole dataset. To enable node-aware connection for each node (e.g., a pixel), α, β, γ can be also formulated as sample-dependent ones:</p><formula xml:id="formula_1">γi = γi(x) = wγ i ,2f (wγ i ,1 j=i xjuij; j⊆N (i) xjvij; ∀j xjwij ),<label>(3)</label></formula><p>where [; ; ] denotes a concatenation operation and w γ,· denotes a linear transformation. α and β are defined in the similar way, which are omitted here. In the experimental section we will show that the above two kinds of formulation have the similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Relation to Prior Works</head><p>CNN. We take CNN as an illustrative example. For notation simplification, we omit the non-linear activation f , which does not affect the derivation process of the formulation. Let x be the input data represented by a 3D tensor (C, H, W ). Let x i and y i be a node (pixel) of the input data and the output respectively, where i, j ∈ [1, H × W ]. Then a general 3×3 convolution can be formulated as</p><formula xml:id="formula_2">yi = j⊆S xjvij (4)</formula><p>where S is the set that containing the nodes which have interactions with the given i-th node. Specifically, S denotes the set of eight neighbors for the i-th node, in addition to the i-th node itself, i.e.,</p><formula xml:id="formula_3">S = {i − W − 1, i − W, i − W + 1, i − 1, i, i + 1, i + W − 1, i + W, i + W + 1}.</formula><p>MLP. MLP shares the formulation of Eqn.4, but it uses different sets of nodes to perform the linear combination. In other words, MLP enables more nodes to interact with the given i-th node, performing a global inference. For MLP,</p><formula xml:id="formula_4">S = {1, 2, 3, . . . , H × W }.</formula><p>In summary, ACNet can be seen as a pure data-driven combination of CNN and MLP, fully exploiting the advantage of these two kinds of basic neural networks. For instance, let α = 0, β = 1, γ = 0 in Eqn.1, ACNet degrades into CNN; let α = 0, β = 0, γ = 1 in Eqn.1, ACNet degrades into MLP. More importantly, ACNet dynamically switches between them by learning α, β and γ, providing more reasonable inferences. This allows us to build a richer hierarchy that combines both global and local information adaptively.</p><p>NLN. NLN also shares the formulation of Eqn.4, with S = {1, 2, 3, . . . , H × W }, which is similar to MLP. However, there is a limitation in NLN. The v ij in NLN is obtained by computing the similarity of the i-th and the j-th nodes, which is very computation-consuming and easy to overfit. Therefore, NLN is rarely employed for image classification tasks. Instead of directly computing v ij , our proposed AC-Net absorbs the advantage of MLP (i.e., regarding v ij as a learnable weight) and tackles its heavy computation problem by employing downsampling operation to perform the global inference. The relations between ACNet and prior works have been summarized in <ref type="figure" target="#fig_1">Fig. 2</ref> </p><formula xml:id="formula_5">(d).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generalization to Non-Euclidean Data</head><p>We present the difference between Euclidean and non-Euclidean data, and then give a general definition of ACNet to handle both Euclidean and non-Euclidean data.</p><p>Euclidean data include the image, audio, and video, while non-Euclidean data contains graph and manifold. The difference is that Euclidean data are structured and non-Euclidean data are unstructured. Mathematically, for Euclidean data, we can denote the neighborhood of the i-th node in Eqn. 1 as N (i) = {i − W − 1, . . . , i + W + 1}, representing the {upper left, ..., low right } neighbors. But for non-Euclidean data we have difficulties. Besides, each node in Euclidean data has a fixed number of neighbors, while the number of neighbors is flexibly adapted to non-Euclidean data. Consequently, there is a gap in using Eqn.1 between Euclidean and non-Euclidean data. For Euclidean data v ij has different values at different j. But for non-Euclidean data v ij is shared among different j in Eqn.1. This weakens the representation capacity for non-Euclidean data due to the lack of position encoding. The similar phenomenon also occurs in w ij .</p><p>To fill the gap, Eqn. 1 is rewritten into a general form:</p><formula xml:id="formula_6">yi = αi j=i xju + βi j⊆N (i) pij(xjv) + γi ∀j qij(xjw).<label>(5)</label></formula><p>where u, v, and w are shared among all kinds of j, which may be considered as 1 × 1 convolution in computer vision. Note that here α, β, γ is defined by using Eqn. 3. In compensation for the information loss in local structure, another two position encoding functions, i.e., p ij and q ij , are proposed to encode the index. These functions are just simple linear transformations using constant Gaussian noise. Specifically,</p><formula xml:id="formula_7">pij(xjv) = xjvζij, qij(xjw) = xjwξij<label>(6)</label></formula><p>where ζ ij and ξ ij are constant variables sampled from a Gaussian noise.</p><p>Remark 1. Let ζ ij and ξ ij in Eqn.6 be learnable parameters instead of constant variables, then Eqn.6 turns out to be Eqn.1.</p><p>Remark 1 reveals that Eqn.5 is a lightweight version of Eqn.1, because a number of parameters are represented as constant variables in Eqn.5, exception the 1 × 1 convolution kernels u, v, and w. In the experimental section we will show that compared to the state-of-the-art CNNs that use large kernels, ACNet with considerably fewer parameters can also achieve their strengths in feature learning, by only exploiting highly efficient 1 × 1 convolution operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training, Inference, and Implementation</head><p>Training &amp; Inference. Let Θ be a set of network parameters (e.g. convolution filters and fully-connected weights) and Φ be a set of control parameters that control the network architecture. In ACNet, we have Φ = {λ α , λ β , λ γ }.</p><p>Training an ACNet network is to minimize a loss function L(Θ, Φ), where Θ and Φ can be optimized jointly by backpropagation (BP). ACNet is tested in the same way as standard networks such as CNN and MLP.</p><p>Compatibility with CNN Tricks and Techniques. Our proposed ACNet is quite compatible with most existing tricks and techniques for CNNs. For instance, through embedding a batch normalization <ref type="bibr" target="#b13">[14]</ref> layer into every nonlinear mapping function f (·), our ACNet can support a large learning rate for high learning efficiency. Meanwhile, we can also exploit the residual connection strategy <ref type="bibr" target="#b9">[10]</ref> to create a short-cut connection for each layer inside our ACNet.</p><p>Implementation. ACNet can be easily implemented by using the existing software such as TensorFlow and PyTorch. The backward computation of ACNet can be obtained by automatic differentiation techniques (AD) in these software. Without AD, ACNet can also be implemented by regarding Φ = {λ α , λ β , λ γ } as learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section presents the main results of ACNet in multiple challenging problems and benchmarks, such as ImageNet-1k classification <ref type="bibr" target="#b36">[35]</ref>, COCO 2017 detection and segmentation <ref type="bibr" target="#b28">[27]</ref>, CUHK03 person re-identification <ref type="bibr" target="#b21">[20]</ref>, CIFAR <ref type="bibr" target="#b15">[16]</ref> classification, and Cora document categorization <ref type="bibr" target="#b38">[37]</ref>, where the effectiveness of ACNet is demonstrated by comparing with the existing state-of-the-art CNNs/NLNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet-1k Classification</head><p>We first compare our ACNet with the most representative CNNs/NLNs on the ImageNet classification dataset of 1k categories. All the models are trained on the 1.28M training images and evaluated on the 50k validation images. Our baseline model is the representative ResNet50. We examine top-1 accuracy on the 224×224 single/center-crop-singlescale images. Note that the top-1 accuracies of the baseline   <ref type="table" target="#tab_0">Table 1</ref>. As depicted, our ACNet-ResNet50 performs approximately 1.1% better than the compared CNN-ResNet50 (77.5% vs 76.4%). The training and validation curves in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref> and (d) also show the sustainable competitive advantage of our ACNet-ResNet50 over CNN-ResNet50. This improvement is quite significant due to the challenge of ImageNet-1k.</p><p>The superior performance of our ACNet is attributed to two reasons. First, ACNet adaptively performs global and local inference for different pixels of internal feature maps from each layer, leading to a flexible discriminative representation learning fashion, which contributes to capturing the local and global dependencies for improving classification accuracy. Second, the mechanism of ACNet may implicitly act as comprehensive data-driven ensembling, which aggregates the advantage of both global and local information.</p><p>Pixel-aware Connection. As is mentioned in Section 3.1, different pixels can have different pixel-aware connection by using Eqn. 3. We report the accuracies of pixel-aware and dataset-aware connection in <ref type="table" target="#tab_0">Table 1</ref> respectively. For the pixel-aware connection, we let α = 0, β = 1 and only learn γ to save parameters and memory. The results show that these two kinds of connection have the same top-1 accuracy. While the pixel-aware connection has more parameters (31.85M vs 29.38M).  <ref type="table" target="#tab_0">Table 1</ref>). The extra parameters are from the global inference ( i.e. ∀j x j w ij in Eqn.1). Thanks to downsampling operation, the extra parameters only introduce negligible computation time and memory usage, which will be examined later. To eliminate the confounding factor of extra parameters and justify the gain of ACNet, we present more comparisons:</p><p>1. We compare ACNet-ResNet50 with CNN-ResNet60, which has the same level of parameters. The result in <ref type="table" target="#tab_1">Table 2</ref> shows that ACNet-ResNet50 obtains a slightly higher accuracy (77.5% vs 76.7%) than CNN-ResNet60, demonstrating the superiority of ACNet over CNN with the nearly same number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The general form of ACNet is also compared with CNN.</head><p>As is discussed in Sect. 3.3, ACNet can be rewritten to a general form for supporting both Euclidean and non-Euclidean data. Remark 1 in Sect. 3.3 reveals that the general form is much more (0.77×) lightweight. The experimental result in <ref type="table" target="#tab_1">Table 2</ref> confirms this remark, and further shows that ACNet with considerably fewer parameters can also achieve their strengths in feature learning(76.2%vs76.4%), by only exploiting highly efficient 1 × 1 convolution operations.</p><p>Computation Complexity. <ref type="table" target="#tab_2">Table 3</ref> reports the computation complexity of ACNet, CNN and NLN. For a fair comparison, all methods are trained in the same desktop with 8 Titan Xp GPUs. We observe that ACNet and CNN have similar computational costs. Specifically, the memory consumption of both ACNet and CNN are the same, i.e. 8.6GB. But the speed of ACNet is slightly slower than CNN (144 vs 198 images/second/GPU). As a comparison, NLN is intractable because NLN requires a vast amount of memory to calculate the similarity between any two pixels of a feature map. The memory required is beyond the testing desktop can provide. Actually, NLN performs significantly slower than ACNet and CNN according to our observation.</p><p>Visualization of importance degrees. The importance degrees in each ACNet layer are visualized in <ref type="figure">Fig. 3</ref>, from which we have two observations. First, the importance degrees differ from pixel to pixel. This is due to the global and local inference are pixel-aware, i.e. different pixels have different inference modes. Second, the importance degrees also differs from layer to layer -there is much more global inference in lower-level layers than in higher-level layers.  <ref type="figure">Figure 3</ref>: Visualization of the nodes with different types of inference generated by our ACNet, which is trained on ImageNet. One node painted by the yellow color indicates its the output of the global inference from the preceding layer (i.e., it connects to all nodes in the preceding layer), while the opposite black nodes indicate the outputs of the local inference from the preceding layer. Although CNN somewhat can capture a few global dependencies in high-level layers by stacking a number of local convolutional layer, it has difficulties in local inference in lower-level layers, as shown in <ref type="figure">Fig. 3</ref>. Fortunately, our AC-Net provides compensatory global inference for these lowerlevel layers. Overall, examining the necessity of global inference in lower layer discloses interesting characteristics and impacts in DNNs, and sheds light on model design in many research fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis on CIFAR10</head><p>As the ImageNet-1k dataset is quite large and the training from scratch is extremely time-consuming, we conduct more ablation studies on CIFAR10 <ref type="bibr" target="#b15">[16]</ref> classification benchmark to deeply analyze ACNet. CIFAR-10 consists of 50k training images and 10k testing image in 10 classes. The presented experiments are trained on the training set and evaluated on the testing set as <ref type="bibr" target="#b10">[11]</ref>. Our focus is to analyze the components of ACNet instead of achieving the state-of-the-art results, therefore we use the representative ResNet32 proposed in <ref type="bibr" target="#b10">[11]</ref>. All the implementation details and experiment settings are the same as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b48">47]</ref>.</p><p>The role of global inference. We first evaluate the effectiveness of global inference y constructing two different networks, i.e. with and without the third term ∀j q ij (x j w) in Eqn.5. As shown in <ref type="table" target="#tab_7">Table 8</ref>, without global inference, ACNet has a performance degradation of 1.1%. As we know, ACNet without global inference equals to CNN. This com-parison verifies the superiority of ACNet over CNN.</p><p>The role of local inference. Next, we investigate the necessity of local inference. In <ref type="table" target="#tab_7">Table 8</ref>, we compare two operations, i.e. with and without local inference. <ref type="table" target="#tab_7">Table 8</ref> shows that equipped with local inference, ACNet has a significant performance gain of 18%, verifying the contribution of local inference. This is natural in the image domain. The lack of local inference leads to neglecting some critical information. Intuitively, we can easily represent an image as an adjacent matrix. But we can never recover the original image from the adjacent matrix. demonstrating an information loss by discarding the local inference.</p><p>Adaptively global+local vs fixed global+local. Next, we investigate the necessity of adaptively switching between global and local inference. We fixed the importance degrees α, β and γ as constant variables, forming the fixed global+local version of ACNet. We have an interesting observation in <ref type="table" target="#tab_7">Table 8</ref>: imposing global information to every pixel has poorer performance than adaptively adding global information (6.0% vs 6.8%). In other words, the global inference is unessential for every pixel, because it may hurt the training. This implies the superiority of adaptively connected neural networks over the fully non-local networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">COCO Object Detection and Segmentation</head><p>We have demonstrated the adaptive inference capacity of ACNet in ImageNet classification task, whose receptive filed is quite large due to 5 times of subsampling and a global pooling. Next, we investigate an inevitable smaller receptive field task, i.e. COCO 2017 detection &amp; segmentation task <ref type="bibr" target="#b28">[27]</ref>. These computer vision tasks in general benefit from higher-resolution input and output. Therefore, the global pooling and some subsampling are removed from the backbone of ResNet50, leading to a smaller receptive filed. As a result, the adaptively global and local inference is in desire.</p><p>We finetune the models trained on ImageNet <ref type="bibr" target="#b36">[35]</ref> for transferring to detection and segmentation. The batch normalization parameters are frozen during the finetuning.</p><p>We conduct experiments on the Mask RCNN baselines [9] using a ResNet50-FPN backbone. We replace CNN layers with ACNet layers. The models are trained in the COCO train2017 set and evaluated in the COCO val2017 set. We use the standard training setting following the COCO model zoo. We report the standard COCO metrics of Average Precision (AP) for bounding box detection (AP bbox ) and instance segmentation (AP mask ). <ref type="table" target="#tab_4">Table 5</ref> shows the comparison of ACNet vs NLN vs CNN. ACNet improves over CNN by 1.5% box AP and 0.6% mask AP. This may be contributed to the fact that CNN lacks adaptive inference capacity. We have also found NLN is 0.5% box AP worse than ACNet. In summary, although NLN is also suitable global inference, its representational power is slightly weaker than ACNet according to our current evaluation. The inferiority of NLN is attributed to the overglobalization. Specifically, the redundant global context may hurt but NOT help the model learning. This phenomenon has also been observed experimentally by <ref type="bibr" target="#b59">[58]</ref> and theoretically by <ref type="bibr" target="#b29">[28]</ref>, confirming that the over-globalization is a shortcoming of NLN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">CUHK03 Person Re-identification</head><p>To demonstrate the good generalization performance of our proposed ACNet on the other recognition tasks, we have conducted the extensive experiments on the person re-identification challenge, which refers to the problem of re-identifying individuals across cameras. Though quite challenging, person re-identification is fundamental and beneficial from many applications in video surveillance for keeping the security of safety of the whole society <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b61">60,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b23">22]</ref>.</p><p>Dataset. We conduct experiments on the CUHK03 dataset <ref type="bibr" target="#b21">[20]</ref>, which is one of the largest databases for person re-identification. This database contains 14,096 images of 1,467 pedestrians. Each person is observed by two disjoint camera views and is shown in 4.8 images on average in each view. We follow the new standard setting of using CUHK03 <ref type="bibr" target="#b57">[56]</ref>, where 767 individuals are regarded as the training set and another 700 individuals are considered as the testing set without sharing the same individuals.</p><p>Evaluation metric. For the evaluation, the testing set is further divided into a gallery set of images and a probe set. We use the standard rank-1 as the evaluation metric.</p><p>Result Analysis. In <ref type="table" target="#tab_5">Table 6</ref>, we compare with the current best models. A total of 11 representative state-of-the-art methods, BOW+XQDA <ref type="bibr" target="#b54">[53]</ref>, PUL <ref type="bibr" target="#b6">[7]</ref>, LOMO+XQDA <ref type="bibr" target="#b26">[25]</ref>, IDE <ref type="bibr" target="#b55">[54]</ref>, IDE+DaF <ref type="bibr" target="#b52">[51]</ref>, IDE+XQ.+Re-ranking <ref type="bibr" target="#b56">[55]</ref>, PAN, DPFL <ref type="bibr" target="#b3">[4]</ref>, and the newly proposed methods SVDNet <ref type="bibr" target="#b40">[39]</ref>, TriNet + Era. <ref type="bibr" target="#b57">[56]</ref>, and TriNet + Era. + Reranking <ref type="bibr" target="#b57">[56]</ref> , are used as the competing methods. All the settings of the above methods are consistent with the common training settings as <ref type="bibr" target="#b57">[56]</ref>. ACNet has achieved a new state-of-the-art performance. Specifically, ACNet achieves a rank-1 accuracy of 64.8%. We can also observe that ACNet surpasses its baseline by a clear margin ( 3.6%, <ref type="table" target="#tab_5">Table 6</ref>). This verifies the effectiveness of ACNet on the person re-identification task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis on Cora : a Non-Euclidean Domain</head><p>A common form of graph-structured data is a network of documents. For example, scientific documents in a database are related to each other through citations and references. Administrators of such large networks may desire to automatically label documents according to their relationships to the remainder of the literature. To demonstrate the compatibility of ACNet for non-Euclidean data, we adapt our proposed ACNet to tackle such a vertex classification task on the Cora benchmark <ref type="bibr" target="#b38">[37]</ref>, which is a large network of scientific publications connected through citations. The vertex features, in this case, are binary word vectors that indicate the presence of a word from a dictionary of 1,433 unique words. There are 2708 publications classified under 7 different categories -case-based, genetic algorithms, neural networks, probabilistic methods, reinforcement learning, rule learning, and theory. There is an edge connection from a cited article to a citing article and another edge connection from a citing article to a cited article. These edge features are also binary representations. We use a quite simple architecture following <ref type="bibr" target="#b14">[15]</ref>, which only contains two graph convolutional layers. The first layer is used for feature learning, and the second layer is used for classifier learning. We replace the first graph convolutional layer in <ref type="bibr" target="#b14">[15]</ref> with our ACNet layer. Note that in our ACNet α, β, γ is defined by using Eqn. 3. Considering the Cora dataset is quite small-scale, we let α = 0, β = 1 and only learn γ to avoid overfitting. We perform 10-fold cross validations to form the training and test set for a fair comparison as the majority of methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref> did.</p><p>Comparisons with the state-of-the-art methods. We first compare with the current best models. A total of 11 representative state-of-the-art methods, i.e., ManiReg <ref type="bibr" target="#b0">[1]</ref>, SemiEmb <ref type="bibr" target="#b50">[49]</ref>, LP <ref type="bibr" target="#b58">[57]</ref>, DeepWalk <ref type="bibr" target="#b33">[32]</ref>, ICA <ref type="bibr" target="#b30">[29]</ref>, Planetoid <ref type="bibr" target="#b51">[50]</ref>, the newly proposed methods Graph-CNN <ref type="bibr" target="#b14">[15]</ref>, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy (%)</p><p>ManiReg <ref type="bibr" target="#b0">[1]</ref> 59.5 SemiEmb <ref type="bibr" target="#b50">[49]</ref> 59.0 LP <ref type="bibr" target="#b58">[57]</ref> 68.0 DeepWalk <ref type="bibr" target="#b33">[32]</ref> 67.2 ICA <ref type="bibr" target="#b30">[29]</ref> 75.1 Planetoid* <ref type="bibr" target="#b51">[50]</ref> 75.7 Graph-CNN <ref type="bibr" target="#b14">[15]</ref> 81.5 MoNet <ref type="bibr" target="#b31">[30]</ref> 81.7 GAT <ref type="bibr" target="#b42">[41]</ref> 83.0 LGCN <ref type="bibr" target="#b7">[8]</ref> 83.3 Dual GCN <ref type="bibr" target="#b60">[59]</ref> 83.5 ACNet 83.5 The role of global inference. We first evaluate the effectiveness of global inference by constructing two different networks, i.e. with and without the third term ∀j q ij (x j w) in Eqn.5. As shown in <ref type="table" target="#tab_7">Table 8</ref>, without global inference, ACNet has a performance degradation of 1.4%. This is reasonable for a document categorization problem like Cora. A document categorization problem is slightly different from a conventional image classification one because each article is not isolated. All the articles are connected with each other in the form of citations. In this sense, a document categorization problem is more like a semantic image segmentation problem in computer vision. Therefore global inference in ACNet is essential for Cora.</p><p>The role of local inference. Next, we investigate the necessity of local inference. In <ref type="table" target="#tab_7">Table 8</ref>, we compare two operations, i.e. with and without local inference. <ref type="table" target="#tab_7">Table 8</ref> shows that equipped with local inference, ACNet obtains a gain of 7.2%, verifying the contribution of local inference. The lack of local inference leads to neglecting some critical information. Specifically, each article in Cora cites several other articles, as well as being cited by other articles. Actu-ally, the citing articles and the cited articles may share the same category with the given article. Without local inference, ACNet cannot capture the citation information. The performance degradation of "w/o local inference" may be due to ignoring this knowledge.</p><p>Adaptively global+local vs fixed global+local. We fixed the importance degrees α, β and γ as constant variable, forming the fixed global+local version of ACNet. Similar to the CIFAR10 case, the results in <ref type="table" target="#tab_7">Table 8</ref> confirms the effectiveness of adaptively global and local inference, with a gain of 0.8%. The reason can be attributed to the property of the document. Some article can be easier to categorize when considered in local range than in wide range. For example, at first, we can easily categorize the reinforcementlearning-based article into the "reinforcement learning" area. But after reading more and more article, we may confuse it with "neural networks" area with the emergence of deep reinforcement learning.</p><p>The role of position encoding. At last, we investigate the impact of position encoding. We remove the position encoding in Eqn.5 to obtain the counterpart. <ref type="table" target="#tab_7">Table 8</ref> shows that without the position encoding, ACNet suffers a performance drop of 0.5%. This is because the non-Euclidean data is unstructured compared with the Euclidean data. Without a position encoding, the non-Euclidean data is with too many degrees of freedom (i.e., the same graph data may have different representations because theoretically, a graph has endless isomorphic graphs). This freedom leads to lower learning efficiency. By introducing the position encoding the training inefficiency has been alleviated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presented a concise ACNet to be a promising substitute for overcoming the limitations of widely used deep CNNs without losing their strengths in feature learning. Specifically, ACNet advances in adaptively switching between global and local inference in a flexible and pure data-driven manner. We further applied our proposed AC-Net for the recognition tasks of both Euclidean data and non-Euclidean data. Extensive experimental analyses from a variety of aspects justify the superiority of ACNet. In the future, we will extend our work to be suitable for more general tasks to demonstrate its superiority.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Some pixels prefer global dependencies, while others prefer local inference. For example, without global inference we cannot recognize the chair in (a). While in (b), the representation capacity of the dog is weakened by global information. Thanks to the adaptively determining the global/local inference, our AC-Net achieves lower top-1 training/validation error than ResNet on ImageNet-1k shown in (c) and (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>"Nodes" are presented in form of orange cylinder in (a) an image, (b) an audio, and (c) a general graph. (d) ACNet can be considered as a generalization of MLP and CNN on these "nodes".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of ImageNet val top-1 accuracies and parameter numbers on ResNet50. ACNet ‡: pixel-aware ACNet using Eqn. 3; ACNet: dataset-aware ACNet with α, β, γ being scalar variables;</figDesc><table><row><cell></cell><cell>top-1 accuracies</cell><cell>#params</cell></row><row><cell>CNN-ResNet50</cell><cell>76.4 ↑0.0</cell><cell>25.56M ×1.00</cell></row><row><cell>ACNet-ResNet50</cell><cell>77.5 ↑1.1</cell><cell>29.38M ×1.15</cell></row><row><cell>ACNet ‡-ResNet50</cell><cell>77.5 ↑1.1</cell><cell>31.85M ×1.25</cell></row><row><cell>generalized ACNet</cell><cell>76.2 ↓0.2</cell><cell>19.80M ×0.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Comparison between ACNet-Resnet50 and CNN-</cell></row><row><cell cols="3">ResNet60 in terms of ImageNet val top-1 accuracies and parameter</cell></row><row><cell>numbers.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>top-1 accuracies (%)</cell><cell>#params</cell></row><row><cell>CNN-ResNet60</cell><cell>76.7 ↑0.0</cell><cell>30.03M ×1.00</cell></row><row><cell>ACNet-ResNet50</cell><cell>77.5 ↑0.8</cell><cell>29.38M ×0.98</cell></row><row><cell cols="3">approximately equals to the official results and the model zoo</cell></row></table><note>3 (Caffe; Tensorflow; Pytorch). CNN-ResNet50 is exactly the original ResNet50. For ACNet-ResNet50, all the 3 × 3 convolution in CNN-ResNet50 are replaced with ACNet layers. And for NLN-ResNet50, the non-local operations are attached to every 3 × 3 convolution in CNN-ResNet50. Classification accuracies. The comparison results of top-1 validation accuracies are illustrated in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Computational complexity analysis on the ImageNet-1k. In fact, ACNet has introduced extra parameters by 0.15× (29.4M vs 25.6M,</figDesc><table><row><cell>Networks</cell><cell>CNN-ResNet50</cell><cell>NLN-ResNet50</cell><cell>ACNet-ResNet50</cell></row><row><cell>Speed images/sec</cell><cell>198 ×1.00</cell><cell>nan</cell><cell>144 ×0.77</cell></row><row><cell>Memory GB</cell><cell>8.579 ×1</cell><cell>out of memory</cell><cell>8.580 ×1</cell></row><row><cell cols="2">Extra Parameters.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on CIFAR10.</figDesc><table><row><cell>Method</cell><cell>Error (%)</cell></row><row><cell>Standard ACNet</cell><cell>6.0 ↓0.0</cell></row><row><cell>w/o global inference</cell><cell>7.1 ↓1.1</cell></row><row><cell>w/o local inference</cell><cell>24.0 ↓18</cell></row><row><cell>Fixed global+local</cell><cell>6.8 ↓0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Detection and segmentation ablation studies on COCO2017 using Mask RCNN.</figDesc><table><row><cell>backbone</cell><cell>AP bbox</cell><cell>AP mask</cell></row><row><cell>CNN</cell><cell cols="2">38.0 ↑0.0 34.6 ↑0.0</cell></row><row><cell>NLN</cell><cell cols="2">39.0 ↑1.0 35.5 ↑0.9</cell></row><row><cell>ACNet</cell><cell cols="2">39.5 ↑1.5 35.2 ↑0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison on a Person Re-identification task (CUHK03, where 'bs' denotes batch size.)</figDesc><table><row><cell>Rank-1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison with state-of-the-art on Cora document classification dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Ablation studies on Cora document classification dataset.MoNet<ref type="bibr" target="#b31">[30]</ref>, GAT<ref type="bibr" target="#b42">[41]</ref>, LGCN<ref type="bibr" target="#b7">[8]</ref>, and Dual GCN<ref type="bibr" target="#b60">[59]</ref> are used as competing methods.Table. 7shows that ACNet achieves comparable performance to the best of all competitive methods, e.g., Dual GCN<ref type="bibr" target="#b60">[59]</ref> (83.5%). This comparison once again verifies the generalization performance of ACNet. Next, we investigate which component of ACNet contributes to the non-Euclidean data to shed light on future researches.</figDesc><table><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>Standard ACNet</cell><cell>83.5 ↓0.0</cell></row><row><cell>w/o global inference</cell><cell>82.1 ↓1.4</cell></row><row><cell>w/o local inference</cell><cell>76.3 ↓7.2</cell></row><row><cell>w/o position encoding</cell><cell>83.0 ↓0.5</cell></row><row><cell>Fixed global+local</cell><cell>82.7 ↓0.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/Cadene/pretrained-models. pytorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12814</idno>
		<title level="m">Jiashi Feng, and Yannis Kalantidis. Graph-based global reasoning networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person reidentification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2590" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Largescale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix capsules with em routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frosst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">PhD thesis: Modeles connexionnistes de l&apos;apprentissage (connectionist learning models)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Universite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Curie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>Paris 6)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Aognets: Deep and-or grammar networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Krim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05847</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A deep joint learning approach for age invariant face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyou</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCF Chinese Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distance metric optimization driven convolutional neural network for age invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="51" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">M2m-gan: Many-to-many generative adversarial transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyong</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03768</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-domain visual matching via generalized similarity measure and feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1089" to="1102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discovering causal signals in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6979" to="6987" />
		</imprint>
	</monogr>
	<note>Bernhard Scholkopf, and Léon Bottou</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning (ICML-03)</title>
		<meeting>the 20th International Conference on Machine Learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1743" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning internal representations by backpropagating errors. Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust spatial filtering with graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shagan</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Domínguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhas</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">D</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">W</forename><surname>Ptucha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sel. Topics Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="884" to="896" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05693</idno>
		<title level="m">Svdnet for pedestrian retrieval</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peigen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03282</idno>
		<title level="m">Spatial-temporal person re-identification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">P2snet: Can an image match a video for person re-identification in an end-to-end way?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2777" to="2787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Discovering underlying person structure pattern with relative local distance for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10100</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dari: Distance metric and representation integration for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Kalman normalization: Normalizing internal representations across network layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="21" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Batch kalman normalization: Towards training deep neural networks with micro-batches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03133</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07971</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Divide and fuse: A re-ranking approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04169</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08398</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semisupervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
		<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11168</idno>
		<title level="m">Deformable convnets v2: More deformable, better results</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dual graph convolutional networks for graph-based semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyi</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
