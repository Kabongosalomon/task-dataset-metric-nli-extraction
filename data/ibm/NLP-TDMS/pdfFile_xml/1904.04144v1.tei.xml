<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning monocular depth estimation infusing traditional stereo knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
							<email>fabio.tosi5@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering (DISI)</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
							<email>filippo.aleotti2@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering (DISI)</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
							<email>m.poggi@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering (DISI)</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
							<email>stefano.mattoccia@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering (DISI)</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning monocular depth estimation infusing traditional stereo knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth estimation from a single image represents a fascinating, yet challenging problem with countless applications. Recent works proved that this task could be learned without direct supervision from ground truth labels leveraging image synthesis on sequences or stereo pairs. Focusing on this second case, in this paper we leverage stereo matching in order to improve monocular depth estimation. To this aim we propose monoResMatch, a novel deep architecture designed to infer depth from a single input image by synthesizing features from a different point of view, horizontally aligned with the input image, performing stereo matching between the two cues. In contrast to previous works sharing this rationale, our network is the first trained end-to-end from scratch. Moreover, we show how obtaining proxy ground truth annotation through traditional stereo algorithms, such as Semi-Global Matching, enables more accurate monocular depth estimation still countering the need for expensive depth labels by keeping a self-supervised approach. Exhaustive experimental results prove how the synergy between i) the proposed monoResMatch architecture and ii) proxy-supervision attains state-of-theart for self-supervised monocular depth estimation. The code is publicly available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Inferring accurate depth information of a sensed scene is paramount for several applications such as autonomous driving, augmented reality and robotics. Although technologies such as LiDAR and time-of-flight are quite popular, obtaining depth from images is often the preferred choice. Compared to other sensors, those based on standard cameras potentially have several advantages: they are inexpensive, have a higher resolution and are suited for almost any environment. In this field, stereo is the preferred choice to infer disparity (i.e., the inverse of depth) from two or more images sensing the same area from different points of view and Semi-Global Matching (SGM) <ref type="bibr" target="#b14">[15]</ref> is a popular, yet effective algorithm to accomplish this task. However, inferring depth from a single image is particularly attractive because it does not require a stereo rig and overcomes some intrinsic limitations of a binocular setup (e.g., occlusions). On the other hand, it is an extremely challenging task due to the ill-posed nature of the problem. Nonetheless, deep learning enabled to achieve outstanding results for this task <ref type="bibr" target="#b6">[7]</ref>, although the gap with state-of-the-art stereo solutions is still huge <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>. Self-supervised learning paradigms for monocular depth estimation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b57">58]</ref> became very popular to overcome the need for costly ground truth annotations, usually obtained employing expensive active sensors and human post-processing <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b51">52]</ref>. Following this strategy, Convolutional Neural Networks (CNNs) can be trained to tackle depth estimation as an image synthesis task from stereo pairs or monocular sequences <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b62">63]</ref>. For this purpose, using stereo pairs rather than monocular sequences as supervision turned out to be more effective according to the literature. Although the former strategy is more constrained since a stereo setup is necessary for training, it does neither require to infer relative pose between adjacent frames in a sequence nor to segment moving objects in the scene. Moreover, a stereo setup does not require camera motion, conversely to a monocular setup, to provide meaningful supervision. Other means for self-supervision consist into distilling proxy labels in place of more expensive annotations for various tasks <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>In this paper, we propose monocular Residual Matching (shorten, monoResMatch), a novel end-to-end architecture trained to estimate depth from a monocular image leveraging a virtual stereo setup. In the first stage, we map input image into a features space, then we use such representation to estimate a first depth outcome and consequently synthesize features aligned with a virtual right image. Finally, the last refinement module performs stereo matching between the real and synthesized representations. Differently from other frameworks following a similar rationale <ref type="bibr" target="#b29">[30]</ref> that combines heterogeneous networks for synthesis <ref type="bibr" target="#b54">[55]</ref> and stereo <ref type="bibr" target="#b33">[34]</ref>, we use a single architecture trained in end-toend fashion yielding a notable accuracy improvement compared to the existing solutions. Moreover, we leverage traditional knowledge from stereo to obtain accurate proxy labels in order to improve monocular depth estimation supervised by stereo pairs. We will show that, despite the presence of outliers in the produced labels, training according to this paradigm results in superior accuracy compared to image warping approaches for self-supervision. Experimental results on the KITTI raw dataset <ref type="bibr" target="#b8">[9]</ref> will show that the synergy between the two aforementioned key components of our pipeline enables to achieve state-of-the-art results compared to other self-supervised frameworks for monocular depth estimation not requiring any ground truth annotation. <ref type="figure" target="#fig_0">Figure 1</ref> shows an overview of our framework, depicting an input frame and the outcome of monoResMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review the literature relevant to our work concerned with stereo/monocular depth estimation and proxy label distillation. Stereo depth estimation. Most conventional dense stereo algorithms rely on some or all the well-known four steps thoroughly described in <ref type="bibr" target="#b45">[46]</ref>. In this field, SGM <ref type="bibr" target="#b14">[15]</ref> stood out for the excellent trade-off between accuracy and efficiency thus becoming very popular.Zbontar and Le-Cun <ref type="bibr" target="#b60">[61]</ref> were the first to apply deep learning to stereo vision replacing the conventional matching costs calculation with a siamese CNN network trained to predict the similarity between patches. Luo et al. <ref type="bibr" target="#b28">[29]</ref> cast the correspondence problem as a multi-class classification task, obtaining better results. Mayer et al. <ref type="bibr" target="#b33">[34]</ref> backed away from the previous approaches and proposed an end-to-end trainable network called DispNetC able to infer disparity directly from images. While DispNetC applies a 1-D correlation to mimic the cost volume, GCNet by Kendall et al. <ref type="bibr" target="#b16">[17]</ref> exploited 3-D convolutions over a 4-D volume to obtain matching costs and finally applied a differentiable version of argmin to se-lect the best disparity along this volume. Other works followed these two main strategies, building more complex architectures starting from DispNetC <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b46">47]</ref> or GCNet <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b17">18]</ref> respectively. The domain shift issue affecting these architectures (e.g. synthetic to real) has been addressed in either offline <ref type="bibr" target="#b48">[49]</ref> or online <ref type="bibr" target="#b49">[50]</ref> fashion, or greatly reduced by guiding them with external depth measurements (e.g. Lidar) <ref type="bibr" target="#b41">[42]</ref>. Monocular depth estimation. Before the deep learning era, some works tackled depth-from-mono with MRF <ref type="bibr" target="#b44">[45]</ref> or boosted classifiers <ref type="bibr" target="#b21">[22]</ref>. However, with the increasing availability of ground truth depth data, supervised approaches based on CNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b6">7]</ref> rapidly outperformed previous techniques. An attractive trend concerns the possibility of learning depth-from-mono in a selfsupervised manner, avoiding the need for expensive ground truth depth labels that are replaced by multiple views of the sensed scene. Then, supervision signals can be obtained by image synthesis according to the estimated depth, camera pose or both. In general, acquiring images from a stereo camera enables a more effective training than using a single, moving camera, since the pose between frames known. Concerning stereo supervision, Garg et al. <ref type="bibr" target="#b7">[8]</ref> first followed this approach, while Godard et al. <ref type="bibr" target="#b10">[11]</ref> introduced spatial transform network <ref type="bibr" target="#b15">[16]</ref> and a left-right consistency loss. Other methods improved efficiency <ref type="bibr" target="#b39">[40]</ref>, deploying a pyramidal architecture, and accuracy by simulating a trinocular setup <ref type="bibr" target="#b43">[44]</ref> or including joint semantic segmentation <ref type="bibr" target="#b59">[60]</ref>. In <ref type="bibr" target="#b37">[38]</ref>, a strategy was proposed to reduce further the energy efficiency of <ref type="bibr" target="#b39">[40]</ref> leveraging fixed-point quantization. The semi-supervised framework by Kuznietsov et al. <ref type="bibr" target="#b20">[21]</ref> combined stereo supervision with sparse LiDAR measurements. The work by Zhou et al. <ref type="bibr" target="#b62">[63]</ref> represents the first attempt to supervise a depth-from-mono framework with single camera sequences. This approach was improved including additional cues such as point-cloud alignment <ref type="bibr" target="#b31">[32]</ref>, differentiable DVO <ref type="bibr" target="#b52">[53]</ref> and multi-task learning <ref type="bibr" target="#b63">[64]</ref>. Zhan et al. <ref type="bibr" target="#b61">[62]</ref> combined the two supervision approaches outlined so far deploying stereo sequences. Another class of methods [2, 1, 5] applied a generative adversarial paradigm to the monocular scenario.</p><p>Finally, relevant to our work is Single View Stereo matching (SVS) <ref type="bibr" target="#b29">[30]</ref>, processing a single image to obtain a second synthetic view using Deep3D <ref type="bibr" target="#b54">[55]</ref> and then computing a disparity map between the two using DispNetC <ref type="bibr" target="#b33">[34]</ref>. However, these two architectures are trained independently. Moreover, DispNetC is supervised with ground truth labels from synthetic <ref type="bibr" target="#b33">[34]</ref> and real domains <ref type="bibr" target="#b34">[35]</ref>. Differently, the framework we are going to introduce requires no ground truth at all and is elegantly trained in an end-to-end manner, outperforming SVS by a notable margin.</p><p>Proxy labels distillation. Since for most tasks ground truth labels are difficult and expensive to source, some works recently enquired about the possibility to replace them with easier to obtain proxy labels. Tonioni et al. <ref type="bibr" target="#b48">[49]</ref> proposed to adapt deep stereo networks to unseen environments leveraging traditional stereo algorithms and confidence measures <ref type="bibr" target="#b42">[43]</ref>, Tosi et al. <ref type="bibr" target="#b50">[51]</ref> learned confidence estimation selecting positive and negative matches by means of traditional confidence measures, Makansi et al. <ref type="bibr" target="#b32">[33]</ref> and Liu et al. <ref type="bibr" target="#b27">[28]</ref> generated proxy labels for training optical flow networks using conventional methods. Specifically relevant to monocular depth estimation are the works proposed by Yang et al. <ref type="bibr" target="#b57">[58]</ref>, using stereo visual odometry to train monocular depth estimation, by Klodt and Vedaldi <ref type="bibr" target="#b19">[20]</ref>, leveraging structure from motion algorithms and by Guo et al. <ref type="bibr" target="#b12">[13]</ref>, obtaining labels from a deep network trained with supervision to infer disparity maps from stereo pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Monocular Residual Matching</head><p>In this section, we describe in detail the proposed monocular Residual Matching (monoResMatch) architecture designed to infer accurate and dense depth estimation in a self-supervised manner from a single image. <ref type="figure" target="#fig_1">Figure 2</ref> recaps the three key components of our network. First, a multi-scale feature extractor takes as input a single raw image and computes deep learnable representations at different scales from quarter resolution F 2 L to full-resolution F 0 L in order to toughen the network to ambiguities in photometric appearance. Second, deep high-dimensional features at input image resolution are processed to estimate, through an hourglass structure with skip-connections, multi-scale inverse depth (i.e., disparity) maps aligned with the input and a virtual right view learned during training. By doing so, our network learns to emulate a binocular setup, thus allowing further processing in the stereo domain <ref type="bibr" target="#b29">[30]</ref>. Third, a disparity refinement stage estimates residual corrections to the initial disparity. In particular, we use deep features from the first stage and back-warped features of the virtual right image to construct a cost volume that stores the stereo matching costs using a correlation layer <ref type="bibr" target="#b33">[34]</ref>. Our entire architecture is trained from scratch in an endto-end manner, while SVS <ref type="bibr" target="#b29">[30]</ref> by training its two main components, Deep3D <ref type="bibr" target="#b54">[55]</ref> and DispNetC <ref type="bibr" target="#b33">[34]</ref>, on image synthesis and disparity estimation tasks separately (with the latter requiring additional, supervised depth labels from synthetic imagery <ref type="bibr" target="#b33">[34]</ref>).</p><p>Extensive experimental results will prove that monoRes-Match enables much more accurate estimations compared to SVS and other state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-scale feature extractor</head><p>Inspired by <ref type="bibr" target="#b24">[25]</ref>, given one input image I we generate deep representations using layers of convolutional filters. In particular, the first 2-stride layer convolves I with 64 learnable filters of size 7 × 7 followed by a second 2-stride convolutional layer composed of 128 filters with kernel size 4 × 4. Two deconvolutional blocks, with stride 2 and 4, are deployed to upsample features from lower-spatial resolution to full input resolution producing 32 features maps each. A 1 × 1 convolutional layer with stride 1 further processes upsampled representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Initial Disparity Estimation</head><p>Given the features extracted by the first module, this component is in charge of estimating an initial disparity map. In particular, an encoder-decoder architecture inspired by DispNet processes deep features at quarter resolution from the multi-scale feature extractor (i.e., conv2) and outputs disparity maps at different scales, specifically from 1 128 to full-resolution. Each down-sampling module, composed of two convolutional blocks with stride 2 and 1 each, produces a growing number of extracted features, respectively 64, 128, 256, 512, 1024, and each convolutional layer uses 3 × 3 kernels followed by ReLU non-linearities. Differently from DispNet, which computes matching costs in the early part of this stage using features from the left and right images of a stereo pair, our architecture lacks such neces-sary information required to compute a cost volume since it processes a single input image. Thus, no 1-D correlation layer can be imposed to encode geometrical constraints in this stage of our network. Then, upsampling modules are deployed to enrich feature representations through skipconnections and to extract two disparity maps, aligned respectively with the input frame and a virtual viewpoint on its right as in <ref type="bibr" target="#b10">[11]</ref>. This process is carried out at each scale using 1-stride convolutional layers with kernel size 3 × 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Disparity Refinement</head><p>Given an initial estimate of the disparity at each scale obtained in the second part of the network, often characterized by errors at depth discontinuities and occluded regions, this stage predicts corresponding multi-scale residual signals <ref type="bibr" target="#b13">[14]</ref> by a few stacked nonlinear layers that are then used to compute the final left-view aligned disparity map. This strategy allows us to simplify the end-to-end learning process of the entire network. Moreover, motivated by <ref type="bibr" target="#b29">[30]</ref>, we believe that geometrical constraints can play a central role in boosting the final depth accuracy. For this reason, we embed matching costs in feature space computed employing a horizontal correlation layer, typically deployed in deep stereo algorithms. To this end, we rely on the rightview disparity map computed previously to generate rightview featuresF 0 R from the left ones F 0 L using a differentiable bilinear sampler <ref type="bibr" target="#b15">[16]</ref>. The network is also fed with error e L , i.e. the absolute difference between left and virtual right features at input resolution, with the latter backwarped at the same coordinates of the former, as in <ref type="bibr" target="#b23">[24]</ref>.</p><p>We point out once more that, differently from <ref type="bibr" target="#b29">[30]</ref>, our architecture produces both a synthetic right view, i.e. its features representation, and computes the final disparity map following stereo rationale. This makes monoRes-Match a single end-to-end architecture, effectively performing stereo out of a single input view rather than the combination of two models (i.e., Deep3D <ref type="bibr" target="#b54">[55]</ref> and DispNetC <ref type="bibr" target="#b33">[34]</ref> for the two tasks outlined) trained independently as in <ref type="bibr" target="#b30">[31]</ref>. Moreover, exhaustive experiments will highlight the superior accuracy achieved by our fully self-supervised, end-toend approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Loss</head><p>In order to train our multi-stage architecture, we define the total loss as a sum of two main contributions, a L init term from the initial disparity estimation module and a L ref term from the disparity refinement stage. Following <ref type="bibr" target="#b11">[12]</ref>, we embrace the idea to up-sample the predicted low-resolution disparity maps to the full input resolution and then compute the corresponding signals. This simple strategy is designed to force the inverse depth estimation to reproduce the same objective at each scale, thus leading to much better outcomes. In particular, we obtain the final training loss as:</p><formula xml:id="formula_0">L total = ni s=1 L init + nr s=1 L ref<label>(1)</label></formula><p>where s indicates the output resolution, n i and n r the numbers of considered scales during loss computation, while L init and L ref are formalised as:</p><formula xml:id="formula_1">L init =α ap (L l ap + L r ap ) + α ds (L l ds + L r ds ) + α ps (L l ps + L r ps )<label>(2)</label></formula><formula xml:id="formula_2">L ref = α ap L l ap + α ds L l ds + α ps L l ps<label>(3)</label></formula><p>where L ap is an image reconstruction loss, L ds is a smoothness term and L ps is a proxy-supervised loss. Each term contains both the left and right components for the initial disparity estimator, and the left components only for the refinement stage.</p><p>Image reconstruction loss. A linear combination of l 1 loss and structural similarity measure (SSIM) <ref type="bibr" target="#b53">[54]</ref> encodes the quality of the reconstructed imageĨ with respect to the original image I:</p><formula xml:id="formula_3">L ap = 1 N i,j α 1 − SSIM (I ij ,Î ij ) 2 + (1 − α)|I ij −Î ij |<label>(</label></formula><p>4) Following <ref type="bibr" target="#b10">[11]</ref>, we set α = 0.85 and use a SSIM with 3 × 3 block filter.</p><p>Disparity smoothness loss. This cost encourages the predicted disparity to be locally smooth. Disparity gradients are weighted by an edge-aware term from image domain:</p><formula xml:id="formula_4">L ds = 1 N i,j |∂ x d ij |e −|∂xIij | + |∂ y d ij |e −|∂yIij |<label>(5)</label></formula><p>Proxy-supervised loss. Given the proxy disparity maps obtained by a conventional stereo algorithm, detailed in Section 4, we coach the network using reverse Huber (berHu) loss <ref type="bibr" target="#b35">[36]</ref>:</p><formula xml:id="formula_5">L ps = 1 N i,j berHu(d ij , d st ij , c) (6) berHu(d ij , d st ij , c) = |d ij − d st ij | if |d ij − d st ij | ≤ c |dij −d st ij | 2 −c 2 2c otherwise (7)</formula><p>where d ij and d st ij are, respectively, the predicted disparity and the proxy annotation for pixel at the coordinates i, j of the image, while c is adaptively set as α max i,j |d ij −d st ij |, with α = 0.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proxy labels distillation</head><p>To generate accurate proxy labels, we use the popular SGM algorithm <ref type="bibr" target="#b14">[15]</ref>, a fast yet effective solution to infer depth from a rectified stereo pair without training. In our implementation, initial matching costs are computed for each pixel p and disparity hypothesis d applying a 9×7 census transform and computing Hamming distance on pixel strings. Then, scanline optimization along eight different paths refines the initial cost volume as follows:</p><formula xml:id="formula_6">E(p, d) =C(p, d) + min j&gt;1 [C(p , d), C(p , d ± 1) + P 1, C(p , d ± q) + P 2] − min k&lt;Dmax (C(p , k))<label>(8)</label></formula><p>being C(p, d) the matching cost for pixel p and hypothesis d, P 1 and P 2 two smoothness penalties, discouraging disparity gaps between p and previous pixel p along the scanline path. The final disparity map D is obtained applying a winner-takes-all strategy to each pixel of the reference image. Although SGM generates quite accurate disparity labels, outliers may affect the training of a depth model negatively, as noticed by Tonioni et al. <ref type="bibr" target="#b48">[49]</ref>. They applied a learned confidence measure <ref type="bibr" target="#b40">[41]</ref> to filter out erroneous labels when computing the loss. Differently, we run a nonlearning based left-right consistency check to detect outliers. Purposely, by extracting both disparity maps D L and D R with SGM, respectively for the left and right images, we apply the following criteria to invalidate (i.e., set to -1) pixels having different disparities across the two maps:</p><formula xml:id="formula_7">D(p) = D(p) if |D L (p) − D R (p − D L (p))| ≤ ε −1 otherwise<label>(9)</label></formula><p>The left-right consistency check is a simple strategy that removes many wrong disparity assignments, mostly near depth discontinuities, without needing any training that would be required by <ref type="bibr" target="#b48">[49]</ref>. Therefore, our proxy labels generation process does not rely at all on ground truth depth labels. <ref type="figure" target="#fig_2">Figure 3</ref> shows an example of distilled labels (b), where black pixels correspond to outliers filtered out by left-right consistency. Although some of them persist, we can notice how they do not affect the final prediction by the trained network and how our proposal can recover accurate disparity values in occluded regions on the left side of the image (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>In this section, we describe the datasets, implementation details and then present exhaustive evaluations of monoRes-Match on various training/testing configurations, showing that our proposal consistently outperforms self-supervised state-of-the-art approaches. As standard in this field, we assess the performance of monocular depth estimation techniques following the protocol by Eigen et al. <ref type="bibr" target="#b5">[6]</ref>, extracting data from the KITTI <ref type="bibr" target="#b8">[9]</ref> dataset, using sparse LiDAR measurements as ground truth for evaluation. Additionally, we also perform an exhaustive ablation study proving that proxy supervision from SGM algorithm and effective architectural choices enable our strategy to improve predicted depth map accuracy by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>For all our experiments we compute standard monocular metrics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>: Abs rel, Sq rel, RMSE and RMSE log represent error measures while δ &lt; ζ the percentage of predictions whose maximum between ratio and inverse ratio with respect to the ground truth is lower than a threshold ζ. Two main datasets are involved in our evaluation, that are KITTI <ref type="bibr" target="#b8">[9]</ref> and CityScapes <ref type="bibr" target="#b3">[4]</ref>.</p><p>KITTI. The KITTI stereo dataset <ref type="bibr" target="#b8">[9]</ref> is a collection of rectified stereo pairs made up of 61 scenes (containing about 42,382 stereo frames) mainly concerned with driving scenarios. Predominant image size is 1242 × 375 pixels. A LiDAR device, mounted and calibrated in proximity to the left camera, was deployed to measure depth information.</p><p>Following other works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>, we divided the overall dataset into two subsets, composed respectively of 29 and  <ref type="table">Table 1</ref>. Ablation studies on the Eigen split <ref type="bibr" target="#b5">[6]</ref>, with maximum depth set to 80m. All networks run post-processing as in <ref type="bibr" target="#b10">[11]</ref> unless otherwise specified. <ref type="bibr" target="#b31">32</ref> scenes. We used 697 frames belonging to the first group for testing purposes and 22600 more taken from the second for training. We refer to these subsets as Eigen split.</p><p>CityScapes. The CityScapes dataset <ref type="bibr" target="#b3">[4]</ref> contains stereo pairs concerning about 50 cities in Germany taken from a moving vehicle in various weather conditions. It consists of 22,973 stereo pairs with a shape of 2048 × 1024 pixels. Since most of the images include the hood of the car, mostly reflective and thus leading to wrong estimates, we discarded the lower 20% of the frame before applying the random crop during training <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>Following the standard protocol in this field, we used CityScapes followed by KITTI for training. We refer to these two training sets as Cityscapes (CS) and Eigen KITTI split (K) from now on. We implemented our architecture using the TensorFlow framework, counting approximately 42.5 millions of parameters, summing variables from the multi-scale feature extractor (0.51 M), the initial disparity stage (41.4 M) and the refinement module (0.6 M). In the experiments, we pre-trained monoResMatch on CS running about 150k iteration using a batch size of 6 and random crops of size 512 × 256 on 1024 × 512 resized images from the original resolution. We used Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with β 1 = 0.9, β 2 = 0.999 and = 10 −8 . We set the initial learning rate to 10 −4 , manually halved after 100k and 120k steps, then continuing until convergence. After the first preinitialisation procedure, we perform fine-tuning of the overall architecture on 22600 KITTI raw images from K. Specifically, we run 300k steps using a batch size of 6 and extracting random crops of size 640 × 192 from resized images at 1280 × 384 resolution. At this stage, we employed a learning rate of 10 −4 , halved after 180k and 240k iterations. We fixed the hyper-parameters of the different loss components to α ap = 1, α ds = 0.1 and α ps = 1, while n i = 4 and n r = 3. As in <ref type="bibr" target="#b10">[11]</ref>, data augmentation procedure has been applied to both images from CS and K at training, in order to increase the robustness of the network. At test time, we post-process disparity as in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b57">58]</ref>. Nevertheless, we preliminary highlight that, differently from the strategies mentioned above, effects such as disparity ramps on the left border are effectively solved by simply picking random crops on proxy disparity maps generated by SGM, as clearly visible in <ref type="figure" target="#fig_2">Figure 3 (c)</ref>.</p><p>Proxy supervision is obtained through SGM implementation from <ref type="bibr" target="#b47">[48]</ref>, which allows us to quickly generate disparity maps aligned with the left and right images for both CS and K. We process such outputs using left-right consistency check in order to reduce the numbers of outliers, as discussed in Section 4 using an of 1. We assess the accuracy of our proxy generator on 200 high-quality disparity maps from KITTI 2015 training dataset <ref type="bibr" target="#b34">[35]</ref>, measuring 96.1% of pixels having disparity error smaller than 3. Compared to Tonioni et al. <ref type="bibr" target="#b48">[49]</ref>, we register a negligible drop in accuracy from 99.6% reported in their paper. However, we do not rely on any learning-based confidence estimator as they do <ref type="bibr" target="#b40">[41]</ref>, so we maintain label distillation detached from the need for ground truth as well. Since SGM runs over images at full resolution while monoResMatch inputs are resized to 1280 × 384 before extracting crops, we enforce a scaling factor to SGM disparities given by 1280 W , where W is the original image width. Consequently, the depth map estimated by monoResMatch must be properly multiplied by W 1280 at test time. The architecture is trained end-to-end on a single Titan XP GPU without any stage-wise procedure and infers depth maps in 0.16s per frame at test time, processing images at KITTI resolution (i.e., about 1280 × 384 to be compatible with monoResMatch downsampling factors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation study</head><p>In this section we examine the impact of i) proxysupervision from SGM and ii) the different components of monoResMatch. The outcomes of these experiments, con-  <ref type="table">Table 2</ref>. Quantitative evaluation on the test set of KITTI dataset <ref type="bibr" target="#b8">[9]</ref> using the split of Eigen et al. <ref type="bibr" target="#b5">[6]</ref>, maximum depth: 80m. Last four entries include post-processing <ref type="bibr" target="#b10">[11]</ref>. Ko, Kr, Ko are splits from K, defined in <ref type="bibr" target="#b57">[58]</ref>. Best results are shown in bold.</p><p>ducted on the Eigen split, are collected in <ref type="table">Table 1</ref>.</p><p>Proxy-supervised loss analysis. We train monodepth framework by Godard et al. <ref type="bibr" target="#b10">[11]</ref> from scratch adding our proxy-loss, then we compare the obtained model with the original one, as well as with the more effective strategy used by 3Net <ref type="bibr" target="#b43">[44]</ref>. We can observe that proxy-loss enables a more accurate monodepth model (row 3) compared to <ref type="bibr" target="#b10">[11]</ref>, moreover it also outperforms virtual trinocular supervision proposed in <ref type="bibr" target="#b43">[44]</ref>, attaining better metrics with respect of both, but δ &lt; 1.25 for 3Net. Specifically, by recalling <ref type="figure" target="#fig_2">Figure 3</ref>, the proxy distillation couples well with a cropping strategy, solving well-known issues for stereo supervision such as disparity ramps on the left border. We refer to supplementary material for additional qualitative examples.</p><p>Component analysis. Still referring to <ref type="table">Table 1</ref>, we evaluate different configurations of our framework by ablating the key modules peculiar to our architecture. First, we train monoResMatch on K without proxy supervision (row 3) to highlight that our architecture already outperforms <ref type="bibr" target="#b10">[11]</ref> (row 1). Training on CS+K with proxy labels, we can notice how without any refinement module (no-refinement), our framework already outperforms the proxy-supervised ResNet50 model of Godard et al. <ref type="bibr" target="#b10">[11]</ref>. Adding the disparity refinement component without encoding any matching relationship (no-corr) enables small improvements, becoming much larger on most metrics when a correlation layer is introduced (no-pp) to process real and synthesized features as to resemble stereo matching. Finally, post-processing as in <ref type="bibr" target="#b10">[11]</ref> (row 11) still ameliorates all scores, although the larger contribution is given by the correlation-based refinement module, as perceived by comparing no-refinement and no-pp entries. Finally, by comparing rows 4 and 11 we can also perceive the impact given by CS pretraining on our full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with self-supervised frameworks</head><p>Having studied in detail the contribution of both monoResMatch architecture and proxy supervision, we compare our framework with state-of-the-art selfsupervised approaches for monocular depth estimation. <ref type="table">Table 2</ref> collects results obtained evaluating different models on the aforementioned Eigen split <ref type="bibr" target="#b5">[6]</ref>. In this evaluation, we consider only competitors trained without any supervision from ground truth labels (e.g., synthetic datasets <ref type="bibr" target="#b33">[34]</ref>) involved in any phase of the training process <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13]</ref>. We refer to methods using monocular supervision (Seq), binocular (Stereo) or both (Seq+Stereo). Most methods are trained on CS and K, except Yang et al. <ref type="bibr" target="#b57">[58]</ref> that leverages on different sub-splits of K. From the table, we can notice that monoResMatch outperforms all of them significantly.</p><p>To compete with methods exploiting supervision from dense synthetic ground truth <ref type="bibr" target="#b33">[34]</ref>, we run additional experiments using very few annotated samples from KITTI as in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref>, for a more fair comparison. <ref type="table" target="#tab_3">Table 3</ref> collects the outcome of these experiments according to different degrees of supervision, in particular using accurate ground truth labels from the KITTI 2015 training split (200-acrt) or different amounts of samples from K with LiDAR measurements, respectively 100, 200, 500 and 700 as proposed in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref>, running only 5k iterations for each configuration. We point out that monoResMatch, on direct comparisons to methods trained with the same amount of labeled images, consistently achieves better scores, with rare exceptions. Moreover, we highlight in red for each metric the best score among all the considered configurations, figuring out that monoResMatch trained with 200-acrt plus 500 samples from K attains the best accuracy on all metrics. This fact points out the high effectiveness of the proposed architecture, able to outperform state-of-the-art techniques <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13]</ref> trained with much more supervised data (i.e., more than 30k stereo pairs from <ref type="bibr" target="#b33">[34]</ref> and pre-trained weights from ImageNet). Leveraging on the traditional SGM algorithm instead of a deep stereo network as in <ref type="bibr" target="#b12">[13]</ref> for proxysupervision ensures a faster and easier to handle training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Performance on single view stereo estimation</head><p>Finally, we further compare monoResMatch directly with Single View Stereo (SVS) by Luo et al. <ref type="bibr" target="#b29">[30]</ref>, being both driven by the same rationale. We fine-tuned monoRes-   Method D1-bg D1-fg D1-all monodepth <ref type="bibr" target="#b10">[11]</ref> 27.00 28. <ref type="bibr" target="#b23">24</ref>   <ref type="bibr" target="#b34">[35]</ref>. Percentage of pixels having error larger than 3 or 5% of the ground truth. Best results are shown in bold. <ref type="table" target="#tab_3">Table 3</ref> and submitted to the online stereo benchmark <ref type="bibr" target="#b34">[35]</ref> as performed in <ref type="bibr" target="#b30">[31]</ref>. <ref type="table" target="#tab_4">Table 4</ref> compares monoResMatch with SVS and other techniques evaluated in <ref type="bibr" target="#b30">[31]</ref>, respectively monodepth <ref type="bibr" target="#b10">[11]</ref> and OpenCV Block-Matching (OCV-BM). D1 scores represent the percentages of pixels having a disparity error larger than 3 or 5% of the ground truth value on different portions of the image, respectively background (bg), foreground (fg) or its entirety (all). We can observe from the table a margin larger than 3% on D1-bg and near to 1% for D1-fg, resumed in a total reduction of 2.72%. This outcome supports once more the superiority of monoRes-Match, although SVS is trained on many, synthetic images with ground truth <ref type="bibr" target="#b33">[34]</ref>. Finally, <ref type="figure" target="#fig_3">Figure 4</ref> depicts qualitative examples retrieved from the KITTI online benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Match on the KITTI 2015 training set as in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we proposed monoResMatch, a novel framework for monocular depth estimation. It combines i) pondered design choices to tackle depth-from-mono in analogy to stereo matching, thanks to a correlation-based refinement module and ii) a more robust self-supervised training leveraging on proxy ground truth labels generated through a traditional (i.e. non-learning based) algorithm such as SGM. In contrast to state-of-the-art models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b57">58]</ref>, our architecture is elegantly trained in an end-to-end manner. Through exhaustive experiments, we prove that plugging proxy-supervision at training time leads to more accurate networks and, coupling this strategy with monoResMatch architecture, is state-of-the-art for self-supervised monocular depth estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the proposed depth-from-mono solution. Input image from KITTI dataset (top). Estimated depth map by our monoResMatch (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of our monoResMatch architecture. Given one input image, the multi-scale feature extractor (in red) generates highlevel representations in the first stage. The initial disparity estimator (in blue) yields multi-scale disparity maps aligned with the left and right frames of a stereo pair. The disparity refinement module (in orange) is in charge of refining the initial left disparity relying on features computed in the first stage, disparities generated in the second stage, matching costs between high-dimensional features F 0 L extracted from input and syntheticF 0 R from a virtual right viewpoint, together with absolute error eL between F 0 L and back-warpedF 0 R (see Section 3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Examples of proxy labels computed by SGM. Given the source image (a), the network exploits the SGM supervision filtered with left-right consistency check (b) in order to train monoResMatch to estimate the final disparity map (c). No post-processing from<ref type="bibr" target="#b10">[11]</ref> is performed on (c) in this example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Stereo evaluation of our depth-from-mono framework. From left to right the input image, the predicted depth and the errors with respect to ground truth. The last line reports the color code used to display the seriousness of the shortcomings (same of<ref type="bibr" target="#b34">[35]</ref>)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Method SupervisionTrain set Abs Rel Sq Rel RMSE RMSE log δ &lt;1.25 δ &lt; 1.25 2 δ &lt; 1.25 3</figDesc><table><row><cell>Image SGM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Godard et al. [11] ResNet50</cell><cell>K</cell><cell>0.128</cell><cell>1.038</cell><cell>5.355</cell><cell>0.223</cell><cell>0.833</cell><cell>0.939</cell><cell>0.972</cell></row><row><cell>Poggi et al. [44] ResNet50</cell><cell>K</cell><cell>0.126</cell><cell>0.961</cell><cell>5.205</cell><cell>0.220</cell><cell>0.835</cell><cell>0.941</cell><cell>0.974</cell></row><row><cell>monoResMatch</cell><cell>K</cell><cell>0.116</cell><cell>0.986</cell><cell>5.098</cell><cell>0.214</cell><cell>0.847</cell><cell>0.939</cell><cell>0.972</cell></row><row><cell>monoResMatch</cell><cell>K</cell><cell>0.111</cell><cell>0.867</cell><cell>4.714</cell><cell>0.199</cell><cell>0.864</cell><cell>0.954</cell><cell>0.979</cell></row><row><cell>Godard et al. [11] ResNet50</cell><cell>CS,K</cell><cell>0.114</cell><cell>0.898</cell><cell>4.935</cell><cell>0.206</cell><cell>0.861</cell><cell>0.949</cell><cell>0.976</cell></row><row><cell>Poggi et al. [44] ResNet50</cell><cell>CS,K</cell><cell>0.111</cell><cell>0.849</cell><cell>4.822</cell><cell>0.202</cell><cell>0.865</cell><cell>0.952</cell><cell>0.978</cell></row><row><cell>Godard et al. [11] ResNet50</cell><cell>CS,K</cell><cell>0.110</cell><cell>0.822</cell><cell>4.675</cell><cell>0.199</cell><cell>0.862</cell><cell>0.953</cell><cell>0.980</cell></row><row><cell>monoResMatch (no-refinement)</cell><cell>CS,K</cell><cell>0.107</cell><cell>0.781</cell><cell>4.588</cell><cell>0.195</cell><cell>0.869</cell><cell>0.957</cell><cell>0.980</cell></row><row><cell>monoResMatch (no-corr)</cell><cell>CS,K</cell><cell>0.104</cell><cell>0.766</cell><cell>4.553</cell><cell>0.192</cell><cell>0.875</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell>monoResMatch (no-pp)</cell><cell>CS,K</cell><cell>0.098</cell><cell>0.711</cell><cell>4.433</cell><cell>0.189</cell><cell>0.888</cell><cell>0.960</cell><cell>0.980</cell></row><row><cell>monoResMatch</cell><cell>CS,K</cell><cell>0.096</cell><cell>0.673</cell><cell>4.351</cell><cell>0.184</cell><cell>0.890</cell><cell>0.961</cell><cell>0.981</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Abs Rel Sq Rel RMSE RMSE log δ &lt;1.25 δ &lt; 1.25 2 δ &lt; 1.25<ref type="bibr" target="#b2">3</ref> Zou et al.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Lower is better</cell><cell cols="2">Higher is better</cell><cell></cell></row><row><cell>Method</cell><cell>Supervision</cell><cell>Train set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[64]</cell><cell>Seq</cell><cell>CS,K</cell><cell>0.146</cell><cell>1.182</cell><cell>5.215</cell><cell>0.213</cell><cell>0.818</cell><cell>0.943</cell><cell>0.978</cell></row><row><cell>Mahjourian et al. [32]</cell><cell>Seq</cell><cell>CS,K</cell><cell>0.159</cell><cell>1.231</cell><cell>5.912</cell><cell>0.243</cell><cell>0.784</cell><cell>0.923</cell><cell>0.970</cell></row><row><cell>Yin et al. [59] GeoNet ResNet50</cell><cell>Seq</cell><cell>CS,K</cell><cell>0.153</cell><cell>1.328</cell><cell>5.737</cell><cell>0.232</cell><cell>0.802</cell><cell>0.934</cell><cell>0.972</cell></row><row><cell>Wang et al. [53]</cell><cell>Seq</cell><cell>CS,K</cell><cell>0.148</cell><cell>1.187</cell><cell>5.496</cell><cell>0.226</cell><cell>0.812</cell><cell>0.938</cell><cell>0.975</cell></row><row><cell>Poggi et al. [40] PyD-Net (200)</cell><cell>Stereo</cell><cell>CS,K</cell><cell>0.146</cell><cell>1.291</cell><cell>5.907</cell><cell>0.245</cell><cell>0.801</cell><cell>0.926</cell><cell>0.967</cell></row><row><cell>Godard et al. [11] ResNet50</cell><cell>Stereo</cell><cell>CS,K</cell><cell>0.114</cell><cell>0.898</cell><cell>4.935</cell><cell>0.206</cell><cell>0.861</cell><cell>0.949</cell><cell>0.976</cell></row><row><cell>Poggi et al. [44] 3Net ResNet50</cell><cell>Stereo</cell><cell>CS,K</cell><cell>0.111</cell><cell>0.849</cell><cell>4.822</cell><cell>0.202</cell><cell>0.865</cell><cell>0.952</cell><cell>0.978</cell></row><row><cell>Pilzer et al. [39] (Teacher)</cell><cell>Stereo</cell><cell>CS,K</cell><cell>0.098</cell><cell>0.831</cell><cell>4.656</cell><cell>0.202</cell><cell>0.882</cell><cell>0.948</cell><cell>0.973</cell></row><row><cell>Yang et al. [58]</cell><cell cols="2">Seq+Stereo K o , K r , K o</cell><cell>0.097</cell><cell>0.734</cell><cell>4.442</cell><cell>0.187</cell><cell>0.888</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell>monoResMatch</cell><cell>Stereo</cell><cell>CS,K</cell><cell>0.096</cell><cell>0.673</cell><cell>4.351</cell><cell>0.184</cell><cell>0.890</cell><cell>0.961</cell><cell>0.981</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>MethodSupervision Abs Rel Sq Rel RMSE RMSE log δ &lt;1.25 δ &lt; 1.25 2 δ &lt; 1.25 3</figDesc><table><row><cell>200-acrt 100 200 500 700</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Luo et al. [30]</cell><cell>0.101</cell><cell>0.673</cell><cell>4.425</cell><cell>0.176</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>monoResMatch</cell><cell>0.089</cell><cell>0.575</cell><cell>4.186</cell><cell>0.181</cell><cell>0.897</cell><cell>0.964</cell><cell>0.982</cell></row><row><cell>Luo et al. [30]</cell><cell>0.100</cell><cell>0.670</cell><cell>4.437</cell><cell>0.192</cell><cell>0.882</cell><cell>0.958</cell><cell>0.979</cell></row><row><cell>monoResMatch</cell><cell>0.096</cell><cell>0.573</cell><cell>3.950</cell><cell>0.168</cell><cell>0.897</cell><cell>0.968</cell><cell>0.987</cell></row><row><cell>Luo et al. [30]</cell><cell>0.094</cell><cell>0.635</cell><cell>4.275</cell><cell>0.179</cell><cell>0.889</cell><cell>0.964</cell><cell>0.984</cell></row><row><cell>monoResMatch</cell><cell>0.093</cell><cell>0.567</cell><cell>3.914</cell><cell>0.165</cell><cell>0.901</cell><cell>0.969</cell><cell>0.987</cell></row><row><cell>Luo et al. [30]</cell><cell>0.094</cell><cell>0.626</cell><cell>4.252</cell><cell>0.177</cell><cell>0.891</cell><cell>0.965</cell><cell>0.984</cell></row><row><cell>monoResMatch</cell><cell>0.095</cell><cell>0.567</cell><cell>3.942</cell><cell>0.166</cell><cell>0.899</cell><cell>0.969</cell><cell>0.987</cell></row><row><cell>Guo et al. [13]</cell><cell>0.096</cell><cell>0.641</cell><cell>4.095</cell><cell>0.168</cell><cell>0.892</cell><cell>0.967</cell><cell>0.986</cell></row><row><cell>monoResMatch</cell><cell>0.098</cell><cell>0.597</cell><cell>3.973</cell><cell>0.169</cell><cell>0.895</cell><cell>0.968</cell><cell>0.987</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Experimental results on the Eigen split<ref type="bibr" target="#b5">[6]</ref>, maximum depth: 80m. Comparison between methods supervised by few annotated samples. Best results in direct comparisons are shown in bold, best overall scores are in red, consistently attained by monoResMatch.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>27.21 OCV-BM 24.29 30.13 25.27 SVS [30] 25.18 20.77 24.44 monoResMatch 22.10 19.81 21.72 Quantitative results on the test set of the KITTI 2015 Stereo Benchmark</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Abarghouei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monocular depth prediction using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchendra</forename><forename type="middle">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mukta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Workshop on Deep Learning for Visual SLAM, (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01260</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="484" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semi-global matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stereonet: Guided hierarchical refinement for real-time edge-aware depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Conference on Computer Vision (ECCV 2018)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervising the new with the old: learning sfm from sfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Klodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
	<note>Federico Tombari, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lqlzj</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep stereo matching with explicit cost aggregation subarchitecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lidong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yucheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32th AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Ddflow: Learning optical flow with unlabeled data distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5695" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single view stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="155" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single view stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osama</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06389</idno>
		<title level="m">Fusionnet and augmentedflownet: Selective proxy ground truth for training on unlabeled images</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A robust hybrid of lasso and ridge regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Art</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary Mathematics</title>
		<imprint>
			<biblScope unit="volume">443</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="59" to="72" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Sj</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Enabling energy-efficient unsupervised monocular depth estimation on armv7-based platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentino</forename><surname>Peluso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Cipolletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Calimera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design Automation and Test in Europe (DATE)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Refine and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Lathuiliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards real-time unsupervised monocular depth estimation on cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/JRS Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning from scratch a confidence measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th British Conference on Machine Vision, BMVC</title>
		<meeting>the 27th British Conference on Machine Vision, BMVC</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Guided stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Pallotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Quantitative evaluation of confidence measures in a machine learning world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning monocular depth estimation with unsupervised trinocular assumptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Edgestereo: A context integrated residual pyramid network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Large scale semi-global matching on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Spangenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Adfeldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raúl</forename><surname>Rojas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="195" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised adaptation for deep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Real-time self-adaptive deep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning confidence measures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Miguel</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Segstereo: Exploiting semantic information for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="835" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Geometry meets semantic for semi-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pierluigi Zama Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
