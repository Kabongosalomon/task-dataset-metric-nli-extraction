<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
							<email>sacmehta@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
							<email>mohammadr@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for AI and XNOR.AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Caspi</surname></persName>
							<email>caspian@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
							<email>shapiro@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<email>hannaneh@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Source code: https://github.com/sacmehta/ESPNet</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a fast and efficient convolutional neural network, ES-PNet, for semantic segmentation of high resolution images under resource constraints. ESPNet is based on a new convolutional module, efficient spatial pyramid (ESP), which is efficient in terms of computation, memory, and power. ES-PNet is 22 times faster (on a standard GPU) and 180 times smaller than the state-of-the-art semantic segmentation network PSPNet [1], while its categorywise accuracy is only 8% less. We evaluated ESPNet on a variety of semantic segmentation datasets including Cityscapes, PASCAL VOC, and a breast biopsy whole slide image dataset. Under the same constraints on memory and computation, ESPNet outperforms all the current efficient CNN networks such as , ShuffleNet [17], and ENet [20] on both standard metrics and our newly introduced performance metrics that measure efficiency on edge devices. Our network can process high resolution images at a rate of 112 and 9 frames per second on a standard GPU and edge device, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep convolutional neural network (CNN) models have achieved high accuracy in visual scene understanding tasks <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. While the accuracy of these networks has improved with their increase in depth and width, large networks are slow and power hungry. This is especially problematic on the computationally heavy task of semantic segmentation <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. For example, PSPNet <ref type="bibr" target="#b0">[1]</ref> has 65.7 million parameters and runs at about 1 FPS while discharging the battery of a standard laptop at a rate of 77 Watts. Many advanced real-world applications, such as self-driving cars, robots, and augmented reality, are sensitive and demand on-line processing of data locally on edge devices. These accurate networks require enormous resources and are not suitable for edge devices, which have limited energy overhead, restrictive memory constraints, and reduced computational capabilities.</p><p>Convolution factorization has demonstrated its success in reducing the computational complexity of deep CNNs (e.g. Inception <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, ResNext <ref type="bibr" target="#b13">[14]</ref>, and Xception <ref type="bibr" target="#b14">[15]</ref>). We introduce an efficient convolutional module, ESP (efficient spatial pyramid), which is based on the convolutional factorization principle <ref type="figure">(Fig. 1</ref>  <ref type="figure">Fig. 1: (a)</ref> The standard convolution layer is decomposed into point-wise convolution and spatial pyramid of dilated convolutions to build an efficient spatial pyramid (ESP) module. (b) Block diagram of ESP module. The large effective receptive field of the ESP module introduces gridding artifacts, which are removed using hierarchical feature fusion (HFF). A skip-connection between input and output is added to improve the information flow. See Section 3 for more details. Dilated convolutional layers are denoted as (# input channels, effective kernel size, # output channels). The effective spatial dimensions of a dilated convolutional kernel are n k × n k , where n k = (n − 1)2 k−1 + 1, k = 1, · · · , K. Note that only n × n pixels participate in the dilated convolutional kernel. In our experiments n = 3 and d = M K .</p><p>these ESP modules, we introduce an efficient network structure, ESPNet, that can be easily deployed on resource-constrained edge devices. ESPNet is fast, small, low power, and low latency, yet still preserves segmentation accuracy. ESP is based on a convolution factorization principle that decomposes a standard convolution into two steps: (1) point-wise convolutions and (2) spatial pyramid of dilated convolutions, as shown in <ref type="figure">Fig. 1</ref>. The point-wise convolutions help in reducing the computation, while the spatial pyramid of dilated convolutions re-samples the feature maps to learn the representations from large effective receptive field. We show that our ESP module is more efficient than other factorized forms of convolutions, such as Inception <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> and ResNext <ref type="bibr" target="#b13">[14]</ref>. Under the same constraints on memory and computation, ESPNet outperforms MobileNet <ref type="bibr" target="#b15">[16]</ref> and ShuffleNet <ref type="bibr" target="#b16">[17]</ref> (two other efficient networks that are built upon the factorization principle). We note that existing spatial pyramid methods (e.g. the atrous spatial pyramid module in <ref type="bibr" target="#b2">[3]</ref>) are computationally expensive and cannot be used at different spatial levels for learning the representations. In contrast to these methods, ESP is computationally efficient and can be used at different spatial levels of a CNN network. Existing models based on dilated convolutions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> are large and inefficient, but our ESP module generalizes the use of dilated convolutions in a novel and efficient way.</p><p>To analyze the performance of a CNN network on edge devices, we introduce several new performance metrics, such as sensitivity to GPU frequency and warp execution efficiency. To showcase the power of ESPNet, we evaluate our model on one of the most expensive tasks in AI and computer vision: semantic segmentation. ESPNet is empirically demonstrated to be more accurate, efficient, and fast than ENet <ref type="bibr" target="#b19">[20]</ref>, one of the most power-efficient semantic segmentation networks, while learning a similar number of parameters. Our results also show that ESPNet learns generalizable representations and outperforms ENet <ref type="bibr" target="#b19">[20]</ref> and another efficient network ERFNet <ref type="bibr" target="#b20">[21]</ref> on the unseen dataset. ESPNet can process a high resolution RGB image at a rate of 112 frames per second (FPS) on a high-end GPU, 21 FPS on a laptop, and 9 FPS on an edge device 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multiple different techniques, such as convolution factorization, network compression, and low-bit networks, have been proposed to speed up convolutional neural networks. We, first, briefly describe these approaches and then provide a brief overview of CNNbased semantic segmentation. Convolution factorization: Convolutional factorization decomposes the convolutional operation into multiple steps to reduce the computational complexity. This factorization has successfully shown its potential in reducing the computational complexity of deep CNN networks (e.g. Inception <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, factorized network <ref type="bibr" target="#b21">[22]</ref>, ResNext <ref type="bibr" target="#b13">[14]</ref>, Xception <ref type="bibr" target="#b14">[15]</ref>, and MobileNets <ref type="bibr" target="#b15">[16]</ref>). ESP modules are also built on this factorization principle. The ESP module decomposes a convolutional layer into a point-wise convolution and spatial pyramid of dilated convolutions. This factorization helps in reducing the computational complexity, while simultaneously allowing the network to learn the representations from a large effective receptive field. Network Compression: Another approach for building efficient networks is compression. These methods use techniques such as hashing <ref type="bibr" target="#b22">[23]</ref>, pruning <ref type="bibr" target="#b23">[24]</ref>, vector quantization <ref type="bibr" target="#b24">[25]</ref>, and shrinking <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> to reduce the size of the pre-trained network. Low-bit networks: Another approach towards efficient networks is low-bit networks, which quantize the weights to reduce the network size and complexity (e.g. <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>). Sparse CNN: To remove the redundancy in CNNs, sparse CNN methods, such as sparse decomposition <ref type="bibr" target="#b31">[32]</ref>, structural sparsity learning <ref type="bibr" target="#b32">[33]</ref>, and dictionary-based method <ref type="bibr" target="#b33">[34]</ref>, have been proposed.</p><p>We note that compression-based methods, low-bit networks, and sparse CNN methods are equally applicable to ESPNets and are complementary to our work. Dilated convolution: Dilated convolutions <ref type="bibr" target="#b34">[35]</ref> are a special form of standard convolutions in which the effective receptive field of kernels is increased by inserting zeros (or holes) between each pixel in the convolutional kernel. For a n × n dilated convolutional kernel with a dilation rate of r, the effective size of the kernel is [(n − 1)r + 1] 2 . The dilation rate specifies the number of zeros (or holes) between pixels. However, due to dilation, only n × n pixels participate in the convolutional operation, reducing the computational cost while increasing the effective kernel size.</p><p>Yu and Koltun <ref type="bibr" target="#b17">[18]</ref> stacked dilated convolution layers with increasing dilation rate to learn contextual representations from a large effective receptive field. A similar strategy was adopted in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. Chen et al. <ref type="bibr" target="#b2">[3]</ref> introduced an atrous spatial pyramid (ASP) module. This module can be viewed as a parallelized version of <ref type="bibr" target="#b2">[3]</ref>. These modules are computationally inefficient (e.g. ASPs have high memory requirements and learn many more parameters; see Section 3.2). Our ESP module also learns multi-scale representations using dilated convolutions in parallel; however, it is computationally efficient and can be used at any spatial level of a CNN network. CNN for semantic segmentation: Different CNN-based segmentation networks have been proposed, such as multi-dimensional recurrent neural networks <ref type="bibr" target="#b37">[38]</ref>, encoderdecoders <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, hypercolumns <ref type="bibr" target="#b40">[41]</ref>, region-based representations <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, and cascaded networks <ref type="bibr" target="#b43">[44]</ref>. Several supporting techniques along with these networks have been used for achieving high accuracy, including ensembling features <ref type="bibr" target="#b2">[3]</ref>, multi-stage training <ref type="bibr" target="#b44">[45]</ref>, additional training data from other datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>, object proposals <ref type="bibr" target="#b45">[46]</ref>, CRF-based post processing <ref type="bibr" target="#b2">[3]</ref>, and pyramid-based feature re-sampling <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Encoder-decoder networks: Our work is related to this line of work. The encoderdecoder networks first learn the representations by performing convolutional and downsampling operations. These representations are then decoded by performing up-sampling and convolutional operations. ESPNet first learns the encoder and then attaches a lightweight decoder to produce the segmentation mask. This is in contrast to existing networks where the decoder is either an exact replica of the encoder (e.g. <ref type="bibr" target="#b38">[39]</ref>) or is relatively small (but not light weight) in comparison to the encoder (e.g. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>). Feature re-sampling methods: The feature re-sampling methods re-sample the convolutional feature maps at the same scale using different pooling rates <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and kernel sizes <ref type="bibr" target="#b2">[3]</ref> for efficient classification. Feature re-sampling is computationally expensive and is performed just before the classification layer to learn scale-invariant representations. We introduce a computationally efficient convolutional module that allows feature re-sampling at different spatial levels of a CNN network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ESPNet</head><p>This section elaborates on the details of ESPNET and describes the core ESP module on which it is built. We compare ESP modules with similar CNN modules, such as Inception <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, ResNext <ref type="bibr" target="#b13">[14]</ref>, MobileNet <ref type="bibr" target="#b15">[16]</ref>, and ShuffleNet <ref type="bibr" target="#b16">[17]</ref> modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ESP module</head><p>ESPNet is based on efficient spatial pyramid (ESP) modules, which are a factorized form of convolutions that decompose a standard convolution into a point-wise convolution and a spatial pyramid of dilated convolutions (see <ref type="figure">Fig. 1a</ref>). The point-wise convolution in the ESP module applies a 1 × 1 convolution to project high-dimensional feature maps onto a low-dimensional space. The spatial pyramid of dilated convolutions then re-samples these low-dimensional feature maps using K, n × n dilated convolutional kernels simultaneously, each with a dilation rate of 2 k−1 , k = {1, · · · , K}. This factorization drastically reduces the number of parameters and the memory required by the ESP module, while preserving a large effective receptive field (n − 1)2 K−1 + 1 2 . This pyramidal convolutional operation is called a spatial pyramid of dilated convolutions, because each dilated convolutional kernel learns weights with different receptive fields and so resembles a spatial pyramid. A standard convolutional layer takes an input feature map F i ∈ R W ×H×M and applies N kernels K ∈ R m×n×M to produce an output feature map F o ∈ R W ×H×N , where W and H represent the width and height of the feature map, m and n represent the width and height of the kernel, and M and N represent the number of input and output feature channels. For the sake of simplicity, we will assume that m = n. A standard convolutional kernel thus learns n 2 MN parameters. These parameters are multiplicatively dependent on the spatial dimensions of the n × n kernel and the number of input M and output N channels. Width divider K: To reduce the computational cost, we introduce a simple hyperparameter K. The role of K is to shrink the dimensionality of the feature maps uniformly across each ESP module in the network. Reduce: For a given K, the ESP module first reduces the feature maps from M-dimensional space to N K -dimensional space using a point-wise convolution (Step 1 in <ref type="figure">Fig. 1a</ref>). Split: The low-dimensional feature maps are then split across K parallel branches. Transform: Each branch then processes these feature maps simultaneously using n × n dilated convolutional kernels with different dilation rates given by 2 k−1 , k = {1, · · · , K − 1} (Step 2 in <ref type="figure">Fig. 1a</ref>). Merge: The output of these K parallel dilated convolutional kernels is then concatenated to produce an Ndimensional output feature map 4 . <ref type="figure">Fig. 1b</ref> visualizes the reduce-split-transform-merge strategy used in ESP modules.</p><p>The ESP module has MN K + (nN) 2 K parameters and its effective receptive field is (n − 1)2 K−1 + 1 2 . Compared to the n 2 NM parameters of the standard convolution, factorizing it using the two steps reduces the total number of parameters in the ESP module by a factor of n 2 MK M+n 2 N , while increasing the effective receptive field by ∼ [2 K−1 ] 2 . For example, an ESP module learns ∼ 3.6 times fewer parameters with an effective receptive field of 17 × 17 than a standard convolutional kernel with an effective receptive field of 3 × 3 for n = 3, N = M = 128, and K = 4. Hierarchical feature fusion (HFF) for de-gridding: While concatenating the outputs of dilated convolutions give the ESP module a large effective receptive field, it introduces unwanted checkerboard or gridding artifacts, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. To address the gridding artifact in ESP, the feature maps obtained using kernels of different dilation rates are hierarchically added before concatenating them (HFF in <ref type="figure">Fig. 1b</ref>). This solution is simple and effective and does not increase the complexity of the ESP module, in contrast to existing methods that remove the gridding artifact by learning more parameters using dilated convolutional kernels with small dilation rates <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref>. To improve the gradient flow inside the network, the input and output feature maps of the ESP module are combined using an element-wise sum <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relationship with other CNN modules</head><p>The ESP module shares similarities with the following CNN modules. MobileNet module: The MobileNet module <ref type="bibr" target="#b15">[16]</ref>, visualized in <ref type="figure">Fig. 3a</ref>, uses a depthwise separable convolution <ref type="bibr" target="#b14">[15]</ref> that factorizes a standard convolutions into depth-wise <ref type="bibr" target="#b3">4</ref> In general, N K may not be a perfect divisor, and therefore concatenating K, N K -dimensional feature maps would not result in an N-dimensional output. To handle this, we use N − (K − 1) N K kernels with a dilation rate of 2 0 and N K kernels for each dilation rate 2 k−1 for k = {2, · · · , K}. convolutions (transform) and point-wise convolutions (expand). It learns less parameters, has high memory requirement, and low receptive field than the ESP module. An extreme version of the ESP module (with K = N) is almost identical to the MobileNet module, differing only in the order of convolutional operations. In the MobileNet module, the spatial convolutions are followed by point-wise convolutions; however, in the ESP module, point-wise convolutions are followed by spatial convolutions. Note that the effective receptive field of an ESP module ( (n − 1)2 K−1 + 1 2 ) is higher than a MobileNet module ([n] 2 ).</p><p>ShuffleNet module: The ShuffleNet module <ref type="bibr" target="#b16">[17]</ref>, shown in <ref type="figure">Fig. 3b</ref>, is based on the principle of reduce-transform-expand. It is an optimized version of the bottleneck block in ResNet <ref type="bibr" target="#b46">[47]</ref>. To reduce computation, Shufflenet makes use of grouped convolutions <ref type="bibr" target="#b47">[48]</ref> and depth-wise convolutions <ref type="bibr" target="#b14">[15]</ref>. It replaces 1 × 1 and 3 × 3 convolutions in the bottleneck block in ResNet with 1 × 1 grouped convolutions and 3 × 3 depth-wise separable convolutions, respectively. The Shufflenet module learns many less parameters than the ESP module, but has higher memory requirements and a smaller receptive field.</p><p>Inception module: Inception modules <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> are built on the principle of split-reducetransform-merge. These modules are usually heterogeneous in number of channels and kernel size (e.g. some of the modules are composed of standard and factored convolutions). In contrast to the Inception modules, ESP modules are straightforward and simple to design. For the sake of comparison, the homogeneous version of an Inception module is shown in <ref type="figure">Fig. 3c</ref>. <ref type="figure">Fig. 3g</ref> compares the Inception module with the ESP module. ESP (1) learns fewer parameters, (2) has a low memory requirement, and (3) has a larger effective receptive field.</p><p>ResNext module: A ResNext module <ref type="bibr" target="#b13">[14]</ref>, shown in <ref type="figure">Fig. 3d</ref>, is a parallel version of the bottleneck module in ResNet <ref type="bibr" target="#b46">[47]</ref> and is based on the principle of split-reducetransform-expand-merge. The ESP module is similar to ResNext in the sense that it involves branching and residual summation. However, the ESP module is more efficient in memory and parameters and has a larger effective receptive field.</p><p>Atrous spatial pyramid (ASP) module: An ASP module <ref type="bibr" target="#b2">[3]</ref>, shown in <ref type="figure">Fig. 3e</ref>, is built on the principle of split-transform-merge. The ASP module involves branching with each branch learning kernel at a different receptive field (using dilated convolutions). Though ASP modules tend to perform well in segmentation tasks due to their high effective receptive fields, ASP modules have high memory requirements and learn many more parameters. Unlike the ASP module, the ESP module is computationally efficient.</p><formula xml:id="formula_0">M, 3 × 3, M M, 1 × 1, N Depth-wise Grouped Standard Convolution Type MobileNet (a) MobileNet M, 1 × 1, d d, 3 × 3, d d, 1 × 1, N Sum (b) ShuffleNet · · · M, 1 × 1, d M, 1 × 1, d M, 1 × 1, d · · · d, n × n, d d, n × n, d d, n × n, d Concat (c) Inception · · · M, 1 × 1, d M, 1 × 1, d M, 1 × 1, d · · · d, n × n, d d, n × n, d d, n × n, d · · · d, 1 × 1, N d, 1 × 1, N d, 1 × 1, N Sum Sum (d) ResNext · · · M, n × n, N 2 1 M, n × n, N 2 0 M, n × n, N 2 K−1 Sum (e) ASP M, 1 × 1, d · · · d, n3 × n3, d d, n2 × n2, d d, n1 × n1, d d, nK × nK, d HFF Sum Sum Sum</formula><p>Concat Sum (f) ESP (same as in <ref type="figure">Fig. 1</ref>)</p><formula xml:id="formula_1">Module # Parameters Memory (in MB) Effective Receptive Field MobileNet M(n 2 + N) = 11, 009 (M + N)W H = 2.39 [n] 2 = 3 × 3 ShuffleNet d g (M + N) + n 2 d = 2,180 W H(2 * d + N) = 1.67 [n] 2 = 3 × 3 Inception K(Md + n 2 d 2 ) = 28, 000 2KW Hd = 2.39 [n] 2 = 3 × 3 ResNext K(Md + d 2 n 2 + dN) = 38, 000 KW H(2d + N) = 8.37 [n] 2 = 3 × 3 ASP KMNn 2 = 450, 000 KW HN = 5.98 (n − 1)2 K−1 + 1 2 = 33 × 33 ESP (Fig. 1b) Md + Kn 2 d 2 = 20, 000 W Hd(K + 1) = 1.43 (n − 1)2 K−1 + 1 2 = 33 × 33</formula><p>Here, M = N = 100, n = 3, K = 5, d = N K = 20, g = 2, and W = H = 56.</p><p>(g) Comparison between different modules <ref type="figure">Fig. 3</ref>: Different types of convolutional modules for comparison. We denote the layer as (# input channels, kernel size, # output channels). Dilation rate in (e) is indicated on top of each layer.</p><p>Here, g represents the number of convolutional groups in grouped convolution <ref type="bibr" target="#b47">[48]</ref>. For simplicity, we only report the memory of convolutional layers in (d). For converting the required memory to bytes, we multiply it by 4 (1 float requires 4 bytes for storage).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Semantic segmentation is one of the most expensive task in AI and computer vision. To showcase the power of ESPNet, ESPNet's performance is evaluated on several datasets for semantic segmentation and compared to the state-of-the-art networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental set-up</head><p>Network structure: ESPNet uses ESP modules for learning convolutional kernels as well as down-sampling operations, except for the first layer which is a standard strided convolution. All layers (convolution and ESP modules) are followed by a batch normalization <ref type="bibr" target="#b48">[49]</ref> and a PReLU <ref type="bibr" target="#b49">[50]</ref> non-linearity except for the last point-wise convolution, which has neither batch normalization nor non-linearity. The last layer feeds into a softmax for pixel-wise classification. Different variants of ESPNet are shown in <ref type="figure" target="#fig_1">Fig. 4</ref>. The first variant, ESPNet-A ( <ref type="figure" target="#fig_1">Fig.  4a)</ref>, is a standard network that takes an RGB image as an input and learns representations at different spatial levels 5 using the ESP module to produce a segmentation mask. The second variant, ESPNet-B ( <ref type="figure" target="#fig_1">Fig. 4b)</ref>, improves the flow of information inside ESPNet-A by sharing the feature maps between the previous strided ESP module and the previous ESP module. The third variant, ESPNet-C ( <ref type="figure" target="#fig_1">Fig. 4c</ref>), reinforces the input image inside ESPNet-B to further improve the flow of information. These three variants produce outputs whose spatial dimensions are <ref type="bibr">1 8</ref> th of the input image. The fourth variant, ESPNet <ref type="figure" target="#fig_1">(Fig. 4d</ref>), adds a light weight decoder (built using a principle of reduceupsample-merge) to ESPNet-C that outputs the segmentation mask of the same spatial resolution as the input image.</p><p>To build deeper computationally efficient networks for edge devices without changing the network topology, a hyper-parameter α controls the depth of the network; the ESP module is repeated α l times at spatial level l. CNNs require more memory at higher spatial levels (at l = 0 and l = 1) because of the high spatial dimensions of feature maps at these levels. To be memory efficient, neither the ESP nor the convolutional modules are repeated at these spatial levels. The building block functions used to build the ESPNet (from ESPNet-A to ESPNet) are discussed in Appendix B. Dataset: We evaluated the ESPNet on the Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref>, an urban visual scene understanding dataset that consists of 2,975 training, 500 validation, and 1,525 test high-resolution images. The dataset was captured across 50 cities and in different seasons. The task is to segment an image into 19 classes belonging to 7 categories (e.g. person and rider classes belong to the same category human). We evaluated our networks on the test set using the Cityscapes online server.</p><p>To study the generalizability, we tested the ESPNet on an unseen dataset. We used the Mapillary dataset <ref type="bibr" target="#b50">[51]</ref> for this task because of its diversity. We mapped the annotations (65 classes) in the validation set (# 2,000 images) to seven categories in the Cityscape dataset. To further study the segmentation power of our model, we trained and tested the ESPNet on two other popular datasets from different domains. First, RGB Image l = 0 Conv-3 <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b15">16)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat</head><p>Conv-1 . We denote each module as (# input channels, # output channels). Here, Conv-n represents n × n convolution.</p><formula xml:id="formula_2">(256, C) DeConv (C, C) Conv-1 (131, C) Concat ESP (2C, C) DeConv (C, C) Conv-1 (19, C) Concat Conv-1 (2C, C) DeConv (C, C) Segmentation Mask (d) ESPNet</formula><p>we used the widely known PASCAL VOC dataset <ref type="bibr" target="#b51">[52]</ref> that has 1,464 training images, 1,448 validation images, and 1,456 test images. The task is to segment an image into 20 foreground classes. We evaluate our networks on the test set (comp6 category) using the PASCAL VOC online server. Following the convention, we used additional images from <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>. Secondly, we used a breast biopsy whole slide image dataset <ref type="bibr" target="#b35">[36]</ref>, chosen because tissue structures in biomedical images vary in size and shape and because this dataset allowed us to check the potential of learning representations from a large receptive field. The dataset consists of 30 training images and 28 validation images, whose average size is 10, 000 × 12, 000, much larger than natural scene images. The task is to segment the images into 8 biological tissue labels; details are in <ref type="bibr" target="#b35">[36]</ref>. Performance evaluation metrics: Most traditional CNNs measure network performance in terms of accuracy, latency, number of network parameters, and network size (e.g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b54">55]</ref>). These metrics provide high-level insight about the network, but fail to demonstrate the efficient usage of limited available hardware resources. In addition to these metrics, we introduce several system-level metrics to characterize the performance of a CNN on resource-constrained devices <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>. Segmentation accuracy is measured as a mean Intersection over Union (mIOU) score between the ground truth and the predicted segmentation mask. Latency represents the amount of time a CNN network takes to process an image. This is usually measured in terms of frames per second (FPS). Network parameters represents the number of parameters learned by the network.</p><p>Network size represents the amount of storage space required to store the network parameters. An efficient network should have a smaller network size.</p><p>Sensitivity to GPU frequency measures the computational capability of an application and is defined as a ratio of percentage change in execution time to the percentage change in GPU frequency. A higher value indicates that the application tends to utilize the GPU more efficiently.</p><p>Utilization rates measures the utilization of compute resources (CPU, GPU, and memory) while running on an edge device. In particular, computing units in edge devices (e.g. Jetson TX2) share memory between CPU and GPU.</p><p>Warp execution efficiency is defined as the average percentage of active threads in each executed warp. GPUs schedule threads in the form of warps, and each thread inside the warp is executed in single instruction multiple data fashion. A high value of warp execution efficiency represents efficient usage of GPU.</p><p>Memory efficiency is the ratio of number of bytes requested/stored to the number of bytes transfered from/to device (or shared) memory to satisfy load/store requests. Since memory transactions are in blocks, this metric allows us to determine how efficiently we are using the memory bandwidth.</p><p>Power consumption is the amount of average power consumed by the application during inference.</p><p>Training details: ESPNet networks were trained using PyTorch <ref type="bibr" target="#b57">[58]</ref> with CUDA 9.0 and cuDNN back-ends. ADAM <ref type="bibr" target="#b58">[59]</ref> was used with an initial learning rate of 0.0005, and decayed by two after every 100 epochs and with a weight decay of 0.0005. An inverse class probability weighting scheme was used in the cross-entropy loss function to address the class imbalance <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, the weights were initialized randomly. Standard strategies, such as scaling, cropping and flipping, were used to augment the data. The image resolution in the Cityscape dataset is 2048 × 1024, and all the accuracy results were reported at this resolution. For training the networks, we sub-sampled the RGB images by two. When the output resolution was smaller than 2048 × 1024, the output was up-sampled using bi-linear interpolation. For training on the PASCAL dataset, we used a fixed image size of 512 × 512. For the WSI dataset, the patch-wise training approach was followed <ref type="bibr" target="#b35">[36]</ref>. ESPNet was trained in two stages. First, ESPNet-C was trained with down-sampled annotations. Second, a light-weight decoder was attached to ESPNet-C and then, the entire ESPNet network was trained.  <ref type="bibr" target="#b59">[62]</ref>, were used. In our experiments, we will refer to ESPNet with α 2 = 2 and α 3 = 8 as ESPNet until and otherwise stated explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on the Cityscape dataset</head><p>Comparison with state-of-the-art efficient convolutional modules: In order to understand the ESP module, we replaced the ESP modules in ESPNet-C with state-of-the-art efficient convolutional modules, sketched in <ref type="figure">Fig. 3</ref> (MobileNet <ref type="bibr" target="#b15">[16]</ref>, ShuffleNet <ref type="bibr" target="#b16">[17]</ref>, Inception <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, ResNext <ref type="bibr" target="#b13">[14]</ref>, and ResNet <ref type="bibr" target="#b46">[47]</ref>) and evaluate their performance on the Cityscape validation dataset. We did not compare with ASP <ref type="bibr" target="#b2">[3]</ref>, because it is computationally expensive and not suitable for edge devices. <ref type="figure">Fig. 5</ref> compares the performance of ESPNet-C with different convolutional modules. Our ESP module outperformed Mo-bileNet and ShuffleNet modules by 7% and 12%, respectively, while learning a similar number of parameters and having comparable network size and inference speed. Furthermore, the ESP module delivered comparable accuracy to ResNext and Inception more efficiently. A basic ResNet module (stack of two 3 × 3 convolutions with a skipconnection) delivered the best performance, but had to learn 6.5× more parameters. Comparison with state-of-the-art segmentation methods: We compared the performance of ESPNet with state-of-the-art semantic segmentation networks. These networks either use a pre-trained network (VGG <ref type="bibr" target="#b60">[63]</ref>: FCN-8s <ref type="bibr" target="#b44">[45]</ref> and SegNet <ref type="bibr" target="#b38">[39]</ref>, ResNet <ref type="bibr" target="#b46">[47]</ref>: DeepLab-v2 <ref type="bibr" target="#b2">[3]</ref> and PSPNet <ref type="bibr" target="#b0">[1]</ref>, and SqueezeNet <ref type="bibr" target="#b54">[55]</ref>: SQNet <ref type="bibr" target="#b61">[64]</ref>) or were trained from scratch (ENet <ref type="bibr" target="#b19">[20]</ref> and ERFNet <ref type="bibr" target="#b20">[21]</ref>). <ref type="figure">Fig. 6</ref> compares ESPNet with state-of-the-art methods. ESPNet is 2% more accurate than ENet <ref type="bibr" target="#b19">[20]</ref>, while running 1.27× and 1.16× faster on a desktop and a laptop, respectively. ESPNet makes some mistakes between classes that belong to the same category, and hence has a lower classwise accuracy (see Appendix F for the confusion matrix). For example, a rider can be confused with a person. However, ESPNet delivers a good category-wise accuracy. ES-PNet had 8% lower category-wise mIOU than PSPNet <ref type="bibr" target="#b0">[1]</ref>, while learning 180× fewer parameters. ESPNet had lower power consumption, had lower battery discharge rate, and was significantly faster than state-of-the-art methods, while still achieving a competitive category-wise accuracy; this makes ESPNet suitable for segmentation on edge devices. ERFNet, an another efficient segmentation network, delivered good segmentation accuracy, but has 5.5× more parameters, is 5.44× larger, consumes more power, (a) Accuracy vs. network size (b) Accuracy vs. speed (laptop) <ref type="figure">Fig. 5</ref>: Comparison between state-of-the-art efficient convolutional modules. For a fair comparison between different modules, we used K = 5, d = N K , α 2 = 2, and α 3 = 3. We used standard strided convolution for down-sampling. For ShuffleNet, we used g = 4 and K = 4 so that the resultant ESPNet-C network has the same complexity as with the ESP block. and has a higher battery discharge rate than ESPNet. Also, ERFNet does not utilize limited available hardware resources efficiently on edge devices (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Segmentation results on other datasets</head><p>Unseen dataset: <ref type="table">Table 1a</ref> compares the performance of ESPNet to that of ENet <ref type="bibr" target="#b19">[20]</ref> and ERFNet <ref type="bibr" target="#b20">[21]</ref> on an unseen dataset. These networks were trained on the Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref> and tested on the Mapillary (unseen) dataset <ref type="bibr" target="#b50">[51]</ref>. <ref type="bibr">ENet</ref>   <ref type="table">Table 1</ref>: Results on different datasets. Here, the number of parameters are in million. For more details, please see <ref type="bibr" target="#b63">[66]</ref>. See Appendix F for more qualitative results.</p><p>PASCAL VOC 2012 dataset: <ref type="table">(Table 1c)</ref> On the PASCAL dataset, ESPNet is 4% more accurate than SegNet, one of the smallest network on the PASCAL VOC, while learning 81× fewer parameters. ESPNet is 22% less accurate than PSPNet (one of the most accurate network on the PASCAL VOC) while learning 180× fewer parameters. Breast biopsy dataset: <ref type="table">(Table 1d</ref>) On the breast biopsy dataset, ESPNet achieved the same accuracy as <ref type="bibr" target="#b35">[36]</ref> while learning 9.5× less parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance analysis on an edge device</head><p>We measure the performance on the NVIDIA Jetson TX2, a computing platform for edge devices. Performance analysis results are given in <ref type="figure" target="#fig_2">Fig. 7</ref>. Network size: <ref type="figure" target="#fig_2">Fig. 7a</ref> compares the uncompressed 32-bit network size of ESPNet with ENet and ERFNet. ESPNet had a 1.12× and 5.45× smaller network than ENet and ERFNet, respectively, which reflects well on the architectural design of ESPNet. Inference speed and sensitivity to GPU frequency: <ref type="figure" target="#fig_2">Fig. 7b</ref> compares the inference speed of ESPNet with ENet and ERFNet. ESPNet had almost the same frame rate as ENet, but it was more sensitive to GPU frequency <ref type="figure" target="#fig_2">(Fig. 7c)</ref>. As a consequence, ESPNet achieved a higher frame rate than ENet on high-end graphic cards, such as the GTX-960M and TitanX (see <ref type="figure">Fig. 6</ref>). For example, ESPNet is 1.27× faster than ENet on an NVIDIA TitanX. ESPNet is about 3× faster than ERFNet on an NVIDIA Jetson TX2. Utilization rates: <ref type="figure" target="#fig_2">Fig. 7d</ref> compares the CPU, GPU, and memory utilization rates of different networks. These networks are throughput intensive, and therefore, GPU utilization rates are high, while CPU utilization rates are low for these networks. Memory utilization rates are significantly different for these networks. The memory footprint of ESPNet is low in comparison to ENet and ERFNet, suggesting that ESPNet is suitable for memory-constrained devices. Warp execution efficiency: <ref type="figure" target="#fig_2">Fig. 7e</ref>   divergence and promotes the efficient usage of limited GPU resources available on edge devices. We note that warp execution efficiency gives a better insight into the utilization of GPU resources than the GPU utilization rate. GPU frequency will be busy even if few warps are active, resulting in a high GPU utilization rate. Memory efficiency: <ref type="figure" target="#fig_2">(Fig. 7e</ref>) All networks have similar global load efficiency, but ERFNet has a poor store and shared memory efficiency. This is likely due to the fact that ERFNet spends 20% of the compute power performing memory alignment operations, while ESPNet and ENet spend 4.2% and 6.6% time for this operation, respectively. See Appendix D for the compute-wise break down of different kernels. Power consumption: <ref type="figure" target="#fig_2">Fig. 7f and 7g</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation studies on the Cityscapes: The path from ESPNet-A to ESPNet</head><p>Larger networks or ensembling the output of multiple networks delivers better performance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19]</ref>, but with ESPNet (sketched in <ref type="figure" target="#fig_1">Fig. 4)</ref>, the goal is an efficient network for edge devices. To improve the performance of ESPNet while maintaining efficiency, a systematic study of design choices was performed. <ref type="table" target="#tab_9">Table 2</ref> summarizes the results. ReLU vs PReLU: <ref type="table" target="#tab_9">(Table 2a</ref>) Replacing ReLU <ref type="bibr" target="#b64">[67]</ref> with PReLU <ref type="bibr" target="#b49">[50]</ref> in ESPNet-A improved the accuracy by 2%, while having a minimal impact on the network complexity.</p><p>Residual learning in ESP: <ref type="table" target="#tab_9">(Table 2b)</ref> The accuracy of ESPNet-A dropped by about 2% when skip-connections in ESP <ref type="figure">(Fig. 1b)</ref>  </p><formula xml:id="formula_3">(f) α3</formula><p>ESPNet-C ( <ref type="figure" target="#fig_1">Fig. 4c</ref>) ESPNet <ref type="figure" target="#fig_1">(Fig. 4d</ref>  Here, ERF represents effective receptive field, denotes that strided ESP was used for down-sampling, † indicates that the input reinforcement method was replaced with input-aware fusion method <ref type="bibr" target="#b35">[36]</ref>, and • denotes the values are in million. All networks in (a-c,e-f) are trained for 100 epochs, while networks in (d,g) are trained for 300 epochs. Here, SPC-s denotes that 3 × 3 standard convolutions are used instead of dilated convolutions in the spatial pyramid of dilated convolutions (SPC).</p><p>Down-sampling: <ref type="table" target="#tab_9">(Table 2c</ref>) Replacing the standard strided convolution with the strided ESP in ESPNet-A improved accuracy by 1% with 33% parameter reduction.</p><p>Width divider (K): <ref type="table" target="#tab_9">(Table 2e</ref>) Increasing K enlarges the effective receptive field of the ESP module, while simultaneously decreasing the number of network parameters. Importantly, ESPNet-A's accuracy decreased with increasing K. For example, raising K from 2 to 8 caused ESPNet-A's accuracy to drop by 11%. This drop in accuracy is explained in part by the ESP module's effective receptive field growing beyond the size of its input feature maps. For an image with size 1024 × 512, the spatial dimensions of the input feature maps at spatial level l = 2 and l = 3 are 256 × 128 and 128 × 64, respectively. However, some of the kernels have larger receptive fields (257 × 257 for K = 8). The weights of such kernels do not contribute to learning, thus resulting in lower accuracy. At K = 5, we found a good trade-off between number of parameters and accuracy, and therefore, we used K = 5 in our experiments.</p><p>ESPNet-A → ESPNet-C: <ref type="table" target="#tab_9">(Table 2f</ref>) Replacing the convolution-based network width expansion operation in ESPNet-A with the concatenation operation in ESPNet-B improved the accuracy by about 1% and did not increase the number of network parameters noticeably. With input reinforcement (ESPNet-C), the accuracy of ESPNet-B further improved by about 2%, while not increasing the network parameters drastically. This is likely due to the fact that the input reinforcement method establishes a direct link between the input image and encoding stage, improving the flow of information.</p><p>The closest work to our input reinforcement method is the input-aware fusion method of <ref type="bibr" target="#b35">[36]</ref>, which learns representations on the down-sampled input image and additively combines them with the convolutional unit. When the proposed input reinforcement method was replaced with the input-aware fusion in <ref type="bibr" target="#b35">[36]</ref>, no improvement in accuracy was observed, but the number of network parameters increased by about 10%.</p><p>ESPNet-C vs ESPNet: <ref type="table" target="#tab_9">(Table 2g</ref>) Adding a light-weight decoder to ESPNet-C improved the accuracy by about 6%, while increasing the number of parameters and network size by merely 20,000 and 0.06 MB from ESPNet-C to ESPNet, respectively. Impact of different convolutions in the ESP block: The ESP block uses point-wise convolutions for reducing the high-dimensional feature maps to low-dimensional space and then transforms those feature maps using a spatial pyramid of dilated convolutions (SPCs) (see Sec. 3). To understand the influence of these two components, we performed the following experiments. 1) Point-wise convolutions: We replaced point-wise convolutions with 3 × 3 standard convolutions in the ESP block (see C1 and C2 in <ref type="table" target="#tab_9">Table  2d</ref>), and the resultant network demanded more resources (e.g., 47% more parameters) while improving the mIOU by 1.8%, showing that point-wise convolutions are effective. Moreover, the decrease in number of parameters due to point-wise convolutions in the ESP block enables the construction of deep and efficient networks (see <ref type="table" target="#tab_9">Table 2g</ref>). 2) SPCs: We replaced 3 × 3 dilated convolutions with 3 × 3 standard convolutions in the ESP block. Though the resultant network is as efficient as with dilated convolutions, it is 1.6% less accurate; suggesting SPCs are effective (see C2 and C3 in <ref type="table" target="#tab_9">Table 2d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced a semantic segmentation network, ESPNet, based on an efficient spatial pyramid module. In addition to legacy metrics, we introduced several new system-level metrics that help to analyze the performance of a CNN network. Our empirical analysis suggests that ESPNets are fast and efficient. We also demonstrated that ESPNet learns good generalizable representations of the objects and perform well in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hardware Details</head><p>Three machines were used in our experiments. <ref type="table" target="#tab_11">Table 3</ref> summarizes the details about these machines. A computing platform (e.g. Jetson TX2) on an edge device shares the global memory or RAM between CPU and GPU, while laptop and desktop devices have dedicated CPU and GPU memory.</p><p>NVIDIA Jetson TX2 can run in different modes. In performance mode (Max-P), all CPU cores are enabled in TX2, while in normal mode (Max-Q mode) only 4 out of 6 CPU cores are active. CPU and GPU clock frequencies are different in these modes and therefore, applications will have different power requirements in different modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The path from ESPNet-A to ESPNet</head><p>Different variants of ESPNet are shown in <ref type="figure" target="#fig_3">Fig. 8</ref>. The first variant, ESPNet-A <ref type="figure" target="#fig_3">(Fig. 8a)</ref>, is a standard network that takes an RGB image as an input and learns representations at different spatial levels using the ESP module to produce a segmentation mask. The second variant, ESPNet-B <ref type="figure" target="#fig_3">(Fig. 8b)</ref>, improves the flow of information inside ESPNet-A by sharing the feature maps between the previous strided ESP module and the previous ESP module. The third variant, ESPNet-C ( <ref type="figure" target="#fig_3">Fig. 8c</ref>), reinforces the input image inside ESPNet-B to further improve the flow of information. These three variants produce outputs whose spatial dimensions are <ref type="bibr">1 8</ref> th of the input image. The fourth variant, ESP-Net <ref type="figure" target="#fig_3">(Fig. 8d)</ref>, adds a light weight decoder (built using a principle of reduce-upsamplemerge) to ESPNet-C that outputs the segmentation mask of the same spatial resolution as the input image. The building block functions used to build the ESPNet (from ESPNet-A to ESPNet) are discussed next. Efficient down-sampling: Recent CNN architectures have used strided convolution (e.g. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b65">68]</ref>) instead of pooling operations (e.g. <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b60">63]</ref>) for down-sampling operations, because it allows the non-linear down-sampling operations to be learned while   simultaneously enabling expansion of the network width. Standard strided convolutional operations are expensive; therefore, they are replaced by strided ESP modules for down-sampling. Point-wise convolutions are replaced by n × n strided convolutions in the ESP module for learning non-linear down-sampling operations. The spatial dimensions of the feature maps are changed by down-sampling operations. Following <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b66">69]</ref>, we do not combine the input and output feature maps using the skip-connection during down-sampling operations. The number of parameters learned by strided convolution and strided ESP are n 2 MN and n 2 MN K + n 2 N 2 K 2 · K , respectively. By expressing strided convolution as strided ESP for down-sampling, the number of parameters required is reduced by a factor of KM M+N and the effective receptive field is increased by ∼ [2 K−1 ] 2 times. We will refer to this network as ESPNet-A <ref type="figure" target="#fig_3">(Fig. 8a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network width expansion:</head><p>To maintain the computational complexity at each spatial level, traditional CNNs (e.g. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b60">63]</ref>) double the width of the network after every down-sampling operation, usually using a convolution operation. Following <ref type="bibr" target="#b66">[69]</ref>, we concatenate the feature maps received from the previous strided ESP module and the previous ESP module to increase the width of the network, as shown in <ref type="figure" target="#fig_3">Fig. 8b</ref> with a curved arrow. The concatenation operation establishes a long-range connection between the input and output at the same spatial level and, therefore, improves the flow of information inside the network. We will refer to this network as ESPNet-B <ref type="figure" target="#fig_3">(Fig. 8b</ref>). Input reinforcement: Spatial information is lost due to down-sampling and convolutional operations. To compensate, we reinforce the input image inside the network. We down-sample the input-image and concatenate it with the feature maps from the previous strided ESP module and the previous ESP module. We will refer to ESPNet-B with input reinforcement as ESPNet-C <ref type="figure" target="#fig_3">(Fig. 8c</ref>). Since the input RGB image has only 3 channels, the increase in network complexity due to input reinforcement is minimal. Depth multiplier α: To build deeper computationally efficient networks for edge devices without changing the network topology, we introduce a hyper-parameter α to control the depth of the network. This parameter, α, repeats the ESP module α l times at spatial level l. CNNs require more memory at higher spatial levels i.e. at l = 0 and l = 1 because of the high spatial dimensions of feature maps at these levels. To be memory efficient, we do not repeat ESP or convolutional modules at these spatial levels.</p><p>As we change the values of these parameters, the amount of computational resources required by a network will change. <ref type="figure" target="#fig_4">Fig. 9</ref> shows the impact of α l , l = {2, 3} on the network parameters and its size. As we increase α 2 , the network size increases with little impact on the number of parameters. When we increase α 3 , both the network size and number of parameters increase. Both the number of parameters and network size should increase with depth <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b46">47]</ref>. Therefore, for creating deep and efficient ESPNet networks, we fix the value of α 2 and vary the value of α 3 . RUM for efficient decoding: The spatial resolution of the output produced by ESPNet-C is <ref type="bibr">1 8</ref> th of the input image size. Up-sampling the feature maps directly, say using bilinear interpolation, may give good accuracy on a standard metric, but the output is usually coarse <ref type="bibr" target="#b44">[45]</ref>. We adopt a bottom-up approach (e.g. <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>) to aggregate the multi-level information learned by ESPNet-C using a simple rule: Reduce-Upsample-Merge (RUM). Reduce: The feature map from spatial levels l and l − 1 are projected to a C-dimensional space, where C represents the number of classes in the dataset. Upsample: The reduced feature map from spatial level l is upsampled by a factor of 2 using a 2 × 2 deconvolutional kernel so that it has the same spatial dimensions as that of the feature map at level l − 1. Merge: The up-sampled feature map from level l is then combined with the C-dimensional feature map from level l − 1 using a concatenation operation. This process is repeated until the spatial dimensions of the feature map are the same as the input image. We refer to this network as ESPNet <ref type="figure" target="#fig_3">(Fig. 8d</ref>).</p><p>C Image Size vs. Inference Speed <ref type="figure">Figure 10</ref> summarizes the impact of image size on the inference speed. At smaller image resolutions (224x224 and 640x360), ESPNet is faster than ENet and ERFNet. However, ESPNet delivers a similar inference speed to ENet for-high resolution images. We presume that ESPNet is bottlenecked by the limited and shared resources on the TX2 device. We note that ESPNet processes high resolution images faster than ENet on high-end devices, such as laptop and desktop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Top-10 Kernels in ESPNet, ENet, and ERFNet</head><p>Convolutional operations are implemented using a highly optimized general matrix multiplication (GEMM) operations and memory re-ordering operations such as im2col. For fast and efficient networks, the kernel corresponding to GEMM operations should have high contribution towards compute resource utilization. <ref type="figure">Figure 11</ref> visualizes the top-10 kernels executed by ENet, ERFNet, and ESPNet. We can see that the top-1 kernel in ESPNet is GEMM, and it is responsible for about 38% of the total computational time. Since convolution operations are implemented using the GEMM kernel, this suggest that ESPNet utilizes the limited computational resources available in TX2 efficiently. Similarly, the top-1 kernel in ENet is also GEMM; however, the contribution of this kernel towards computing is not as high as ESPNet. This is why the sensitivity of ENet towards GPU frequency is low and runs 1.27× slower on NVIDIA TitanX than ESPNet while running at almost the same rate on the NVIDIA TX2. On the other hand, the top-1 kernel in ERFNet is the memory alignment kernel. This suggests that ERFNet gets bottlenecked by the memory operations. E Resource Utilization Plots for ENet, ERFNet, and ESPNet <ref type="bibr">Figures 12,</ref><ref type="bibr" target="#b12">13</ref>, and 14 show the utilization of TX2 resources (CPU, GPU, and memory) over time for ENet, ERFNet and ESPNet. The data were collected using Tegrastats in Max-Q mode. These networks are throughput intensive, and therefore, GPU utilization rates are high while CPU utilization rates are low for these networks. Note that the average CPU utilization rate is below 25%; suggesting that these networks are using only one CPU core out of the available four CPU cores and can be bound to a single CPU core for better utilization of CPU resources, if running additional applications on TX2. Memory utilization rates are significantly different for these networks. The memory footprint of ESPNet is low in comparison to ENet and ERFNet, suggesting ESPNet is suitable for memory constrained devices.</p><p>Recall that ESPNet with α 2 = 2 and α 3 = 8 learns the same number of parameters as ENet. However, ESPNet has a low memory footprint than ENet <ref type="figure" target="#fig_1">(Fig. 14)</ref>; suggesting ESPNet is more memory efficient and utilizes the shared memory efficiently.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Results on the Cityscape and the Mapillary Dataset</head><p>A summary of class-wise and category-wise results on the Cityscape <ref type="bibr" target="#b5">[6]</ref> dataset was given in <ref type="table" target="#tab_13">Table 4</ref>, while category-wise results on the Mapillary [51] dataset were given in <ref type="table" target="#tab_14">Table 5</ref>. Though ERFNet outperformed ENet and ESPNet on every class, it performed badly on the Mapillary dataset. In particular, ERFNet struggled classifying simple classes, such as sky, on the Mapillary dataset, while on such classes, ENet and ESPNet performed relatively well. We note that ESPNet learns good generalization representations about the objects and performs well, even in the wild. Qualitative results on the Cityscape and Mapillary dataset were given in <ref type="figure">Figure 16</ref> and <ref type="figure" target="#fig_2">Figure 17</ref>, respectively. ESPNet makes some mistakes between classes that belong to the same category, and hence has lower class-wise accuracy. However, ESPNet delivers a good category-wise accuracy. Here, the class names were represented by the first three characters of a word. For class names with two words, the first character from the first word and the first two characters from the second word were used to represent the class name. Here, Unk denotes the unknown class.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>(a) An example illustrating a gridding artifact with a single active pixel (red) convolved with a 3 ×3 dilated convolutional kernel with dilation rate r = 2. (b) Visualization of feature maps of ESP modules with and without hierarchical feature fusion (HFF). HFF in ESP eliminates the gridding artifact. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>The path from ESPNet-A to ESPNet. Red and green color boxes represent the modules responsible for down-sampling and up-sampling operations, respectively. Spatial-level l is indicated on the left of every module in (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 :</head><label>7</label><figDesc>Performance analysis of ESPNet with ENet and ERFNet on a NVIDIA Jetson TX2: (a) network size, (b) inference speed vs. GPU frequency (in MHz), (c) sensitivity analysis, (d) utilization rates, (e) efficiency rates, and (f, g) power consumption at two different GPU frequencies.In (d), the statistics for the network's initialization phase were not considered, because they were the same across all networks. See Appendix E for time vs. utilization plots. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 :</head><label>8</label><figDesc>The path from ESPNet-A to ESPNet. Red and green color boxes represent the modules responsible for down-sampling and up-sampling operations, respectively. Spatial-level l is indicated on the left of every module in (a). We denote each module as (# input channels, # output channels). Here, Conv-n represents n × n convolution. This figure is the same asFig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 :</head><label>9</label><figDesc>Relationship between depth multipliers α 2 and α 3 for creating efficient networks. Here, circle size ∝ network size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 : 8 )Fig. 11 :</head><label>10811</label><figDesc>The impact of image size on the inference speed on an edge device(a) ENet (b) ERFNet (c) ESPNet (α 2 = 2, α 3 = 3) (d) ESPNet (α 2 = 2, α 3 = 5) (e) ESPNet (α 2 = 2, α 3 = This figure visualizes the top-10 kernels along with their contribution towards compute resource utilization. The top-1 kernel is highlighted in green color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 :</head><label>12</label><figDesc>This figure compares the CPU utilization rates on NVIDIA Jetson TX2. For ESPNet, we used α 2 = 2. Here, 1.0 represents 100% CPU utilization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 :</head><label>13</label><figDesc>This figure compares the GPU utilization rates on NVIDIA Jetson TX2. For ESPNet, we used α 2 = 2. Here, 1.0 represents 100% GPU utilization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 14 :</head><label>14</label><figDesc>This figure compares the memory utilization on NVIDIA Jetson TX2. For ESPNet, we used α 2 = 2. Maximum available memory on TX2 is 8 GB and is shared between CPU and GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 15 :</head><label>15</label><figDesc>ESPNet's (with α 2 = 2 and α 3 = 8) confusion matrix on the Cityscape validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 16 :Fig. 17 :</head><label>1617</label><figDesc>Qualitative results on the Cityscape validation dataset. Qualitative results on the Mapillary validation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Based on arXiv:1803.06815v3 [cs.CV] 25 Jul 2018 × n3, d d, n2 × n2, d d, n1 × n1, d d, nK × nK, d</figDesc><table><row><cell>ESP Strategy</cell><cell></cell><cell></cell></row><row><cell>Reduce</cell><cell></cell><cell>M, 1 × 1, d</cell></row><row><cell>Split</cell><cell></cell><cell></cell></row><row><cell cols="2">Transform d, n3 HFF Sum Sum</cell><cell>· · ·</cell><cell>Sum</cell></row><row><cell>Merge</cell><cell></cell><cell>Concat</cell></row><row><cell></cell><cell></cell><cell>Sum</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Three different GPU devices were used for our experiments: (1) a desktop with a NVIDIA TitanX GPU (3,584 CUDA cores), (2) a laptop with a NVIDIA GTX-960M GPU (640 CUDA cores), and (3) an edge device with NVIDIA Jetson TX2 (256 CUDA cores). See Appendix A for more details about the hardware. Unless and otherwise stated explicitly, statistics, such as power consumption and inference speed, are reported for an RGB image of size 1024 × 512 averaged over 200 trials. For collecting the hardware-level statistics, NVIDIA's and Intel's hardware profiling and tracing tools, such as NVPROF [60], Tegrastats [61], and PowerTop</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>compares the warp execution efficiency of ESPNet with ENet and ERFNet. The warp execution of ESPNet was about 9% higher than ENet and about 14% higher than ERFNet. This indicates that ESPNet has less warp ERFNet 19.7 99.00 61.3 ESPNet 20.3 99.00 44.0</figDesc><table><row><cell cols="2">Network Size</cell><cell>Network</cell><cell cols="2">Sensitivity to GPU freq. 828 to 1134 1134 to 1300</cell><cell>Network</cell><cell>Utilization (%) CPU GPU Memory</cell></row><row><cell>ENet</cell><cell>1.64 MB</cell><cell>ENet</cell><cell>71%</cell><cell>70%</cell><cell>ENet</cell><cell>20.5 99.00 50.6</cell></row><row><cell cols="2">ERFNet 7.95 MB</cell><cell>ERFNet</cell><cell>69%</cell><cell>53%</cell><cell></cell><cell></cell></row><row><cell cols="2">ESPNet 1.46 MB</cell><cell>ESPNet</cell><cell>86%</cell><cell>95%</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell></cell><cell></cell><cell>(d)</cell></row><row><cell></cell><cell>(e)</cell><cell cols="2">(f) GPU freq. @ 828 MHz</cell><cell cols="3">(g) GPU freq. @ 1,134 MHz</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2 :</head><label>2</label><figDesc>The path from ESPNet-A to ESPNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 3 :</head><label>3</label><figDesc>This table summarizes the hardware that we used in our experiments.</figDesc><table><row><cell></cell><cell>RGB Image</cell><cell cols="2">RGB Image</cell><cell>RGB Image</cell><cell>RGB Image</cell></row><row><cell>l = 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(3, 16)</cell><cell>(3, 16)</cell><cell></cell></row><row><cell></cell><cell>(3, 16)</cell><cell cols="2">Conv-3</cell><cell>Conv-3</cell><cell></cell></row><row><cell>l = 1</cell><cell>Conv-3 (16, 64)</cell><cell>ESP</cell><cell>(16, 64)</cell><cell>(19, 64) Concat</cell><cell>Conv-3 (3, 16)</cell></row><row><cell>l = 2 l = 2 l = 3</cell><cell>ESP ESP ×α 2 ESP (64, 128) (64, 64)</cell><cell cols="2">(64, 64) Concat ESP ×α2 (128, 128) ESP (128, 128)</cell><cell>ESP ESP ×α2 ESP Concat (64, 64) (131, 128)</cell><cell>Concat (19, 64) ESP ×α2 ESP (64, 64)</cell></row><row><cell></cell><cell>(128, 128)</cell><cell>ESP</cell><cell></cell><cell>(128, 128)</cell><cell></cell></row><row><cell>l = 3 l = 3</cell><cell>ESP ×α 3 Conv-1 (128, C)</cell><cell cols="2">×α3 Concat Conv-1 (256, C)</cell><cell>ESP ×α3 Concat Conv-1 (256, C)</cell><cell>Concat (131, 128) ESP ×α3 ESP (128, 128)</cell></row><row><cell cols="2">Segmentation Mask</cell><cell cols="2">Segmentation Mask</cell><cell>Segmentation Mask</cell><cell></cell></row><row><cell cols="2">(a) ESPNet-A</cell><cell cols="2">(b) ESPNet-B</cell><cell>(c) ESPNet-C</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Network mIOU Roa Sid Bui Wal Fen Pol TLi TSi Veg Ter Sky Per Rid Car Tru Bus Tra Mot Bic ENet [20] 58.29 96.33 74.24 85.05 32.16 33.23 43.45 34.10 44.02 88.61 61.39 90.64 65.51 38.43 90.60 36.90 50.51 48.08 38.80 55.41 ERFNet [21] 68.02 97.74 80.99 89.83 42.46 47.99 56.25 59.84 65.28 91.38 68.20 94.19 76.75 57.08 92.76 50.77 60.09 51.80 47.27 61.65 ESPNet (Ours) 60.34 95.68 73.29 86.60 32.79 36.43 47.06 46.92 55.41 89.83 65.96 92.47 68.48 45.84 89.90 40.00 47.73 40.70 36.40 54.89 (a) Class-wise comparison on the test set Network mIOU Flat Nature Object Sky Construction Human Vehicle ENet [20] 80.40 97.34 88.28 46.75 90.64 85.40 65.50 88.87 ERFNet [21] 86.46 98.18 91.12 62.42 94.19 90.06 77.43 91.87 ESPNet (Ours) 82.18 95.49 89.46 52.94 92.47 86.67 69.76 88.45 (b) Category-wise comparison on the test set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 4 :</head><label>4</label><figDesc>Comparison on the Cityscape dataset. For comparison with other networks, please see the Cityscape leader-board: https://www.cityscapes-dataset.com/benchmarks/.</figDesc><table><row><cell>Network</cell><cell cols="4">mIOU Flat Nature Object Sky Construction Human Vehicle</cell></row><row><cell>ENet [20]</cell><cell>0.33 0.61 0.57 0.16 0.37</cell><cell>0.35</cell><cell>0.08</cell><cell>0.20</cell></row><row><cell>ERFNet [21]</cell><cell>0.25 0.73 0.29 0.16 0.03</cell><cell>0.23</cell><cell>0.06</cell><cell>0.24</cell></row><row><cell cols="2">ESPNet (Ours) 0.40 0.66 0.69 0.20 0.52</cell><cell>0.32</cell><cell>0.16</cell><cell>0.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 5 :</head><label>5</label><figDesc>Category-wise comparison on the Mapillary validation set. ESPNet learned generalizable representations of objects and outperformed both ENet and ERFNet in the wild.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We used a desktop with NVIDIA TitanX GPU, a laptop with GTX-960M GPU, and NVIDIA Jetson TX2 as an edge device. See Appendix A for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">At each spatial level l, the spatial dimensions of the feature maps are the same. To learn representations at different spatial levels, a down-sampling operation is performed (seeFig. 4a).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This research was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Interior/Interior Business Center (DOI/IBC) contract number D17PC00343, the Washington State Department of Transportation research grant T1461-47, NSF III (1703166), the National Cancer Institute awards (R01 CA172343, R01 CA140560, and RO1 CA200690), Allen Distinguished Investigator Award, Samsung GRO award, and gifts from Google, Amazon, and Bloomberg. We would also like to acknowledge NVIDIA Corporation for donating the Jetson TX2 board and the Titan X Pascal GPU used for this research. We also thank the anonymous reviewers for their helpful comments. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Segmentation-based urban traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cordts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making bertha see</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Knoeppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Herrtwich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DA-RNN: Semantic mapping with data associated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Joint semantic segmentation and 3d reconstruction from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Going deeper with convolutions. In: CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<title level="m">Dilated residual networks. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5474</idno>
		<title level="m">Flattened convolutional neural networks for feedforward acceleration</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08545</idno>
		<title level="m">Icnet for real-time semantic segmentation on high-resolution images</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Speeding up convolutional neural networks with low rank expansions. BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Xnor-net: Imagenet classification using binary convolutional neural networks. In: ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fixed-point feedforward deep neural network design using weights 1, 0, and -1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Signal Processing Systems (SiPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<title level="m">Binarized neural networks: Training neural networks with weights and activations constrained to+ 1 or-1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07061</idno>
		<title level="m">Quantized neural networks: Training neural networks with low precision weights and activations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pensky</surname></persName>
		</author>
		<title level="m">Sparse convolutional neural networks. In: CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<biblScope unit="page" from="2074" to="2082" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Lcnn: Lookup-based convolutional neural network. In: CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tchamitchian</surname></persName>
		</author>
		<editor>Wavelets.</editor>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning to segment breast biopsy whole slide images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mercan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-dimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Conference on Artificial Neural Networks -ICANN</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MICCAI.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Region-based semantic segmentation with end-to-end training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks. In: NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rota Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt; 0.5 MB model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep-dive analysis of the data analytics workload in cloudsuite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben-Asher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workload Characterization (IISWC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Performance characterization of high-level programming models for gpu graph analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workload Characterization (IISWC)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<idno>2018-02-08</idno>
		<ptr target="http://pytorch.org/Accessed" />
		<title level="m">PyTorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/embedded/develop/toolsAccessed" />
	</analytic>
	<monogr>
		<title level="m">ICLR (2015) 60. NVPROF: CUDA Toolkit Documentation</title>
		<imprint>
			<date type="published" when="2018-02" />
			<biblScope unit="page" from="2018" to="2020" />
		</imprint>
	</monogr>
	<note>TegraTools: NVIDIA Embedded Computing</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<ptr target="https://01.org/powertop/Accessed" />
		<title level="m">PowerTop: For PowerTOP saving power on IA isn&apos;t everything. It is the only thing!</title>
		<imprint>
			<biblScope unit="page" from="2018" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Treml</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MLITS, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Y-Net: Joint Segmentation and Classification for Diagnosis of Breast Biopsy Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mercan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
