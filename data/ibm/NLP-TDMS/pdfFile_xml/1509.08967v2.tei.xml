<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VERY DEEP MULTILINGUAL CONVOLUTIONAL NEURAL NETWORKS FOR LVCSR</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
							<email>1cpuhrsch@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VERY DEEP MULTILINGUAL CONVOLUTIONAL NEURAL NETWORKS FOR LVCSR</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional Networks</term>
					<term>Multilingual</term>
					<term>Acous- tic Modeling</term>
					<term>Speech Recognition</term>
					<term>Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3×3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.77% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub5'00 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6% relative) over the best published CNN result so far.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b0">[1]</ref> have recently pushed the state of the art on large-scale tasks in many domains dealing with natural data, most notably in computer vision tasks like image classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, object localization <ref type="bibr" target="#b5">[6]</ref> and segmentation <ref type="bibr" target="#b6">[7]</ref>.</p><p>Early applications of neural nets to speech recognition used Time-Delay Neural Nets <ref type="bibr" target="#b7">[8]</ref> which can be seen as simple forms of CNNs without pooling or subsampling. Full-fledged CNNs with pooling and subsampling were soon applied to speech recognition and combined with dynamic time warping <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. While the globally-trained combination of neural nets and HMMs for speech and handwriting goes back to the 1990s <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b0">1]</ref>, only due to recent developments <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> HMM/DNN hybrid modeling became dominant in ASR. In the context of these hybrid models, the use of CNNs is relatively recent <ref type="bibr" target="#b14">[15]</ref>. CNNs were shown to achieve state of the art performance on the benchmark datasets Broadcast News and Switchboard 300 <ref type="bibr" target="#b15">[16]</ref>. However, in contrast to the trend in other domains where deeper architectures are often shown to gain performance, the classical CNN architecture in LVCSR <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> has only two convolutional layers.</p><p>Our network architecture (Section 2.1) is strongly inspired by the work of Simonyan et al. <ref type="bibr" target="#b2">[3]</ref> (subsequently referred to as "VGG Net") which obtained second place in the classification section of the Imagenet 2014 competition. The central idea of VGG Net is to replace large convolutional kernels by a stack of 3×3 kernels with ReLU nonlinearities without pooling between these layers; The authors argue the advantage of this is twofold: (1) additional nonlinearity hence more expressive power, and (2) a reduced number of parameters. Using these principles, very deep networks are trained with up to 19 weight layers (of which 16 are convolutional and 3 fully connected). By contrast, the classical CNNs deployed in LVCSR have typically only two convolutional layers, use large (9×9) kernels in the first layer, and use sigmoid activation functions. The first goal of this work is to adapt the VGG Net architecture to LVCSR. Most closely related to this is <ref type="bibr" target="#b18">[19]</ref>, which also uses VGG Net-inspired CNNs for LVCSR <ref type="bibr" target="#b0">1</ref> . In contrast to our work, the architectures investigated in <ref type="bibr" target="#b18">[19]</ref> are quite different and the paper only provides results from training on a non-standard Switchboard-51h dataset, with WER not close to state of the art performance on Hub5'00.</p><p>In the context of low-resource language tasks, it can be crucial to leverage training data in languages other than the target language. Therefore we trained multilingual deep CNNs, which we describe in Section 2.2. This is related to multilingual neural networks in hybrid NN-HMM systems <ref type="bibr" target="#b19">[20]</ref> which have been extended to multilingual bottleneck architectures for tandem models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> and have proven valuable for spoken term detection <ref type="bibr" target="#b22">[23]</ref>. To our knowledge, no work has been published that extends the multilingual setup to CNNs.</p><p>The multi-scale features described in Section 2.1 aim at exploiting more context at very low computational cost. They are inspired by the recent success of combining information at multiple scales in tasks like traffic sign recognition <ref type="bibr" target="#b23">[24]</ref>, semantic segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> and depth map prediction <ref type="bibr" target="#b25">[26]</ref>. In LVCSR the multi-scale idea has been explored in tandem systems <ref type="bibr" target="#b26">[27]</ref> and the CLDNN architecture <ref type="bibr" target="#b27">[28]</ref>.</p><p>As training becomes more challenging with increasing depth, we used two recently proposed optimization algorithms, Adadelta <ref type="bibr" target="#b28">[29]</ref> and Adam <ref type="bibr" target="#b29">[30]</ref> (Section 2.4). Both algorithms are first order gradient-based optimization methods, which keep track of an estimate of the first and second order moment of the gradient to tune the step size of each weight separately.</p><p>The rest of the paper is organized as follows. In Section 2 we introduce the novel aspects of our work: <ref type="figure">very</ref>   <ref type="table">Table 1</ref>. The configurations of our very deep CNNs for LVCSR. In all but the classic convnet, convolutional layers have 3×3 kernels, thus kernel size is omitted. The depth of the networks increases from left to right. The deepest configuration, WDX, has 10 convolutional and 4 fully connected layers. The leftmost column indicates the number of output feature maps in each layer. The optional X means there are four fully connected layers instead of three (output layer included).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ARCHITECTURAL AND TRAINING NOVELTIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Very Deep Convolutional Networks</head><p>The very deep convolutional networks we describe here are adaptations of the VGG Net architecture <ref type="bibr" target="#b2">[3]</ref> to the LVCSR domain, where until now networks with two convolutional layers dominated <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. <ref type="table">Table 1</ref> shows the configurations of the deep CNNs. The deepest configuration, WDX, has 14 weight layers: 10 convolutional and 4 fully connected. As in <ref type="bibr" target="#b2">[3]</ref>, we omit the Rectified Linear Unit (ReLU) layers following every convolutional and fully connected layer. The convolutional layers are written as conv({input feature maps}-{output feature maps}) where each kernel is understood to be size 3×3. The pooling layers are written as (time x frequency) with stride equal to the pool size. For architectures VDX and WDX, we apply zero padding of size 1 at every side before every convolution, while for architecture VC(X) and VB(X) we use the convolutions to reduce the size of the feature maps, hence only in the higher layers of VC(X) padding is applied.</p><p>In contrast to <ref type="bibr" target="#b2">[3]</ref>, we do not reinitialize the deeper models with the shallower models. Each model is trained from scratch with random initialization from a uniform distribution in the range [−a, a] where a = (kW × kH × numInputFeatureMaps) − 1 2 . This follows the argument of <ref type="bibr" target="#b30">[31]</ref> to initialize the weights such that the variance of the activations on each layer does not explode or vanish during the forward pass. <ref type="figure">Figure 1</ref> shows a multilingual VBX network, which we used for most of our Babel experiments. It is similar to previous multilingual deep neural networks <ref type="bibr" target="#b19">[20]</ref>, with the main difference that the shared lower layers of the network are convolutional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multilingual Convolutional Networks</head><p>A second difference is that we untie more than only the last layer, meaning that the weights and biases of multiple fully connected layers are different for each language. Since the output dimension of the convolutional stages is typically large when using large context windows, most of the weights are in the first fully connected layer, which acts on the flattened output of the convolutional stages. This is an argument to share this large, first fully connected layer across languages. We experimentally confirmed that for all architectures, untying all fully connected layers except the lowest one gives optimal performance, with strong degradation if the first fully connected layer is also untied. This untying corresponds to a view of the shared layers and the first fully connected layer as a shared multilingual feature extractor, while the fully connected layers higher up form the classifier.</p><p>The multilingual CNN is trained in a round-robin fashion: we process a mini-batch for each language before making an update to the weights. In the shared part of the network the gradients of all mini-batches are accumulated between weight updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-scale feature maps</head><p>The main goal of constructing multi-scale feature maps is to add more context without increasing the computational cost. <ref type="figure">Figure 2</ref> illustrates the concept of multi-scale feature maps, where additional input feature maps contain a larger view of the context of the frame by downsampling larger context windows with different strides. Kernels on the first convolutional layer are able to combine information from multiple scales, i.e. different distances from the central frame. Because the only difference for the convnet configuration is the first convolutional layer having more feature maps, the additional computational cost and number of parameters is small.</p><p>We found this style of multi-scale training to give small gains. Increasing the context size had a stronger positive impact, though at the expense of increased computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Training</head><p>We use Adadelta <ref type="bibr" target="#b28">[29]</ref> and Adam <ref type="bibr" target="#b29">[30]</ref> to do initial training of the deep CNNs. Using Adadelta has two main advantages. Firstly, in our experience the optimization problem converges much faster than with SGD; for the Babel experiments we typically see convergence after about 40 million frames using the 18 hours of Babel training data (after silence removal about 5.8 million frames). Secondly, the optimal working point of Adadelta's hyperparameters and ρ was stable across architectures, always giving optimal performance. This was crucial in order to explore architectural variations. After initial training with Adadelta, we fine tune using SGD with a small learning rate.</p><p>Another aspect of training that improved our results is data balancing (something similar was done in <ref type="bibr" target="#b5">[6]</ref>). We construct batches on the fly by sampling target y = CDi with probability pi, where pi is related to the frequency fi of context dependent state CDi as pi = In our experiments on Babel it proved optimal to start with γ = 0 and raise it during training to its final value of γ = 1. In our experiments on switchboard we varied γ typically from 0.4 to 0.8 and decoded with HMM priors adjusted to match the final pi distribution.  <ref type="table">Table 3</ref>. WER on Babel for monolingual (1L, 3 hours of training data) versus multilingual (6L, 18 hours of training data). When trained on a single language, the classical CNN architecture does slightly worse than the baseline DNN. However, the VC architecture gives an average 2.5 WER improvement even when trained on one language. As expected, for both models training multilingual gives a strong performance boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Babel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN</head><p>Our first set of experiments on Babel focuses on the multilingual and multi-scale aspects of this work. The IARPA Babel program is aimed at developing robust keyword search technology for low resource languages. Though the word error rates reported here are too high to be useful for simple speech to text applications, useful keyword search (KWS) systems can still be built based on these ASR models.</p><p>As training data we use a combination of 6 languages, with 3 hours of training data per language. The languages used for training are languages from the second Option Period of the Babel program, i.e. Kurmanji (KUR), Tok Pisin (TOK), Cebuano (CEB), Kazakh (KAZ), Telugu (TEL), and Lithuanian (LIT). The features used in these experiments are standard log-Mel features, standardized with a global mean and variance shared across the speakers and langauges. Unless explicitly mentioned, we use multi-scale features with context ±20 in the Babel experiments. We report results after crossentropy training with adadelta (ρ = 0.985, = 1e−10), and γ varying from 0 to 1.</p><p>We trained the multilingual deep CNN architecture on 6 Babel languages using alignments from 6 baseline speaker independent HMM/DNN systems using PLP features, with 1000 context dependent states. The context dependent states are specific to each language. Each baseline system is cross-entropy trained on a single language with 3 hours of data. We will report the WER of the CNNs  <ref type="table">Table 5</ref>. Results on Hub5'00 SWB after training on the 262-hour SWB-1 dataset. We obtain 14.5% relative improvement over our baseline adaptation of the classical CNN and 10.6% relative improvement over <ref type="bibr" target="#b16">[17]</ref>. (A+S) means Adadelta + SGD finetuning. (S) means the model was trained from random initialization using SGD. The last column gives the number of frames til convergence.</p><p>We evaluate our deep CNN architecture by training on the 262hour SWB-1 training data, and report the Word Error Rates on Hub5'00 SWB <ref type="table">(table 5)</ref>. The Switchboard experiments focus on the very deep aspect of our work. Apart from not involving multilingual training, we did not use multi-scale features in the Switchboard experiments, but did use speaker-dependent VTLN and deltas and double deltas as this is shown to help performance for classical CNNs <ref type="bibr" target="#b15">[16]</ref>.</p><p>In the switchboard experiments, using a large context only gave marginal gains which were not worth the computational cost, so we worked with context windows of ±8. We use a data balancing value of γ = 0.8, chosen from [0.4, 0.6, 0.8, 1.0].</p><p>After training with multiple combinations of Adam, Adadelta and SGD, we settled on two possible strategies for optimization: the first strategy is to use Adadelta or Adam for initial training, followed by SGD finetuning. This way one can typically achieve good performance in minimal time. The second strategy, training from scratch using only SGD, requires more training, however the performance is slightly superior. Classical momentum yielded no gains and sometimes slight degradation over plain SGD. We provide the results and total number of frames until convergence. Note that with the first strategy, we achieve 12.2% WER after 140M frames, i.e. only 1.5 passes through the dataset (which has 92.1M frames). Using just SGD we achieve 11.8% WER in 3.5 passes through the data.</p><p>We only present results after cross-entropy training, so we compare against the best published cross-entropy trained CNNs. The baseline is the work of Soltau et al. <ref type="bibr" target="#b16">[17]</ref> using classical CNNs with 512 feature maps on both convolutional layers. A second baseline is the work of Saon et al. <ref type="bibr" target="#b17">[18]</ref> which introduces annealed dropout maxout CNN's with a large number of HMM states, achieving 12.6% WER after cross-entropy training (not in the paper, from personal communication). Note that these improvements could readily be integrated with our very deep CNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DISCUSSION</head><p>In this paper we proposed a number of architectural advances in CNNs for LVCSR. We introduced a very deep convolutional network architecture with small 3×3 kernels and multiple convolutional layers before each pooling layer, inspired by the VGG Imagenet 2014 architecture. Our best performing model has 14 weight layers. We also introduced multilingual CNNs which proved valuable in the context of low resource speech recognition. We introduced multi-scale input features aimed at exploiting more acoustic context with minimal computational increase. We showed an improvement of 2.50% WER over a standard DNN PLP baseline using 3 hours of data, and an improvement of 5.77% WER by combining six languages to train on 18 hours of data. We then showed results on Hub5'00 after training on 262 hours of SWB-1 data where we get 11.8% WER, which is an improvement of 2.0% WER (14.5% relative) over our own baseline, and a 1.4% WER (10.6% relative) improvement over the best result published on classical CNNs after cross-entropy training <ref type="bibr" target="#b16">[17]</ref>.</p><p>We expect additional gains from sequence training, joint training with DNNs <ref type="bibr" target="#b16">[17]</ref>, and integrating improvements like annealed dropout and maxout nonlinearities <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGEMENT</head><p>This effort uses the very limited language packs from IARPA Babel Program language collections IARPA-babel205b-v1.0a, IARPA-babel207b-v1.0e, IARPA-babel301b-v2.0b, IARPA-babel302b-v1.0a, IARPA-babel303b-v1.0a, and IARPA-babel304b-v1.0b. This work is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Defense U.S. Army Research Laboratory (DoD/ARL) contract number W911NF-12-C-0012. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Government.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 . 4 Fig. 2 .</head><label>142</label><figDesc>Multilingual VBX network with the last three layers untied. FC stands for Fully Connected layers.Context +/-5Context +/-10, stride 2Context +/-20, stride Multi-scale feature maps with context ±5 and strides {1,2,4} (3S/5). The final size of each feature map along the time dimension is 11. The three 11×40 input feature maps are stacked as input to the CNN, similar to how RGB channels form 3 input feature maps in an image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>.</head><label></label><figDesc>After sampling y, we sample uniformly across all frames with that target. The exponent γ takes values between balanced training (γ = 0) and unbalanced training using the natural frequencies (γ = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>WER for VC multi-scale training with different context windows. 3S/20 stands for three scales with a context of ±20. From table 3 note that even in the monolingual case (3 hours of data) the VBX CNN architecture outperforms both the classical CNN and the baseline DNN.</figDesc><table><row><cell></cell><cell cols="4">DNN 3S/20 1S/20 3S/8 1S/8</cell></row><row><cell>KUR</cell><cell>82.7</cell><cell>78.1</cell><cell>78.4</cell><cell>78.4 79.2</cell></row><row><cell>TOK</cell><cell>62.6</cell><cell>54.2</cell><cell>54.7</cell><cell>55.8 56.7</cell></row><row><cell>CEB</cell><cell>76.3</cell><cell>70.3</cell><cell>70.4</cell><cell>71.6 71.8</cell></row><row><cell>KAZ</cell><cell>77.3</cell><cell>71.1</cell><cell>71.8</cell><cell>72.5 72.8</cell></row><row><cell>TEL</cell><cell>87.0</cell><cell>82.5</cell><cell>83.1</cell><cell>83.5 83.6</cell></row><row><cell>LIT</cell><cell>71.0</cell><cell>66.2</cell><cell>67.3</cell><cell>66.9 67.5</cell></row><row><cell cols="2">IMPR 0.00</cell><cell>5.75</cell><cell>5.20</cell><cell>4.70 4.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>For</cell></row><row><cell cols="5">3S we use strides of 1, 2, and 4, while 1S just has stride 1, i.e. regular</cell></row><row><cell cols="5">input features. Multi-scale features provide a modest gain. Using</cell></row><row><cell cols="5">larger context size gives a better improvement, however this comes</cell></row><row><cell cols="5">at the cost of extra computation proportional to the context size in</cell></row><row><cell cols="2">the convolutional layers.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">compared to the baseline DNN, and summarize this in the average</cell></row><row><cell cols="5">absolute WER improvement over the baseline DNN, which gives</cell></row><row><cell cols="5">one number to compare different models. The WER improvements</cell></row><row><cell cols="5">over the baseline DNN are fairly consistent across languages.</cell></row><row><cell cols="5">Tables 2 through 4 show the results outlining the performance</cell></row><row><cell cols="5">gains from the different architectural improvements discussed in</cell></row><row><cell cols="4">Section 2.1, 2.2, and 2.1 respectively. 3.2. Switchboard 300</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">WER # params (M) #M frames</cell></row><row><cell>Classic 512 [17]</cell><cell></cell><cell>13.2</cell><cell>41.2</cell><cell>1200</cell></row><row><cell cols="3">Classic 256 ReLU (A+S) 13.8</cell><cell>58.7</cell><cell>290</cell></row><row><cell cols="2">VCX (6 conv) (A+S)</cell><cell>13.1</cell><cell>36.9</cell><cell>290</cell></row><row><cell cols="2">VDX (8 conv) (A+S)</cell><cell>12.3</cell><cell>38.4</cell><cell>170</cell></row><row><cell cols="2">WDX (10 conv) (A+S)</cell><cell>12.2</cell><cell>41.3</cell><cell>140</cell></row><row><cell cols="2">VDX (8 conv) (S)</cell><cell>11.9</cell><cell>38.4</cell><cell>340</cell></row><row><cell cols="2">WDX (10 conv) (S)</cell><cell>11.8</cell><cell>41.3</cell><cell>320</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This work was pursued independently of ours, and was published about two weeks before submission of this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We gratefully acknowledge the support of NVIDIA Corporation. The authors would like to thank Pierre Sermanet for the initial code base, George Saon, Vaibhava Goel, Etienne Marcheret and Xiaodong Cui for valuable discussions and comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3626" to="3633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Experiments with time delay networks and dynamic time warping for speaker independent isolated digits recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Soulié</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blanchet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Liénard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speaker-independent isolated digit recognition: multilayer perceptrons vs. dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Soulié</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blanchet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Liénard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Global optimization of a neural network-hidden Markov model hybrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Flammia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kompe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="252" to="259" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conversational speech transcription using context-dependent deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="437" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep belief networks using discriminative features for phone recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael A P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
		<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5060" to="5063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3761" to="3764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4277" to="4280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for lvcsr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint training of convolutional and non-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The IBM 2015 english conversational telephone speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Very deep convolutional neural networks for LVCSR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the use of a multilingual neural network front-end</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scanzio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fissore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gemello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multilingual MLP features for low-resource LVCSR systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Investigation on cross-and multilingual MLP features under matched and mismatched acoustical conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tüske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Investigation of multilingual deep neural networks for spoken term detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with multi-scale convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2011 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2809" to="2813" />
		</imprint>
	</monogr>
	<note>Neural Networks (IJCNN)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Investigation into bottleneck features for meeting speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2947" to="2950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">ADADELTA: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
