<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Dynamic Graph LSTM for Action-driven Video Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xiaodan1@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carneige Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
							<email>xiaolonw@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carneige Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
							<email>dyyeung@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
							<email>abhinavg@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carneige Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Dynamic Graph LSTM for Action-driven Video Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we investigate a weakly-supervised object detection framework. Most existing frameworks focus on using static images to learn object detectors. However, these detectors often fail to generalize to videos because of the existing domain shift. Therefore, we investigate learning these detectors directly from boring videos of daily activities. Instead of using bounding boxes, we explore the use of action descriptions as supervision since they are relatively easy to gather. A common issue, however, is that objects of interest that are not involved in human actions are often absent in global action descriptions known as "missing label". To tackle this problem, we propose a novel temporal dynamic graph Long Short-Term Memory network (TD-Graph LSTM). TD-Graph LSTM enables global temporal reasoning by constructing a dynamic graph that is based on temporal correlations of object proposals and spans the entire video. The missing label issue for each individual frame can thus be significantly alleviated by transferring knowledge across correlated objects proposals in the whole video. Extensive evaluations on a large-scale daily-life action dataset (i.e., Charades) demonstrates the superiority of our proposed method. We also release object bounding-box annotations for more than 5,000 frames in Charades. We believe this annotated data can also benefit other research on video-based object recognition in the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the recent success of data-driven approaches in recognition, there has been a growing interest in scaling up object detection systems <ref type="bibr" target="#b37">[38]</ref>. However, unlike classification, exhaustively annotating object instances with diverse classes and bounding boxes is hardly scalable. Therefore, there has been a surge in exploring in unsupervised and weakly-supervised approaches for object detection. However, fully unsupervised approaches <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b16">17]</ref> without any annotations currently give considerably inferior performance on similar tasks, while conventional weaklysupervised methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">42]</ref> use static images to learn the detectors. These object detectors, however, fail to generalize to videos due to shift in domain. One alternative is to use these weakly-supervised approaches but using video frames themselves. However, current approaches rely heavily on the accuracy of image-level labels and are vulnerable to missing labels (as shown in <ref type="figure">Figure 1</ref>). Can we design a learning framework that is robust to these missing labels ?</p><p>In this paper, we explore a novel slightly-supervised video object detection pipeline that uses human action labels as supervision for object detection. As illustrated in <ref type="figure">Figure 1</ref>, the coarse human action labels spanning multiple frames (e.g., watching a laptop or sitting in a chair) help indicate the presence of participating object instances (e.g., laptop and chair). Compared to prior works, our investigated setting has two major merits: 1) the textual action descriptions for videos are much cheaper to collect, e.g., through text tags, search queries and action recognition datasets <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref>; and 2) the intrinsic temporal coherence in video domain provides more cues to facilitate the recognition of each object instance and help overcome the missing label problem.</p><p>Action-driven supervision for object detection is much more challenging since it can only access object labels for some specific frames, while a considerable number of uninvolved object labels are unknown. As shown in the right column of <ref type="figure">Figure 1</ref>, four action categories are labeled for different periods in the given video. In each period, the action label (e.g., tidying a shelf ) only points out the shelf category and misses the rest of the categories such as laptop, table, chair and refrigerator. On the other hand, the missed categories (e.g., laptop) may appear in other labeled actions in the same video. Inspired by this observation, we propose to alleviate the missing label issue by exploiting the rich temporal correlations of object instances in the video. The core idea is that action labels in a different period may help to infer the presence of some objects in this current period. Specifically, a novel temporal dynamic graph LSTM (TD-Graph LSTM) framework is introduced to model the complex and dynamic temporal graph structure for object proposals in the whole video and thus enable the joint reasoning for all frames. The knowledge of all action labels in  Weakly-supervised object detection Action-driven video object detection <ref type="figure">Figure 1</ref>. (Left) shows the traditional weakly-supervised object detection setting. Each training image has an accurate image-level annotation about object categories. (Right) shows our action-driven weakly-supervised video detection setting. Video-level action labels are provided for each video, indicating what and when (the start and end) the action happened in the video. For each frame, the object categories in its left-below are the participating objects in the action label, while those in its right-below are all objects appearing in the frame.</p><p>the video can thus be effectively transfered into all frames to enhance their frame-level categorizations.</p><p>To incorporate the temporal correlation of object proposals for global reasoning, we resort to the family of recurrent neural networks <ref type="bibr" target="#b10">[11]</ref> due to their good sequential modeling capability. However, existing recurrent networks are largely limited in the constrained information propagation on fixed nodes following predefined routes such as tree-LSTM <ref type="bibr" target="#b38">[39]</ref>, graph-LSTM <ref type="bibr" target="#b19">[20]</ref> and structural-RNN <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>. In contrast, due to the unknown object localizations and temporal motion, it is difficult to find an optimal structure that connects object proposals for routed information propagation to achieve weakly-supervised video object detection. The proposed TD-Graph LSTM, posed as a general dynamic recurrent structure, overcomes these limitations by performing the dynamic information propagation based on an adaptive temporal graph that varies over both time periods in the video and model status in each updating step.</p><p>Specifically, the dynamic temporal graph is constructed based on the visual correlation of object proposals across neighboring frames. The set of graph nodes denotes the entire collection of object proposals in all the frames, while graph edges are adaptively specified for consecutive frames in distinct learning steps. At each iteration, given the updated feature representation of object proposals, we only activate the edge connections with object proposals that have highest similarities with each current proposal. The adaptive graph topology can thus be constructed where different proposals are connected with different temporal correlated neighbors. TD-Graph LSTM alternatively performs the information propagation through each temporal graph topology and updates the graph topology at each iteration. In this way, our model enables the joint optimization of feature learning and temporal inference towards a robust slightlysupervised detection framework.</p><p>The contributions of this paper are summarized as 1)</p><p>We explore a new slightly-supervised video object detection pipeline that leverages convenient action descriptions as the supervision; 2) A novel TD-Graph LSTM framework alleviates the missing label issue by enabling global reasoning over the whole video; 3) TD-Graph LSTM is posed as a general dynamic recurrent structure that performs temporal information propagation on an adaptively updated graph topology at each iteration; 4) We collect and release 5,000 frame annotations with object-level bounding boxes on daily-life videos, with the goal of evaluating our model and also helping advance the object detection community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Weakly-Supervised Object Detection. Though recent state-of-the-art fully-supervised detection pipelines <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23]</ref> have achieved great progress, they heavily rely on large-scale bounding-box annotations. To alleviate this expensive annotation labor, weakly-supervised methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46]</ref> have recently attracted a lot of interest. These approaches use cheaper image-level object labels rather than bounding boxes. Beyond the image domain, another line of research <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">45]</ref> attempts to exploit the temporal information embedded in videos to facilitate the weakly-supervised object detection. Different from all the existing pipelines, we investigate a much cheaper action-driven object detection setting that aims to detect all object instances given only action descriptions. In addition, instead of employing multiple separate steps (e.g., detection and tracking) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref> to capture motion patterns, our TD-graph LSTM is an end-toend framework that incorporates the intrinsic temporal coherence with a designed dynamic recurrent network structure into the action-driven slightly-supervised detection.</p><p>Sequential Modeling. Recurrent neural networks, espe- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action-driven Loss</head><p>Action-driven Loss <ref type="figure">Figure 2</ref>. Our TD-Graph LSTM. Each frame is first passed into a spatial ConvNet to extract region-level features. A temporal graph structure is then constructed by dynamic edge connections between regions in two consecutive frames. TD-Graph LSTM then recurrently propagates information over the updated graph to generate temporal-aware feature representations for all regions. A region-level classification module is then adopted to produce category confidences of all regions in each frame, which are aggregated to obtain frame-level action predictions. The final action-driven loss for each frame is used to feedback signals into the whole model. After each gradient updating, the temporal graph is dynamically updated based on new visual features. For clarity, some edges in the graph are omitted.</p><p>cially Long Short-Term Memory (LSTM) <ref type="bibr" target="#b10">[11]</ref>, have been adopted to address many video processing tasks such as action recognition <ref type="bibr" target="#b23">[24]</ref>, action detection <ref type="bibr" target="#b43">[44]</ref>, video prediction <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b30">31]</ref>, and video summarization <ref type="bibr" target="#b46">[47]</ref>. However, limited by the fixed propagation route of existing LSTM structures <ref type="bibr" target="#b10">[11]</ref>, most of the previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b36">37]</ref> can only learn the temporal interdependency between the holistic frames rather than more fine-grained object-level motion patterns. Some recent approaches develop more complicated recurrent network structures. For instance, structural-RNN <ref type="bibr" target="#b11">[12]</ref> develops a scalable method for casting an arbitrary spatio-temporal graph as a rich RNN mixture. A more recent Graph LSTM <ref type="bibr" target="#b20">[21]</ref> defined over a pre-defined graph topology enables the inference for more complex structured data. However, both of them require a pre-fixed network structure for information propagation, which is impractical for weakly-supervised/slightly-supervised object detection without the knowledge of object localizations and precise object class labels. To handle the propagation over dynamically specified graph structures, we thus propose a new temporal dynamic network structure that supports the inference over the constantly changing graph topologies in different training steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed TD-Graph LSTM</head><p>Overview. We establish a fully-differentiable temporal dynamic graph LSTM (TD-Graph LSTM) framework for the action-driven video object detection task. For each video, the provided annotations are a set of action labels Y = {y 1 , . . . , y N }, each of which describes the action y i =&lt; a i , c i &gt; appearing within a consecutive sequence of frames {I d s i , . . . , I d e i }, where d s i and d e i indicate the action starting and ending frame index. a i denotes the corresponding action noun while c i denotes the object noun. For example, the action tidying a shelf is comprised of the action Tidying and object a shelf. To achieve weakly-supervised object detection, we only extract the object nouns {c i } of action labels in all videos and eliminate the prepositions (e.g., a, the) to produce an object category corpus (e.g., shelf, door, cup) with C classes. Each frame I can be thus assigned with several participating object classes. For example, frames with two actions will be assigned with more than one participating object class, as shown in <ref type="figure">Figure 1</ref>. The action-driven object detection is thus posed as a multi-class weakly-supervised video object detection problem. For simplicity, we eliminate the subscript i of action labels in the following. <ref type="figure">Figure 2</ref> gives an overview of our TD-Graph LSTM. Each frame in the input video is first passed through a spatial ConvNet to obtain spatial visual features for region proposals. Based on visual features, similar regions in two consecutive frames are discovered and associated to indicate the same object across the temporal domain. A temporal graph structure is constructed by connecting all of the semantically similar regions in two consecutive frames, where graph nodes are represented by region proposals. The TD-Graph LSTM unit is then employed to recurrently propagate information over the whole temporal graph, where LSTM units take the spatial visual features as the input states. Benefiting from the graph topology, TD-Graph LSTM is capable of incorporating temporal motion patterns for participating objects in the action in a more efficient and meaningful way. TD-Graph LSTM outputs the enhanced temporal-aware features of all regions. Regionlevel classification is then employed to produce classification confidences. These region-level predictions can finally be aggregated to generate frame-level object class prediction, supervised by the object classes from action labels. The action-driven object categorization loss thus enables the holistic back-propagation into all regions in the video, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region-level Classification Module</head><p>Recurrently update states of frame # &amp; : Update next frame <ref type="figure">Figure 3</ref>. Illustration of the TD-Graph LSTM layer at t-th gradient updating. Given the constructed temporal graph G t , the TD-Graph LSTM recurrently updates the hidden states of each frame Ii, i ∈ {1, . . . , N } as the enhanced temporal-aware visual feature, and then feeds these features into a region-level classification module to compute final category confidences of all regions. Specially, each LSTM unit takes the shared frame-level hidden statesh t i−1 and memory statesm t i−1 , and input features for all regions as the inputs. Then the updated hidden states and memory states for all regions are produced, which are then averaged to generate the new frame-level hidden statesh t i and memory statesm t i for updating next frame Ii+1. The input features of each region consist of the visual features f t i,j and temporal context featuresf t i,j that are aggregated by its connected regions with edge weights in the preceding frame.</p><p>where the prediction of each frame can mutually benefit from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">TD-Graph LSTM Optimization</head><p>The proposed TD-Graph LSTM is comprised by three parametrized modules: spatial ConvNet Φ(·) for visual feature extraction, TD-Graph LSTM unit Ψ(·) for recurrent temporal information propagation, and region-level classification module ϕ(·). These three modules are iteratively updated, targeted at the action-driven object detection.</p><p>At each model updating step t, a temporal graph structure G t =&lt; V, E t &gt; for each video is constructed based on the updated spatial visual features f t of all regions r in the videos, defined as G t = β(Φ t (r)). β(·) is a function to calculate the dynamic edge connections E t conditioning on the updated visual features f t = Φ t (r). The TD-Graph LSTM unit Ψ t recurrently functions on the visual features f t of all frames and propagates temporal information over the graph G t to obtain the enhanced temporal-aware featuresf t = Ψ t (f t |G t ) of all regions in the video. Based on the enhancedf t , the region-level classification module ϕ produces classification confidences rc t for all regions, as rc t = ϕ(f t ). These region-level category confidences rc t can be aggregated to produce frame-level category confidences pc t = γ(rc t ) of all frames by summing the category confidences of all regions of each frame.</p><p>During training, we define the action-driven loss for each frame as a hinge loss function and train a multi-label image classification objective for all frames in the videos:</p><formula xml:id="formula_0">L(Φ, Ψ, ϕ) = 1 CN C c=1 N i=1 max(0, 1 − y c,i pc c,i ) = 1 CN C c=1 N i=1 max(0, 1 − y c,i γ(ϕ(Ψ(f i |G)))),<label>(1)</label></formula><p>where C is the number of classes and y c,i , i ∈ {1, . . . , N } represents action-driven object labels for each frame. For each frame I i , y c,i = 1 only if the action-driven object label c is assigned to the frame I i , otherwise as -1. The objective function defined in Eq. 1 can be optimized by the Stochastic Gradient Descent (SGD) back-propagation. At each t-th gradient updating, the temporal graph structure G t is accordingly updated by β(Φ t (r)) for each video. Thus, the TD-Graph LSTM unit optimizes over a dynamically updated graph structure G t . In the following sections, we introduce the above-defined parametrized modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial ConvNet</head><p>Given each frame I i , we first extract category-agnostic region proposals and then extract their visual features by passing them into a spatial ConvNet Φ(·) following <ref type="bibr" target="#b7">[8]</ref>.</p><p>To provide a fair comparison on action-driven object detection, we adopt the EdgeBoxes <ref type="bibr" target="#b39">[40]</ref> proposal generation method which does not require any object annotations for pretraining. We select the top M = 500 proposals r i = {r i,1 , r i,2 , ..., r i,M } for the frame I i with the highest objectness scores, considering the computation efficiency. At the t-th updating step, visual</p><formula xml:id="formula_1">features f t i = {f t i,1 , f t i,2 , .</formula><p>.., f t i,M } ∈ R M ×D of all regions r i are extracted using the updated spatial ConvNet model, i.e., f t i = Φ t (r i ). The spatial ConvNet Φ(·) consists of several convolutional layers from the base net and one ROI-pooling layer <ref type="bibr" target="#b7">[8]</ref>, and two fully-connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">TD-Graph LSTM Unit</head><p>Dynamic Graph Updating. Given the updated visual features f t i of each frame I i , the temporal graph structure G t =&lt; V, E t &gt; can be accordingly constructed by learning the dynamic edge connections E t . The graph node V = {v i,j }, j = {1, . . . , M } is represented by visual features {f t i,j } of all regions in all frames; that is, M ×N nodes for M region proposals of N frames. Each node v i,j is connected with nodes in the preceding frame I i−1 and the nodes in subsequent frame I i+1 . To incorporate the motion dependency in consecutive frames, the edge connections E t i,i−1 between nodes in I i and I i−1 are mined by considering their appearance similarities in visual features. Specifically, the edge weight between each pair of nodes</p><formula xml:id="formula_2">(v i,j , v i−1,j ) is first calculated as 1 2 exp(−||f t i,j − f t i−1,j || 2 )</formula><p>. To make the model inference efficient and alleviate the missing issue, each node v i,j is only connected to K nodes v i−1,j with the top-K highest edge weights in preceding frame I i−1 , and these activated edge weights are normalized to be summed as 1. We denote the normalized edge weight as ω t i,i−1,j,j . Thus, the updated temporal graph structure G t can be regarded as an undirected K-neighbor graph where each node v i,j is connected with at most K nodes in previous frames.</p><p>TD-Graph LSTM. TD-Graph LSTM layer propagates temporal context over graph and recurrently updates the hidden states {h t i,j } of all regions in each frame I i to construct enhanced temporal-aware feature representations. These features are fed into the region-level classification module to compute the category-level confidences of each region. TD-Graph LSTM updates hidden state of frame i by incorporating information from frame-level hidden statē h t i−1 and memory statem t i−1 . The usage of the shared frame-level hidden state and memory state enables the provision of a compact memorization of temporal patterns in the previous frame and is more suitable for massive and possibly missing graph nodes (e.g., 500 in our setting) in a large temporal graph. After performing N updating steps for all frames, our model effectively embeds the rich temporal dependency to obtain the enhanced temporal-aware feature representations of all regions in all frames. For updating the features of each node v i,j in the frame I i , the TD-Graph LSTM unit takes as the input its own visual features f t i,j , temporal context featuresf t i,j , frame-level hidden statesh t i−1 and memory statesm t i−1 , and outputs the new hidden states h t i,j . Given the dynamic edge connections e i,j = {&lt; v i,j , v i−1,j &gt;}, j ∈ N G (v i,j ), the temporal context featuresf t i,j can be calculated by performing a weighted summation of features of connected regions:</p><formula xml:id="formula_3">f t i,j = j ∈N G (vi,j ) ω t i,i−1,j,j f t i−1,j .<label>(2)</label></formula><p>And the shared frame-level hidden statesh t i−1 and memory statesm t i−1 can be computed as</p><formula xml:id="formula_4">h t i−1 = 1 M M j=1 h t i−1,j ,m t i−1 = 1 M M j=1 m t i−1,j .<label>(3)</label></formula><p>The TD-Graph LSTM unit consists of four gates for each node v i,j : the input gate gu t i,j , the forget gate gf t i,j , the memory gate gc t i,j , and the output gate go t i,j . The W u are the weight parameters specified for frame-level hidden states. The new hidden states and memory states in the graph G t can be calculated as follows:</p><formula xml:id="formula_5">gu t i,j =δ(W u t f t i,j + W ut tf t i,j + U u th t i−1 + b u t ), gf t i,j =δ(W f t f t i,j + W f t tf t i,j + U f th t i−1 + b f t ), go t i,j =δ(W o t f t i,j + W ot tf t i,j + U o th t i−1 + b o t ), gc t i,j = tanh(W c t f t i,j + W ct tf t i,j + U c th t i−1 + b c t ), m t i,j =gf t i,j m t i−1 + gu t i,j gc t i,j , h t i,j =go t i,j tanh(m t i,j ).<label>(4)</label></formula><p>Here δ is a logistic sigmoid function, and indicates a point-wise product. Given the updated hidden states {h t i,j } and memory states {m t i,j } of all regions in frame I i , we can obtain new frame-level hidden statesh t i and memory states m t i for updating the states of regions in frame I i+1 . The TD-LSTM unit recurrently updates the states of all regions in each frame, and thus the past temporal information in preceding frames can be utilized for updating each frame. The TD-Graph LSTM layer is illustrated in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Region-level Classification Module</head><p>Given the updated hidden states h t i,j for each node v i,j , we use a region-level classification module to obtain the category confidences of all regions, that is, rc t i = ϕ(h t i ) of all M regions. Following the two-stream architecture of WS-DDN <ref type="bibr" target="#b1">[2]</ref>, the region-level classification module contains a detection stream and a classification stream, and produces final classification scores by performing element-wise multiplication between them. The classification stream takes the region-level feature vectors h t i of all regions as the input and feeds it to a linear layer that outputs a set of class scores S t i ∈ R M ×C for C classes of all M regions. Here, we use the reproduced WSDDN in <ref type="bibr" target="#b15">[16]</ref> that does not employ an additional softmax in the classification stream. These differences have a minor effect on the detection accuracy as has been discussed in <ref type="bibr" target="#b15">[16]</ref>. The detection stream also takes h t i as the input and feeds it to another linear layer that outputs a set of class scores, giving a matrix of scores L t i ∈ R M ×C . L t i is then fed to another softmax layer to normalize the scores over the regions in the frame. The final scores of all regions rc t i are obtained by taking the element-wise multiplication of the two scoring matrices S t i and L t i . We sum all the region-level class scores rc t i to obtain the frame-level class prediction scores pc t i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Measures</head><p>Dataset Analysis. We evaluate the action-drive weaklysupervised object detection performance on the Charades dataset <ref type="bibr" target="#b31">[32]</ref>. The Charades video dataset is composed of daily indoor activities collected through Amazon Mechanical Turk. There are 157 action classes and on average 6.8 actions in each video, which occur in various orders and contexts. In order to detect objects in videos by using action labels, we only consider the action labels that are related to objects for training. Therefore, there are 66 action labels that are related to 17 object classes in our experiments. We show distribution of object classes (in a random subset of videos) in <ref type="figure" target="#fig_4">Figure 5 (a)</ref>. The training set contains 7,542 videos. Videos are down-sampled to 1 fps and we only sample the frames assigned with action labels in each video. During training, only frame-level action labels are provided for each video.</p><p>In order to evaluate the video object detection performance over 17 daily object classes, we collect the bounding box annotations for 5,000 test frames from 200 videos in the Charades test set. The bounding box number distribution in each frame is shown in <ref type="figure" target="#fig_4">Figure 5</ref> (b), ranging from 1 to 23 boxes appearing in the frame. More than 60% frames have more than 4 bounding boxes and most video frames exhibit severe motion blurs and low resolution. This poses more challenges for the object detection model compared to an image-based object detection dataset, such as the most popular PASCAL VOC <ref type="bibr" target="#b6">[7]</ref> that is widely used in existing weakly-based object detection methods. <ref type="figure" target="#fig_3">Figure 4</ref> further shows example frames with action labels on the Charades dataset. It can be seen that each action label only provides one piece of object class information for the frame that may contain several object classes, which can be regarded as the missing label issue for training a model under this actiondriven setting. Moreover, the video frames often appear with a very cluttered background, blurry objects and diverse viewpoints, which are more challenging and realistic compared to existing image datasets (e.g., MS COCO <ref type="bibr" target="#b21">[22]</ref> and ImageNet <ref type="bibr" target="#b28">[29]</ref>) and video datasets (e.g., UCF101 <ref type="bibr" target="#b35">[36]</ref>).</p><p>Evaluation Measures. We evaluate the performance of both object detection and image classification tasks on Charades. For detection, we report the average precision (AP) at 50% intersection-over-union (IOU) of the detected boxes with the ground truth boxes. For classification, we also report the AP on frame-level object classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Our TD-Graph LSTM adopts the VGG-CNN-F model <ref type="bibr" target="#b2">[3]</ref> pre-trained on ImageNet ILSVRC 2012 challenge data <ref type="bibr" target="#b28">[29]</ref> as the base model, and replaces the last pooling layer pool5 with an SPP layer <ref type="bibr" target="#b8">[9]</ref> to be compatible  with the first fully connected layer. We use the EdgeBoxes algorithm <ref type="bibr" target="#b47">[48]</ref> to generate the top 500 regions that have width and height larger than 20 pixels as candidate regions for each frame. To balance the performance and time cost, we set the number of edges linked to each node K to 100. For training, we use stochastic gradient descent with momentum 0.9 and weight decay 5 × 10 −4 . All weight matrices used in the TD-Graph LSTM units are randomly initialized from a uniform distribution of [−0.1, 0.1]. TD-Graph LSTM predicts the hidden and memory states with the same dimension as the previous region-level CNN features. Each mini-batch contains at most 6 consecutive sampled frames in a video. The network is trained on the Charades training set by using fine-tuning on all layers, including those of the pre-trained base CNN model. The experiments are run for 30 epochs for the model convergence. The learning rates are set to 10 −5 for the first ten epochs, then decreased to 10 −6 . All our models are implemented on the public Torch <ref type="bibr" target="#b4">[5]</ref> platform, and all experiments are conducted on a single NVIDIA GeForce GTX TITAN X GPU with 12 GB memory. The runtime is 2.5 fps and 3.9 fps for training and testing respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results and Comparisons</head><p>We compare the proposed TD-Graph LSTM model with two state-of-the-art weakly-supervised learning methods on the Charades dataset, WSDDN <ref type="bibr" target="#b1">[2]</ref> and ContextLoc-Net <ref type="bibr" target="#b15">[16]</ref>. As both of the two methods were proposed for image-based weakly-supervised image object detection, here we run the source code of ContextLocNet <ref type="bibr" target="#b15">[16]</ref> and their reproduced WSDDN 1 on the Charades dataset to make a fair comparison with our method. Their models are trained by treating the action-related object labels in each frame as the supervision information and are evaluated on each video frame. The difference between our model and WSDDN <ref type="bibr" target="#b1">[2]</ref> is our usage of TD-Graph LSTM layers to leverage rich temporal correlations in the whole video. Similar to WSDDN, ContextLocNet is also a two stream model with an enhanced localization module using various 1 https://github.com/vadimkantorov/contextlocnet    <ref type="figure">Figure 6</ref>. Our TD-Graph LSTM addresses well the missing label issue. It can successfully detect the refrigerator that is not referred to by any action labels (A green box shows the detection result and yellow box the ground truth.) surrounding context. Specifically, we use the contrastive-S setup of ContextLocNet. All of these models use the same base model and region proposal method, i.e., VGG-CNN-F model <ref type="bibr" target="#b2">[3]</ref> and EdgeBoxes <ref type="bibr" target="#b47">[48]</ref>. We report the comparisons with two state-of-the-art on classification mAP and detection mAP in <ref type="table" target="#tab_2">Table 1 and Table 2</ref>, respectively. It can be observed that our TD-Graph LSTM model substantially outperforms two baselines on both classification mAP and detection mAP, particularly, 3.05% higher than ContextLocNet <ref type="bibr" target="#b15">[16]</ref> and 3.85% than WSDDN <ref type="bibr" target="#b1">[2]</ref> in terms of classification mAP. Especially, our TD-Graph LSTM surpasses two baselines in small objects, e.g., over 14.13% for pillow class and 6.93% for cup class. Although our model and two baselines all obtain low detection mAP under this challenging setting, our TD-Graph LSTM still surpasses two baselines on detecting crowded and small objects in the video. The superiority of our TD-Graph LSTM clearly demonstrates its effectiveness in challenging action-driven weakly-supervised object detection where the missing label issue is quite severe and a considerable number of bounding boxes appear in each frame with very low quality. We further show the qualitative comparison with two state-of-the-arts in <ref type="figure" target="#fig_5">Figure 7</ref>. Our model is able to produce more precise object detection for even very small objects (e.g., the cup in the middle row) and objects with heavy occlusion (e.g., the sofa in the bottom row). Our TD-Graph LSTM takes the advantage of exploiting complex temporal correlations between region proposals by propagating knowledge into a whole dynamic temporal graph, which effectively alleviates the critical missing label issue, as shown in <ref type="figure">Figure 6</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>The results of model variants are reported in <ref type="table" target="#tab_2">Table 1</ref>, <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref>.</p><p>The effectiveness of incorporating graph. The main difference between our TD-Graph with a conventional LSTM structure for sequential modeling is in propagating information over a dynamic graph structure. To verify its effectiveness, we thus compare our full model with the variant "TD-Graph LSTM w/o graph" that eliminates the edge connections between regions in consecutive frames, and updates the frame-level hidden and memory states with the original region-level features. Our TD-Graph LSTM consistently obtains better results over "TD-Graph LSTM w/o graph", which speaks to the advantage of incorporating a graph for the challenging action-driven object detection.</p><p>The effectiveness of temporal LSTM. We further verify that recurrent sequential modeling by the LSTM units over the temporal graph is beneficial for exploiting complex object motion patterns in daily videos. "TD-Graph LSTM w/o LSTM" indicates removing the LSTM units and directly aggregating the temporal context features to enhance features of each region. The performance gap between our full model and "TD-Graph LSTM w/o LSTM" verifies the benefits of adopting LSTM.</p><p>Dynamic graph vs Static graph vs Mean graph. Besides the proposed dynamic graph, another commonly used alternative is the fully-connected graph where each region is densely connected with all regions in the preceding frame; that is, "Ours w/ Static Graph" and "Ours w/ Mean Graph". "Ours w/ Static Graph" uses the adaptive edge weights similar to TD-Graph LSTM while "Ours w/ Mean Graph" uses the same weights for all edge connections. It can be seen that applying a dynamic graph structure can help significantly boost both detection and classification performance over other fully-connected graphs. The reason is that meaningful temporal correlations between regions can be discovered by the dynamic graph and leveraged to transfer motion context into the whole video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel temporal dynamic graph LSTM architecture to address action-driven weaklysupervised object detection. It recurrently propagates the temporal context on a constructed dynamic graph structure for each frame. The global action knowledge in the whole video can be effectively leveraged for object detection in each frame, which helps alleviate the missing label problem. Extensive experiments on a large-scale daily-life action dataset Charades demonstrate the superiority of our model over the state-of-the-arts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Several samples of key frames from videos in Charades. The action labels are given at the bottom of the image and the related objects are listed at the top of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>(a) The distribution of object classes appearing in the action labels of the training set. (b) The distribution of the ground truth bounding box numbers in each image of the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparisons with two state-of-the-arts on video object detection. The green boxes indicate detection results and yellow ones are the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>table /</head><label>/</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Action labels</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>person</cell><cell>horse</cell><cell>chair</cell><cell>pottedplant</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Video frames</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cat</cell><cell></cell><cell>motorbike</cell><cell>shelf</cell><cell>shelf</cell><cell></cell><cell>shelf</cell><cell>laptop</cell><cell>shelf</cell><cell>laptop</cell><cell>shelf</cell></row><row><cell></cell><cell></cell><cell></cell><cell>food/sandwich</cell><cell>food/sandwich</cell><cell>desk</cell><cell>laptop</cell><cell>table/desk</cell><cell>laptop</cell><cell>table/desk</cell><cell>laptop</cell></row><row><cell></cell><cell></cell><cell></cell><cell>laptop</cell><cell>laptop</cell><cell></cell><cell>table/desk</cell><cell>chair</cell><cell>table/desk</cell><cell>chair</cell><cell>table/desk</cell></row><row><cell></cell><cell></cell><cell></cell><cell>table/desk</cell><cell>table/desk</cell><cell></cell><cell>chair</cell><cell></cell><cell>chair</cell><cell>chair</cell></row><row><cell></cell><cell></cell><cell></cell><cell>chair</cell><cell>chair</cell><cell></cell><cell>refrigerator</cell><cell></cell><cell>refrigerator</cell><cell>refrigerator</cell></row><row><cell></cell><cell></cell><cell></cell><cell>refrigerator</cell><cell>refrigerator</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Action label:</cell><cell>Tidying a shelf</cell><cell>Watching a laptop</cell><cell></cell><cell>Sitting in a chair</cell><cell></cell><cell>Sitting at a table</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Per-class performance comparison of our proposed models with two state-of-the-art weakly-supervised learning methods when evaluating on the Charades dataset<ref type="bibr" target="#b31">[32]</ref>, test classification average precision (%).Graph  LSTM w/o graph 25.04 6.51 43.79 21.54 15.6 15.86 19.57 5.61 9.32 6.2 9.02 25.95 39.2 8.85 15.27 18.18 5.63 17.13 TD-Graph LSTM 47.62 12.26 45.07 23.55 16.7 15.6 30.9 5.05 17.64 7.43 9.53 19.52 43.29 4.23 12.47 15.03 5.91 19.52</figDesc><table><row><cell>Method</cell><cell cols="7">bed broom chair cup dish door laptop mirror pillow refri shelf sofa table tv towel vacuum window mAP</cell></row><row><cell>WSDDN [2]</cell><cell>39.8 5.85 36.1 21 16.3 11.6 30.5 4.7</cell><cell>2.8</cell><cell>6.5 8.1 14.8 37.8</cell><cell>5</cell><cell>12.5</cell><cell>8.2</cell><cell>4.8 15.67</cell></row><row><cell>ContextLocNet [16]</cell><cell cols="6">43.37 5.65 38.95 16.62 12.46 8.67 27.75 4.5 3.51 11.12 9.79 15.67 37.44 14.39 9.72 16.36</cell><cell>3.97 16.47</cell></row><row><cell cols="7">TD-Graph LSTM w/o LSTM 32.54 5.875 31.69 27.9 15.79 14.19 18.81 6.15 8.35 4.5 9.3 24.33 33 8.26 14.7 7.68</cell><cell>6.72 15.89</cell></row><row><cell>TD-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Per-class performance comparison of our proposed models with two state-of-the-art weakly-supervised learning methods when evaluating on the Charades dataset<ref type="bibr" target="#b31">[32]</ref>, test detection average precision (%).</figDesc><table><row><cell>Method</cell><cell cols="4">bed broom chair cup dish door laptop mirror pillow refri shelf sofa table tv towel vacuum window mAP</cell></row><row><cell>WSDDN [2]</cell><cell cols="4">2.38 0.04 1.17 0.03 0.13 0.31 2.81 0.28 0.02 0.12 0.03 0.41 1.74 1.18 0.07 0.08</cell><cell>0.22 0.65</cell></row><row><cell>ContextLocNet [16]</cell><cell cols="2">7.4 0.03 0.55 0.02 0.01 0.17 1.11 0.66</cell><cell cols="2">0 0.07 1.75 4.12 0.63 0.99 0.03 0.75</cell><cell>0.78 1.12</cell></row><row><cell cols="2">TD-Graph LSTM w/o LSTM 7.41 0.05</cell><cell cols="3">3 0.05 0.02 0.56 0.11 0.65 0.04 0.16 0.25 1.67 2.46 1.24 0.11 0.46</cell><cell>1.46 1.16</cell></row><row><cell cols="5">TD-Graph LSTM w/o graph 9.69 0.02 2.85 0.34 0.05 0.87 1.95 0.69 0.05 0.44 2.11 3.34 1.91 1.05 0.05 0.29</cell><cell>0.69 1.55</cell></row><row><cell>TD-Graph LSTM</cell><cell cols="3">9.19 0.04 4.18 0.49 0.11 1.17 2.91 0.3 0.08 0.29 3.21 5.86 3.35 1.27 0.09</cell><cell>0.6</cell><cell>0.47 1.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison of using different graph topologies when evaluating on the Charades dataset, test detection mAP (%) and classification mAP (%).</figDesc><table><row><cell>Method</cell><cell cols="2">det mAP cls mAP</cell></row><row><cell>Ours w/o Graph</cell><cell>1.55</cell><cell>17.13</cell></row><row><cell>Ours w/ Mean Graph</cell><cell>1.41</cell><cell>16.92</cell></row><row><cell>Ours w/ Static Graph</cell><cell>1.89</cell><cell>17.97</cell></row><row><cell>Ours</cell><cell>1.98</cell><cell>19.52</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , W f t , W c t , W o t are the recurrent gate weight matrices specified for input visual features and W ut t , W f t t , W ct t , W ot t are those for temporal context features. U u t , U f t , U c t , U o t</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>We visualize more detection results of the TD-Graph LSTM on the Charades dataset.    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Torch7: A Matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Localizing objects while learning their appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ActivityNet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structural-RNN: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient image and video co-localization with Frank-Wolfe algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ContextLoc-Net: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and tracking in video collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interpretable structure-evolving LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards computational baby learning: A weakly-supervised approach for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic object parsing with local-global long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Track and transfer: Watching videos to simulate strong human supervision for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">In defence of negative mining for annotating weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02968</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Video object discovery and co-segmentation with extremely weak supervision. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Revealing event saliency in unconstrained video collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1746" to="1758" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a self-paced multiple-instance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="865" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video summarization with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Edge Boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
