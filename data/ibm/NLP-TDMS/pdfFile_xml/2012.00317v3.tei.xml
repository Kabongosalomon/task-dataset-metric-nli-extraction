<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
							<email>yw.lee@etri.re.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung-Il</forename><surname>Kim</surname></persName>
							<email>hikim@etri.re.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Yun</surname></persName>
							<email>kimin.yun@etri.re.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Moon</surname></persName>
							<email>jymoon@etri.re.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video classification researches have recently attracted attention in the fields of temporal modeling and efficient 3D architectures. However, the temporal modeling methods are not efficient, and there is little interest in how to deal with temporal modeling in a 3D efficient architecture. For bridging the gap between them, we propose an efficient 3D architecture for temporal modeling, called VoV3D, that consists of a temporal one-shot aggregation (T-OSA) module and depthwise factorized component, D(2+1)D. The T-OSA is devised to build a feature hierarchy by aggregating spatiotemporal features with different temporal receptive fields. Stacking this T-OSA enables the network itself to model short-range as well as long-range temporal relationships across frames without any external modules. We also design a depthwise spatiotemporal factorization module, named, D(2+1)D that decomposes a 3D depthwise convolution into two spatial and temporal depthwise convolutions for making our network more lightweight and efficient. Through the proposed temporal modeling method (T-OSA) and the efficient factorized module (D(2+1)D), we construct two types of VoV3D networks, VoV3D-M and VoV3D-L. Thanks to its efficiency and effectiveness of temporal modeling, VoV3D-L has 6× fewer model parameters and 14× less computation, surpassing state-of-the-arts on both Something-Something and Kinetics-400 datasets. Furthermore, VoV3D shows better abilities for temporal modeling than state-of-the-art efficient 3D architectures. We hope that VoV3D can serve as a baseline for efficient temporal modeling architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, many works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b29">30]</ref> for video classification have been studied to handle temporal modeling that characterizes the temporal variation and dynamics of an action (i.e., visual tempo <ref type="bibr" target="#b34">[35]</ref>). Unlike 2D image classification, video classification should distinguish visual tempo variation as well as its semantic appearance. In other words, appearance information alone is not sufficient to distinguish between moving something up and down or between walking and running, which requires to capture visual tempo variations. Thus, effectively modeling visual tempo is a key factor for video classification.</p><p>Previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref> for temporal modeling utilize 2D CNN architecture due to its efficiency rather than 3D CNN one, which usually process per-frame inputs and aggregate these results to produce a final output by adopting temporal shift module <ref type="bibr" target="#b17">[18]</ref> or motion information embedding module <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. However, these methods depend heavily on the 2D ResNet <ref type="bibr" target="#b7">[8]</ref> backbone, which is neither lightweight nor efficient compared to state-of-the-art efficient 2D CNN models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27]</ref>. 3D CNN-based temporal modeling methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref> are also proposed to construct input frame-level pyramid <ref type="bibr" target="#b3">[4]</ref> with different input frame rates or feature-level pyramid <ref type="bibr" target="#b34">[35]</ref> with dynamic visual tempo modeling. However, these methods require extra model capacity by adding a separate network path or a fusion module. In short, since previous works are add-on style modules on top of the backbone network, they are constrained under the backbone.</p><p>Another research that has recently attracted attention for video understanding is to build efficient network architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3]</ref>. These works exploit 3D depthwise convolution for reducing model parameters and computations like 2D efficient CNN architectures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref> that replace a convolution with a combination of depthwise convolution and pointwise convolution. This depthwise separable convolution can be called channel factorization. However, these 3D efficient networks only focus on building architectures and do not consider temporal modeling.</p><p>For addressing these issues, in this work, we propose an efficient and effective temporal modeling 3D architecture, called VoV3D. The proposed VoV3D consists of temporal one-shot aggregation (T-OSA) building blocks, which are made of the proposed depthwise factorized module (i.e., D(2+1)D). The T-OSA is devised to build a temporal feature hierarchy by aggregating features with different temporal receptive fields. As illustrated in <ref type="figure">Fig. 1</ref>, having multi-ple temporal receptive fields is helpful to capture the visual tempo variation of an action. From this perspective, stacking the T-OSAs enables the network itself to model shortrange as well as long-range temporal relationships across frames without any external modules <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Inspired by the optimization benefit from kernel factorization <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref> and the efficiency of channel factorization <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3]</ref>, we also design a depthwise spatiotemporal factorized module, called D(2+1)D that decomposes 3D depthwise convolution into spatial depthwise convolution and temporal depthwise convolution for making our network more lightweight and efficient. In practice, we have confirmed that combining the two factorization methods achieves better performance and efficiency than each one. Moreover, the efficiency of D(2+1)D makes our network possible to use more input frames (over 16 frames), which is advantageous for temporal modeling.</p><p>By using the proposed temporal modeling method, T-OSA, and the efficient factorized module, D(2+1)D, we construct two types of 3D CNN architectures, VoV3D-M and VoV3D-L models. In order to evaluate the proposed method in terms of modeling temporal variations, we validate VoV3D on the Something-Something dataset <ref type="bibr" target="#b5">[6]</ref> which has been well-known to be challenging to classify an action due to the temporal complexity <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18]</ref>. Moreover, we show the performance on Kinetics-400 dataset <ref type="bibr" target="#b12">[13]</ref> to compare the proposed network to the many state-of-the-arts. Thanks to its efficiency and effectiveness of the proposed temporal modeling mechanism, VoV3D-M outperforms the state-of-the-art both 2D <ref type="bibr" target="#b16">[17]</ref> and 3D <ref type="bibr" target="#b3">[4]</ref> temporal modeling methods, while having much 7× and 11× fewer parameters and 27× and 10× less computation on Something-Something dataset <ref type="bibr" target="#b5">[6]</ref>. Furthermore, the proposed VoV3D shows better temporal modeling ability than the state-ofthe-art efficient 3D architecture, X3D <ref type="bibr" target="#b2">[3]</ref> having comparable model capacity. We hope that the ideas contained within the proposed VoV3D are widely used in other methods.</p><p>The main contributions of this work are summarized as below:</p><p>• We propose an effective temporal modeling method, Temporal One-Shot Aggregation (T-OSA) that can capture visual tempo variations by aggregating features having different temporal receptive fields.</p><p>• We propose an efficient depthwise factorized module, D(2+1)D that decomposes a 3D convolution into spatial and temporal depthwise convolutions, making T-OSA modules better accuracy and efficiency.</p><p>• We design an efficient and effective 3D CNN architecture, VoV3D based on the proposed T-OSA and D(2+1)D modules, which outperforms the state-ofthe-arts in terms of both temporal modeling and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TRF = 7</head><p>Temporal Receptive Field = TRF TRF = 5 TRF = 3 TRF = 9</p><p>"diverse temporal representation" concat <ref type="figure">Figure 1</ref>: Illustration of the temporal receptive field. The features having multiple temporal receptive fields are advantageous to capture visual tempo variation of an action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works 2.1. Temporal modeling for video classification</head><p>Recent attempts for temporal modeling for video classification could be divided into two categories: 2D CNNbased and 3D CNN-based methods. 2D CNN-based methods such as TSN <ref type="bibr" target="#b29">[30]</ref>, TSM <ref type="bibr" target="#b17">[18]</ref>, STM <ref type="bibr" target="#b11">[12]</ref> and TEA <ref type="bibr" target="#b16">[17]</ref> prefer to use 2D CNN, e.g., ResNet-50 as a backbone, due to its efficiency than 3D CNN models. They process perframe inputs and aggregate these results to produce a final output on top of 2D ResNet. TSN <ref type="bibr" target="#b29">[30]</ref> proposes to form a clip by sampling evenly from divided segments and this sparse sampling method becomes a common strategy for many works. TSM <ref type="bibr" target="#b17">[18]</ref> is proposed to model temporal motion by utilizing memory shift operation along the temporal dimension. Since motion information is also an important cue for temporal modeling as a short-term temporal relationship, attempts to model feature-level motion features are proposed in STM <ref type="bibr" target="#b11">[12]</ref> and TEA <ref type="bibr" target="#b16">[17]</ref>. STM and TEA propose to differentiate between adjacent features for representing motion features and then add the spatiotemporal features and motion encoding together. TEA <ref type="bibr" target="#b16">[17]</ref> also has a temporal aggregation module to capture long-range temporal dependency. However, TEA is based on 2D CNN features that are not jointly convolved along with spatial and temporal axis. This means that the interaction between spatial and temporal features in <ref type="bibr" target="#b16">[17]</ref> is limited than 3D spatiotemporal methods.</p><p>For modeling various visual tempos using spatiotempo-  <ref type="table">Table 2</ref>.</p><formula xml:id="formula_0">T-OSA T-OSA T-OSA T-OSA stage-2 stage-3 stage-4 stage-5 × × × 5 × × × × × × 1×1×1 TRF=9 TRF=7 TRF=5 TRF=3 D(2+1)D D(2+1)D D(2+1)D D(2+1)D ⊕ T-OSA</formula><p>ral 3D CNN, many works have been proposed by building an input frame-level pyramid <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref> or feature-level pyramid <ref type="bibr" target="#b34">[35]</ref>. SlowFast <ref type="bibr" target="#b3">[4]</ref> has two network inputs with different frame rates to capture different types of visual information, e.g., semantic appearance or motion. DTPN <ref type="bibr" target="#b35">[36]</ref> also uses a different sampling rate for arbitrary-length input video, which builds up the input frame-level hierarchy. Unlike these methods, TPN <ref type="bibr" target="#b34">[35]</ref> leverages the feature hierarchy on top of the backbone network, instead of the input frame-level hierarchy by building a temporal feature pyramid network. In short, since temporal modeling methods are based on the existing backbone networks, e.g., ResNet-50, they are constrained under the nature of the backbone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Efficient 3D CNN architecture</head><p>Since channelwise separable convolution is densely exploited by efficient 2D CNN architectures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref>, 3D CNN architectures <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14]</ref> based on the extended depthwise separable convolution have been explored. CSN <ref type="bibr" target="#b27">[28]</ref> adopts 3D depthwise separable convolution into the residual bottleneck block <ref type="bibr" target="#b7">[8]</ref> by replacing the 3 × 3 × 3 convolution and adding a 1 × 1 × 1 convolution in front of the 3D depthwise convolution for interaction between channels. X3D <ref type="bibr" target="#b2">[3]</ref> explores 3D CNN architecture along with spatial, temporal, depth, channel axis for maximizing the efficacy of the 3D CNN model. The depthwise bottleneck is also utilized as a key component in X3D, while X3D is progressively expanded from a lightweight to a large-scale model by scale-up all kinds of axes. As a result, X3D achieves state-of-the-art performance with a much smaller model capacity on various video classification datasets such as Kinetics-400. However, this method focuses on building an efficient network while temporal modeling is not considered deeply. Therefore, we focus on building an efficient 3D CNN architecture as well as temporal modeling simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VoV3D</head><p>Temporal modeling (i.e., capturing visual tempo variation) plays an important role in action recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35]</ref>. In particular, in the case of a video that lacks semantic variations of the features, video classification networks should rely heavily on visual tempo. Moreover, it is necessary to model long-term as well as short-term temporal relationship because short-term information is not sufficient to distinguish visual tempo variations such as walking vs. running. The conventional temporal modeling methods based on 3D CNN try to model the visual tempo through the input frame-level <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4]</ref> or feature-level pyramids <ref type="bibr" target="#b34">[35]</ref>. However, these methods have to add separate networks on top of the existing 3D backbone as an external (i.e., plugin) module, which requires more parameters and computations. To address these challenges, in this paper, we aim to propose a lightweight and efficient video backbone network having temporal modeling ability by itself without external modules. To this end, we design a new 3D CNN architecture inspired by VoVNet <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> that represents hierarchical and diverse spatial features at a small cost.</p><p>First, we propose an effective temporal modeling method, named Temporal One-Shot Aggregation (T-OSA) inspired by the OSA module in VoVNet <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. For making a network lightweight and efficient, we also devise a depthwise spatiotemporal factorization component, D(2+1)D. Lastly, we design a new video classification network, called VoV3D, which is comprised of the proposed T-OSA and D(2+1)D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Temporal One-Shot Aggregation (T-OSA)</head><p>VoVNet <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> is a computation and energy-efficient 2D CNN architecture devised to learn diverse feature representations by stacking One-Shot-Aggregation (OSA) modules. The OSA module consists of successive 3 × 3 convolutions and aggregates those feature maps into one feature map at once in a concatenate manner, followed by a 1 × 1 convolution. The OSA allows the network to represent diverse features by capturing multiple receptive fields in one feature map, which results in the effect of the feature pyramid. Due to the diverse feature representation power of OSA, VoVNet outperforms ResNet <ref type="bibr" target="#b7">[8]</ref> and HRNet <ref type="bibr" target="#b24">[25]</ref> in object detection and segmentation tasks that require more complex representation.</p><p>Inspired by the spatial feature's hierarchy of OSA in VoVNet, we propose temporal one-shot aggregation, called T-OSA, to capture multiple temporal receptive fields in one 4D feature map, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref></p><formula xml:id="formula_1">. In detail, the i-th affine transform F 3×3 i (e.g., 3×3 2DConv) can be replaced with F t×3×3 i (e.g., t × 3 × 3 3DConv) for i ∈ {1, 2, ..., n}</formula><p>where t is the temporal kernel size (we set to 3) and n is the number of t × 3 × 3 3D convolutions in T-OSA. It is noted that we keep temporal dimension T (frames) for feature aggregation. Each feature map X i ∈ R C×T ×W ×H that is the result from F t×3×3 i has progressively increasing temporal receptive field due to its successive connection. For example, if the temporal receptive field (TRF) of the feature map X 1 is 3 and temporal kernel size t is 3, the TRF of the next X 2 is 5. Thus, once the features are concatenated in channel-axis, the aggregated feature map X agg ∈ R (n+1)C×T ×W ×H comprised of {X in , X 1 , ..., X n } has diverse temporal and spatial receptive fields in one feature map, where X in ∈ R C×T ×W ×H is the input feature and n is set to 4 in <ref type="figure" target="#fig_0">Fig. 2</ref>. Then, a 1 × 1 × 1 convolution is followed for reducing channel size (n + 1)C to C and the residual connection is added to the final feature map. Therefore, stacking T-OSA makes it possible to model short-range as well as long-range temporal dependency across frames, which has a similar effect with feature pyramid in the same spatial feature space.</p><p>In practice, simply expanding 2D VoVNet to 3D CNN architecture is limited in terms of optimization because 3D CNN models have additional parameter space along with temporal-axis and thus need optimization strategy. Therefore, we elaborate the T-OSA with additional design choices for adaptation of OSA in 3D temporal feature space. While the OSA module in 2D VoVNet uses only 3 × 3 2D Conv as an affine tranform, the proposed T-OSA adopts 3D bottleneck architecture <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28]</ref> </p><formula xml:id="formula_2">(e.g., 1 × 1 × 1 3DConv, 3 × 3 × 3 3DConv, 1 × 1 × 1 3DConv</formula><p>) with more nonlinearity operations. As a 3D bottleneck architecture, we propose D(2+1)D in the next section. Also, we add an inner residual connection to facilitate optimization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Depthwise Sptaiotemporal Factorization</head><p>There are two types of factorization concept on 3D convolution (3DConv): 1) Depthwise (or Channelwise) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14]</ref> and 2) Kernelwise <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref> methods. Inspired by efficient 2D image classification network <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, depthwise separable convolution is also mainly used as a key building block for efficient video backbone networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14]</ref>. 3D depthwise separable convolution (3DWConv) is utilized to factorize a 3DConv into a t×k×k depthwise 3DConv followed by 1×1×1 pointwise 3DConv. CSN [28] adds a 1 × 1 × 1 3DConv in front of the 3DWConv for preserving the interaction between channels, which results in improving accuracy. Tran et al. <ref type="bibr" target="#b27">[28]</ref> found that the 3DWConv has two advantages: 1) significant reduction of parameters and computational cost (FLOPs) without sacrificing accuracy, 2) regularization effect. In addition to channel factorization, kernel factorization also has been widely used in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref> for curtailing computation and boosting accuracy. The kernel factorization is also called spatiotemporal factorization as it is decomposed into a 1×k×k spatial convolution (space) followed by a t×1×1 temporal convolution (time) as shown in <ref type="figure" target="#fig_1">Fig. 3 (top)</ref>.</p><p>Our motivation lies in the fusion of these two factorization methods for realizing an efficient video classification network. We design a depthwise spatiotemporal factorized module, D(2+1)D, that decomposes a 3DWConv into a spatial DWConv and a temporal DWConv as shown in <ref type="figure" target="#fig_1">Fig. 3 (bottom)</ref>. We analyze each resource requirement of models in <ref type="table" target="#tab_0">Table 1</ref> illustrating the number of parameters and computation (FLOPs) of a 3DConv in the middle of bottleneck architecture. The input tensor of the 3DConv Type Param. FLOPs has C × T × H × W shape, where T and C are the number of frames and channels, and H, W is the size of height and width, respectively. Assuming the number of filters (output channel) is the same (C), the 3D filter has t × k × k kernel size, where t, k denote temporal and spatial kernel, respectively. As demonstrated in <ref type="table" target="#tab_0">Table 1</ref>, compared to the basic bottleneck 3DConv in (a), 3DWConv in (c) is C× more efficient because it has only one sub-filter for the input tensor as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. We design two types of factorized modules based on the order of spatial and temporal dimensions: D(1+2)D and D(2+1)D. It is noted that spatial down-sampling is operated in the spatial convolution and the temporal convolution keeps temporal dimension. Compared with 3DWConv in (c), both D(1+2)D and D(2+1)D have about one order of magnitude fewer parameters and computations. In comparison between the two factorized modules, an important difference arises in spatial down-sampling. The number of parameters is the same, while the computation cost is different due to different spatial sizes. Specifically, for D(1+2)D, the temporal DWConv is operated first with C × T × H × W input tensor followed by the spatial DWConv with stride s. It is summarized as:</p><formula xml:id="formula_3">(a) bottleneck C 2 tk 2 C 2 tk 2 (HW T )/s 2 (b) R(2+1)D C 2 (t + k 2 ) C 2 (t + k 2 )(HW T )/s 2 (c) dw-bottleneck Ctk 2 Ctk 2 (HW T )/s 2 (d) D(1+2)D C(t + k 2 ) C(s 2 t + k 2 )(HW T )/s 2 (e) D(2+1)D C(t + k 2 ) C(t + k 2 )(HW T )/s 2</formula><formula xml:id="formula_4">FLOPs = Ct × HW T + Ck 2 × HW T /s 2 = (s 2 t + k 2 )CHW T /s 2 .<label>(1)</label></formula><p>For D(2+1)D, since spatial DWConv with down-sampling goes ahead, the temporal DWConv operates the spatially down-sized input tensor, which results in reducing overall computation. This is summarized as:</p><formula xml:id="formula_5">FLOPs = Ct × HW T /s 2 + Ck 2 × HW T /s 2 = (t + k 2 )CHW T /s 2 .<label>(2)</label></formula><p>It is worth noting that the proposed D(2+1)D shows better efficiency than kernel or channel factorization alone. We also expect that the D(2+1)D can be widely used for other 3D CNN architectures to boost their performances. We also have confirmed the effect through the combination of the state-of-the-art method (i.e., X3D <ref type="bibr" target="#b2">[3]</ref>) and our D(2+1)D, which will be described in the experimental section.  </p><formula xml:id="formula_6">Stage VoV3D-M (L) output size T × H × W conv1 1 × 3 2 , 5 × 1 2 , 24 T × 112 × 112 T-OSA2 (D(2+1)D, 40(48))×5 1 × 1 2 , 24 ×1(1) T × 56 × 56 T-OSA3 (D(2+1)D, 80(96))×5 1 × 1 2 , 48 ×1(2) T × 28 × 28 T-OSA4 (D(2+1)D, 160(192))×5 1 × 1 2 , 96 ×2(5) T × 14 × 14 T-OSA5 (D(2+1)D, 320(384))×5 1 × 1 2 , 160(192) ×2(3) T × 7 × 7 conv5 1 × 1 2 , 320(384) T × 7 × 7 pool5 T × 7 × 7 1 × 1 × 1 fc1 1 × 1 2 , 2048 1 × 1 × 1 fc2 1 × 1 2 #classes 1 × 1 × 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">VoV3D Architecture</head><p>Finally, we construct a lightweight and efficient 3D CNN architecture, VoV3D, that can model various visual tempos effectively with the proposed T-OSA and D(2+1)D modules. We design two types of lightweight models: VoV3D-M and VoV3D-L which have only 3.3M and 5.8M parameters, respectively. VoV3D is comprised of the proposed T-OSA blocks which consist of five D(2+1)D modules followed by a 1 × 1 × 1 3DConv. In stage level (same spatial resolution), VoV3D has multiple T-OSAs (e.g., 5), in series, which leads to representing diverse temporal features. conv1 is also the (2+1)D style-convolution where 1 × 3 2 spatial 3DConv is operated and followed by a 3×1 2 temporal 3DConv. Following <ref type="bibr" target="#b2">[3]</ref>, we also add a channel attention module, SE block <ref type="bibr" target="#b10">[11]</ref>, into the D(2+1)D with reduction ratio of 1/16. The lightweight and efficient D(2+1)D allows VoV3D to reduce significant computation cost, so it can use longer frames (≥16) to capture longer visual tempo. The details are illustrated in <ref type="table">Table.</ref> 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We validate the proposed VoV3D on Something-Something (V1 &amp; V2) <ref type="bibr" target="#b5">[6]</ref> and Kinetics-400 <ref type="bibr" target="#b12">[13]</ref>. In contrast to Kinetics-400 <ref type="bibr" target="#b12">[13]</ref> that is less sensitive to visual tempo variations, Something-Something <ref type="bibr" target="#b5">[6]</ref> is focused on human-object interaction which requires a more temporal relationship than appearance <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18]</ref>. Since Something-Something is widely used as a benchmark for evaluating the effectiveness of temporal modeling, the effectiveness of the proposed VoV3D is mainly investigated for this dataset. Something-Something V1 <ref type="bibr" target="#b5">[6]</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Training. Our models are trained from scratch without using ImageNet <ref type="bibr" target="#b21">[22]</ref> pretrained model unless specified. For Something-Something <ref type="bibr" target="#b5">[6]</ref> dataset, we use segment-based input frame sampling <ref type="bibr" target="#b17">[18]</ref>, which splits each video into N segments and picks one frame to form a clip (N frames) from each segment. We note that thanks to the memory efficient VoV3D, our model can be trained with more input frames, e.g., from 16 to 32. For Kinetics-400 <ref type="bibr" target="#b12">[13]</ref>, we sample 16 frames with a temporal stride of 5 as <ref type="bibr" target="#b2">[3]</ref>. We apply the random cropping of 224 × 224 pixels from a clip and random horizontal flip with a shorter side randomly sampled in [256, 320] pixels <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> for VoV3D-M and VoV3D-L models. In case of Something-Something, it requires discriminating between directions, so the random flip is not applied. Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, we use the same parameters for training Something-Something V1 &amp; V2: SGD optimizer, 100 epochs, mini-batch size 64 (8 clips per a GPU), initial learning rate 0.1, half-period cosine learning rate schedule <ref type="bibr" target="#b18">[19]</ref>, linear warm-up strategy <ref type="bibr" target="#b4">[5]</ref>, and weight  <ref type="table">Table 5</ref>: Comparison to X3D on Something-Something V1. #F denotes the number of input frames. X and V denote X3D and VoV3D, respectively. For model parameters, X3D-M and VoV3D-M have 3.3M respectively and X3D-L and VoV3D-L have 5.6M and 5.8M, respectively. decay 5 × 10 −5 . Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>, we also fine-tune VoV3D using Kinetics-400 pretrained model. We use a linear warm-up <ref type="bibr" target="#b4">[5]</ref> for 2k iterations from 0.0001 and a weight decay of 5 × 10 −5 . We finetune the model for 50 epochs with a base learning rate of 0.05 decreased at 35 and 45 epoch by 0.1 and use sync bathcnorm. For Kinetics-400, we use the same training parameters except for 256 epochs and mini-batch size 128. We train all models using a 8-GPU machine and implementation is based on PySlowFast <ref type="bibr" target="#b1">[2]</ref>. To compare VoV3D-M/L to the strong state-of-the-art X3D <ref type="bibr" target="#b2">[3]</ref>, we also train X3D-M/L having similar parameters and FLOPs with the same training protocols. Note that for X3D-L, unlike origin X3D paper <ref type="bibr" target="#b2">[3]</ref>, we use the same spatial sample size [256, 320], not <ref type="bibr">[356,</ref><ref type="bibr">446]</ref>. The reason why we invest computation budget to more input frames (≥16) is that the Something-Something dataset <ref type="bibr" target="#b5">[6]</ref> requires more temporal modeling than appearance information. Inference. Following common practice in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, we sample multiple clips per video (e.g., 10 for Kinetics and 2 for Something-Something). We scale the shorter spatial side to 256 pixels and take 3 crops of 256×256, as an approximation of fully-convolutional testing <ref type="bibr" target="#b30">[31]</ref> called full resolution image testing in TSM <ref type="bibr" target="#b17">[18]</ref>. Then, we average the softmax scores for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>In order to verify the effectiveness of the proposed method in terms of temporal modeling and computational complexity, we conduct ablation studies on Something-Something V1 <ref type="bibr" target="#b5">[6]</ref> that requires more temporal modeling ability <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18]</ref> than Kinetics-400. Component contributions. We study the effect of the individual component of VoV3D and results are shown in <ref type="table" target="#tab_3">Table 3</ref>. We use X3D as a baseline and T-OSA without D(2+1)D consists of the same depthwise bottleneck as  X3D. T-OSA boosts performance by large margins in both M and L models, demonstrating the diverse temporal representation of T-OSA improves temporal modeling capability. D(2+1)D also achieves higher accuracy, which suggests that the factorization of spatial and temporal features helps the network to optimize easily.</p><p>Comparison with the different bottleneck. We compare the proposed depthwise spatiotemporal factorization module (i.e., D(2+1)D) with other architectures <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28]</ref> in <ref type="table" target="#tab_4">Table 4</ref>. We alternatively plug the bottleneck architectures into the T-OSA. While R(2+1)D <ref type="bibr" target="#b28">[29]</ref> reduces both parameters and GFLOPs with higher accuracy than the standard bottleneck <ref type="bibr" target="#b6">[7]</ref> in (a), the depthwise bottleneck <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref> in (c) also significantly reduces the computations but obtains lower performance than R(2+1)D. However, both D(1+2)D and D(2+1)D achieves better accuracy with less computation than dw-bottleneck in (c). In particular, D(2+1)D outperforms all other architectures with a minimum computation and model size. In addition, we also investigate the effect of D(2+1)D by replacing dw-bottleneck with D(2+1)D in X3D. As a result, D(2+1)D improves 1%p Top-1 accuracy gain while reducing model parameters and GFLOPs.</p><p>Comparison to X3D under various conditions. We compare VoV3D with X3D under the following conditions: the number of input frames (#F in <ref type="table">Table 5</ref>) and whether a backbone is pre-trained with Kinetics-400 or not. We train VoV3D and X3D with 16 and 32 input frames from scratch or using Kinetics-400 pretraining. <ref type="table">Table 5</ref> summa-rizes the results. We can find that using more frames boosts performance in both VoV3D and X3D and VoV3D consistently outperforms X3D. This demonstrates that using more frames helps the networks to capture visual tempo variation and the ability of the proposed T-OSA to represent diverse temporal receptive fields enables VoV3D to yield better temporal modeling than X3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison to state-the-of-art</head><p>Results on Something-Something. We validate the efficiency and effectiveness of the proposed VoV3D on Something-Something V1&amp;V2 requiring more temporal modeling ability than spatial appearance.  These results break the prejudice that 3D CNN architectures require an expensive computation budget than 2D CNN. We also note that VoV3D architecture alone shows sufficient performance and efficiency than the add-on style temporal modeling methods on top of 2D backbone networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17]</ref>. It shows that VoV3D can serve as a strong baseline for temporal modeling. VoV3D is also superior to those 3D CNN-based temporal modeling methods, such as SlowFast <ref type="bibr" target="#b3">[4]</ref> and CSN <ref type="bibr" target="#b27">[28]</ref>. Even without Kinetics-pretraining, VoV3D-M with 32 frames achieves higher accuracy than SlowFast pretrained on Kinetics-400 with 11× more model parameters. It demonstrates that a 3D single network path is enough to model visual tempo variations. Although CSN <ref type="bibr" target="#b27">[28]</ref> contains the depthwise bottleneck architecture, its accuracy is lower than that of VoV3D-M. This result shows that the proposed T-OSA plays an important role in temporal modeling. Results on Kinetics-400. We also compare VoV3D to other state-of-the-art methods on Kinetics-400. VoV3D-L achieves 76.3%/92.9% Top-1/5 accuracy, and it shows better performance than the state-of-the-art temporal modeling 2D method, TEA <ref type="bibr" target="#b16">[17]</ref>, even without ImageNet pretraining. VoV3D-L also surpasses 3D temporal modeling methods, SlowFast <ref type="bibr" target="#b3">[4]</ref> 4×16 based on ResNet-50 while having about 5× and 4× fewer model parameters and FLOPs, respectively. Compared to ip-CSN-152 <ref type="bibr" target="#b27">[28]</ref> as an efficient 3D CNN, VoV3D-L shows slightly lower Top-1 accuracy, but it achieves higher Top-5 accuracy with much less model capacity. While VoV3D-M shows comparable accuracy with X3D-M, VoV3D-L achieves higher Top-1/Top-5 accuracy.  <ref type="figure">Figure 4</ref>: Robustness to temporal variation. Changing sampling rates (or temporal stride) induces temporal variation. Compared to X3D, VoV3D is more robust to temporal variation due to its temporal modeling ability of T-OSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Robustness Analysis to Temporal Variation</head><p>Inspired by TPN <ref type="bibr" target="#b34">[35]</ref>, we investigate the robustness to temporal variation of VoV3D and X3D. VoV3D-M and X3D-M are trained with the same sampling rate (temporal stride τ ) of 5 on Kinetics-400 with 16 frames as input. At the test phase, we measure the top-1 accuracy drop depending on the change of the sampling rate (e.g., τ ∈ {5, 8, 10, 12, 14, 16}) used for adjusting the visual tempo of a given action instance. The accuracy drop is used for measuring the robustness to temporal variations. <ref type="figure">Fig. 4</ref> shows the accuracy curves of varying visual tempos for VoV3D and X3D. When changing the sampling rate, VoV3D shows less accuracy drop than X3D, which supports the fact that VoV3D is more robust to temporal variation and thus has a better ability to model temporal relationship across frames than X3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed an efficient and effective temporal modeling 3D architecture, called VoV3D, that consists of Temporal One-Shot Aggregation (T-OSA) and depthwise spatiotemporal factorized module, D(2+1)D. The T-OSA is able to effectively model various visual tempos by aggregating features having different temporal receptive fields. The D(2+1D) module decomposes 3D depthwise convolution into a spatial and temporal depthwise convolution, which makes the proposed VoV3D significantly lightweight and efficient while improving accuracy. Thanks to T-OSA and D(2+1)D, our VoV3D outperforms the state-of-the-art 2D efficient CNN as well as 3D CNN methods in terms of temporal modeling, with lower computational complexity. We hope that it can serve as a strong baseline for video action recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>VoV3D has Temporal One-Shot Aggregation (T-OSA) building blocks. T-OSA consists of depthwise spatiotemporal factorized modules, D(2+1)D. Please refer to the details of the VoV3D architecture in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Depthwise spatiotemporal factorization. k, t, c denote the size of spatial kernel, temporal kernel, and channel in 3D convolution, respectively. Compared to Spatiotemporal factorization (top) as in R(2+1)D [29], depthwise spatiotemporal factorization (bottom) in the proposed D(2+1)D further decomposes the features along the channel axis, which improves the efficiency of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 :</head><label>2</label><figDesc>VoV3D architectures: VoV3D-M and VoV3D-L. T denotes the number of input frames. VoV3D has two types of models: VoV3D-M and VoV3D-L. They are comprised of Temporal One-Shot Aggregation (T-OSA) building blocks made of D(2+1)D modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>This work was supported by Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2020-0-00004, Development of Previsional Intelligence based on Long-term Visual Memory Network and No.B0101-15-0266, Development of High Performance Visual BigData Discovery Platform for Large-Scale Realtime Data Analysis)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of parameters and computation.This table considers only a 3D convolution located in the middle of the bottleneck. t, k, and s denote temporal, spatial kernel size, and stride, respectively. C, H, W , T denote channel, height, width, the number of frames in the input 3D feature map, assuming input/output channel size is same.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>+2.4 76.7 +1.4 48.5 +2.3 76.9 +1.6 49.0 +2.6 78.2 +2.9</figDesc><table><row><cell>Model</cell><cell>T-OSA D(2+1)D Top-1</cell><cell>Top-5</cell></row><row><cell>Baseline (M)</cell><cell>46.4</cell><cell>75.3</cell></row><row><cell cols="2">48.0 Baseline (L) 47.1</cell><cell>76.5</cell></row><row><cell></cell><cell cols="2">48.9 +1.8 77.6 +1.1</cell></row><row><cell></cell><cell cols="2">48.8 +1.7 77.4 +0.9</cell></row><row><cell></cell><cell cols="2">49.6 +2.5 78.1 +1.6</cell></row><row><cell></cell><cell></cell><cell>contains 108k videos with</cell></row><row><cell></cell><cell></cell><cell>174 categories, and the second release (V2) of the dataset</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Contributions of the proposed components inVoV3D on Something-Something V1.</figDesc><table><row><cell>VoV3D-M</cell><cell cols="4">Param. GFLOPs Top-1 Top-5</cell></row><row><cell>(a) bottleneck [7]</cell><cell cols="2">42.9M 103.2 × 6</cell><cell>48.6</cell><cell>76.8</cell></row><row><cell>(b) R(2+1)D [29]</cell><cell>20.9M</cell><cell>48.9 × 6</cell><cell>48.6</cell><cell>77.6</cell></row><row><cell>(c) dw-bottleneck [3, 28]</cell><cell>3.3M</cell><cell>7.0 × 6</cell><cell>48.0</cell><cell>76.7</cell></row><row><cell>(d) D(1+2)D (ours)</cell><cell>3.2M</cell><cell>6.5 × 6</cell><cell>48.0</cell><cell>77.2</cell></row><row><cell>(e) D(2+1)D (ours)</cell><cell>3.2M</cell><cell>6.4 × 6</cell><cell>49.0</cell><cell>78.2</cell></row><row><cell>X3D-M [3]</cell><cell>3.3M</cell><cell>6.1 × 6</cell><cell>46.4</cell><cell>75.3</cell></row><row><cell>X3D-M [3] w/ D(2+1)D</cell><cell>3.2M</cell><cell>5.8 × 6</cell><cell>47.4</cell><cell>75.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison to different bottleneck architectures on Something-Something V1.is increased to 220k videos. Kinetics-400 dataset<ref type="bibr" target="#b12">[13]</ref> includes 400 categories and provides download URL links over 240k training and 20k validation videos. Because of the expirations of some YouTube links, we collect 234,619 training and 19,761 validation videos. For the fair comparison with X3D, we train X3D and VoV3D on the same collected Kinetics-400 dataset by ourselves.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison with the state-of-the-art architectures on Something-Something V1&amp; V2 validation set. Note that Something-Something dataset requires more temporal relationship than Kinetics-400<ref type="bibr" target="#b12">[13]</ref> (appearance-oriented). For fair comparison, X3D and VoV3D are trained with the same training protocols on PySlowFast<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>shows the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparison with the state-of-the-art architectures on Kinetics-400. IN denotes ImageNet pretraining.</figDesc><table><row><cell>Note that both VoV3D and X3D are trained with the same</cell></row><row><cell>training protocols on the same environment such as GPU</cell></row><row><cell>server, training set, and scale size [256, 320] and imple-</cell></row><row><cell>mented on PySlowFast [2].</cell></row><row><cell>VoV3D-L with 32 frames pretrained on Kinetics-400 sur-</cell></row><row><cell>passes that of the best model among 2D CNN methods,</cell></row><row><cell>TEA [17] by a large margin (2.4% / 2.3% @Top-1) on</cell></row><row><cell>both SSv1/v2, while having about 14× fewer computa-</cell></row><row><cell>tion.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pyslowfast</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/slowfast" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Resource efficient 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Köpüklü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neslihan</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Gunduz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An energy and gpu-computation efficient backbone network for real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joong-Won</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangrok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Centermask: Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">Wenyu Liu, and Jingdong Wang. High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<title level="m">Platform-aware neural architecture search for mobile</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kaiming He, Christoph Feichtenhofer, and Philipp Krahenbuhl. A multigrid method for efficiently training video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal pyramid network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic temporal pyramid network: A closer look at multi-scale modeling for activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
