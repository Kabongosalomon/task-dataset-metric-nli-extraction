<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MetricGAN: Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Wei</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Feng</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shou-De</forename><surname>Lin</surname></persName>
						</author>
						<title level="a" type="main">MetricGAN: Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adversarial loss in a conditional generative adversarial network (GAN) is not designed to directly optimize evaluation metrics of a target task, and thus, may not always guide the generator in a GAN to generate data with improved metric scores. To overcome this issue, we propose a novel MetricGAN approach with an aim to optimize the generator with respect to one or multiple evaluation metrics. Moreover, based on Metric-GAN, the metric scores of the generated data can also be arbitrarily specified by users. We tested the proposed MetricGAN on a speech enhancement task, which is particularly suitable to verify the proposed approach because there are multiple metrics measuring different aspects of speech signals. Moreover, these metrics are generally complex and could not be fully optimized by L p or conventional adversarial losses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b11">(Goodfellow et al., 2014)</ref> has shown its powerful generative ability in many different applications. In particular, for conditional GANs (CGANs) <ref type="bibr" target="#b28">(Mirza &amp; Osindero, 2014)</ref>, in addition to the adversarial loss, there is an L p loss, to guide the learning of generators. Ideally, the adversarial loss should make generated data indistinguishable from real (target) data. However, some applications of image <ref type="bibr" target="#b21">(Ledig et al., 2017;</ref> and speech processing <ref type="bibr" target="#b33">(Pandey &amp; Wang, 2018;</ref><ref type="bibr" target="#b42">Wang &amp; Chen, 2018;</ref><ref type="bibr" target="#b5">Donahue et al., 2018;</ref><ref type="bibr" target="#b27">Michelsanti &amp; Tan, 2017)</ref> show that this loss term provides very marginal improvement (sometimes even degrade the performance) in terms of objective evaluation scores (in the case of image processing, the subjective score can be improved). For in-Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s). stance, <ref type="bibr" target="#b5">Donahue et al. (2018)</ref> applied CGAN on speech enhancement (SE) for automatic speech recognition (ASR); however, the following conclusion was obtained: "Our experiments indicate that, for ASR, simpler regression approaches may be preferable to <ref type="bibr">GAN-based enhancement."</ref> This may be because the method that the discriminator uses to judge whether each sample is real or fake is not fully related to the metrics that we consider. In other words, similar to L p loss, the way the adversarial loss guides the generator to generate data is still not matched to the evaluation metrics. We call this problem discriminator-evaluation mismatch (DEM). In this study, we propose a novel MetricGAN to solve this problem. We tested the proposed approach on the SE task because the metrics for SE are generally complex and difficult to directly optimize or adjust.</p><p>For human perception, the primary goal of SE is to improve the intelligibility and quality of noisy speech <ref type="bibr" target="#b2">(Benesty et al., 2005)</ref>. To evaluate a SE model in different aspects, several objective metrics have been proposed. Among the human perception-related objective metrics, the perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b35">(Rix et al., 2001</ref>) and short-time objective intelligibility (STOI) <ref type="bibr" target="#b38">(Taal et al., 2011)</ref> are two popular functions to evaluate speech quality and intelligibility, respectively. The design of these two metrics considers human auditory perception and has shown higher correlation to subjective listening tests than simple L 1 or L 2 distance between clean and degraded speech.</p><p>In recent years, various deep learning-based models have been developed for SE <ref type="bibr" target="#b23">(Lu et al., 2013;</ref><ref type="bibr" target="#b46">Xu et al., 2014;</ref><ref type="bibr" target="#b44">Wang et al., 2014;</ref><ref type="bibr" target="#b47">Xu et al., 2015;</ref><ref type="bibr" target="#b32">Ochiai et al., 2017;</ref><ref type="bibr" target="#b24">Luo &amp; Mesgarani, 2018;</ref><ref type="bibr" target="#b12">Grais et al., 2018;</ref><ref type="bibr" target="#b10">Germain et al., 2018;</ref><ref type="bibr" target="#b3">Chai et al., 2018;</ref><ref type="bibr" target="#b4">Choi et al., 2019)</ref>. Most of these models were trained in a supervised fashion by preparing pairs of noisy and clean speeches. The deep models were then optimized by minimizing the distance between generated speech and clean speech. However, the distance (objective function) is usually based on simple L p loss (where p = 1 or 2), which does not reflect human auditory perception or ASR accuracy <ref type="bibr" target="#b1">(Bagchi et al., 2018)</ref> well. In fact, several researches have indicated that an enhanced speech with a smaller L p distance, does not guarantee a higher quality or intelligibility score <ref type="bibr" target="#b7">(Fu et al., 2018b;</ref><ref type="bibr" target="#b18">Koizumi et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1905.04874v1 [cs.SD] 13 May 2019</head><p>Therefore, optimizing the evaluation metrics (i.e., STOI, PESQ, etc.) may be a reasonable direction to connect the model training with the goal of SE. Some latest studies <ref type="bibr" target="#b7">(Fu et al., 2018b;</ref><ref type="bibr" target="#b18">Koizumi et al., 2018;</ref><ref type="bibr" target="#b48">Zhang et al., 2018;</ref><ref type="bibr" target="#b49">Zhao et al., 2018a;</ref><ref type="bibr" target="#b30">Naithani et al., 2018;</ref><ref type="bibr" target="#b19">Kolbaek et al., 2018;</ref><ref type="bibr" target="#b50">Zhao et al., 2018b)</ref> have focused on STOI score optimization to improve speech intelligibility. A waveform based utterance-level enhancement manner is proposed to optimize the STOI score <ref type="bibr" target="#b7">(Fu et al., 2018b)</ref>. The results of a listening test showed that by combining STOI with MSE as an objective function, the speech intelligibility can be further increased. On the other hand, because the PESQ function is not fully differentiable and significantly more complex compared with STOI, only few <ref type="bibr" target="#b18">(Koizumi et al., 2018;</ref><ref type="bibr" target="#b48">Zhang et al., 2018;</ref><ref type="bibr" target="#b17">Koizumi et al., 2017;</ref><ref type="bibr" target="#b26">Martín-Doñas et al., 2018)</ref> have considered it as an objective function. Reinforcement learning (RL) techniques such as deep Q-network (DQN) and policy gradient were employed to solve non-differentiable problems, as <ref type="bibr" target="#b17">(Koizumi et al., 2017)</ref> and <ref type="bibr" target="#b18">(Koizumi et al., 2018)</ref>, respectively.</p><p>In summary, the abovementioned existing techniques can be categorized into two types depending on whether the details of evaluation metrics have to be obtained: (1) white-box: these methods approximate the complex evaluation metrics with a hand-crafted, simpler one; thus, it is differentiable and easy to be applied as a loss function. However, the details of the metrics have to be known; (2) black-box: these methods mainly treat the metrics as a reward and apply RL-based techniques to increase the scores. However, because of less efficiency in training, most of them have to be pre-trained by conventional supervised learning.</p><p>In this study, to solve the drawbacks of the abovementioned methods and the DEM problem, the discriminator in GAN is associated with the evaluation metrics of interest (Although these evaluation functions are complex, <ref type="bibr" target="#b6">Fu et al. (2018a)</ref> showed that they can be approximated by neural networks). In particular, when training the discriminator, instead of always giving a false label (e.g., "0") to the generated speech, the labels of MetricGAN are given according to the evaluation metrics. Therefore, the target space of discriminator transforms from discrete (1 (true) or 0 (false)) to continuous (evaluation scores). Through this modification, the discriminator can be treated as a learned surrogate of the evaluation metrics. In other words, the discriminator iteratively estimates a surrogate loss that approximates the sophisticated metric surface, and the generator uses this surrogate to decide a gradient direction for optimization. Compared with previous existing methods, the main advantages of Metric-GAN are as follows:</p><p>(1) The surrogate function (discriminator) of the complex evaluation metrics is learned from data. In other words, it is still in a black-box setting and no computational details of the metric function have to be known.</p><p>(2) Experiment result shows that the training efficiency of MetricGAN to increase metric score is even higher than conventional supervised learning with L p loss.</p><p>(3) Because the label space of the discriminator is now continuous, any desired metric scores can be assigned to the generator. Therefore, MetricGAN has the flexibility to generate speech with specific evaluation scores.</p><p>(4) Under some non-extreme conditions, MetricGAN can even achieve multi-metrics assignments by employing multiple discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CGAN for SE</head><p>GAN has recently attracted a significant amount of attention in the community. By employing an alternative mini-max training scheme between a generator network (G) and a discriminator network (D), adversarial training can model the distribution of real data. One of its applications is to serve as a trainable objective function for a regression task. Instead of explicitly minimizing the L p losses, which may cause over smoothing problems, D provides a high-level abstract measurement of realness <ref type="bibr" target="#b22">(Liao et al., 2018)</ref>.</p><p>In the applications of GAN on SE, CGAN is usually employed to generate enhanced speech. To achieve this, G is trained to map noisy speech x to its corresponding clean speech y by minimizing the following loss function (as in <ref type="bibr" target="#b34">(Pascual et al., 2017)</ref>. The least-squares GAN (LSGAN) approach <ref type="bibr" target="#b25">(Mao et al., 2017)</ref> is used with binary coding (1 for real, 0 for fake)):</p><formula xml:id="formula_0">L G(CGAN ) = E x [λ(D(G(x), x)−1) 2 ]+||G(x)−y|| 1 (1)</formula><p>Because G usually simply learned to ignore the noise prior z in the CGAN <ref type="bibr" target="#b15">(Isola et al., 2017)</ref>, we directly neglected it here. The first term in Eq. (1) is called adversarial loss for cheating D with a weighting factor λ. The goal of D is to distinguish between real data and generated data by minimizing the following loss function:</p><formula xml:id="formula_1">L D(CGAN ) = E x,y [(D(y, x) − 1) 2 + (D(G(x), x) − 0) 2 ]<label>(2)</label></formula><p>We argue that to optimize the metric scores, the training of D should be associated with the metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MetricGAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Associating the Discriminator with the Metrics</head><p>The main difference between the proposed MetricGAN and the conventional CGAN is how the discriminator is trained.</p><p>Here, we first introduce a function Q(I) to represent the evaluation metric to be optimized, where I is the input of the metric. For example, for PESQ and STOI, I is the pair of speech that we want to evaluate and the corresponding clean speech y. Therefore, to ensure that D behaves similar to Q, we simply modify the objective function of D:</p><formula xml:id="formula_2">L D(M etricGAN ) = E x,y [(D(y, y) − Q(y, y)) 2 + (D(G(x), y) − Q(G(x), y)) 2 ]<label>(3)</label></formula><p>Because we can always map Q to Q , which is between 0 and 1 (here, 1 represents the best evaluation score), Eq. <ref type="formula" target="#formula_2">(3)</ref> can be reformulated as</p><formula xml:id="formula_3">L D(M etricGAN ) = E x,y [(D(y, y) − 1) 2 + (D(G(x), y) − Q (G(x), y)) 2 ]<label>(4)</label></formula><p>where 0 ≤ Q (G(x), y) ≤ 1. There are two main differences between Eq. (4) and Eq. <ref type="formula" target="#formula_1">(2):</ref> 1.) In CGAN, as long as the data is generated, its label for D is always a constant 0. However, the target label of the generated data in our MetricGAN is based on its metric score. Therefore, D can evaluate the degree of realness (clean speech), instead of just distinguishing real and fake. (Therefore, maybe "D" should be called an evaluator; however, here we just follow the convention of GAN.)</p><p>2.) The condition used in the D of CGAN is the noisy speech x, which is different from the condition used in the proposed MetricGAN (clean speech y). This is because we want D and Q to have similar behavior. Therefore, the input argument of D is chosen to be the same as Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Continuous Space of the Discriminator Label</head><p>The training of G is similar to Eq. (1). However, we found that the gradient provided by D in our MetricGAN is more efficient than the L p loss. Therefore, the training of G can completely rely on the adversarial loss :</p><formula xml:id="formula_4">L G(M etricGAN ) = E x [(D(G(x), y) − s) 2 ]<label>(5)</label></formula><p>where s is the desired assigned score. For example, to generate clean speech, we can simply assign s to be 1. On the contrary, we can also generate more noisy speech by assigning a smaller s. This flexibility is caused by the label of the generated speech in D, which is now continuous and related to the metric. Unlike surrogate loss learning in the multi-class classification <ref type="bibr" target="#b13">(Hsieh et al., 2018)</ref>, because the output space of our G is continuous, the local neighbors need not be explicitly selected to learn the behavior of metric surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Explanation of MetricGAN</head><p>In MetricGAN, the target of G is to cheat D to reach specified score, and D tries to not be cheated by learning the true score. Here, we also explain the learning process of MetricGAN in a different manner. As shown in <ref type="figure">Figure 1</ref>, training of D can be treated as learning a local surrogate of Q; and training of G is to adjust its weights W G toward the optimum value of D. Because D may only approximate Q well in the observed region <ref type="bibr" target="#b8">(Fu et al., 2019)</ref>, this learning framework should be alternatively trained until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Architecture</head><p>The input features x for G is the normalized noisy magnitude spectrogram utterance. The generator used in this experiment is a BLSTM <ref type="bibr" target="#b45">(Weninger et al., 2015)</ref> with two bidirectional LSTM layers, each with 200 nodes, followed by two fully connected layers, each with 300 LeakyReLU nodes and 257 sigmoid nodes for mask estimation, respectively. When this mask (between 0 to 1) is multiplied with the noisy magnitude spectrogram, the noise components should be removed. In addition, as reported in <ref type="bibr" target="#b18">(Koizumi et al., 2018)</ref>, to prevent musical noise, flooring was applied to the estimated mask before T-F-mask processing. Here, we used the lower threshold of the T-F mask as 0.05.</p><p>The discriminator herein is a CNN with four twodimensional (2-D) convolutional layers with the number of filters and kernel size as follows: <ref type="bibr">[15, (5, 5)</ref>], <ref type="bibr">[25, (7, 7)</ref>], <ref type="bibr">[40, (9, 9)</ref>], and <ref type="bibr">[50, (11, 11)</ref>]. To handle the variablelength input (different speech utterance has different length), a 2-D global average pooling layer was added such that the features can be fixed at 50 dimensions (50 is the number of feature maps in the previous layer). Three fully connected layers were added subsequently, each with 50 and 10 LeakyReLU nodes, and 1 linear node. In addition, to make D a smooth function (we do not want a small change in the input spectrogram can result in a significant difference to the estimated score), it is constrained to be 1-Lipschitz continuous by spectral normalization <ref type="bibr" target="#b29">(Miyato et al., 2018)</ref>. Our preliminary experiments found that adding this constraint can stabilize the training of D. All models are trained using Adam <ref type="bibr" target="#b16">(Kingma &amp; Ba, 2014)</ref> with β 1 = 0.9 and β 2 = 0.999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment on the TIMIT Dataset</head><p>In this section, we show the experiments about PESQ and STOI scores. PESQ was designed to evaluate the quality of processed speech, and the score ranges from -0.5 to 4.5. STOI was designed to compute the speech intelligibility, and the score ranges from 0 to 1. Both the two metrics are the higher the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">DATASET</head><p>In this experiments, the TIMIT corpus <ref type="bibr" target="#b9">(Garofolo et al., 1988)</ref> was used to prepare the training, validation, and test sets. 300 utterances were randomly selected from the training set of the TIMIT database for training in this experiment. These utterances were further corrupted with 10 noise types (crowd, 2 machine, alarm and siren, traffic and car, animal sound, water sound, wind, bell, and laugh noise) from (Hu), at five SNR levels (from -8 dB to 8 dB with steps of 4 dB) to form 15000 training utterances. To monitor the training process and choose the hyperparameters, we randomly selected another clean 100 utterances from the TIMIT training set to form our validation set. Each utterance was further corrupted with one of the noise types (different from those already used in the training set) from (Hu) at five different SNR levels (from -10 dB to 10 dB with steps of 5 dB). To evaluate the performance of different training methods, 100 utterances from the TIMIT test set were randomly selected as our test set. These utterances were mixed with four un-seen noise types (engine, white, street, and baby cry), at five SNR levels (-12 dB, -6 dB, 0 dB, 6 dB, and 12 dB). In summary, 2000 utterances exist in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">OBJECTIVE EVALUATION WITH DIFFERENT LOSS FUNCTIONS</head><p>In this experiment, to evaluate the performance of different objective functions, the structure of G is fixed and trained with different losses. As one of our baseline models, we adopt ideal ratio mask (IRM) <ref type="bibr" target="#b31">(Narayanan &amp; Wang, 2013)</ref> based mask estimation with L 1 loss (denoted as IRM <ref type="formula">(L1)</ref>).</p><p>The other baseline (denoted as IRM (CGAN)) is the CGAN with the loss function of G shown in Eq. (1). Compared to IRM (L1), IRM (CGAN) has an additional adversarial loss term with λ = 0.01 as in <ref type="bibr" target="#b1">(Bagchi et al., 2018;</ref><ref type="bibr" target="#b34">Pascual et al., 2017)</ref>. A parameter exploring policy gradients <ref type="bibr" target="#b36">(Sehnke et al., 2010)</ref> based black-box optimization, which is similar to the one used in <ref type="bibr" target="#b48">(Zhang et al., 2018)</ref>, is also compared. However, we found that this method is very sensitive to the hyperparameters (e.g., weight initialization, step size of jitter, etc.). We could only obtain improved results for PESQ optimization (denoted as PE policy grad (P)). In addition, because of the lower training efficiency, its generator was first pre-trained from IRM (L1). The proposed MetricGAN with PESQ or STOI metric as Q, is indicated as MetricGAN (P) and MetricGAN (S), respectively. <ref type="table" target="#tab_1">Table 1</ref> presents the results of the average PESQ and STOI scores on the test set for the baselines and proposed methods. From this table, we can first observe that the performance of IRM (CGAN) is similar to or slightly worse than the simple IRM (L1), which is in agreement with the results presented in previous papers. <ref type="bibr" target="#b33">(Pandey &amp; Wang, 2018;</ref><ref type="bibr" target="#b5">Donahue et al., 2018)</ref>. This implies that the adversarial loss term used to cheat D is not helpful in this application. One possible reason for this result may be that the decision boundary of D is very different from the metrics we consider. We also attempted to train IRM (CGAN) with larger λ; however, their evaluation scores were worse than the reported scores. Although PE policy grad (P) can obtain some PESQ scores improvements, the STOI scores decreased compared to its initialization, IRM (L1). On the contrary, when we employed PESQ as Q in our MetricGAN, it could achieve the highest PESQ scores among all the models with the second highest STOI score. Note that unlike L p loss, the loss function of G in MetricGAN is Eq. (5), and there is no specific target for each T-F bin. In terms of the STOI score, MetricGAN (S) outperforms the other models, and the improvement is most evident for the low SNR conditions (where speech intelligibility improvement is most critical).</p><p>In addition to the final results of the test set, the learning process of different loss functions evaluated on the validation set are also presented in <ref type="figure" target="#fig_0">Figure 2</ref>. For both the scores, we can observe that the learning efficiency (in terms of the number of iterations) of MetricGAN is higher than the others. This implies that the gradient provided by D (surrogate of Q) is the most accurate toward the maximum value of Q. However, if the Q used to train MetricGAN does not match the evaluation metric, the performance is sub-optimal. Therefore, the information from Q is important; our preliminary experiment also shows that without Q, the learning cannot converge. The conventional adversarial loss term in  <ref type="figure">Figure 3</ref>. Results of AB preference test (with 95% confidence intervals) on speech quality compared between proposed Metric-GAN(P) and the two baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IRM (CGAN)</head><p>is not helpful for improving the scores and training efficiency.</p><p>Finally, an example of the enhanced spectrograms by different training objective functions are shown in <ref type="figure" target="#fig_1">Figure 4</ref>. The spectrogram generated by IRM (CGAN) is similar to that of IRM (L1). If we simply increase the weight λ of the adversarial loss term in Eq.(1), some unpleasant artifacts begin to appear (this is not shown here, owing to limited space). Interestingly, in comparison to others, the spectrogram (f) generated by MetricGAN (S) can best recover the speech components with clear structures (as shown by the black-dashed rectangles) and hence, obtain the highest STOI score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">SUBJECTIVE EVALUATION</head><p>To evaluate the perceptual quality of the enhanced speech, we conducted AB preference tests to compare the proposed method with the baseline models. Three pairs of listening tests were conducted: IRM(CGAN) versus IRM(L1), MetricGAN(P) versus IRM(CGAN), and MetricGAN(P) versus IRM(L1). Each pair of samples are presented in a randomized order. For each listening test, 20 sample pairs were randomly selected from the test set; 15 listeners participated. Listeners were instructed to select the sample with the better quality. The stimuli were played to the subjects in a quiet environment through a set of Sennheiser HD headphones at a comfortable listening level. In <ref type="figure">Figure 3 (a)</ref>, we can observe that the preference score between IRM (L1) and IRM (CGAN) overlap in the confidence interval, which is in agreement with the result of the objective evaluation. Further, as shown in <ref type="figure">Figure 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">ASSIGNING ANY DESIRED SCORE TO THE GENERATOR</head><p>Because the label of D in the conventional GAN is in a discrete space, there is no guarantee that the generated data can gradually improve toward real data when the label assigned to G (i.e., the constant in the first term of Eq.(1)) increases from 0 (fake) toward 1 (real). For example, the generated data from label 0.9 is not necessarily better (more like real data) than that from label 0.8. However, as pointed out in section 3.2, because the output of D in MetricGAN is continuous according to Q, we can assign any desired score during the training of G as in Eq. (5). Therefore, different s in Eq. (5) correspond to generated speech with different qualities. Interestingly, setting s as a small value can convert the generator from a speech enhancement model to a speech degradation model. This provides us with another method to understand the factors that affect the metric. To achieve this, a uniform mask constraint (penalize estimated mask away from 0.5) was also applied to G so that G has to choose the most efficient way to attain the assigned score s without significantly changing the initialized mask. (Owing to the sigmoid activation used in the output layer of G, all the initially estimated mask values were close to 0.5). <ref type="figure" target="#fig_2">Figure 5</ref> shows an example of assigning different s to G, and the learning process evaluated on the validation set is also illustrated in <ref type="figure" target="#fig_2">Figure 5</ref> (c) and (g). Compared to the generation of clean speech (the entire learning process for generating clean speech is presented in <ref type="figure" target="#fig_0">Figure 2</ref>), Metric-GAN can attain the desired score more easily when s is small. This phenomenon is because the number of solutions decreases gradually when s increases (it is easier to obtain noisy speech than a clean speech). Therefore, the solution for a large s is considerably difficult to obtain. <ref type="figure" target="#fig_2">Figures 5  (d</ref>) to (f) and (h) to (j) present the generated speech by assigning different s with STOI and PESQ as Q, respectively. Intriguingly, the speech components gradually disappear when we attempt to generate a speech with low STOI score (the speech components are almost removed as shown by the black rectangle in <ref type="figure" target="#fig_2">Figure 5 (f)</ref>). Because STOI measures the intelligibility of speech, it is reasonable that the speech component is most crucial in this metric. On the contrary, because PESQ measures the quality of speech, the generated speech with lower s seems to become more noisy (for extremely low s values ( <ref type="figure" target="#fig_2">Figure 5 (j)</ref>), in spite of not as serious as the STOI case, there is also some speech components being removed). These results verify that the MetricGAN can generate data according to the designate metric score and make the label space of D continuous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5.">MULTI-METRIC SCORES ASSIGNMENT</head><p>In this section, we further explore the assignment of scores for multiple metrics simultaneously. Compared with single metric assignment, this is a more difficult task because the requirement to achieve other metrics can be treated as adding constraints. (h) generated speech, s=4.5 (i) generated speech, s=1.5 (j) generated speech, s=1.0 (g) Algorithm 1 Multi-Metric Scores Assignment Input: desired score s 1 for metric Q 1 (.) to s N for metric Q N (.) (assume there are N different metrics). repeat 1) Find metric index i with the largest distance between achieved and assigned score: i = argmax n |Q n (G(x), y) − s n | 2) Train G to minimize the loss from D i :</p><formula xml:id="formula_5">L G(M etricGAN ) = E x [(D i (G(x), y) − s i ) 2 ]</formula><p>3) Train all D n to minimize the distance from Q n :</p><formula xml:id="formula_6">L D(M etricGAN ) = E x,y [(D n (y, y) − 1) 2 + (D n (G(x), y) − Q n (G(x), y)) 2 ] until converge</formula><p>Algorithm 1 shows the proposed training method for multimetric scores assignment. Assuming that there are N different metrics, we have to employee N discriminators. In each iteration, only D with the largest distance between achieved score, Q n (G(x), y), and assigned score, s n , would guide the learning of G (steps 1 and 2). However, in the training of D, all the discriminators D n are updated, irrespective of whether it is used to provide loss to G (step 3). <ref type="figure" target="#fig_3">Figure 6</ref> shows the learning curves for the case of N =2. To explore more possible combinations, these results are based on the subset (top 10% metric score) of the original validation set. To clearly illustrate the results of multi-metric learning, in each column of this figure, the assignment of STOI score is fixed with different PESQ scores. Because different metrics may have some positive correlation between each other, MetricGAN is difficult to converge when the score assignments are too extreme (in this case, the solution may not even exist). However, we still obtain some flexibility to generate speech with desired multiple scores. This experiment verifies that MetricGAN can approximate and distinguish different metrics well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Other State-of-the-Art SE Models</head><p>To further compare the proposed MetricGAN with other state-of-the-art methods, we use a publicly available dataset released by <ref type="bibr" target="#b39">(Valentini-Botinhao et al., 2016)</ref>. This dataset contains a large amount of pre-mixed noisy-clean paired data and is already used by several SE models. By using the  <ref type="figure">figure)</ref>. Given a specified STOI score, the upper row and lower row is the maximum and minimum PESQ scores MetricGAN can reach, respectively. Note that the PESQ score is normalized between 0 to 1 with the original score shown in the parentheses. exact same training and test dataset split, we can establish a fair comparison with them easily.</p><p>Experimental Setup and Results: Details about the data can be found in the original paper. Except for input features and activation functions, the network architecture and training strategy are the same as described in the previous section. In addition to the PESQ score, we also report another three metrics over the test set to compare with previous works: CSIG predicts the mean opinion score (MOS) of the signal distortion, CBAK predicts the MOS of the background noise interferences, and COVL predicts the MOS of the overall speech quality, these three metrics range from 1 to 5.</p><p>Five baseline models that rely on another network to provide loss information are compared with the proposed Met-ricGAN (P). We briefly explain these models as follows: SEGAN <ref type="bibr" target="#b34">(Pascual et al., 2017)</ref> directly operates on the raw waveform and the model is trained to minimize the combination of adversarial and L 1 losses. MMSE-GAN <ref type="bibr" target="#b37">(Soni et al., 2018)</ref> is a time-frequency masking-based method that uses a GAN objective along with L 2 loss. Similar to the structure of SEGAN, WGAN-GP and SERGAN <ref type="bibr" target="#b0">(Baby &amp; Verhulst, 2019)</ref> introduced Wasserstein loss and relativistic least-square loss for GAN training, respectively. Finally, Deep Feature Loss <ref type="bibr" target="#b10">(Germain et al., 2018)</ref> also operates on the raw waveform and is trained with a deep feature loss from another network that classifies acoustic environments. <ref type="table" target="#tab_4">Table 2</ref> summarizes that our proposed method outperforms all previous works with respect to three metrics. This implies that although MetricGAN is only trained to optimize a certain score (PESQ), it also has a great generalization ability to other metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel MetricGAN approach to directly optimize generators based on one or multiple evaluation metric scores. By associating a discriminator with the metrics of interest, MetricGAN can be treated as an iterative process between surrogate loss learning and generator learning. This surrogate can successfully capture the behavior of the metrics and provides accurate gradients guiding the generator updates. In addition to outperforming other loss functions and state-of-the-art models in SE, MetricGAN can also be trained to generate data according to the designate metric scores. To the best of our knowledge, this is the first work that employs GAN to directly train the generator with respect to multiple evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Learning curve of different objective functions evaluated on the validation set (structure of G is fixed). In terms of: (a) PESQ score and (b) STOI score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>(b) and Figure 3 (c), Metric-GAN(P) significantly outperforms both baseline systems, without an overlap in the confidence intervals. Spectrograms of a TIMIT utterance in the teset set: (a) clean target, (b) noisy speech (engine noise at 0 dB). (c) to (f): enhanced speech with different loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Results of assigning different s to Eq. (5) for the generator training. Note that the learning curves of generating clean speech in (c) and (g) are not yet converged. For more complete learning processes, please refer to Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Learning curves of assigning different pairs of (STOI, PESQ) scores (shown in the title of each</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Learning process of MetricGAN to optimize the evaluation metric Q (the horizontal axis represents the weights of G). In each iteration, there are three steps. First, some data are generated by G with weights W G(t) . Second, training D to have similar behavior as metric Q in those points. Third, training G according to the gradient provided by D (cheat D).</figDesc><table><row><cell>Evaluation metrics Q</cell><cell>5. Training D at t+1</cell><cell>4. Generated</cell></row><row><cell></cell><cell>3. Training G</cell><cell>data at t+1</cell></row><row><cell></cell><cell>at t</cell><cell></cell></row><row><cell></cell><cell cols="2">2. Training D at t</cell></row><row><cell></cell><cell>1. Generated</cell><cell></cell></row><row><cell></cell><cell>data at iteration t</cell><cell></cell></row><row><cell>Figure 1.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance comparisons of different loss functions in terms of PESQ and STOI (* represents pre-trained from another model).</figDesc><table><row><cell></cell><cell>Noisy</cell><cell cols="2">IRM (L1)</cell><cell>IRM (CGAN)</cell><cell cols="2">PE policy grad*(P)</cell><cell cols="2">MetricGAN (P)</cell><cell cols="2">MetricGAN (S)</cell></row><row><cell>SNR (dB)</cell><cell>PESQ STOI</cell><cell cols="2">PESQ STOI</cell><cell>PESQ STOI</cell><cell>PESQ</cell><cell>STOI</cell><cell>PESQ</cell><cell>STOI</cell><cell>PESQ</cell><cell>STOI</cell></row><row><cell>12</cell><cell>2.375 0.919</cell><cell cols="2">2.913 0.935</cell><cell>2.879 0.936</cell><cell>2.995</cell><cell>0.927</cell><cell>2.967</cell><cell>0.936</cell><cell>2.864</cell><cell>0.939</cell></row><row><cell>6</cell><cell>1.963 0.831</cell><cell>2.52</cell><cell>0.878</cell><cell>2.479 0.876</cell><cell>2.595</cell><cell>0.869</cell><cell>2.616</cell><cell>0.881</cell><cell>2.486</cell><cell>0.885</cell></row><row><cell>0</cell><cell>1.589 0.709</cell><cell cols="2">2.086 0.787</cell><cell>2.053 0.786</cell><cell>2.144</cell><cell>0.776</cell><cell>2.200</cell><cell>0.796</cell><cell>2.086</cell><cell>0.802</cell></row><row><cell>-6</cell><cell>1.242 0.576</cell><cell cols="2">1.583 0.655</cell><cell>1.551 0.653</cell><cell>1.634</cell><cell>0.644</cell><cell>1.711</cell><cell>0.668</cell><cell>1.599</cell><cell>0.679</cell></row><row><cell>-12</cell><cell>0.971 0.473</cell><cell cols="2">1.061 0.508</cell><cell>1.046 0.507</cell><cell>1.124</cell><cell>0.500</cell><cell>1.169</cell><cell>0.521</cell><cell>1.090</cell><cell>0.533</cell></row><row><cell>Avg.</cell><cell>1.628 0.702</cell><cell cols="2">2.033 0.753</cell><cell>2.002 0.751</cell><cell>2.098</cell><cell>0.743</cell><cell>2.133</cell><cell>0.760</cell><cell>2.025</cell><cell>0.768</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Compared MetricGAN with other state-of-the-art methods. The highest score per metric is highlighted with bold text.</figDesc><table><row><cell></cell><cell>PESQ</cell><cell cols="3">CSIG CBAK COVL</cell></row><row><cell>Noisy</cell><cell>1.97</cell><cell>3.35</cell><cell>2.44</cell><cell>2.63</cell></row><row><cell>SEGAN</cell><cell>2.16</cell><cell>3.48</cell><cell>2.94</cell><cell>2.80</cell></row><row><cell>MMSE-GAN</cell><cell>2.53</cell><cell>3.80</cell><cell>3.12</cell><cell>3.14</cell></row><row><cell>WGAN-GP</cell><cell>2.54</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Deep Feature Loss</cell><cell>-</cell><cell>3.86</cell><cell>3.33</cell><cell>3.22</cell></row><row><cell>SERGAN</cell><cell>2.62</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MetricGAN (P)</cell><cell>2.86</cell><cell>3.99</cell><cell>3.18</cell><cell>3.42</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">National Taiwan University, Taiwan 2 Academia Sinica, Taiwan. Correspondence to: Yu Tsao &lt;yu.tsao@citi.sinica.edu.tw&gt;, Shou-De Lin &lt;sdlin@csie.ntu.edu.tw&gt;.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speech enhancement using relativistic generative adversarial networks with gradient penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verhulst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="106" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Spectral feature mapping with mimic loss for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bagchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Plantinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stiff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09816</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Speech Enhancement</title>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Error modeling via asymmetric laplace distribution for deep neural network based single-channel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3269" to="3273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Phase-aware speech enhancement with deep complex u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring speech enhancement with generative adversarial networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5024" to="5028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Quality-net: An end-to-end non-intrusive speech quality assessment model based on blstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end waveform utterance enhancement for direct evaluation metrics optimization by fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1570" to="1584" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning with learned loss function: Speech enhancement with quality-net to improve perceptual evaluation of speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Getting started with the darpa timit cdrom: An acoustic phonetic continuous speech database. National Institute of Standards and Technology (NIST)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">16</biblScope>
			<pubPlace>Gaithersburgh, MD</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10522</idno>
		<title level="m">Speech denoising with deep feature losses</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Raw multi-channel audio source separation using multiresolution convolutional auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Grais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00702</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A deep model with local surrogate loss for general cost-sensitive multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">100 nonspeech environmental sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dnn-based source enhancement selfoptimized by reinforcement learning using sound quality measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Haneda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="81" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dnn-based source enhancement to increase objective sound quality assessment score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Haneda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Monaural speech enhancement using deep neural networks by maximizing a short-time objective intelligibility measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00604</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Crafting papers on machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Machine Learning</title>
		<editor>Langley, P.</editor>
		<meeting>the 17th International Conference on Machine Learning<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07501</idno>
		<title level="m">Noise adaptive speech enhancement using domain adversarial training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speech enhancement based on deep denoising autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="436" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Surpassing ideal timefrequency masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tasnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07454</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A deep learning loss function based on the perceptual evaluation of the speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Martín-Doñas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Peinado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1680" to="1684" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial networks for speech enhancement and noise-robust speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michelsanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01703</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep neural network based speech separation optimizing an objective estimator of intelligibility for low latency applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Naithani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nikunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bramslow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Acoustic Signal Enhancement (IWAENC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="386" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ideal ratio mask estimation using deep neural networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7092" to="7096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multichannel end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2632" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On adversarial training and loss functions for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5414" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parameter-exploring policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sehnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rückstieß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="551" to="559" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Time-frequency masking-based speech enhancement using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of timefrequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Investigating rnn-based speech enhancement methods for noise-robust text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yamagishi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th ISCA Speech Synthesis Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">End-to-end networks for supervised single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02568</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Performance based cost functions for end-to-end speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Higa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00511</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esrgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00219</idno>
		<title level="m">Enhanced superresolution generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Speech enhancement with lstm recurrent neural networks and its application to noise-robust asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An experimental study on speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="68" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="7" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Training supervised speech separation system to improve stoi and pesq directly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5374" to="5378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Perceptually guided speech enhancement using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5074" to="5078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Convolutional neural networks to enhance coded speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
