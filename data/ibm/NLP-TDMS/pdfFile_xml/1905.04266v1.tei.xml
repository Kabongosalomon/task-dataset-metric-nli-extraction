<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
							<email>aarnab@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
							<email>doersch@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<email>zisserman@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a bundle-adjustment-based algorithm for recovering accurate 3D human pose and meshes from monocular videos. Unlike previous algorithms which operate on single frames, we show that reconstructing a person over an entire sequence gives extra constraints that can resolve ambiguities. This is because videos often give multiple views of a person, yet the overall body shape does not change and 3D positions vary slowly. Our method improves not only on standard mocap-based datasets like Human 3.6M -where we show quantitative improvements -but also on challenging in-the-wild datasets such as Kinetics. Building upon our algorithm, we present a new dataset of more than 3 million frames of YouTube videos from Kinetics with automatically generated 3D poses and meshes. We show that retraining a single-frame 3D pose estimator on this data improves accuracy on both real-world and mocap data by evaluating on the 3DPW and HumanEVA datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding the 3D configuration of the human body has numerous real-life applications in robotics, augmented and virtual reality, and animation, among other fields. However, it is an inherently under-constrained problem when only a single image is available, as there are many 3D poses which project to the same 2D image. Data-driven methods to resolve this ambiguity are promising, but they are typically trained and evaluated on motion capture datasets recorded in constrained and unrealistic environments <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>To resolve some of the ambiguities in monocular 3D pose estimation, we exploit temporal consistency across frames of a video. The temporal dimension of ordinary videos encodes valuable information: multiple views of people are observed, where the body shape and bone lengths remain constant throughout a video, and joint positions in * Equal contribution. † Work done during an internship at DeepMind both 2D and 3D change slowly over time. These priors constrain the space of possible poses and thus help reduce the ambiguity of this ill-posed problem as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Despite its value, the temporal information in mocap datasets is discarded by all current leading 3D pose estimation algorithms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b27">28]</ref> which use only single, ambiguous frames. Our approach incorporates temporal information through a form of bundle adjustment, a method used in multi-view geometry for estimating cameras and 3D structure of rigid scenes from image correspondences <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b47">48]</ref>. We repurpose bundle adjustment to deal with non-rigid (articulated) human motion in a video sequence. In contrast to previous recurrent models for human pose <ref type="bibr" target="#b14">[15]</ref>, our method can jointly reason about all frames in the video, and errors made in initial frames do not accumulate over time. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the current state-of-art single frame estimation network for the SMPL model <ref type="bibr" target="#b19">[20]</ref> fails on a number of frames of "in the wild" videos, such as when there is occlusion, unusual poses, poor lighting or motion blur. Our bundle adjustment method is able to correct these estimates and infer 3D human pose for these frames.</p><p>To address the lack of real-world data in 3D pose estimation, we apply our bundle adjustment framework to "in the wild" clips from the Kinetics dataset <ref type="bibr" target="#b21">[22]</ref> comprised of YouTube videos, and show how we can leverage our predictions on real-world videos as a source of weak supervision to improve existing 3D pose estimation models. By encouraging temporal consistency with bundle adjustment and using YouTube videos as a source of weakly supervised data, we make the following novel contributions:</p><p>First, we show that multi-frame bundle adjustment can be specialized to human pose estimation, which improves performance on the Human 3.6M dataset over single frame estimation. Our method achieves the state-of-the-art for SMPL <ref type="bibr" target="#b25">[26]</ref> models on this dataset.</p><p>We then apply our bundle adjustment method to 107 000 YouTube videos from the Kinetics dataset <ref type="bibr" target="#b21">[22]</ref> and generate a large-scale dataset of 3D human poses aligned with the video frames. This dataset contains great diversity in pose,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Perframe <ref type="bibr" target="#b19">[20]</ref>   <ref type="bibr" target="#b19">[20]</ref> do not use temporal information to constrain the problem. Coupled with the fact that 3D supervision is only available from lab-captured mocap datasets, they often fail on "in the wild" videos, e.g., from Kinetics <ref type="bibr" target="#b21">[22]</ref>. As shown in the second row, the failure modes of <ref type="bibr" target="#b19">[20]</ref> vary even though the image has barely changed. Our proposed bundle adjustment considers all frames in the video jointly and uses temporal coherence to prevent major failures (column 2 and 3) and to resolve ambiguities (column 5). We then apply our method on YouTube videos to obtain weakly-supervised data to improve per-frame methods. Note that we are only showing 5 out of 190 frames in the clip. Best viewed in colour on screen.</p><p>with 400 different human actions, and is available publicly 1 . As we are fitting SMPL body models <ref type="bibr" target="#b25">[26]</ref> to the data, other information such as 2D keypoints and body-part segmentations can also be obtained automatically as done by <ref type="bibr" target="#b22">[23]</ref>.</p><p>By retraining the single-frame 3D pose estimator using our automatically-generated dataset, we obtain a more robust network that performs better on real-world (3DPW <ref type="bibr" target="#b51">[52]</ref>) and mocap (HumanEVA <ref type="bibr" target="#b41">[42]</ref>) datasets. We are thus the first paper, to our knowledge, to show how we can use masses of unlabelled real-world data to improve 3D pose estimation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D human pose is typically represented in the literature as either a point cloud of 3D joint positions or the parameters of a body model. A common approach with the former representation is to "lift" 2D keypoints (either ground truth or from a 2D pose detector) to 3D. This has been recently done with neural networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b30">31]</ref> and previously using a dictionary of 3D skeletons <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b53">54]</ref> or other priors <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b1">2]</ref> to constrain the problem. The point cloud representation also allows one to train a CNN to regress directly from an image (instead of 2D keypoints) to 3D joints using supervision from motion capture datasets like Human 3.6M <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b33">34]</ref>. However, this approach overfits to the constrained environments of lab-captured motion capture datasets and does not generalise well to real-world images. Whilst methods based on "lifting" are more robust to this 1 https://github.com/deepmind/ Temporal-3D-Pose-Kinetics domain shift, they discard valuable information from the image as they depend solely on the input 2D keypoints.</p><p>Training models with supervision from both 2D keypoints (from real-world datasets such as <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18]</ref>) and 3D joints (from mocap datasets) has been shown to help with generalisation to real-world images <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45</ref>]. However, greater success has been achieved in this scenario by fitting parametric models of human body meshes to images. Human body models, such as <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b4">[5]</ref>, encapsulate more prior knowledge, thus reducing the ambiguity of the 3D pose estimation problem. Explicit priors such as bone length ratios remaining constant <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b8">9]</ref> and limbs being symmetric <ref type="bibr" target="#b8">[9]</ref> are enforced naturally by body models. Moreover, this mesh representation also enables a direct mapping to body part segmentations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Early work used the SCAPE body model <ref type="bibr" target="#b4">[5]</ref> and fitted it to images using manually annotated keypoints and silhouettes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>. More recent works use the SMPL model <ref type="bibr" target="#b25">[26]</ref> and fit it automatically. This is done by either solving an optimisation problem to fit the model to the data <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b5">6]</ref> or by regressing the model parameters directly using a neural network <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref> or random forest <ref type="bibr" target="#b22">[23]</ref>. Optimisation-based approaches minimise an energy function that depends on the reprojection error of the 3D joints onto 2D <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>, priors on joint angle and shape parameters <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>, and/or the discrepancy between the silhouette of the 3D model and its foreground mask in the 2D image <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6]</ref>. Direct regression methods, in contrast, train a neural network where the keypoint <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref> or silhouette reprojection errors are used in its training objective <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Kanazawa et al. <ref type="bibr" target="#b19">[20]</ref> also use an adversarial loss that distinguishes between real and fake joint angles of SMPL models. This effectively acts as a joint-angle prior, allowing the authors to utilise existing ground truth SMPL model fits from <ref type="bibr" target="#b26">[27]</ref> without requiring them to be paired to images.</p><p>Our approach uses the per-frame neural network model of <ref type="bibr">Kanazawa et al. [20]</ref> as the initialisation of our optimisation problem. Despite efforts by <ref type="bibr" target="#b19">[20]</ref> to train it with realistic 2D data, we show (as illustrated by <ref type="figure" target="#fig_0">Fig. 1</ref>, 2) how this model often fails on challenging real-world videos and how these errors can be corrected with bundle adjustment. Moreover, we show how we can improve the performance of this network by finetuning it using the results of our bundle adjustment as ground truth on originally difficult sequences.</p><p>We note that despite there being previous efforts to produce temporally consistent fits of the SMPL model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b36">37]</ref>, none of these works have been able to use these results to improve a per-frame model as we have. Furthermore, <ref type="bibr" target="#b55">[56]</ref> and <ref type="bibr" target="#b36">[37]</ref> have not explicitly evaluated on 3D pose estimation either. Additionally, we do not assume knowledge of calibrated cameras like <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>There are also several methods which enforce temporal consistency without body models: The works of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b23">24]</ref> were based on Non-Rigid Structure from Motion whilst <ref type="bibr" target="#b3">[4]</ref> lifted tracked 2D keypoints into 3D. More recently, Hossain et al. <ref type="bibr" target="#b14">[15]</ref> also lifted 2D keypoints using an LSTM in a sequence-to-sequence <ref type="bibr" target="#b45">[46]</ref> model. However, it is difficult to retain memory over long sequences as evidenced by their model performing best with a temporal context of only five frames. Dabral et al. <ref type="bibr" target="#b8">[9]</ref> use a feedforward network using the predictions of the previous 20 frames as input. Our optimisation based approach, in contrast, can consider all frames (our experiments have as many as 1175 frames) in the video to produce more globally coherent results. Furthermore, as we consider all frames jointly, rather than sequentially like <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>, errors do not accumulate over time.</p><p>Finally, we note that there are several works which synthesise additional training data using rendering engines <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b7">8]</ref>. Although this approach provides additional diversity compared to motion capture datasets, the resultant data, although fully labelled, is not photorealistic. Our approach is complementary in that we leverage unlabelled, but real-world YouTube videos from the Kinetics dataset. Concurrently to this paper, <ref type="bibr" target="#b20">[21]</ref> have also used additional videos from Instagram to improve 3D pose estimation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Bundle Adjustment using the SMPL Model</head><p>We jointly optimise the parameters of a SMPL statistical body shape model <ref type="bibr" target="#b25">[26]</ref> and a camera over an entire video sequence. The whole-video approach contrasts with recurrent networks such as <ref type="bibr" target="#b14">[15]</ref> which are only effective using a temporal context of only five frames, and allows for better global solutions. As shown in <ref type="figure">Fig. 2</ref>, the input to our method is a sequence of video frames, 2D keypoint predictions for a single person for each frame using a stateof-art 2D pose detector <ref type="bibr" target="#b32">[33]</ref> and initial SMPL parameters produced per-frame using the HMR network of <ref type="bibr" target="#b19">[20]</ref>. From this, our method outputs SMPL-and camera parameters for each frame in the video that are consistent with each other and reproject to the 2D keypoints. In Sec. 3.1, we briefly describe the SMPL body model that we are fitting to videos. Thereafter, in Sec. 3.2, we detail the objective function that we minimise in order to fit this model to the video. Section. 3.3 we provide details on the optimisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Body representation</head><p>The SMPL body model <ref type="bibr" target="#b25">[26]</ref> parameterises a triangulated mesh with N = 6890 vertices of a human body. It factorises the 3D mesh into shape parameters, β ∈ R 10 and pose θ ∈ R 3K , where K = 23 joints. The shape parameters model the variations in body proportions, height and weight. They are the coefficients of a low-dimensional shape space that was originally learned by <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7]</ref> from a training set of approximately 4000 registered human-body scans. The pose parameters model the deformation of the body as a result of the articulation of its K internal joints. They are an axis-angle representation of the relative rotation of a joint with respect to its parent in the model's kinematic tree. SMPL is a differentiable function that outputs a mesh and positions of joints in 3D. We denote the latter as X = SMPL(β, θ) ∈ R J×3 where J is the number of joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Formulation</head><p>We optimise an objective function that considers the reprojection of 3D keypoints onto 2D, temporal consistency of SMPL parameters, 3D-and 2D-keypoints, and a prior:</p><formula xml:id="formula_0">E(β, θ, Ω) = E R (β, θ, Ω) + E T (β, θ, Ω) + E P (θ, β) (1)</formula><p>Reprojection error: We assume that we have 2D keypoint detections, x det,i with a confidence score of w i for the i th joint. This error term penalises deviations of the projections of our estimated 3D joints onto 2D over all T frames in the video for all J body joints:</p><formula xml:id="formula_1">E R (β, θ, Ω) = λ R T t J i w i ρ(x t i − x t det,i ).<label>(2)</label></formula><p>Here, ρ is the robust Huber error function which we favour over a squared error as it can deal better with noisy estimates that we sometimes obtain on "in-the-wild" sequences, and the superscript t denotes time.</p><p>x is the 2D projection of the 3D joint X,</p><formula xml:id="formula_2">x t = s t Π(RX t ) + u t<label>(3)</label></formula><formula xml:id="formula_3">X t = SMPL(β, θ t ),<label>(4)</label></formula><p>….</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>….</head><p>Bundle adjustment ….</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>….</head><p>…. ….  <ref type="figure">Figure 2</ref>. Overview of our method: Using initial per-frame estimates of 2D keypoints, SMPL-and camera parameters, we jointly optimise over the whole video comprising T frames by encouraging temporal consistency. As a result, we can overcome poor 2D keypoint detection (first row) and poor initial SMPL estimates (all rows) to output accurate SMPL-and camera-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-frame Joint</head><p>where Π is an orthographic projection, R ∈ R 3×3 is the global rotation matrix parameterised by a Rodrigues vector and Ω t = {s t , u t } are the camera parameters comprising of scale, s ∈ R and translation u ∈ R 2 and time-step t. Note that the parameters β and θ are mapped to 3D joint positions X by SMPL, and that we use a single β parameter for the whole sequence as the body shape of the video's subject remains constant.</p><p>Temporal error: This error, E T is defined as:</p><formula xml:id="formula_4">E T (β, θ, Ω) = T t=2 J i=1 λ 1 ρ(X t i − X t−1 i ) + λ 2 ρ(x t i − x t−1 i ) + λ 3 ρ(Ω t − Ω t−1 ).<label>(5)</label></formula><p>The temporal error on 3D joints, X, and camera parameters, Ω, encourages smooth motions that are typical of humans in videos. This is also applied on the 2D keypoint projections, x, as it helps to compensate for spurious errors of the 2D keypoint detector at a particular frame in the video.</p><p>3D Prior: There are many 3D poses (including some that are not humanly possible) that project correctly onto the 2D keypoints while also having low temporal error (for example, having all keypoints in a flat plane actually minimises the change with time). We use a single β for the entire sequence, meaning that changes in distance between 2D keypoints must be explained by pose changes, but telling which keypoint is in front of the other often remains ambiguous. Therefore, we include a prior term that encourages realistic 3D poses which match the appearance, as illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>. We use two terms: the same joint angle prior used by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>, and another term that robustly encourages the solution to stay close to our initialisation, (β,θ), which was estimated by the single-frame HMR model. It is thus defined as:</p><formula xml:id="formula_5">E P (β, θ) = T t E J (θ t ) + λ I E I (θ t , β) (6) E J (θ) = − log i g i N θ t ; µ i , Σ i (7) E I (θ t , β) = J i ρ(X t i −X t i ) + λ β ρ(β −β t ). (8)</formula><p>The joint angle prior, E J (θ), is the negative log-likelihood of a Gaussian Mixture Model that was fitted to the joint angles on the CMU Mocap dataset <ref type="bibr" target="#b0">[1]</ref>. g i are the mixture model weights of 8 Gaussians <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>, and µ i and Σ i are the mean and covariance of the i th Gaussian. Multiple modes are used to represent the diverse range of poses which a human can be in. Note that though our initialisation prior (8) penalises deviations in 3D joint positions, these are functions of the SMPL parameters according to (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimisation</head><p>We optimise (1) with respect to all SMPL and camera parameters, for all frames in the video, jointly using L-BFGS and Tensorflow. The solution is first initialised using the results of the per-frame, HMR neural network <ref type="bibr" target="#b19">[20]</ref>. In total there are 10 + 75F parameters to be optimised for, where F is the number of frames in the video. On a typical clip from Kinetics <ref type="bibr" target="#b21">[22]</ref> consisting of 250 frames, the optimisation takes about 8 minutes on a standard CPU or GPU (as we did not implement customised kernels for this task), or only 2 seconds per frame. The time-and memory-efficiency  of our method is thus suited for batch, offline processing of videos as done in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>As previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b29">30]</ref> have incorporated temporal information into 3D pose estimation using bundle adjustment before, we discuss the differences of our approach: First, in contrast to <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b55">56]</ref>, we use a robust Huber penalty function, and unlike previous approaches, also incorporate additional robustness into our reprojection term for Kinetics data in the next section. Second, our temporal consistency term is not only on 3D joint positions, but also on 2D joint projections and camera parameters (note that <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b29">30]</ref> assume known intrinsics). Third, unlike previous works, we use our bundle adjustment results to improve a per-frame model. Fourth, <ref type="bibr" target="#b36">[37]</ref> optimises in the feature space of HMR, whilst we optimise SMPLand camera-parameters directly. Additional segmentation masks for model fitting as also used by <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b54">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Leveraging Kinetics for weak supervision</head><p>Kinetics-400 <ref type="bibr" target="#b21">[22]</ref> is a large-scale dataset of human actions collected from YouTube. It contains 400 or more 10s video clips for each of 400 action classes. Each clip is from a different YouTube video, and consequently the dataset contains considerable diversity in people, scenes and camera viewpoints as shown in <ref type="figure" target="#fig_0">Fig. 1,2,3</ref>. We perform bundle adjustment on this dataset to obtain real-world, weaklysupervised training data for 3D pose models. Bundle adjustment is challenging on Kinetics since there are often multiple people in a frame, shaking cameras, and people are often occluded or move off-camera. The diversity also results in more frequent failures of our multi-person 2D pose detector <ref type="bibr" target="#b32">[33]</ref> and HMR <ref type="bibr" target="#b19">[20]</ref>.</p><p>Dealing with multiple people: We could handle multiple people with our formulation in Sec. 3 by first tracking a single person through the video, and applying our method to only the tracked region. However, we found this approach too sensitive to missing detections and tracking failures. Consequently, we perform tracking to initialise the solution but also augment the per-frame component of our loss function, (1), to deal with multiple (or potentially no) people, and allow for outliers to be ignored:</p><formula xml:id="formula_6">E R (β, θ t , Ω t ; x t det,i ) = (9) min min p∈P t J i w i h(x t i − x t,p det,i ), τ R , E I (β, θ t ) = (10) min min p∈P t J i ρ(X t,p i −X t i ) + λ β ρ(β −β t ), τ I .</formula><p>Here, τ R and τ I are constants, and p indexes the different person detections P t in frame t. Intuitively, the "inner min" means that the loss is with respect to the current bestmatching 2D pose for each frame. However, if estimates from either the 2D pose detection or the HMR model are too far from the current bundle adjustment estimates, they are considered outliers, and the loss is set to a constant (performed by the "outer min"). This means that they no longer affect the bundle adjustment procedure. There is also substantial jitter in keypoint prediction in Kinetics, due to both 2D detector inaccuracy and camera shake. This causes significant problems if a bone is close to parallel with the camera plane: in such cases, jitter in 2D keypoints can often only be explained by large changes in 3D orientation. Since we are penalising 3D changes, this encourages the overall algorithm to avoid poses where bones are near parallel with the camera plane. To mitigate this, we replace the Huber loss, ρ, in the reprojection term with a hinge loss, h, which is 0 if the error is less than 5 pixels, and behaves like the Huber loss (i.e. L1 error) otherwise. Finally, to deal with camera motion, we find it advantageous to put an upper bound on the camera translations in <ref type="bibr" target="#b4">(5)</ref>, which is equal to 10% of the image width, and we do not penalise camera scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialisation by tracking:</head><p>The possibility of outliers means that initialisation is important, which we do by first tracking people in 2D using our multi-person pose detector <ref type="bibr" target="#b32">[33]</ref> that outputs 2D keypoints and bounding boxes for each person in the image. We select bounding boxes by computing the shortest path from the start to the end of the video: distances between detected people in subsequent frames are equal to the mean-squared-error in pixels between detected keypoints. As there may be missing person detections, we allow the shortest-path algorithm to skip frames with a penalty of 100 pixels. Given a selected person detection for each frame, we initialise the 3D pose parameters for each frame using the estimates from HMR (for any skipped frames, we initialise using the pose from the nearest non-skipped frame).</p><p>Training data selection: After optimising, we measure the success of the algorithm by the total loss (1). However, we find that the loss tends to be lowest for people who aren't moving, producing videos that are not suitable to use as training data. This problem is alleviated by normalising the total loss by the the 3D trajectory length,</p><formula xml:id="formula_7">E norm (β, θ, Ω) = E(β, θ, Ω) T t J i X t i − X t−1 i .<label>(11)</label></formula><p>To obtain training data, we process all videos in Kinetics that do not have more than 6 detected people in a single frame, as our 2D pose detector and HMR usually fail on crowded scenes. After running bundle adjustment, we then discard any videos where E norm is above a threshold, retaining roughly 10% of the original videos. From these videos, we keep the frames where the 2D reprojections of the 3D poses are inliers with respect to our detected key-</p><formula xml:id="formula_8">points (i.e. min p∈P t J i w i ρ(x t i − x t,p det,i ) &lt; τ R ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>After describing common experimental details in Sec. 5.1, we first analyse our bundle adjustment method on the Human 3.6M dataset in Sec. 5.2. Although this labcaptured dataset is not particularly realistic, it has metric ground truth 3D which allows us to conduct an ablation study and compare to previous work on 3D pose estimation using the SMPL model. Thereafter, in Sec. 5.3 we run our method large-scale on Kinetics videos before using these predictions in Sec. 5.4 as weakly-supervised ground truth to retrain a per-frame 3D pose estimation model as described previously in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Set-up</head><p>We initialise the solution to bundle adjustment using the state-of-art HMR neural network <ref type="bibr" target="#b19">[20]</ref> which is input an image and outputs SMPL and orthographic camera parameters. Unless otherwise specified, we use the publicly released model that has been trained on 3D mocap datasets: Human 3.6M <ref type="bibr" target="#b16">[17]</ref> and MPI-3DHP <ref type="bibr" target="#b28">[29]</ref>, 2D pose datasets: COCO <ref type="bibr" target="#b24">[25]</ref>, MPII <ref type="bibr" target="#b2">[3]</ref> and LSP <ref type="bibr" target="#b17">[18]</ref>, and an adversarial prior that was trained on SMPL model fits using <ref type="bibr" target="#b26">[27]</ref>. The keypoints that we use for bundle adjustment are obtained using <ref type="bibr" target="#b32">[33]</ref>, which was trained on the same 2D pose data as HMR and additional data from Flickr collected by the authors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on Human 3.6M</head><p>Human 3.6M <ref type="bibr" target="#b16">[17]</ref> is a popular motion capture dataset and 3D pose benchmark. Following previous work <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b19">20]</ref>, we downsample the videos from 50fps to 10fps and evaluate on the validation set. Even so, some videos contain as many as 1175 frames, which we are still able to jointly optimise over. We report the mean per joint position error (MPJPE) <ref type="bibr" target="#b16">[17]</ref>, and also this error after rigid alignment of the prediction with respect to the ground truth using Procrustes Analysis <ref type="bibr" target="#b10">[11]</ref> which we denote as PA-MPJPE. <ref type="table" target="#tab_0">Table 1</ref> shows the effect of the various terms of our objective function in <ref type="bibr" target="#b0">(1)</ref>. We initialise the solution to our bundle adjustment using the public HMR model of <ref type="bibr" target="#b19">[20]</ref>, and the error increases if we only use the reprojection error. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, optimising for reprojection error alone can result in impossible poses. Note that we are using a single β shape parameter across the whole video, but this alone is too weak a constraint. The addition of the prior term (6) improves results substantially: MPJPE reduces by 6.2mm compared to the HMR initialisation. Although HMR was also trained with 2D reprojection as one of its loss functions, we obtain better results by explicitly optimising for this term and using HMR as an initialisation method. Note that the 2D pose detector that we use <ref type="bibr" target="#b32">[33]</ref> has not been trained on Human 3.6M at all. Our final model, which enforces temporal consistency with not only a single β parameter, but smoothness of joints and camera parameters, achieves the best results, significantly improving the MPJPE error of the initial HMR model by 9.4% and PA-MPJPE by 5.6%.</p><p>The final three rows of Tab. 1 use ground truth 2D keypoints. Note that here, as the ground truth is the projection of 3D joints into the image using the known camera, we have keypoints for occluded joints too. Each term of our objective function (1) has the same effect on the overall error as before. However, the MPJPE and PA-MPJPE improve considerably more over the initialisation of HMR: Our final model reduces these errors by 26.2 and 27.2% respectively. This shows the significant benefits that we can obtain if we have knowledge of occluded keypoints since this further reduces the ambiguity in the problem.</p><p>Finally, Tab. 2 shows we achieve the best results on Hu- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on Kinetics</head><p>Given that our algorithm can reliably improve 3D estimates, we apply our method to a large-scale video dataset to produce training data for single-frame 3D pose estimation. We used the entirety of Kinetics-400 <ref type="bibr" target="#b21">[22]</ref> (400+ clips of 400 action classes), after automatically selecting videos as described in Sec. 4. <ref type="table">Table 3</ref> shows the statistics of the important stages in this process. We first pre-select about 16.7K videos based on the normalized bundle adjustment loss <ref type="bibr" target="#b10">(11)</ref>, resulting in 4.1M frames. The bundle adjustment matched the prediction of the 2D pose detector <ref type="bibr" target="#b32">[33]</ref> for 3.4M out of 4.1M frames (we used a threshold of τ R = 50 pixels total error to determine outliers). Visual inspection showed that the 3D pose detector was fairly reliable: for the majority of outlier frames, the person was occluded or had simply left the frame. <ref type="table">Table 4</ref> lists the action classes from Kinetics that were selected most often, showing that none of them appear in existing mocap datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b28">29]</ref>. Mocap datasets only contain actions performed by a single person, in contrast <ref type="table">Table 4</ref>. The most common action classes of the videos selected from Kinetics. Our bundle adjustment method works well on action classes that do not appear in motion capture datasets, e.g., those that occur outdoors or contain multiple people. to classes such as "tap dancing" and "salsa dancing" which bundle adjustment performs well on. Similarly, our method is effective on outdoor activities such as "roller skating" and "spinning poi" which cannot be recorded by mocap. There were no classes without any selected videos, but for several classes (e.g., "knitting" and "tying tie"), where a person is rarely fully visible, we only selected 1 video each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action class</head><p>Some qualitative examples of the diversity of our dataset are shown in <ref type="figure">Fig. 4</ref>. All experimental hyperparameters are included in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Weak supervision from Kinetics</head><p>We utilise the training data that we automatically obtained in the previous section to retrain a new HMR model from Imagenet initialisation. We use the original training data (described in Sec. 5.1) too, and use a model only trained on this data as our baseline. We evaluate on the recently released 3D Poses in the Wild dataset (3DPW) <ref type="bibr" target="#b51">[52]</ref> in Sec. 5.4.1, which consists of outdoor videos captured in real-world conditions, HumanEVA <ref type="bibr" target="#b42">[43]</ref>, a mocap dataset in Sec. 5.4.2 and Ordinal Depth <ref type="bibr" target="#b33">[34]</ref> which provides a good proxy task for 3D pose esitmation on unconstrained, real-world internet images. Our network has never been trained on images from either of these datasets. To verify the effect of Kinetics training, we trained a model with all frames from our automatically-generated dataset (Kinetics 3M), and also with a random subset of 10% of the frames in our dataset (Kinetics 300K).</p><p>When retraining the HMR model on Kinetics data, we made modifications to the HMR training procedure <ref type="bibr" target="#b19">[20]</ref>. These are detailed in Sec. 5.4.4, where we also show that our modifications only help for training on Kinetics data, and not when using only the original training data used by HMR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">3D Poses in the Wild</head><p>This recently released dataset contains 60 clips, consisting of outdoor videos captured from a moving mobile phone and 17 IMUs attached to the subjects <ref type="bibr" target="#b51">[52]</ref>. The IMU data <ref type="figure">Figure 4</ref>. The dataset we automatically generated from Kinetics has a diverse range of scenes, people, camera viewpoints and action classes not found in motion capture. We show the input frame, results for a single tracked person (which are cropped for display) of HMR (pink) and bundle adjustment (blue), and the bundle adjustment result from another view respectively. Note how bundle adjustment typically improves the per-frame estimates of HMR. <ref type="table">Table 5</ref>. Results on the 3DPW <ref type="bibr" target="#b51">[52]</ref>, HumanEVA <ref type="bibr" target="#b42">[43]</ref> and Ordinal Depth <ref type="bibr" target="#b33">[34]</ref> datasets when training with our Kinetics datasets. We evaluate the HMR model retrained by us on its original training data using the author's public code, and the HMR model trained on its original data and 300K and 3M frames from our Kinetics dataset. For 3DPW and HumanEVA, we report the PA-MPJPE error in mm (lower is better), and for Ordinal Depth, we report the accuracy in % (higher is better). allowed the authors to accurately compute 3D poses which we use as ground truth. We evaluate on the test set comprising 24 videos, using the 14 keypoints that are common across both MS-COCO and SMPL skeletons, as also done by <ref type="bibr" target="#b20">[21]</ref>. We only evaluate on frames where enough of the person is visible to estimate a 3D pose for it. This is performed by discarding examples where less than 7 groundtruth 2D keypoints are visible. We compute the Procrustesaligned error independently for each pose, and then average errors for each tracked person within each video, before finally averaging over the entire dataset (thus videos with two people count twice as much as videos with one). <ref type="table">Table 5</ref> shows how using additional data from Kinetics improves results on this dataset. Training with 300K frames of Kinetics data improves the PA-MPJPE by 3.4mm, and our model trained with all 3M frames of Kinetics improves further by 5 mm over the baseline. Our Kineticstrained model also outperforms the public HMR model <ref type="bibr" target="#b19">[20]</ref> (trained by the authors) which obtains a PA-MPJPE error of 74.9. While isolated checkpoints from our reimplementation of HMR perform as well as the public model, not all do; Tab 5 computes the mean of 20 checkpoints (roughly 1500 training iterations apart) to minimise variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">HumanEVA</head><p>HumanEVA <ref type="bibr" target="#b42">[43]</ref> is an indoor motion-capture dataset where we follow the evaluation protocol of <ref type="bibr" target="#b6">[7]</ref> on the validation set. Although HumanEVA does not contain "in the wild" data, it is a dataset which our HMR model has not been trained on at all. <ref type="table">Table 5</ref> shows how adding additional data from our Kinetics dataset improves performance on this dataset compared to our baselines that were trained without Kinetics. Our model trained with 300K frames of Kinetics data improves the PA-MPJPE by 2.2 mm, and the model trained with 3M Kinetics frames improves further by 3.6 mm over our baseline. The public HMR model obtains a PA-MPJPE error of 83.5, which is also worse than our Kinetics-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Ordinal Depth</head><p>A key challenge with 3D pose estimation in-the-wild is the lack of ground truth for people performing arbitrary, unconstrained actions in-the-wild (as typically found on images scraped from the internet). However, a suitable proxy for 3D pose estimation quality is ordinal depth <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b33">34]</ref> i.e. given two keypoints, predict the relative depth ordering by specifying which keypoint is in front of the other. This utility of this task was demonstrated by Taylor <ref type="bibr" target="#b46">[47]</ref>, who showed that the 3D skeleton of a person could be reconstructed perfectly if exact 2D keypoint correspondences, bone lengths and ordinal relations between keypoints were known, assuming an orthographic camera.</p><p>Although humans cannot annotate 3D pose or absolute depth, they can reliably label ordinal depth <ref type="bibr" target="#b33">[34]</ref>. We thus evaluate on the Ordinal Depth dataset <ref type="bibr" target="#b33">[34]</ref> which added ordinal depth annotations to the MPII <ref type="bibr" target="#b2">[3]</ref> and LSP <ref type="bibr" target="#b17">[18]</ref> 2D pose datasets of real-world internet images. We evaluate on 2606 images from the validation sets of MPII and LSP, as images from the training set were used to train HMR (we do not use any of the ordinal depth information during training). For each person, each pair of keypoints is labelled either "in front", "behind", or "ambiguous". To evaluate, we compute the 3D pose for each person and then obtain ordinal depth for each pair of keypoints. We report the average accuracy, ignoring keypoint pairs labelled as ambiguous. <ref type="table">Table 5</ref> shows the benefits we get for this task by training on Kinetics. Even a relatively small amount of Kinetics data provides noticeable improvements on this dataset, with further benefits from our entire dataset. As expected, training on real-world data from Kinetics helps on ordinal depth predictions of real-world images.</p><p>These experiments thus show how we can effectively use Kinetics data to improve the per-frame HMR model on multiple datasets. We also achieve greater improvements on the real-world 3DPW dataset, compared to the mocap Hu-manEVA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">HMR training modification and ablation</head><p>When training with Kinetics data, we find that it is beneficial to not use any of the original 2D data used by HMR, and thus also to not use the adversarial pose prior since it is only used on 2D pose datasets <ref type="bibr" target="#b19">[20]</ref>. We suspect that this is because the adversarial pose prior encourages predictions that are closer to the mean pose, and since we use HMR to initialise our bundle adjustment, our Kinetics data may also have a slight bias towards this mean pose. Applying the same prior while retraining may aggravate this problem. <ref type="table">Table 6</ref>. Ablation study of our HMR retraining schemes. PA-only 3D means during our retraining of HMR, we discard the losses on SMPL joints and absolute 3D locations and only use losses on joints after Procrustes alignment. No 2D means disabling all HMR datasets that contain only 2D data (and therefore disabling the adversarial prior which is only used on 2D datasets). We also find it's important to train only on 3D keypoints after Procrustes alignment, rather than training directly on SMPL joint angles and absolute 3D keypoint locations. Note this means that HMR only learns to predict the camera orientation by minimizing 2D reprojection error. We suspect that this strategy is effective because Kinetics has a very large range of camera orientations, which may not match well with evaluation datasets that have less variety in camera pose. <ref type="table">Table 6</ref> shows that our modifications to the HMR training procedure help only when we train with additional Kinetics data. When using the original training data, our modified training procedure does not improve results. Removing the original 2D data from training also has a large negative impact on performance. This is because the original training data has a relatively small amount of 3D supervision (Human 3.6M <ref type="bibr" target="#b16">[17]</ref> and MPI-3DHP <ref type="bibr" target="#b28">[29]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We presented a bundle-adjustment algorithm to leverage the temporal context in a video in order to improve estimates of the 3D pose of a person. Furthermore, we applied this to YouTube videos from Kinetics and automatically generated a dataset which we used to improve perframe 3D pose estimators, demonstrating how we can effectively use large amounts of unlabelled data to improve existing models.</p><p>Bundle adjustment was effective because videos are shot in a 3D world where people move slowly (relative to the camera framerate), and the person's size and appearance remain consistent over time. If properly characterised, these constraints can give strong supervision to algorithms, which allows us to break out of the environments which motion capture devices are restricted to. We believe there is far more 3D structure to exploit, because people don't behave in a vacuum. People act under gravity, are supported by ground planes and interact with objects. Therefore, we aim in future to use physical constraints and information about human actions to constrain poses and predict the objects that people are interacting with to estimate their affordances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→ Althoughmonocular 3D pose estimation is an ill-posed problem, state-of-art methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Without the prior (6), the SMPL model fit can project well onto 2D keypoints without being in a valid human pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation study on Human 3.6M, considering the effect of different terms of our objective function<ref type="bibr" target="#b0">(1)</ref>. Mean errors over the validation set are reported.</figDesc><table><row><cell>Method</cell><cell cols="2">MPJPE (mm) PA-MPJPE (mm)</cell></row><row><cell>HMR initialisation [20]</cell><cell>85.8</cell><cell>57.5</cell></row><row><cell>E R</cell><cell>154.3</cell><cell>99.7</cell></row><row><cell>E R + E P</cell><cell>79.6</cell><cell>55.3</cell></row><row><cell>E R + E P + E T</cell><cell>77.8</cell><cell>54.3</cell></row><row><cell>E R (gt. keypoints)</cell><cell>89.2</cell><cell>64.5</cell></row><row><cell>E R + E P (gt. keypoints)</cell><cell>66.5</cell><cell>45.7</cell></row><row><cell>E R + E P + E T (gt. keypoints)</cell><cell>63.3</cell><cell>41.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of approaches fitting the SMPL model<ref type="bibr" target="#b25">[26]</ref> on Human 3.6M. We did not use additional Kinetics data here.</figDesc><table><row><cell>Method</cell><cell cols="2">MPJPE (mm) PA-MPJPE (mm)</cell></row><row><cell>Self-Sup [49]</cell><cell>-</cell><cell>98.4</cell></row><row><cell>Lassner et al. direct fitting [23]</cell><cell>-</cell><cell>93.9</cell></row><row><cell>SMPLify [7]</cell><cell>-</cell><cell>82.3</cell></row><row><cell>Lassner et al. optimisation [23]</cell><cell>-</cell><cell>80.7</cell></row><row><cell>Pavlakos et al. [36]</cell><cell>-</cell><cell>75.9</cell></row><row><cell>NBF [32]</cell><cell>-</cell><cell>59.9</cell></row><row><cell>MuVS (Note uses 4 cameras) [16]</cell><cell>-</cell><cell>58.4</cell></row><row><cell>HMR [20]</cell><cell>88.0</cell><cell>56.8</cell></row><row><cell>Ours</cell><cell>77.8</cell><cell>54.3</cell></row><row><cell cols="3">Table 3. Statistics of our bundle-adjustment dataset from Kinetics-</cell></row><row><cell cols="3">400. 2D inliers refers to frames where 2D reprojection error was</cell></row><row><cell>small: ER &lt; τR.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Count</cell></row><row><cell>Total videos</cell><cell></cell><cell>106 589</cell></row><row><cell cols="2">Selected videos (E norm &lt; τ R )</cell><cell>16 720</cell></row><row><cell cols="3">Total frames in selected videos 4 141 436</cell></row><row><cell>BA inliers</cell><cell></cell><cell>3 407 686</cell></row><row><cell cols="3">man 3.6M among other methods utilising the SMPL model.</cell></row><row><cell cols="3">Note that Mehta et al. [30] also perform bundle adjust-</cell></row><row><cell cols="3">ment to improve the predictions of a CNN model, obtain-</cell></row><row><cell cols="3">ing an MPJPE of 80.5. However, as [30] do not use the</cell></row><row><cell cols="3">SMPL model, they are not directly comparable. Addition-</cell></row><row><cell cols="3">ally, although direct CNN-regression methods such as [9]</cell></row><row><cell cols="3">obtain MPJPE errors of as low as 52.1, they overfit to the</cell></row><row><cell cols="3">Human 3.6M dataset and have been shown to be signifi-</cell></row><row><cell cols="3">cantly outperformed by SMPL-based approaches on real-</cell></row><row><cell cols="3">world datasets such as 3DPW [52] by Kanazawa et al. [21].</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We would like to thank Jean-Baptiste Alayrac, Relja Arandjelović, João Carreira, Rohit Girdhar, Viorica Pȃtrȃucean and Jacob Walker for valuable discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Section A lists the hyperparameters we used for our bundle adjustment, whilst Sec. B provides some more details about the dataset we automatically generated from Kinetics. <ref type="table">Table 7</ref> shows the values of our bundle adjustment hyperparameters for our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Details</head><p>Note that the 2D joint positions, x are measured in pixels, and that the largest spatial dimensions of a video frame is typically around 450. On the other hand, the 3D joint positions, X and camera parameters are typically in the range [−1, 1]. As the range of the 2D joint positions is higher, the values of λ R and λ 2 , are small, even though they have a significant effect on the bundle adjustment.</p><p>λ I and λ β are higher on Human 3.6M than they are on Kinetics. These weights are used in the prior term that encourages the bundle adjustment result to stay close to the initialisation (Eq. 8 of main paper). The initialisation that we get from HMR <ref type="bibr" target="#b19">[20]</ref> is far better on Human 3.6M than on Kinetics, which is why λ I and λ β are higher on Human 3.6M. It is expected that HMR performs better on Human 3.6M as it has been trained with 3D supervision from this dataset. <ref type="figure">Figure 5</ref> visualises the distribution of Kinetics action classes in our dataset. We can see that the distribution has a fairly long tail: Our bundle adjustment method works well for a variety of object classes, including many types of dancing and various outdoor activities, where there are usually not many people in the video clip and the whole body is visible. There are also many classes for which only a handful of videos are automatically selected. These are typically classes such as "tying tie", "bookbinding" and "knitting" where the person is usually not fully visible. Note that there are 400+ clips for each action in the Kinetics-400 dataset <ref type="bibr" target="#b21">[22]</ref> that we use, and that we have always selected at least one video of each action class.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset statistics</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://mocap.cs.cmu.edu/.4" />
		<title level="m">CMU graphics lab motion capture database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="408" to="416" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detailed human shape and pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Haussecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structure-aware and temporally coherent 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-Rigid Structure from Motion with complementary rank-3 spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Gotardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generalized procrustes analysis. Psychometrika</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="33" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1381" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">521540518</biblScope>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilinear pose and body shape estimation of dressed subjects from image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1823" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Classner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.24.12.2</idno>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Endto-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structure from recurrent motion: From rigidity to recurrency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3032" to="3040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>248:1-248:16</idno>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">MoSh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>220:1-220:13</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3D human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1561" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Neural body fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sfv: Reinforcement learning of physical skills from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03599</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reconstructing 3D human pose from 2D image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Image-based synthesis for deep 3D human pose estimation. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LCR-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">How robust is 3D human pose estimation to occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sárándi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09316</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1337" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HumanEVA: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Reconstruction of articulated objects from point correspondences in a single uncalibrated image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bundle adjustment: A modern synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Algorithms: Theory and Practice</title>
		<editor>W. Triggs, A. Zisserman, and R. Szeliski</editor>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="298" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deterministic 3D human pose estimation using rigid structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">3D reconstruction of human motion from monocular image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1505" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust estimation of 3D human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Mosculp: Interactive visualization of shape and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05491</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A simple, fast and highly-accurate algorithm to recover 3D shape from 2D landmarks on a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3D human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
